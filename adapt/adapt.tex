% 2-8 pages, 10pt font
%
% Topics:
%
% * Machine learning based autotuning.
% * Representative benchmarking.
% * Automatic fault tolerance.
% * Run-time adaption.
%

% The following \documentclass options may be useful:
%
% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.
\documentclass[nonatbib,preprint,10pt]{sigplanconf}

\include{preamble}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ADAPT '16}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish,
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers,
                                  % short abstracts)

% \titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{ADAPT workshop '16}   % 'preprint' option specified.

% \title{Autotuning Stencil Codes using Synthetic Benchmarks}
% \title{Autotuning OpenCL Workgroup Sizes for Stencil Codes}
% \title{Machine learning for OpenCL Workgroup Sizes of Stencil Codes}
% \title{Machine learning for OpenCL Workgroup Sizes of Stencil Codes}
% \title{Methods for Autotuning Workgroup Size of OpenCL Stencil Codes}
\title{Autotuning OpenCL Workgroup Size for Stencil Codes}

% Comparison of multiple approaches to autotuning stencil patterns:
%
% * Compare classifiers with regressors
% * Compare synthetic vs real training

% \subtitle{Subtitle Text, if any}

\authorinfo{Chris Cummins\and Pavlos Petoumenos\and Hugh Leather}
           {University of Edinburgh}
           {c.cummins@ed.ac.uk,\{ppetoume,hleather\}@inf.ed.ac.uk}

\maketitle

\begin{abstract}
  Selecting the appropriate workgroup size for an OpenCL kernel
  requires knowledge of the underlying hardware, the data being
  operated on, and properties of the kernel. This makes portable
  performance tuning a difficult task. To address this, we propose the
  use of machine learning-enabled autotuning to predict performant
  workgroup sizes across a range of architectures, kernels, and
  datasets.

  % This makes portable performance tuning a difficult task.

  % Algorithmic skeletons simplify parallel programming by providing
  % high-level, reusable patterns of computation.

  % The optimal workgroup size can often only be discovered through
  % empirical performance comparisons, and generalising a successful
  % workgroup size for multiple kernels becomes a near impossible
  % task. In general,

  % In addition, arbitrary values are not acceptable. Hardware typically
  % imposes limitations and

  % this paper attempts to

  We propose three methodologies for autotuning the workgroup size of
  unseen OpenCL kernels for stencil programming. The first, using
  classifiers to predict the optimal workgroup size. The second and
  third proposed methodologies employ the novel use of regressors for
  performing classification by predicting the \emph{runtime} and
  \emph{relative performance} of different workgroup sizes.%
  % What's the difference between each method?
  Each method is evaluated with respect to its portability, the
  performance achieved, and the time required. We find that over
  $2.7\times 10^5$ test cases, autotuning provides a median
  $3.79\times$ speedup over the best possible performance which can be
  achieved without autotuning, achieving 94\% of the available
  performance.
\end{abstract}

% \category{CR-number}{subcategory}{third-level}

% % general terms are not compulsory anymore,
% % you may leave them out
% % \terms
% % term1, term2

% \keywords
% keyword1, keyword2

\section{Introduction}\label{sec:introduction}

Stencil codes have a variety of computationally expensive uses from
fluid dynamics to quantum mechanics. Efficient, tuned stencil kernels
are highly sought after, with early work in \citeyear{Bolz2003} by
\citeauthor{Bolz2003} demonstrating the capability of GPUs for
massively parallel stencil operations~\cite{Bolz2003}. Since then, the
introduction of OpenCL has enabled greater programmability of
heterogeneous devices by providing a vendor-independent layer of
abstraction for data parallel programming of CPUs, GPUs, DSPs, and
other devices. However, achieving portable performance of OpenCL
programs is a hard task --- such programs are sensitive to properties
of the underlying hardware, to the program being executed, and even to
the \emph{dataset} that is operated upon. This forces developers to
laboriously hand tune performance on a per-program basis. Higher level
programming abstractions aim to address this issue by simplifying the
programmability to challenge, raising the level of abstraction above

In this paper, we implement autotuning of OpenCL workgroup size for
the SkelCL stencil skeleton. SkelCL is a C++ algorithmic skeleton
framework which allows users to easily harness the power of CPUs and
multi-GPU systems. The

Introduced in~\cite{Steuwer2011}, SkelCL is an object oriented C++
library that provides OpenCL implementations of data parallel
algorithmic skeletons for heterogeneous parallelism using CPUs or
multi-GPUs. SkelCL addresses the parallel programmability challenge by
allowing users to easily harness the power of GPUs and CPUs for data
parallel computing.

The optimisation space presented by the workgroup size of OpenCL
kernels is large, complex, and non-linear. Successfully applying
machine learning to such a space requires plentiful training data, the
careful selection of features, and classification approach.

Stencil workgroup sizes presents a two dimensional parameter space,
consisting of a number of rows and columns. It is constrained by
properties of both the stencil code and underlying architecture.

Autotuning of the SkelCL stencil skeleton is achieved using OmniTune,
a framework for runtime autotuning using machine learning.

\section{The SkelCL Stencil Skeleton}

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{img/stencil}
\caption[Stencil border region]{%
  The components of a stencil: an input matrix is decomposed into
  workgroups, consisting of $w_r \times w_c$ elements. Each element is
  mapped to a work-item. Each work-item operates its corresponding
  element and a surrounding border region $S$, consisting of the four
  independent components describing the number of elements north
  $S_n$, east $S_e$, west $S_w$, and south $S_s$ (in this example, 1
  element to the south, and 2 elements in all other directions). Each
  tile is allocated in local memory for fast access of repeated read
  operations.%
}
\label{fig:stencil-shape}
\end{figure}

SkelCL provides a stencil skeleton in which a user-provided
\emph{customising function} is applied to each element of a 2D
matrix. The value of each element is updated based on its current
value and the value of one or more neighbouring elements, called the
\emph{border region}. The border region is described by a
\emph{stencil shape}, which defines an $i \times j$ rectangular region
about each cell which is used to update the cell value. Stencil shapes
may be asymmetrical, and are defined in terms of the number of cells
in the border region to the north, east, south, and west of each cell,
shown in Figure~\ref{fig:stencil-shape}. Where elements of a border
region which fall outside of the matrix bounds, values are substituted
from either a predefined padding value, or the value of the nearest
cell within the matrix, depending on user preference.

Each element of a stencil input matrix is mapped to a single OpenCL
work-item; and this collection of work-items is then divided into
\emph{workgroups} for execution on the target hardware. Each work-item
reads the value of the corresponding element and the surrounding
elements defined by the border region. Since the border regions of
neighbouring elements overlap, the value of all elements within a
workgroup are stored in a \emph{tile}, allocated as a contiguous block
of local memory. As local memory access times are much shorter than
that of global memory, this greatly reduces the latency of the border
region reads performed by each work-item. Changing the workgroup size
thus affects the amount of local memory required for each workgroup,
which in turn affects the number of workgroups which may be
simultaneously active. So while the user defines the size, type, and
border region of the of the grid being operated upon, it is the
responsibility of the SkelCL stencil implementation to select an
appropriate workgroup size to use.


\section{Autotuning Workgroup Size}

Selecting the appropriate workgroup size for a kernel depends on the
properties of the kernel itself, underlying architecture, and
dataset. For a given \emph{scenario} (that is, a particular
combination of kernel, architecture, and dataset), the goal of this
work is to harness machine learning to \emph{predict} a performant
workgroup size to use, based on some prior knowledge of the
performance of workgroup sizes for other scenarios. The space of
possible workgroup sizes $W$ is constrained by properties of both the
architecture and kernel.

\subsection{Constraints}

Each OpenCL device imposes a maximum workgroup size which can be
statically checked through the OpenCL Device API. This constraint
reflects architectural limitations of how code is mapped to the
underlying execution hardware. Typical values are powers of two, e.g.\
1024, 4096, 8192. Additionally, kernels enforce a maximum workgroup
size. This value can be queries at runtime once a program has been
compiled for a specific execution device. Factors which affect a
kernel's maximum workgroup size include the number of registers
required for a kernel, and the available number of SIMD execution
units for each type of instructions in a kernel.

While in theory, any workgroup size which satisfies the device and
kernel workgroup size should provide a functioning program, in
practise we did not find this to be the case. As such, we define
\emph{refused parameters} as workgroup sizes which satisfy the kernel
and architectural constraints, yet cause a
\texttt{CL\_OUT\_OF\_RESOURCES} error to be thrown when the kernel is
enqueued. Note that in many OpenCL implementations, this error type
acts as a generic placeholder and may not necessarily indicate that
the underlying cause of the error was due to finite resources
constraints. Further discussion on the possible causes and effects of
refused parameters is contained in Section~XXX, but for now we use the
concept of refused parameters to formalise the constraints on the
workgroup size space. We define a \emph{legal} workgroup size as one
satisfies the architectural and kernel constraints of a given
scenario, and is not refused. The subset of all possible workgroup
sizes $W_{legal}(s) \subset W$ that are legal is then:
%
\begin{equation}
  \footnotesize
  W_{legal}(s) = \left\{w | w \in W, w < W_{\max}(s) \right\} - W_{refused}(s)
\end{equation}
%
Where $W_{\max}(s)$ can be determined at runtime prior to the kernels
execution, but the set $W_{refused}(s)$ can only be discovered
emergently.


%\subsection{Optimisation Target}
%
% %
% We can quantify the performance $p(s,w)$ of a particular workgroup
% size relative to the oracle, within the range $0 \le p(s,w) \le 1$,
% using $p(s,w) = \frac{t(s,\Omega(s)}{t(s,w)}$. Across a set of
% scenarios $S$, the average performance $\bar{p}(w)$ of a workgroup
% size is calculated using the geometric mean of performances:
% % Across the set of all scenarios $S$, the average performance
% % $\bar{p}(w)$ of the workgroup size is found using the geometric mean
% % of performances:
% %
% % \begin{align}
% %   p(s,w) &= \frac{t(s,\Omega(s)}{t(s,w)}\\
% %   \bar{p}(w) &=
% %   \left(
% %     \prod_{s \in S} p(s,w))
% %   \right)^{1/|S|}
% % \end{align}
% \begin{equation}
%   \bar{p}(w) =
%   \left(
%     \prod_{s \in S} p(s,w))
%   \right)^{1/|S|}
% \end{equation}
% %
% The \emph{baseline} workgroup size is the value which provides the
% best average case performance across all scenarios. Such a baseline
% value represents the \emph{best} possible performance which can be
% achieved using a single, statically chosen workgroup size. By defining
% $W_{safe} \in W$ as the intersection of legal workgroup sizes, the
% baseline workgroup size $\bar{w}$ can be found using:
% %
% \begin{align}
% W_{safe} &= \cap \left\{ W_{legal}(s) | s \in S \right\}\\
% \bar{w} &= \argmax_{w \in W_{safe}} \bar{p}(w)
% \end{align}


\subsection{Stencil Features}

Since properties of the architecture, program, and dataset all
contribute to the performance of a workgroup size, the success of a
machine learning system depends on the ability to translate these
properties into meaningful explanatory variables ---
\emph{features}. For each scenario, 102 features are extracted
describing the architecture, kernel, and dataset.

Architecture features are extracted using the OpenCL Device API to
query properties such as the size of local memory, maximum work group
size, and number of compute units. For kernel feature, the source code
for a stencil kernel is compiled to LLVM IR bitcode, and a statistics
pass is used to obtain static counts for each type of instruction
present in the kernel, as well as the total number of
instructions. The instruction counts for each type are divided by the
total number of instructions to produce a \emph{density} of
instruction for that type. Examples include total static instruction
count, ratio of instructions per type, and ratio of basic blocks per
instruction. Dataset features include the input and output data types,
and the 2D matrix dimensions.

Feature extraction (particularly compilation to LLVM IR) introduces a
runtime overhead to the classification process. To minimise this, a
relational database stores lookup tables for device and dataset
features, indexed by device IDs and checksums of kernel source codes,
respectively. During autotuning, before feature extraction for either
occurs a lookup is performed in the relevant table, meaning that the
cost of feature extraction is amortised over time.


\subsection{Training Data}\label{subsec:training}

Training data are collected by measuring the runtimes of stencil
programs using different workgroup sizes. These stencil programs are
generated synthetically using a parameterised template substitution
engine. A stencil template is parameterised first by stencil shape
(one parameter for each of the four directions), input and output data
types (either integers, or single or double precision floating
points), and \emph{complexity} --- a simple boolean metric for
indicating the desired number of memory accesses and instructions per
iteration, reflecting the relatively bi-modal nature of stencil codes,
either compute intensive (e.g.\ finite difference time domain and
other PDE solvers), or lightweight (e.g.\ Game of Life and Gaussian
blur).


\section{Machine Learning Methods}

The challenge is to design a system which, given a set of prior
observations of the empirical performance of stencil codes with
different workgroup sizes, predicts workgroup sizes for \emph{unseen}
scenarios which maximise the performance. This section presents three
methods for achieving this goal.


\subsection{Predicting Oracle Workgroup Size}

\begin{algorithm}[t]
\begin{algorithmic}[1]
\Require scenario $s$, classifier $C(x)$
\Ensure workgroup size $w$

\Procedure{Baseline}{s}
% \Comment Select the best $w$ from $W_{safe}$.
\State $w \leftarrow C(f(s))$
\If{$w \in W_{legal}(s)$}
    \State \textbf{return} $w$
\Else
  \State \textbf{return} $\underset{w \in W_{safe}}{\argmax}
\left(
  \prod_{s \in S_{training}} p(s, w)
\right)^{1/|S_{training}|}$
\EndIf
\EndProcedure
\item[] % line break

\Procedure{Random}{s}
% \Comment Select a random workgroup size.
\State $w \leftarrow C(f(s))$
\While{$w \not\in W_{legal}(s)$}
  \State $W \leftarrow \left\{ w | w < W_{max}(s), w \not\in W_{refused}(s) \right\}$
  \State $w \leftarrow $ random selection $w \in W$
\EndWhile
\State \textbf{return} $w$
\EndProcedure
\item[] % line break

\Procedure{NearestNeighbour}{s}
% \Comment Select the closest workgroup size to prediction.
\State $w \leftarrow C(f(s))$
\While{$w \not\in W_{legal}(s)$}
  \State $d_{min} \leftarrow \infty$
  \State $w_{closest} \leftarrow \text{null}$
  \For{$c \in \left\{ w | w < W_{\max}(s), w \not\in W_{refused}(s) \right\}$}
    \State $d \leftarrow \sqrt{\left(c_r - w_r\right)^2 + \left(c_c - w_c\right)^2}$
    \If{$d < d_{min}$}
      \State $d_{min} \leftarrow d$
      \State $w_{closest} \leftarrow c$
    \EndIf
  \EndFor
  \State $w \leftarrow w_{closest}$
\EndWhile
\State \textbf{return} $w$
\EndProcedure
\end{algorithmic}
\caption{Predicting using classifiers}
\label{alg:autotune-classification}
\end{algorithm}

The first approach to predicting workgroup sizes is to consider the
set of possible workgroup sizes as a hypothesis space and to use a
classifier to predict, for a given set of features, the \emph{oracle}
workgroup size $\Omega(s)$, which is the workgroup size which provides
the lowest mean runtime $t(s,w)$:
%
\begin{equation}
  \Omega(s) = \argmin_{w \in W_{legal}(s)} t(s,w)
\end{equation}
%
Training data consists of pairs of features $f(s)$ labelled with their
oracle workgroup size for a set of training scenarios $S_{training}$:
%
\begin{equation}
  D_{training} = \left\{ \left(f(s), \Omega(s)\right) | s \in S_{training} \right\}
\end{equation}
%
After training, the classifier predicts workgroup sizes for unseen
scenarios from the set of oracle workgroup sizes from the training
set. This is a common and intuitive approach to autotuning, in that
machine learning is used to predict the best parameter value for a new
scenario based on what were the best parameter values for previous
scenarios. However, given the constrained space of workgroup sizes,
this presents a problem that after training, there is no guarantee
that the set of workgroup sizes which may be predicted is within the
set of legal workgroup sizes for future scenarios:
%
\begin{equation}
  \left\{ \Omega(s) | s \in S_{training} \right\} \nsubseteq \bigcup_{\forall s \in S_{future}} W_{legal}(s)
\end{equation}
%
This results in a classifier which may predict workgroup sizes that
are not legal for scenarios, either because it exceeds $W_{\max}(s)$,
or because the parameter is refused, $w \in W_{refused}(s)$. For these
cases, we evaluate the effectiveness of three \emph{fallback
  strategies} to select a legal workgroup size:
%
\begin{enumerate}
\item \emph{Baseline} --- select the workgroup size which is known to
  be safe $w < W_{safe}$, and provides the highest average case
  performance on training data.
\item \emph{Random} --- select a random workgroup size which is
  expected from prior observations to be legal $w \in W_{legal}(s)$.
\item \emph{Nearest Neighbour} --- select the workgroup size which
  from prior observations is expected to be legal, and has the lowest
  Euclidian distance to the prediction.
\end{enumerate}
%
See Algorithm~\ref{alg:autotune-classification} for definitions.


\subsection{Predicting Kernel Runtime}

\begin{algorithm}[t]
\begin{algorithmic}[1]
\Require scenario $s$, regressor $R(x, w)$, fitness function $\Delta(x)$
\Ensure workgroup size $w$

\State $W \leftarrow \left\{ w | w < W_{\max}(s) \right\} -
W_{refused}(s)$
\Comment Candidates.
\State $w \leftarrow \underset{w \in W}{\argmax} \Delta(R(f(s), w))$
\Comment Select best candidate.
\While{$w \not\in W_{legal}(s)$}
  \State $W_{refused}(s) = W_{refused}(s) + \{w\}$
  \State $W \leftarrow W - \left\{ w \right\}$
  \Comment Remove candidate from selection.
  \State $w \leftarrow \underset{w \in W}{\argmax} \Delta(R(f(s), w))$
  \Comment Select next best candidate.
\EndWhile
\State \textbf{return} $w$
\end{algorithmic}
\caption{Predicting using regressors}
\label{alg:autotune-regression}
\end{algorithm}

A problem of predicting oracle workgroup sizes is that it requires
each training point to be labelled with the oracle workgroup size
which can be only be evaluated using an exhaustive search. An
alternative approach is to build a model to attempt to predict the
\emph{runtime} of a kernel given a specific workgroup size. This
allows for training on data for which the oracle workgroup size is
unknown, and has the secondary advantage that this allows for
additional training data point to be gathered every time a kernel is
evaluated. Given training data consisting of $(f(s),w,t)$ tuples,
where $s$ is a scenario, $w$ is the workgroup size, and $t$ is the
observed runtime, we train a regressor $R(f(s), w)$ which predicts the
runtime of unseen scenario and workgroup size combinations. The
selected workgroup size $\bar{\Omega}(s)$ is then the workgroup size
from a pool of candidates which minimises the output of the regressor,
as shown in Algorithm~\ref{alg:autotune-regression}. The fitness
function $\Delta(x)$ is merely a reciprocal, so as to favour shorter
runtimes rather than longer. Note that the algorithm is self
correcting --- if a workgroup size is refused, it is removed from the
candidate pool, and the next best candidate is chosen.


\subsection{Predicting Relative Performance}

Accurately predicting the runtime of an arbitrary stencil kernel is a
difficult problem due to the impacts of flow control. In such cases,
it may be more effective to instead predict the \emph{relative}
performance of two different workgroup sizes for the same kernel. To
do this, we can predict speedup of a workgroup size over a
\emph{baseline}. The \emph{baseline} value is the workgroup size which
provides the best average case performance across all scenarios. Such
a baseline value represents the \emph{best} possible performance which
can be achieved using a single, statically chosen workgroup size. By
defining $W_{safe} \in W$ as the intersection of legal workgroup
sizes, the baseline workgroup size $\bar{w}$ can be found using:
%
\begin{align}
W_{safe} &= \cap \left\{ W_{legal}(s) | s \in S \right\}\\
\bar{w} &= \argmax_{w \in W_{safe}} \bar{p}(w)
\end{align}
%
We then train a regressor $R(f(s), w)$ to predict the relative
performance of workgroup sizes over this baseline parameter. The
prediction algorithm is the same as when predicting runtimes, only the
regressor returns a predicted runtime, and the fitness function does
not need to invert the prediction in order to rank workgroup sizes by
expected speedup. This has the same advantageous properties as
predicting runtimes, but by training using relative performance, we
minimise the risk of control flow leading to inaccurate predictions,
as the flow of control in user code is independent of the size of the
workgroup.


\section{Experimental Setup}

\begin{table*}
\scriptsize
\centering
\begin{tabular}{l l | l l l l l l}
\toprule
Host & Host Memory &  OpenCL Device &  Compute units & Frequency & Local Memory & Global Cache & Global Memory \\
\midrule
Intel i5-2430M & 8 GB  & CPU              &              4 &   2400 Hz &        32 KB &       256 KB &       7937 MB \\
Intel i5-4570  & 8 GB  & CPU              &              4 &   3200 Hz &        32 KB &       256 KB &       7901 MB \\
Intel i7-3820  & 8 GB  & CPU              &              8 &   1200 Hz &        32 KB &       256 KB &       7944 MB \\
Intel i7-3820  & 8 GB  & AMD Tahiti 7970  &             32 &   1000 Hz &        32 KB &        16 KB &       2959 MB \\
Intel i7-3820  & 8 GB  & Nvidia GTX 590   &              1 &   1215 Hz &        48 KB &       256 KB &       1536 MB \\
Intel i7-2600K & 16 GB & Nvidia GTX 690   &              8 &   1019 Hz &        48 KB &       128 KB &       2048 MB \\
Intel i7-2600  & 8 GB  & Nvidia GTX TITAN &             14 &    980 Hz &        48 KB &       224 KB &       6144 MB \\
\bottomrule
\end{tabular}
\caption{Specification of experimental platforms and OpenCL devices.}
\label{tab:hw}
\end{table*}

To evaluate the performance of the presented autotuning techniques, an
exhaustive enumeration of the workgroup size optimisation space for
429 combinations of architecture, program, and dataset were performed.

Table~\ref{tab:hw} describes the experimental platforms and OpenCL
devices used. Each platform was unloaded, frequency governors were
disabled, and the benchmark processes were set to the highest priority
available to the task scheduler. Datasets and programs were stored in
an in-memory file system. All runtimes were recorded with millisecond
precision using OpenCL's Profiling API to record the kernel execution
time.

\begin{table}
\scriptsize
\centering
\begin{tabular}{lrrrrp{1.3cm}}
\toprule
      Name &  North &  South &  East &  West &  Instruction Count \\
\midrule
   synthetic-a & 1--30 & 1--30 & 1--30 & 1--30 & 67--137\\
   synthetic-b & 1--30 & 1--30 & 1--30 & 1--30 & 592--706\\
   gaussian    & 1--10 & 1--10 & 1--10 & 1--10 & 82--83 \\
   gol         &      1 &      1 &     1 &     1 &                190 \\
   he          &      1 &      1 &     1 &     1 &                113 \\
   nms         &      1 &      1 &     1 &     1 &                224 \\
   sobel       &      1 &      1 &     1 &     1 &                246 \\
   threshold   &      0 &      0 &     0 &     0 &                 46 \\
\bottomrule
\end{tabular}
\caption{%
  Stencil kernels, border sizes (north, south, east, and west),
  and static instruction counts.
}
\label{tab:kernels}
\end{table}

In addition to the synthetic stencil benchmarks described in
Section~\ref{subsec:training}, six stencil kernels taken from four
reference implementations of standard stencil applications from the
fields of image processing, cellular automata, and partial
differential equation solvers are used: Canny Edge Detection, Conway's
Game of Life, Heat Equation, and Gaussian
Blur. Table~\ref{tab:kernels} shows details of the stencil kernels for
these reference applications and the synthetic training benchmarks
used. For each benchmark, dataset sizes of size $512\times512$,
$1024\times1024$, $2048\times2048$, and $4096\times4096$ were used.

Program behaviour is validated by comparing program output against a
gold standard output; collected by executing each of the real-world
benchmarks programs using the baseline workgroup size. The output of
real-world benchmarks with other workgroup sizes is compared to this
gold standard output to test for correct program execution. The
optimisation space is enumerated by continuously iterating over each
benchmark, collecting 500 iterations for each and randomly selecting a
workgroup size from the space of legal workgroup sizes for each
iteration. Data collection is stopped once there is a minimum of 30
samples for each legal workgroup size.

To evaluate the classification approach, five different classification
algorithms are used, chosen for their contrasting properties: Naive
Bayes, SMO, Logistic Regression, J48 Decision tree, and Random
Forest. For regression, a Random Forest with regression trees is used,
chosen because of its efficient handling of large feature sets
compared to linear models.


\section{Results and Analysis}

The experimental results consist of measured runtimes for a set of
\emph{test cases}, where a test case $\tau_i$ consists of a scenario,
workgroup size pair $\tau_i = (s_i,w_i)$, and is associated with a
\emph{sample} of observed runtimes from multiple runs of the
program. A total of 269813 test cases were evaluated, which represents
an exhaustive enumeration of the workgroup size optimisation space for
429 scenarios. For each scenario, runtimes for an average of 629 (max
7260) unique workgroup sizes were measured. The average sample size
for each test case is 83 (min 33, total 16917118).


\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/num_params_oracle.pdf}
\caption[Oracle accuracy vs.\ number of workgroup sizes]{%
  Accuracy compared to the oracle as a function of the number of
  workgroup sizes used. The best accuracy that can be achieved using a
  single statically chosen workgroup size is 15\%. Achieving 50\%
  oracle accuracy requires a minimum of 14 distinct workgroup sizes.%
}
\label{fig:oracle-accuracy}
\end{figure}

For each scenario $s$, the oracle workgroup size $\Omega(s)$ is the
workgroup size which resulted in the lowest mean runtime. If the
performance of stencils were independent of workgroup size, we would
expect that the oracle workgroup size would remain constant across all
scenarios $s \in S$. Instead, we find that there are 135 unique oracle
workgroup sizes, with 31.5\% of scenarios having a unique workgroup
size. This demonstrates the difficult in attempting to tune for
\emph{optimal} parameter values, since 14 distinct workgroup sizes are
needed to achieve just 50\% of the oracle accuracy
(Figure~\ref{fig:oracle-accuracy}), although it is important to make
the distinction that oracle \emph{accuracy} and \emph{performance} are
not equivalent.

Figure~\ref{fig:oracle-wgsizes} shows the distribution of oracle
workgroup sizes, demonstrating that there is clearly no ``silver
bullet'' workgroup size which is optimal for all scenarios, and that
the space of oracle workgroup sizes is non linear and complex. The
workgroup size which is most frequently optimal is
$w_{(64 \times 4)}$, which is optimal for 15\% of scenarios. Note that
this is not adequate to use as a baseline for static tuning, as it
does not respect legality constraints, that is
$w_{(64 \times 4)} \not\in W_{safe}$.

% \begin{figure}
% \begin{subfigure}[t]{0.98\columnwidth}
% \centering
% \includegraphics[width=\columnwidth]{img/oracle_param_space.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:oracle-wgsizes}
% \end{subfigure}
% \\
% \begin{subfigure}[t]{0.98\columnwidth}
% \centering
% \includegraphics[width=\columnwidth]{img/coverage_space.pdf}
% \vspace{-1.5em} % Shrink vertical padding
% \caption{}
% \label{fig:coverage}
% \end{subfigure}
% \caption[Workgroup size legality and optimality]{%
%   Log frequency counts for: (\subref{fig:oracle-wgsizes}) optimality,
%   and (\subref{fig:coverage}) legality for a subset of the aggregated
%   workgroup size optimisation space, $w_c \le 100, w_r \le 100$. The
%   space of oracle workgroup size frequencies is highly irregular and
%   uneven, with a peak frequency of $w_{(64 \times 4)}$. Legality
%   frequencies are highest for smaller row and column counts (where
%   $w < W_{\max}(s) \forall s \in S$), and $w_c$ and $w_r$ values which
%   are multiples of 8.%
% }
% \label{fig:heatmaps}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=\columnwidth]{img/max_wgsizes.pdf}
%   \vspace{-1.5em} % Shrink vertical padding
%   \caption[Workgroup size coverage]{%
%     A subset of the aggregated workgroup size optimisation space,
%     $w_c \le 100, w_r \le 100$, showing the \emph{coverage} of each
%     workgroup size, i.e.\ the ratio of scenarios for which a workgroup
%     size satisfies architecture and kernel enforced constraints
%     ($W_{\max}(s)$). Workgroup sizes with a coverage of $< 1$ fail to
%     satisfy these constraints for one or more scenarios. Only
%     workgroup sizes with a coverage of 1 may be used for static
%     tuning, which greatly reduces the size of the optimisation
%     space. Observed $W_{\max}(s)$ values are multiples of 256, hence
%     the abrupt ``steps'' in coverage.%
%   }
% \label{fig:max-wgsizes}
% \end{figure}

% We define the \emph{coverage} of a workgroup size to be the ratio
% $0 \le x \le 1$ between the number of scenarios for which the
% workgroup size was less than $W_{\max}(s)$, normalised to the total
% number of workgroup sizes. A coverage of 1 implies a workgroup size
% which is always legal for all combinations of stencil and
% architecture. A workgroup size with a coverage of 0 is never legal.

% Note that since $W_{\max}(s)$ defines a hard limit for a given $s$, if
% statically selecting a workgroup size, one must limit the optimisation
% space to the smallest $W_{\max}(s)$ value, i.e.\ only the workgroup
% sizes with a coverage of 1. The observed $W_{\max}(s)$ values range
% from 256--8192, which results in up to a 97\% reduction in the size of
% the optimisation space when $W_{\max}(s) = 8192$, even though only
% 14\% of scenarios have the minimum value of $W_{\max}(s) = 256$.

\begin{figure}
  \centering
  \centering
  \includegraphics[width=.75\columnwidth]{img/refused_params_by_device}
  \caption[Refused workgroup sizes by device and vendor]{%
    The ratio of test cases with refused workgroup sizes, grouped by
    OpenCL device ID. %
    % Parameters were refused most frequently by Intel i5
    % CPUs, then by previous-generation NVIDIA GPUs.
    No parameters were refused by AMD devices.%
  }
\label{fig:refused-params-by-dev}
\end{figure}

In addition to the hard constraints imposed by the maximum workgroup
size, there are also refused parameters, which are workgroup sizes
which are rejected by the OpenCL runtime and do not provide a
functioning program. Of the 8504 unique workgroup sizes tested, 11.4\%
were refused in one or more test cases. An average of 5.5\% of all
test cases lead to refused parameters. For a workgroup size to be
refused, it must satisfy the architectural and program-specific
constraints which are exposed by OpenCL, but still lead to a
\texttt{CL\_OUT\_OF\_RESOURCES} error when the kernel is enqueued.
While uncommon, a refused parameter is an obvious inconvenience to the
user, as one would expect that any workgroup size within the specified
maximum should behave \emph{correctly}, if not
efficiently. Figure~\ref{fig:coverage} visualises the space of legal
workgroup sizes by showing the frequency counts that a workgroup size
is legal. Smaller workgroup sizes are legal most frequently due to the
$W_{\max}(s)$ constraints. Beyond that, workgroup sizes which contain
$w_c$ and $w_r$ values which are multiples of eight are more
frequently legal, which is a common width of SIMD vector
operations~\cite{IntelCorporation2012}.

Experimental results suggest that the problem is vendor --- or at
least device --- specific. By grouping the refused test cases by
device, we see a much greater quantity of refused parameters for test
cases on Intel CPU devices than any other type, while no workgroup
sizes were refused by the AMD
GPU. Figure~\ref{fig:refused-params-by-dev} shows these groupings. The
exact underlying cause for these refused parameters is unknown, but
can likely by explained by inconsistencies or errors in specific
OpenCL driver implementations. As these OpenCL implementations are
still in active development, it is anticipated that errors caused by
unexpected behaviour will become more infrequent as the technology
matures. The ratio of refused parameters decreases across the three
generations of Nvidia GPUs: GTX 590 (2011), GTX 690 (2012), and GTX
TITAN (2013). It is imperative that any autotuning system is capable
of adapting to these refused parameters by suggesting alternatives
when they occur.

% BASELINE

The baseline parameter $\bar{w}$ is the workgroup size which provides
the best overall performance while being legal for all scenarios. It
is the workgroup size $w \in W_{safe}$ which maximises the output of
the performance function $\bar{p}(w)$.
% As shown in Table~\ref{tab:highest-legality},
Only a \emph{single} workgroup size $w_{(4 \times 4)}$ from the set of
experimental results is found to have a legality of 100\%, suggesting
that an adaptive approach to setting workgroup size is necessary not
just for the sake of maximising performance, but also for guaranteeing
program correctness.
% TODO: Justify

The utility of the baseline parameter is that it represents the best
performance that can be achieved through static tuning of the
workgroup size parameter. We can evaluate the performance of
suboptimal workgroup sizes by calculating the geometric mean of their
\emph{performance} for a particular scenario $p(s, w)$ across all
scenarios, $\bar{p}(w)$. The baseline parameter $\bar{p}(\bar{w})$
achieves only 24\% of the available
performance. Figure~\ref{fig:performance-legality} plots workgroup
size \emph{legality} and \emph{performance}, showing that there is no
clear correlation between the two. In fact, the workgroup sizes with
the highest mean performance are valid only for scenarios with the
largest $W_{\max}(s)$ value, which account for less than 1\% of all
scenarios, further reinforcing the case for adaptive tuning.
% The workgroup sizes with the highest legality are listed in
% Table~\ref{tab:highest-legality}, and the workgroup sizes with the
% highest performance are listed in
% Table~\ref{tab:highest-performance}.

Figure~\ref{fig:speedups} shows the speedup of the oracle workgroup
size over the baseline parameter $w_{(4 \times 4)}$ for all
scenarios. If we assume that sufficiently pragmatic developer with
enough time would eventually find this static optimal, then this
provides a reasonable comparison for calculating speedups of an
autotuner for workgroup size. Comparing the runtime of workgroup sizes
relative to the oracle allows us to calculate upper bounds on the
possible performance which can be expected from autotuning.


% \begin{figure}
% \centering
% \includegraphics[width=\columnwidth]{img/params_summary.pdf}
% \caption[Workgroup size legality vs.\ performance]{%
%   Average legality and performance relative to the oracle of all
%   workgroup sizes. Clearly, the relationship between legality and
%   performance is not linear. Distinct vertical ``bands'' appear
%   between regions of legality caused by the different $W_{\max}(s)$
%   values of devices. The irregular jitter between these vertical bands
%   is caused by refused parameters.%
% }
% \label{fig:performance-legality}
% \end{figure}


% \begin{table}
%   \centering
%   \scriptsize
%   \begin{tabular}{lrr}
%     \toprule
%     Parameter &  Legality (\%) &  Performance (\%) \\
%     \midrule
%     $4 \times 4$ &          100.0 &              34.6 \\
%     $32 \times 4$ &           97.4 &              79.2 \\
%     $40 \times 4$ &           96.5 &              77.5 \\
%     $16 \times 4$ &           96.3 &              65.5 \\
%     $56 \times 4$ &           95.6 &              81.0 \\
%     $16 \times 8$ &           95.6 &              75.7 \\
%     $8 \times 4$ &           95.6 &              51.7 \\
%     $24 \times 4$ &           95.3 &              69.6 \\
%     $24 \times 8$ &           95.1 &              78.4 \\
%     $48 \times 4$ &           94.6 &              83.1 \\
%     $8 \times 8$ &           94.6 &              63.4 \\
%     $4 \times 16$ &           94.6 &              50.1 \\
%     $4 \times 8$ &           94.4 &              41.5 \\
%     $8 \times 16$ &           94.2 &              70.6 \\
%     $4 \times 40$ &           94.2 &              50.2 \\
%     $4 \times 32$ &           93.9 &              53.6 \\
%     $4 \times 48$ &           93.9 &              51.9 \\
%     $4 \times 56$ &           93.9 &              50.3 \\
%     $4 \times 24$ &           93.5 &              50.4 \\
%     $8 \times 24$ &           92.5 &              69.5 \\
%     $64 \times 4$ &           84.1 &              87.5 \\
%     $16 \times 16$ &           84.1 &              74.5 \\
%     $24 \times 16$ &           83.2 &              75.1 \\
%     $32 \times 8$ &           83.0 &              81.3 \\
%     $40 \times 16$ &           82.8 &              80.3 \\
%     \bottomrule
%   \end{tabular}
%   \caption[Workgroup sizes with greatest legality]{%
%     The 25 workgroup sizes with the greatest legality.%
%   }
%   \label{tab:highest-legality}
% \end{table}

% \begin{table}
%   \centering
%   \scriptsize
%   \begin{tabular}{lrr}
%     \toprule
%     Parameter &  Legality (\%) &  Performance (\%) \\
%     \midrule
%     $270 \times 24$ &            0.2 &              97.5 \\
%     $174 \times 38$ &            0.2 &              94.4 \\
%     $310 \times 20$ &            0.2 &              94.2 \\
%     $546 \times 10$ &            0.2 &              93.0 \\
%     $282 \times 16$ &            0.2 &              93.0 \\
%     $520 \times 10$ &            0.2 &              92.9 \\
%     $520 \times 12$ &            0.2 &              92.3 \\
%     $746 \times 8$ &            0.2 &              92.0 \\
%     $38 \times 140$ &            0.2 &              91.9 \\
%     $300 \times 18$ &            0.2 &              91.3 \\
%     $700 \times 6$ &            0.2 &              91.1 \\
%     $96 \times 24$ &           24.7 &              90.6 \\
%     $88 \times 48$ &            0.7 &              90.6 \\
%     $820 \times 6$ &            0.2 &              90.4 \\
%     $88 \times 32$ &           24.9 &              90.0 \\
%     $96 \times 32$ &           25.4 &              89.8 \\
%     $88 \times 40$ &           21.0 &              89.7 \\
%     $80 \times 16$ &           30.3 &              89.6 \\
%     $722 \times 10$ &            0.2 &              89.5 \\
%     $280 \times 24$ &            0.2 &              89.5 \\
%     $910 \times 6$ &            0.2 &              89.4 \\
%     $88 \times 24$ &           28.7 &              89.4 \\
%     $64 \times 24$ &           29.4 &              89.2 \\
%     $80 \times 32$ &           24.5 &              89.2 \\
%     $72 \times 16$ &           29.4 &              89.2 \\
%     \bottomrule
%   \end{tabular}
%   \caption[Workgroup sizes with greatest performance]{%
%     The 25 workgroup sizes with the greatest mean
%     performance.%
%   }
%   \label{tab:highest-performance}
% \end{table}


\subsubsection{Speedup Upper Bounds}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{img/max_speedups}
%   \caption[Workgroup size speedups]{%
%     Speedup of oracle workgroup size over: the worst performing
%     workgroup size for each scenario (\emph{Max}), the statically
%     chosen workgroup size that provides the best overall performance
%     ($w_{(4 \times 4)}$), and the human expert selected parameter
%     ($w_{(32 \times 4)}$). Note that the human expert parameter is not
%     legal for all scenarios.%
%   }
% \label{fig:speedups}
% \end{figure}

For a given scenario $s$, the ratio of the workgroups sizes from
$W_{legal}(s)$ which provide the longest and shortest mean runtimes is
used to calculate an upper bound for the possible performance
influence of workgroup size:
%
\begin{equation}
r_{max}(s) = r(s, \argmax_{w \in W_{legal}(s)} t(s,w), \Omega(s))
\end{equation}
%
When applied to each scenario $s \in S$ of the experimental results,
we find the average of speedup upper bounds to be $15.14\times$ (min
$1.03\times$, max $207.72\times$). This demonstrates the importance of
tuning stencil workgroup sizes --- if chosen incorrectly, the runtime
of stencil programs can be extended by up to $207.72\times$. Note too
that for 5 of the scenarios, the speedup of the best over worst
workgroup sizes is $\le 5\%$.
% TODO: t-test for this!
For these scenarios, there is little benefit to autotuning; however,
this represents only 1.1\% of the tested scenarios. For 50\% of the
scenarios, the speedup of the best over worst workgroup sizes is
$\ge 6.19\times$.


\subsubsection{Human Expert}

% SCENARIOS IN WHICH 32x4 WAS NOT LEGAL:
%
% sqlite> select distinct name,kernels.north,kernels.south,kernels.east,kernels.west,device,dataset,name from runtime_stats left join scenarios on runtime_stats.scenario=scenarios.id left join kernel_names on scenarios.kernel=kernel_names.id left join kernels on scenarios.kernel=kernels.id where scenario NOT IN (select scenario from runtime_stats where params="32x4");
% name        north       south       east        west        device                                     dataset                name
% ----------  ----------  ----------  ----------  ----------  -----------------------------------------  ---------------------  ----------
% complex     30          30          30          30          1xIntel(R) Core(TM) i5-4570 CPU @ 3.20GHz  1024.1024.float.float  complex
% simple      0           0           0           0           1xIntel(R) Core(TM) i5-2430M CPU @ 2.40GH  1024.1024.float.float  simple
% complex     30          30          30          30          1xIntel(R) Core(TM) i5-2430M CPU @ 2.40GH  2048.2048.float.float  complex
% complex     1           10          30          30          1xIntel(R) Core(TM) i5-4570 CPU @ 3.20GHz  512.512.float.float    complex
% simple      30          30          30          30          1xIntel(R) Core(TM) i5-2430M CPU @ 2.40GH  512.512.float.float    simple
% complex     30          30          30          30          1xGeForce GTX 690                          512.512.float.float    complex
% simple      20          10          20          10          1xIntel(R) Core(TM) i5-2430M CPU @ 2.40GH  4096.4096.float.float  simple
% simple      1           10          30          30          1xIntel(R) Core(TM) i5-2430M CPU @ 2.40GH  2048.2048.float.float  simple
% complex     30          30          30          30          1xGeForce GTX 690                          1024.1024.float.float  complex
% complex     1           10          30          30          1xIntel(R) Core(TM) i5-2430M CPU @ 2.40GH  2048.2048.float.float  complex
% simple      30          30          30          30          1xGeForce GTX 690                          512.512.float.float    simple

In the original implementation of the SkelCL stencil
skeleton~\cite{Breuer2013}, \citeauthor{Breuer2013} selected a
workgroup size of $w_{(32 \times 4)}$ in an evaluation of 4 stencil
operations on a Tesla S1070 system. We can use this as an additional
parameter to compare the relative performance of workgroup sizes
against. However, the $w_{(32 \times 4)}$ workgroup size is invalid
for 2.6\% of scenarios, as it is refused in 11 test cases. By device,
those are: 3 on the GTX 690, 6 on the i5-2430M, and 2 on the i5-4570.
For the scenarios where $w_{(32 \times 4)}$ \emph{is} legal, the human
expert chosen workgroup size achieves an impressive geometric mean of
79.2\% of the oracle performance. The average speedup of oracle
workgroup sizes over the workgroup size selected by a human expert is
$1.37\times$ (min $1.0\times$, max $5.17\times$). Since the workgroup
size selected by the human expert is not legal for all scenarios, we
will examine the effectiveness of heuristics for tuning workgroup
size.


\subsubsection{Heuristics}

In this subsection we will consider the effectiveness of instead
selecting workgroup size using two types of heuristics. The first,
using the maximum workgroup size returned by the OpenCL device and
kernel APIs to select the workgroup size adaptively. The second, using
per-device heuristics, in which the workgroup size is selected based
on the specific architecture that a stencil is operating on.

\paragraph{Using maximum legal size}

A common approach taken by OpenCL developers is to set the workgroup
size for a kernel based on the maximum legal workgroup size queried
from the OpenCL APIs. For example, to set the size of 2D workgroup, a
developer the square root of the (scalar) maximum wgsize to set the
number of columns and rows (since $w_c \cdot w_r$ must be
$< W_{\max}(s)$). To consider the effectiveness of this approach, we
group the workgroup size performances based on the ratio of the
maximum allowed for each scenario. We can also perform this for each
of the two dimensions --- rows and columns --- of the stencil
workgroup size.

Figure~\ref{fig:performance-wgsizes} shows the distribution of
runtimes when grouped this way, demonstrating that the performance of
(legal) workgroup sizes are not correlated with the maximum workgroup
sizes $W_{\max}(s)$. However, when considering individual components,
we observe that the best median workgroup size performances are
achieved with a number of columns that is between 10\% and 20\% of the
maximum, and a number of rows that is between 0\% and 10\% of the
maximum.

% \paragraph{Per-device workgroup sizes}

% \begin{table}
%   \scriptsize
%   \centering
%   \begin{tabular}{lllp{1cm}p{1cm}}
%     \toprule
%     Device &         Oracle & Legality & Perf.\ min & Perf.\ avg. \\
%     \midrule
%     AMD Tahiti 7970 &   $48\times 4$ &      1.0 &       0.54 &        0.91 \\
%     Intel i5-2430M &  $64\times 16$ &      0.8 &       0.37 &        0.91 \\
%     Intel i5-4570 &   $88\times 8$ &     0.89 &       0.33 &        0.89 \\
%     Intel i7-3820 &  $40\times 24$ &     0.95 &       0.76 &        0.97 \\
%     NVIDIA GTX 590 &  $12\times 2$ &      0.8 &        0.2 &         0.9 \\
%     NVIDIA GTX 690 &   $64\times 4$ &     0.93 &       0.32 &        0.84 \\
%     NVIDIA GTX TITAN &   $64\times 4$ &      1.0 &       0.26 &        0.81 \\
%     \textbf{CPUs} &   $88\times 8$ &     0.88 &       0.33 &        0.91 \\
%     \textbf{GPUs} &   $64\times 4$ &     0.76 &       0.26 &        0.86 \\
%     \bottomrule
%   \end{tabular}
%   \caption[Performance of tuning with a per-device heuristic]{%
%     Selecting workgroup size using a per-device heuristic. The mode
%     optimal workgroup size for each device type $\bar{w}$ is evaluated
%     based on legality, and performance relative to the oracle (minimum
%     and average) when legal.%
%   }
%   \label{tab:heuristic-dev}
% \end{table}

% One possible technique to selecting workgroup size is to tune
% particular values for each targeted execution device. This approach is
% sometimes adopted for cases with particularly high requirements for
% performance on a single platform, so it produces an interesting
% contrast to evaluating a machine learning approach, which attempts to
% predict workgroup sizes for unseen platforms without the need for an
% expensive exploration phase on the new platform.

% Figure~\ref{fig:performances} shows the performance of workgroup sizes
% relative to the oracle across scenarios grouped by: kernel, device,
% and dataset. When grouped like this, a number of observations can
% made. First is that not all of the kernels are sensitive to tuning
% workgroup size to the same degree. The \emph{sobel} kernel has the
% lowest median performance, indicating that it is the most profitable
% to tune, while the \emph{threshold} kernel is the least
% profitable. Similarly, the Intel i7-3820 is far less amenable to
% tuning than the other devices, while the Intel i5-4570 is the most
% sensitive to the workgroup size parameter. Such variances in the
% distributions of workgroup sizes suggest that properties underlying
% the architecture, kernel, and dataset all contribute towards the
% proper selection of workgroup size.

% To test the performance of a per-device heuristic for selecting
% workgroup size, we group the scenarios by device, and compare the
% relative performance of all workgroup sizes for each group of
% scenarios. The most frequently optimal workgroup size $\bar{w}$ for
% each device is selected, and the legality and performance of each
% scenario using that workgroup size is evaluated.
% Table~\ref{tab:heuristic-dev} shows the results of this evaluation.
% The GTX 690 and GTX TITAN share the same $\bar{w}_{(64 \times 4)}$
% value, while every other device has a unique optimum. The general case
% performance of these per-device parameters is very good, although
% legality is only assured for the GTX TITAN and AMD 7970 (which did not
% refuse any parameters). However, the worst case performance of
% per-device workgroup sizes is poor for all except the i7-3820 (which
% is least sensitive to tuning), suggesting that device alone is not
% capable of reliably informing the choice of workgroup size.


% \begin{figure}
%   \begin{subfigure}[h]{\columnwidth}
%     \centering
%     \includegraphics[width=\columnwidth]{img/performance_max_wgsize}
%     \vspace{-1.5em} % Shrink vertical padding
%     \caption{}
%     \label{fig:performance-max-wgsize}
%   \end{subfigure}
%   \\
%   \begin{subfigure}[h]{.48\columnwidth}
%     \centering
%     \includegraphics[width=\columnwidth]{img/performance_max_c}
%     \vspace{-1.5em} % Shrink vertical padding
%     \caption{}
%     \label{fig:performance-wg-c}
%   \end{subfigure}
%   ~%
%   \begin{subfigure}[h]{.48\columnwidth}
%     \centering
%     \includegraphics[width=\columnwidth]{img/performance_max_r}
%     \vspace{-1.5em} % Shrink vertical padding
%     \caption{}
%     \label{fig:performance-wg-r}
%   \end{subfigure}

%   \caption[Workgroup size performances vs.\ size]{%
%     Comparing workgroup performance relative to the oracle as function
%     of: (\subref{fig:performance-max-wgsize})~maximum legal size,
%     (\subref{fig:performance-wg-c})~number of columns, and
%     (\subref{fig:performance-wg-r})~ number of rows. Each workgroup
%     size is normalised to the maximum allowed for that scenario, $W_{\max}(s)$.%
%   }
%   \label{fig:performance-wgsizes}
% \end{figure}

\begin{figure}
  \begin{subfigure}[h]{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_kernels.pdf}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-kernels}
  \end{subfigure}
  \\
  \begin{subfigure}[h]{.48\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_devices.pdf}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-devices}
  \end{subfigure}
  ~%
  \begin{subfigure}[h]{.48\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{img/performance_datasets.pdf}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:performance-datasets}
  \end{subfigure}
  \caption[Workgroup size performances across device, kernel, and dataset]{%
    Performance relative to the oracle of workgroup sizes across all
    test cases, grouped by: (\subref{fig:performance-kernels})~kernels,
    (\subref{fig:performance-devices})~devices, and
    (\subref{fig:performance-datasets})~datasets.%
  }
  \label{fig:performances}
\end{figure}


\subsection{Autotuning}

This section evaluates the performance of OmniTune when tasked with
selecting workgroup sizes for SkelCL stencil codes. First I discuss
measurement noise present in the experimental results, and the methods
used to accommodate for it. Then I examine the observed effect that
workgroup size has on the performance of SkelCL stencils. The
effectiveness of each of the autotuning techniques described in the
previous sections is evaluated using multiple different machine
learning algorithms. The prediction quality of OmniTune is scrutinised
for portability across programs, devices, and datasets.


In this subsection, we evaluate the performance of OmniTune for
predicting workgroup sizes of SkelCL skeletons using the prediction
techniques described in Subsection~\ref{subsec:omnitune-ml}. For each
technique, we partition the experimental data into training and
testing sets, $S_{training} \subset S$ and
$S_{testing} = S - S_{training}$. A set of labelled training data
$D_{training}$ is derived from $S_{training}$, and the prediction
quality is testing using the validation set $D_{testing}$ derived from
$S_{training}$. We use multiple approaches to partitioning test data
to evaluate the prediction quality under different scenarios. The
processes for generating validation sets are:
%
\begin{itemize}
\item 10-fold --- shuffle the set of all data and divide into 10
  validation sets, each containing 10\% of the data. This process is
  repeated for 10 rounds, resulting in 100 validations of 10
  permutations of the data.
\item Synthetic --- divide the training data such that it consists
  solely of data collected from synthetic benchmarks, and use data
  collected from real-world benchmarks to test.
\item Device --- partition the training data into $n$ sets, one for
  each device. Use $n-1$ sets for training, repeating until every
  partition has been used for testing once.
\item Kernel --- partition the training data into $n$ sets, one for
  each kernel. Use $n-1$ sets for training, repeating until every
  partition has been used for testing once.
\item Dataset --- partition the training data into $n$ sets, one for
  each type of dataset. Use $n-1$ sets for training, repeating until
  every partition has been used for testing once.
\end{itemize}
%
For each autotuning technique, the results of testing using the
different validation sets are reported separately. The autotuning
techniques evaluated are: using classifiers to predict the optimal
workgroup size of a stencil, with fallback strategies to handle
illegal predictions; using regressors to predict the runtime of a
stencil using different workgroup sizes, and selecting the legal
workgroup size which has the lowest predicted runtime; and using
regressors to predict the relative performance of workgroup sizes over
a baseline, and selecting the workgroup size which has the highest
predicted relative performance. We first describe the evaluation
strategies for each technique, before presenting experimental results
and analysis.


\subsubsection{Evaluating Classifiers}

Training data consists of pairs of feature vectors $f(s)$ and oracle
workgroup sizes $\Omega(s)$:
%
\begin{equation}
  D_{training} = \left\{ (f(s),\Omega(s)) | s \in S_{training} \right\}
\end{equation}
%
Testing data are not labelled with oracle workgroup sizes:
%
\begin{equation}
  D_{testing} = \left\{ f(s) | s \in S_{testing} \right\}
\end{equation}
%
Each classifier is evaluated using the three different classification
techniques: \textsc{Baseline}, \textsc{Random}, and
\textsc{NearestNeighbour}, which differ in the way in which they
handle illegal predictions. Illegal predictions occur either because
the classifier has suggested a parameter which does not satisfy the
maximum workgroup size constraints $w < W_{\max}(s)$, or because the
workgroup size is refused by OpenCL $w \in W_{refused}(s)$. Workgroup
sizes are predicted for each scenario in the testing set, and the
quality of the predicted workgroup size is evaluated using the
following metrics:
%
\begin{itemize}
\item accuracy (binary) --- whether the predicted workgroup size is
  the true oracle, $p(f(s)) = \Omega(s)$.
\item validity (binary) --- whether the classifier predicted a
  workgroup size which satisfies the workgroup size constraints
  constraints, $p(f(s)) < W_{\max}(s)$.
\item refused (binary) --- whether the classifier predicted a
  workgroup size which is refused, $p(f(s)) \in W_{refused}(s)$.
\item performance (real) --- the relative performance of the predicted
  workgroup size relative to the oracle,
  $0 \le r(p(f(s)), \Omega(s)) \le 1$.
\item speedups (real) --- the relative performance of the predicted
  workgroup size relative to the baseline workgroup size
  $w_{(4 \times 4)}$, and human expert workgroup size
  $w_{(32 \times 4)}$ (where applicable).
\item time (real) --- the round trip time required to make the prediction,
  as measured by the OmniTune client. This includes classification
  time and inter-process communication overheads between the client
  and server.
\end{itemize}
%
The \emph{validty} and \emph{refused} metrics measure how often
fallback strategies are required to select a legal workgroup size
$w \in W_{legal}(s)$.


\subsubsection{Evaluating Regressors}

The evaluation approach for predicting runtimes and speedups is the
same; only the training data differs. For predicting runtimes,
training data consists of feature vectors, labelled with the mean
observed runtime $t(s,w)$ for all legal workgroup sizes:
%
\begin{equation}
  D_{training} = \bigcup_{\forall s \in S_{training}} \left\{ (f(s),t(s,w)) | w \in W_{legal}(s)
  \right\}
\end{equation}
For predicting speedups, the features vectors are labelled with
observed speedup over the baseline parameter $\bar{w}$ for all legal
workgroup sizes:
\begin{equation}
\begin{split}
  D_{training} = \cup \left\{ (f(s),r(s,w,\bar{w})) | w \in W_{legal}(s)
  \right\} \forall \\
  s \in S_{training}
\end{split}
\end{equation}
%
Test data consists of unlabelled feature vectors:
%
\begin{equation}
  D_{testing} = \left\{ f(s) | s \in S_{testing} \right\}
\end{equation}
%
The quality of predicted workgroup sizes is evaluated using the
following metrics:
%
\begin{itemize}
\item accuracy (binary) --- whether the predicted workgroup size is
  the true oracle, $p(f(s)) = \Omega(s)$.
\item performance (real) --- the relative performance of the predicted
  workgroup size relative to the oracle,
  $0 \le r(p(f(s)), \Omega(s)) \le 1$.
\item speedups (real) --- the relative performance of the predicted
  workgroup size relative to the baseline workgroup size
  $w_{(4 \times 4)}$, and human expert workgroup size
  $w_{(32 \times 4)}$ (where applicable).
\item time (real) --- the round trip time required to make the
  prediction, as measured by the OmniTune client. This includes
  classification time and inter-process communication overheads
  between the client and server.
\end{itemize}
%
Unlike with classifiers, the process of selecting workgroup sizes
using regressors is resistant to refused parameters, so no fallback
strategies are required, and the \emph{validity} and \emph{refused}
metrics are not used.


\subsubsection{Results and Analysis}

The purpose of this evaluation is to test the effectiveness of machine
learning-enabled autotuning for predicting workgroup sizes of SkelCL
stencils codes. First, we consider the prediction performance of
classifiers.


\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/classification-syn-real}
\caption[Classification results using synthetic benchmarks]{%
  Classification results for synthetic benchmarks. Each classifier is
  trained on data from synthetic stencils, and tested for prediction
  quality using data from 6 real world benchmarks.%
}
\label{fig:class-syn}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/classification-arch}
\caption[Classification results using cross-device evaluation]{%
  Classification results of cross-device evaluation. Each classifier
  is trained using data from $n-1$ devices, and tested for prediction
  quality using data for the $n^{th}$ device.%
}
\label{fig:class-arch}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[h]{.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/runtime-class-xval}
\caption{}
\label{fig:runtime-class-xval}
\end{subfigure}
\begin{subfigure}[h]{.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/speedup-class-xval}
\caption{}
\label{fig:speedup-class-xval}
\end{subfigure}
\caption[Autotuning performance using regressors]{%
  Evaluating the effectiveness of classification using regressors, by
  predicting: (\subref{fig:runtime-class-xval}) the workgroup size
  with the minimal runtime, and (\subref{fig:speedup-class-xval}) the
  workgroup size with the greatest speedup over a baseline.%
}
\label{fig:regression-class}
\end{figure}

With the exception of the ZeroR, which predicts \emph{only} the
baseline workgroup size $w_{\left( 4 \times 4 \right)}$, the
classifiers achieve good speedups over the baseline. Average
classification speedups across all validation sets range between
$4.61\times$ and $5.05\times$. Figures~\ref{fig:class-syn}
and~\ref{fig:class-arch} show a summary of results using 10-fold
cross-validation and cross-device validation, respectively.  The
highest average speedup is achieved by SMO, and the lowest by Naive
Bayes. The difference between average speedups is not significant
between the types of classifier, with the exception of SimpleLogistic,
which performs poorly when trained with synthetic benchmarks and
tested against real-world programs. This suggests the model
over-fitting to features of the synthetic benchmarks which are not
shared by the real-world
tests.

% \begin{figure}
% \centering
% \includegraphics[width=\columnwidth]{img/fallback_speedups}
% \caption[Comparison of fallback handler speedups]{%
%   Comparison of fallback handlers, showing the speedup over baseline
%   parameter for all test cases where a classifier predicted an illegal
%   workgroup size.%
% }
% \label{fig:fallback-speedups}
% \end{figure}

By isolating the test cases where classifiers predicted an illegal or
refused parameter, we can directly compare the relative effectiveness
of each fallback handler. The fallback handler with the best average
case performance is \textsc{NearestNeighbour}, with an average speedup
across all classifiers and validation sets of $5.26\times$. The
speedup of \textsc{Random} fallback handler is $3.69\times$, and
$1.0\times$ for \textsc{Baseline}. Figure~\ref{fig:fallback-speedups}
plots the speedups of each fallback handler for all of these isolated
test cases. Interestingly, both the lowest and highest speedups are
achieved by the \textsc{Random} fallback handler, since it essentially
performs a random exploration of the optimisation space. However, the
\textsc{NearestNeighbour} fallback handler provides consistently
greater speedups for the majority of test cases, indicating that it
successfully exploits the structure of the optimisation spaces.

Figures~\ref{fig:runtime-class-xval} and ~\ref{fig:speedup-class-xval}
show a summary of results for classification using regressors to
predict kernel runtimes and speedups, respectively. Of the two
regression techniques, predicting the \emph{speedup} of workgroup
sizes is much more successful than predicting the \emph{runtime}. This
is most likely caused by the inherent difficulty in predicting the
runtime of arbitrary programs, where dynamic factors such as flow
control and loop bounds are not captured by the kernel features used
in OmniTune, which instead use simple static static instruction counts
and densities. The average speedup achieved by predicting runtimes is
$4.14\times$. For predicting speedups, the average is $5.57\times$.
Tables~\ref{tab:class}, \ref{tab:runtime-class},
and~\ref{tab:speedup-class} show mean performances and speedups for:
J48 classifier using the \textsc{NearestNeighour} fallback strategy,
classification using runtime regression, and classification using
speedup regression, respectively.

If we eliminate the 2.6\% of test cases for which the workgroup size
of $w_{(32 \times 4)}$ is illegal, we can compare the performance of
OmniTune directly against the human expert chosen workgroup
size. Figure~\ref{fig:speedup-distributions} compares the speedups of
all such validation instances over the human expert parameter, for
each autotuning technique. The speedup distributions show consistent
classification results for the five classification techniques, with
the speedup at the lower quartile for all classifiers being
$\ge 1.0\times$. The IQR for all classifiers is $< 0.5$, but there are
outliers with speedups both well below $1.0\times$ and well above
$2.0\times$. In contrast, the speedups achieved using runtime
regression have a lower range, but also a lower median and a larger
IQR. Clearly, runtime regression is the least effective of the
evaluated autotuning techniques. Speedup regression is more
successful, with the highest median speedup of all the
techniques. However, it also has a large IQR and the lower quartile
has a speedup value well below 1, meaning that for more than 25\% of
test instances, the workgroup size selected did not perform as well as
the human expert selected workgroup size.

% The lower average speedup attained by speedup regression over the
% J48 classifier belies the fact that the \emph{median} speedup is
% much greater, at $1.33 \times$.


\begin{figure}
\centering
\includegraphics[width=\columnwidth]{img/speedup-distributions}
\caption[Speedup results over human expert]{%
  Distributions of speedups over \emph{human expert}, ignoring cases
  where human expert prediction is invalid. Classifiers are using
  \textsc{NearestNeighbour} fallback handlers. The speedup axis is
  fixed to the range 0--2.5 to highlight the IQRs, which results in
  some outliers > 2.5 being clipped.%
}
\label{fig:speedup-distributions}
\end{figure}


% \begin{table}
% \scriptsize
% \centering
% \begin{tabular}{llll}
% \toprule
%               Job &    Performance &            Speedup &       Human Expert \\
% \midrule
%           10-fold &           92\% &       $5.65\times$ &       $1.26\times$ \\
%         Synthetic &           92\% &       $4.79\times$ &       $1.13\times$ \\
%            Device &           85\% &       $5.23\times$ &       $1.17\times$ \\
%            Kernel &           89\% &       $5.43\times$ &       $1.21\times$ \\
%           Dataset &           91\% &       $5.63\times$ &       $1.25\times$ \\
%  \textbf{Average} &  \textbf{90\%} &  $\bm{5.45\times}$ &  $\bm{1.22\times}$ \\
% \bottomrule
% \end{tabular}
% \caption{Validation results for J48 and \textsc{NearestNeighbour}
%   classification.}
% \label{tab:class}
% \end{table}
% \begin{table}
% \scriptsize
% \centering
% \begin{tabular}{llll}
% \toprule
%               Job &    Performance &            Speedup &       Human Expert \\
% \midrule
%           10-fold &           68\% &       $4.13\times$ &       $0.88\times$ \\
%         Synthetic &           78\% &       $3.81\times$ &       $1.06\times$ \\
%            Device &           69\% &       $3.89\times$ &       $0.97\times$ \\
%            Kernel &           74\% &       $4.36\times$ &       $1.04\times$ \\
%           Dataset &           72\% &       $4.33\times$ &       $0.98\times$ \\
%  \textbf{Average} &  \textbf{70\%} &  $\bm{4.14\times}$ &  $\bm{0.92\times}$ \\
% \bottomrule
% \end{tabular}
% \caption{Validation results for runtime regression.}
% \label{tab:runtime-class}
% \end{table}
% \begin{table}
% \scriptsize
% \centering
% \begin{tabular}{llll}
% \toprule
%               Job &    Performance &            Speedup &       Human Expert \\
% \midrule
%           10-fold &           89\% &       $5.67\times$ &       $1.10\times$ \\
%         Synthetic &           86\% &       $4.48\times$ &       $1.19\times$ \\
%            Device &           85\% &       $5.18\times$ &       $1.15\times$ \\
%            Kernel &           88\% &       $5.38\times$ &       $1.15\times$ \\
%           Dataset &           88\% &       $5.53\times$ &       $1.13\times$ \\
%  \textbf{Average} &  \textbf{89\%} &  $\bm{5.57\times}$ &  $\bm{1.12\times}$ \\
% \bottomrule
% \end{tabular}
% \caption{Validation results for speedup regression.}
% \label{tab:speedup-class}
% \end{table}


% \subsubsection{Visualising Prediction Errors}

\begin{figure}
\centering
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-1}
\end{subfigure}
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-2}
\end{subfigure}
\\
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-3}
\end{subfigure}
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_5}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-4}
\end{subfigure}
\\
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/reg_runtime_heatmap}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-5}
\end{subfigure}
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/reg_speedup_heatmap}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-6}
\end{subfigure}
\caption[Classification error heatmaps]{%
  Heatmaps of classification errors for 10-fold cross-validation,
  showing a subset of the optimisation space. The shading in each
  cells indicates if it is predicted less frequently (blue), ore more
  frequently (red) than it is optimal. Colour gradients are normalised
  across plots.%
}
\label{fig:class-hmaps}
\end{figure}


The prediction costs using regression are significantly greater than
using classifiers. This is because, while a classifier makes a single
prediction, the number of predictions required of a regressor grows
with the size of $W_{\max}(s)$, since classification with regression
requires making predictions for all
$w \in \left\{ w | w < W_{\max}(s) \right\}$. The fastest classifier
is J48, due to the it's simplicity (it can be implemented as a
sequence of nested \texttt{if}/\texttt{else} statements).

Figure~\ref{fig:class-hmaps} visualises the classification errors of
each of the autotuning techniques. It shows that while the performance
of all of the classifiers is comparable, the distribution of
predictions is not. Only the NaiveBayes and RandomForest classifiers
predicted the human expert selected workgroup size of
$w_{(32 \times 4)}$ as frequently, or more frequently, than it was
optimal. The two regression techniques were the least accurate of all
of the autotuning techniques.


\section{Related Work}\label{sec:related}

This section begins with a brief survey of the broad field of
literature that is relevant to algorithmic skeletons. This is followed
by a review of the current state of the art in autotuning research,
focusing on heterogeneous parallelism, algorithmic skeletons, and
stencil codes. It presents the context and rationale for the research
undertaken for this thesis.


% \subsection{Automating Parallelism}

% It is widely accepted that parallel programming is difficult, and the
% continued repetition of this claim has become something of a trite
% mantra for the parallelism research community. An interesting
% digression is to discuss some of the ways in which researchers have
% attempted to tackle this difficult problem, and why, despite years of
% research, it remains an ongoing challenge.

% The most ambitious and perhaps daring field of parallelism research is
% that of automatic parallelisation, where the goal is to develop
% methods and systems to transform arbitrary sequential code into
% parallelised code. This is a well studied subject, with the typical
% approach being to perform these code transformations at the
% compilation stage. In \citeauthor{Banerjee1993}'s thorough
% review~\cite{Banerjee1993} on the subject, they outline the key
% challenges of automatic parallelisation:
% %
% \begin{itemize}
% \item determining whether sequential code can be legally transformed
%   for parallel execution; and
% \item identifying the transformation which will provide the highest
%   performance improvement for a given piece of code.
% \end{itemize}
% %
% Both of these challenges are extremely hard to tackle. For the former,
% the difficulties lie in performing accurate analysis of code
% behaviour. Obtaining accurate dynamic dependency analysis at compile
% time is an unsolved problem, as is resolving pointers and points-to
% analysis~\cite{Atkin-granville2013, Hind2001,Ghiya2001}.

% The result of these challenges is that reliably performant, automatic
% parallelisation of arbitrary programs remains a far from reached goal;
% however, there are many note worthy variations on the theme which have
% been able to achieve some measure of success.

% One such example is speculative parallelism, which circumvents the
% issue of having incomplete dependency information by speculatively
% executing code regions in parallel while performing dependency tests
% at runtime, with the possibility to fall back to ``safe'' sequential
% execution if correctness guarantees are not
% met~\cite{Prabhu2010,Trachsel2010}.  In~\cite{Jimborean2014},
% \citeauthor{Jimborean2014} present a system which combines polyhedral
% transformations of user code with binary algorithmic skeleton
% implementations for speculative parallelisation, reporting speedups
% over sequential code of up to $15.62\times$ on a 24 core processor.

% Another example is PetaBricks, which is a language and compiler
% enabling parallelism through ``algorithmic choice''~\cite{Ansel2009,
%   Ansel2010}. With PetaBricks, users provide multiple implementations
% of algorithms, optimised for different parameters or use cases. This
% creates a search space of possible execution paths for a given
% program. This has been combined with autotuning techniques for
% enabling optimised multigrid programs~\cite{Chan2009}, with the wider
% ambition that these autotuning techniques may be applied to all
% algorithmic choice programs~\cite{Ansel2014}. While this helps produce
% efficient parallel programs, it places a great burden on the
% developer, requiring them to provide enough contrasting
% implementations to make a search of the optimisation space fruitful.

% Annotation-driven parallelism takes a similar approach. The user
% annotates their code to provide ``hints'' to the compiler, which can
% then perform parallelising transformations. A popular example of this
% is OpenMP, which uses compiler pragmas to mark code sections for
% parallel or vectorised execution~\cite{Dagum1998}. Previous work has
% demonstrated code generators for translating OpenMP to
% OpenCL~\cite{Grewe2013} and CUDA~\cite{Lee2009}. Again,
% annotation-driven parallelism suffers from placing a burden on the
% developer to identify the potential areas for parallelism, and lacks
% the structure that algorithmic skeletons provide.

% Algorithmic skeletons contrast the goals of automatic parallelisation
% by removing the challenge of identifying potential parallelism from
% compilers or users, instead allowing users to frame their problems in
% terms of well defined patterns of computation. This places the
% responsibility of providing performant, well tuned implementations for
% these patterns on the skeleton author.


% \subsection{Iterative Compilation \& Machine
%   Learning}\label{subsec:iterative-compilation}

% Iterative compilation is the method of performance tuning in which a
% program is compiled and profiled using multiple different
% configurations of optimisations in order to find the configuration
% which maximises performance. One of the the first formalised
% publications of the technique appeared in \citeyear{Bodin1998} by
% \citeauthor{Bodin1998}~\cite{Bodin1998}.  Iterative compilation has
% since been demonstrated to be a highly effective form of empirical
% performance tuning for selecting compiler optimisations.

% Given the huge number of possible compiler optimisations (there are
% 207 flags and parameters to control optimisations in GCC v4.9), it is
% often unfeasible to perform an exhaustive search of the entire
% optimisation space, leading to the development of methods for reducing
% the cost of evaluating configurations. These methods reduce evaluation
% costs either by shrinking the dimensionality or size of the
% optimisation space, or by guiding a directed search to traverse a
% subset of the space.

% Machine learning has been successful applied to this problem,
% in~\cite{Stephenson2003}, using ``meta optimisation'' to tune compiler
% heuristics through an evolutionary algorithm to automate the search of
% the optimisation space. \citeauthor{Fursin2011} continued this with
% Milepost GCC, the first machine learning-enabled self-tuning
% compiler~\cite{Fursin2011}. A recent survey of the use of machine
% learning to improve heuristics quality by \citeauthor{Burke2013}
% concludes that the automatic \emph{generation} of these self-tuning
% heuristics but is an ongoing research challenge that offers the
% greatest generalisation benefits~\cite{Burke2013}.

% An approach to online tuning of parallel programs is presented
% in~\cite{Ansel2012} which partitions the available parallel resources
% of a device in to two partitions and then executes two different
% configurations simultaneously using each partition. The configuration
% used for one of the configuration is guaranteed to be ``safe'', and
% the performance

% % Eastep, J., Wingate, D., & Agarwal, A. (2011). Smart Data
% % Structures: An Online Machine Learning Approach to Multicore Data
% % Structures. In Proceedings of the 8th ACM International Conference
% % on Autonomic Computing (pp. 1120). New York, NY, USA:
% % ACM. doi:10.1145/1998582.1998587
% \TODO{Online reinforcement learning for optimising data structures
%   online, \cite{Tesauro2005}}

% % Tesauro, G. (2005). Online Resource Allocation Using Decompositional
% % Reinforcement Learning. In AAAI (pp. 886891).
% \TODO{Reinforcement learning for resource allocation~\cite{Eastep2011}}

% % W. F. Ogilvie, P. Petoumenos, Z. Wang, and H. Leather, Intelligent
% % Heuristic Construction with Active Learning, in 18th International
% % Workshop on Compilers for Parallel Computing, 2015.
% \TODO{Using Active Learning to speed up the learning of compiler
%   heuristics~\cite{Ogilvie2015}. Towards online autotuning, albeit
%   only with binary optimisation parameter.}

%
% SOME EXAMPLES OF ML IN THE WILD:
%

% % R. Bitirgen, E. Ipek, and J. F. Martinez, Coordinated Management of
% % Multiple Interacting Resources in Chip Multiprocessors: A Machine
% % Learning Approach, in 2008 41st IEEE/ACM International Symposium on
% % Microarchitecture, 2008, pp. 318329.
% \TODO{Artificial Neural Networks for resource allocation of CMPS:
% \cite{Bitirgen2008}}

% % Z. Wang and M. F. P. O. Boyle, Partitioning Streaming Parallelism
% % for Multi-cores: A Machine Learning Based Approach, in Proceedings
% % of the 19th international conference on Parallel architectures and
% % compilation techniques, 2010, pp. 307318.
% \TODO{Offline ML for partitioning streaming applications:
% \cite{Wang2010}}

% In~\cite{Saclay2010,Memon2013,Fursin2014}, \citeauthor{Fursin2014}
% advocate a collaborative and ``big data'' driven approach to
% autotuning, arguing that the challenges facing the widespread adoption
% of autotuning and machine learning methodologies can be attributed to:
% a lack of common, diverse benchmarks and datasets; a lack of common
% experimental methodology; problems with continuously changing hardware
% and software stacks; and the difficulty to validate techniques due to
% a lack of sharing in publications. They propose a system for
% addressing these concerns, the Collective Mind knowledge system,
% which, while in early stages of ongoing development, is intended to
% provide a modular infrastructure for sharing autotuning performance
% data and related artifacts across the internet. In addition to sharing
% performance data, the approach taken in this thesis emphasises the
% collective \emph{exploitation} of such performance data, so that data
% gathered from one device may be used to inform the autotuning
% decisions of another. This requires each device to maintain local
% caches of shared data to remove the network overhead that would be
% present from querying a single centralised data store during execution
% of a hot path. The current implementation of Collective Mind uses a
% NoSQL JSON format for storing performance data. The relational schema
% used in this thesis offers greater scaling performance and lower
% storage overhead as the amount of performance data grows.

% Whereas iterative compilation requires an expensive offline training
% phase to search an optimisation space, dynamic optimisers perform this
% optimisation space exploration at runtime, allowing programs to
% respond to dynamic features ``online''. This is a challenging task, as
% a random search of an optimisation space may result in configurations
% with vastly suboptimal performance. In a real world system, evaluating
% many suboptimal configurations will cause a significant slowdown of
% the program. Thus a requirement of dynamic optimisers is that
% convergence time towards optimal parameters is minimised.

% Existing dynamic optimisation research has typically taken a low level
% approach to performing optimisations. Dynamo is a dynamic optimiser
% which performs binary level transformations of programs using
% information gathered from runtime profiling and
% tracing~\cite{Bala2000}. While this provides the ability to respond to
% dynamic features, it restricts the range of optimisations that can be
% applied to binary transformations. These low level transformations
% cannot match the performance gains that higher level parameter tuning
% produces.

% An interesting related tangent to iterative compilation is the
% development of so-called ``superoptimisers''. In~\cite{Massalin1987},
% the smallest possible program which performs a specific function is
% found through a brute force enumeration of the entire instruction
% set. Starting with a program of a single instruction, the
% superoptimiser tests to see if any possible instruction passes a set
% of conformity tests. If not, the program length is increased by a
% single instruction and the process repeats. The exponential growth in
% the size of the search space is far too expensive for all but the
% smallest of hot paths, typically less than 13 instructions. The
% optimiser is limited to register to register memory transfers, with no
% support for pointers, a limitation which is addressed
% in~\cite{Joshi2002}. Denali is a superoptimiser which uses constraint
% satisfaction and rewrite rules to generate programs which are
% \emph{provably} optimal, instead of searching for the optimal
% configuration through empirical testing. Denali first translates a low
% level machine code into guarded multi-assignment form, then uses a
% matching algorithm to build a graph of all of a set of logical axioms
% which match parts of the graph, before using boolean satisfiability to
% disprove the conjecture that a program cannot be written in $n$
% instructions. If the conjecture cannot be disproved, the size of $n$
% is increased and the process repeats.


% \subsubsection{Training with Synthetic Benchmarks}

% The use of synthetic benchmarks for providing empirical performance
% evaluations dates back to as early as 1974~\cite{Curnow1976}. The
% \emph{automatic generation} of such synthetic benchmarks is a more
% recent innovation, serving the purpose initially of stress-testing
% increasingly complex software systems for behaviour validation and
% automatic bug detection~\cite{Verplaetse2000,Godefroid2008}. A range
% of techniques have been developed for these purposes, ranging from
% applying random mutations to a known dataset to generate test stimuli,
% to so-called ``whitebox fuzz testing'' which analyses program traces
% to explore the space of a program's control flow. Csmith is one such
% tool which generates randomised C source programs for the purpose of
% automatically detecting compiler bugs~\cite{Yang2012}.

% A method for the automatic generation of synthetic benchmarks for the
% purpose of \emph{performance} tuning is presented
% in~\cite{Chiu2015}. \citeauthor{Chiu2015} use template substitution
% over a user-defined range of values to generate training programs with
% a statistically controlled range of features. A Perl preprocessor
% generates output source codes from an input description using a custom
% input language Genesis. Genesis is more flexible than the system
% presented in this thesis, supporting substitution of arbitrary
% sources. The authors describe an application of their tool for
% generating OpenCL stencil kernels, but do not report any performance
% results.


% \subsection{Performance Tuning for Heterogeneous Parallelism}

The complex interactions between optimisations and heterogeneous
hardware makes performance tuning for heterogeneous parallelism a
difficult task. Performant GPGPU programs require careful attention
from the developer to properly manage data layout in DRAM, caching,
diverging control flow, and thread communication. The performance of
programs depends heavily on fully utilising zero-overhead thread
scheduling, memory bandwidth, and thread
grouping. \citeauthor{Ryoo2008a} illustrate the importance of these
factors by demonstrating speedups of up to $432\times$ for matrix
multiplication in CUDA by appropriate use of tiling and loop
unrolling~\cite{Ryoo2008a}. The importance of proper exploitation of
local shared memory and synchronisation costs is explored
in~\cite{Lee2010}.

In~\cite{Chen2014}, data locality optimisations are automated using a
description of the hardware and a memory-placement-agnostic
compiler. The authors demonstrate impressive speedups of up to
$2.08\times$, although at the cost of requiring accurate memory
hierarchy descriptor files for all targeted hardware. The descriptor
files must be hand generated, requiring expert knowledge of the
underlying hardware in order to properly exploit memory locality.

Data locality for nested parallel patterns is explored in~\cite{Lee}.
The authors use an automatic mapping strategy for nested parallel
skeletons on GPUs, which uses a custom intermediate representation and
a CUDA code generator, achieving $1.24\times$ speedup over hand
optimised code on 7 of 8 Rodinia benchmarks.

Reduction of the GPGPU optimisation space is demonstrated
in~\cite{Ryoo2008}, using the common subset of optimal configurations
across a set of training examples. This technique reduces the search
space by 98\%, although it does not guarantee that for a new program,
the reduced search space will include the optimal configuration.

\citeauthor{Magni2014} demonstrated that thread coarsening of OpenCL
kernels can lead to speedups in program performance between
$1.11\times$ and $1.33\times$ in~\cite{Magni2014}. The authors achieve
this using a machine learning model to predict optimal thread
coarsening factors based on the static features of kernels, and an
LLVM function pass to perform the required code transformations.

A framework for the automatic generation of OpenCL kernels from
high-level programming concepts is described in~\cite{Steuwer2015}. A
set of rewrite rules is used to transform high-level expressions to
OpenCL code, creating a space of possible implementations. This
approach is ideologically similar to that of PetaBricks, in that
optimisations are made through algorithmic choice, although in this
case the transformations are performed automatically at the compiler
level. The authors report performance on a par with that of hand
written OpenCL kernels.


\subsection{Autotuning Algorithmic Skeletons}

An enumeration of the optimisation space of Intel Thread Building
Blocks in~\cite{Contreras2008} shows that runtime knowledge of the
available parallel hardware can have a significant impact on program
performance. \citeauthor{Collins2012} exploit this
in~\cite{Collins2012}, first using Principle Components Analysis to
reduce the dimensionality of the space of possible optimisation
parameters, followed by a search of parameter values to optimise
program performance by a factor of $1.6\times$ over values chosen by a
human expert. In~\cite{Collins2013}, they extend this using static
feature extraction and nearest neighbour classification to further
prune the search space, achieving an average 89\% of the oracle
performance after evaluating 45 parameters.

\citeauthor{Dastgeer2011} developed a machine learning based autotuner
for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data
is used to predict the optimal execution device (i.e.\ CPU, GPU) for a
given program by predicting execution time and memory copy overhead
based on problem size. The autotuner only supports vector operations,
and there is limited cross-architecture
evaluation. In~\cite{Dastgeer2015a}, the authors extend SkePU to
improve the data consistency and transfer overhead of container types,
reporting up to a $33.4\times$ speedup over the previous
implementation.


\subsection{Code Generation and Autotuning for Stencils}

% Stencil codes have a variety of computationally expensive uses from
% fluid dynamics to quantum mechanics. Efficient, tuned stencil kernels
% are highly sought after, with early work in \citeyear{Bolz2003} by
% \citeauthor{Bolz2003} demonstrating the capability of GPUs for
% massively parallel stencil operations~\cite{Bolz2003}. In the
% resulting years, stencil codes have received much attention from the
% performance tuning research community.

\citeauthor{Ganapathi2009} demonstrated early attempts at autotuning
multicore stencil codes in~\cite{Ganapathi2009}, drawing upon the
successes of statistical machine learning techniques in the compiler
community, as discussed in
Subsection~\ref{subsec:iterative-compilation}. They present an autotuner
which can achieve performance up to 18\% better than that of a human
expert. From a space of 10 million configurations, they evaluate the
performance of a randomly selected 1500 combinations, using Kernel
Canonical Correlation Analysis to build correlations between tunable
parameter values and measured performance targets. Performance targets
are L1 cache misses, TLB misses, cycles per thread, and power
consumption. The use of KCAA restricts the scalability of their system
as the complexity of model building grows exponentially with the
number of features. In their evaluation, the system requires two hours
of compute time to build the KCAA model for only 400 seconds of
benchmark data. They present a compelling argument for the use of
energy efficiency as an optimisation target in addition to runtime,
citing that it was the power wall that lead to the multicore
revolution in the first place. Their choice of only 2 benchmarks and 2
platforms makes the evaluation of their autotuner somewhat limited.

\citeauthor{Berkeley2009} targeted 3D stencils code performance
in~\cite{Berkeley2009}. Stencils are decomposed into core blocks,
sufficiently small to avoid last level cache capacity misses. These
are then further decomposed to thread blocks, designed to exploit
common locality threads may have within a shared cache or local
memory. Thread blocks are divided into register blocks in order to
take advantage of data level parallelism provided by the available
registers. Data allocation is optimised on NUMA systems. The
performance evaluation considers speedups of various optimisations
with and without consideration for host/device transfer overhead.

\citeauthor{Kamil2010} present an autotuning framework
in~\cite{Kamil2010} which accepts as input a Fortran 95 stencil
expression and generates tuned shared-memory parallel implementations
in Fortan, C, or CUDA. The system uses an IR to explore autotuning
transformations, enumerating a subset of the optimisation space and
recording only a single execution time for each configuration,
reporting the fastest. They demonstrate their system on 4
architectures using 3 benchmarks, with speedups of up to $22\times$
compared to serial implementations. The CUDA code generator does not
optimise for the GPU memory hierarchy, using only global memory. As
demonstrated in this thesis, improper utilisation of local memory can
hinder program performance by two orders of magnitude. There is no
directed search or cross-program learning.

In~\cite{Zhang2013a}, \citeauthor{Zhang2013a} present a code generator
and autotuner for 3D Jacobi stencil codes. Using a DSL to express
kernel functions, the code generator performs substitution from one of
two CUDA templates to create programs for execution on GPUs. GPU
programs are parameterised and tuned for block size, block dimensions,
and whether input data is stored in read only texture memory. This
creates an optimisation space of up to 200 configurations. In an
evaluation of 4 benchmarks, the authors report impressive performance
that is comparable with previous implementations of iterative Jacobi
stencils on GPUs~\cite{Holewinski2012, Phillips2010}. The dominating
parameter is shown to be block dimensions, followed by block size,
then read only memory. The DSL presented in the paper is limited to
expressing only Jacobi Stencils applications. Critically, their
autotuner requires a full enumeration of the parameter space for each
program. Since there is no indication of the compute time required to
gather this data, it gives the impression that the system would be
impractical for the needs of general purpose stencil computing. The
autotuner presented in this thesis overcomes this drawback by learning
parameter values which transfer to unseen stencils, without the need
for an expensive tuning phase for each program and architecture.
% TODO: Depending on results of cross-architecture validation, this
% last claim may not hold up.
%
% The majority of applications tested are memory bound. Does this
% transfer to computer bound?

In~\cite{Christen2011}, \citeauthor{Christen2011} presentf a DSL for
expressing stencil codes, a C code generator, and an autotuner for
exploring the optimisation space of blocking and vectorisation
strategies. The DSL supports stencil operations on arbitrarily
high-dimensional grids. The autotuner performs either an exhaustive,
multi-run Powell search, Nelder Mead, or evolutionary search to find
optimal parameter values. They evaluate their system on two CPUS and
one GPU using 6 benchmarks. A comparison of tuning results between
different GPU architectures would have been welcome, as the results
presented in this thesis show that devices have different responses to
optimisation parameters. The authors do not present a ratio of the
available performance that their system achieves, or how the
performance of optimisations vary across benchmarks or devices.

A stencil grid can be decomposed into smaller subsections so that
multiple GPUs can operate on each subsection independently. This
requires a small overlapping region where each subsection meets ---
the halo region --- to be shared between devices. For iterative
stencils, values in the halo region must be synchronised between
devices after each iteration, leading to costly communication
overheads. One possible optimisation is to deliberately increase the
size of the halo region, allowing each device to compute updated
values for the halo region, instead of requiring a synchronisation of
shared state. This reduces the communication costs between GPUs, at
the expense of introducing redundant computation. Tuning the size of
this halo region is the goal of PARTANS~\cite{Lutz2013}, an autotuning
framework for multi-GPU stencil computations. \citeauthor{Lutz2013}
explore the effect of varying the size of the halo regions using six
benchmark applications, finding that the optimal halo size depends on
the size of the grid, the number of partitions, and the connection
mechanism (i.e.\ PCI express). The authors present an autotuner which
determines problem decomposition and swapping strategy offline, and
performs an online search for the optimal halo size. The selection of
overlapping halo region size compliments the selection of workgroup
size which is the subject of this thesis. However, PARTANS uses a
custom DSL rather than the generic interface provided by SkelCL, and
PARTANS does not learn the results of tuning across programs, or
across multiple runs of the same program.


% \subsection{Summary}

% There is already a wealth of research literature on the topic
% autotuning which begs the question, why isn't the majority of software
% autotuned? In this section I attempted to answer the question by
% reviewing the state of the art in the autotuning literature, with
% specific reference to auotuning for GPUs and stencil codes. The bulk
% of this research falls prey of one of two shortcomings. Either they
% identify and develop a methodology for tuning a particular
% optimisation space but then fail to develop a system which can
% properly exploit this (for example, by using machine learning to
% predict optimal values across programs), or they present an autotuner
% which targets too specific of a class of optimisations to be widely
% applicable. This project attempts to address both of those
% shortcomings by expending great effort to deliver a working
% implementation which users can download and use without any setup
% costs, and by providing a modular and extensible framework which
% allows rapid targeting of new autotuning platforms, enabled by a
% shared autotuning logic and distributed training data. The following
% section outlines the design of this system.


\section{Conclusions}\label{sec:conclusions}

% As the trend towards higher core counts and increasing parallelism
% continues, the need for high level, accessible abstractions to manage
% such parallelism will continue to go. Autotuning proves a valuable aid
% for achieving these goals, providing the benefits of low level
% performance tuning while maintaining ease of use, without burdening
% developers with optimisation concerns. As the need for autotuned
% parallelism rises, the desire for collaborative techniques for sharing
% performance data must be met with systems capable of supporting this
% cross-platform learning.

By comparing the relative performance of an average of 629 workgroup
sizes for each of 429 scenarios, the following conclusions can be
drawn:
%
\begin{itemize}
\item The performance of a workgroup size for a particular scenario
  depends properties of the hardware, software, and dataset.
\item The performance gap between the best and workgroup sizes for a
  particular combination of hardware, software, and dataset is up to
  $207.72\times$.
\item Not all workgroup sizes are legal, and the space of legal
  workgroup sizes cannot statically be determined. Adaptive tuning of
  workgroup size is required to ensure reliable performance.
\item Differing scenarios have wildly different optimal workgroup
  sizes, and the best performance can be achieved using static tuning
  is optimal for only 15\% of scenarios.
\end{itemize}
%
From an evaluation of 17 different autotuning techniques using 5
different types of validation sets, the following conclusions about
autotuning performance can be drawn:
%
\begin{itemize}
\item In the case of classifiers predicting illegal workgroup sizes,
  the best fallback strategy is to select the closest legal workgroup
  size.
\item The performance of predicted workgroup sizes for unseen devices
  is within 8\% of the performance for known devices.
\item Predicting the \emph{runtime} of stencils is the least effective
  of the evaluated autotuning techniques, achieving an average of only
  68\% of the available performance.
\item Predicting the \emph{speedup} of workgroup sizes provides the
  highest median speedup, but more frequently predicts a poorly
  performing workgroup size then the classifiers.
\item Classification using regression costs an order of magnitude more
  time than using classifiers. The J48 classifier has the lowest
  overhead.
\end{itemize}

% In this thesis, I have presented my attempt at providing such a
% system, by designing a novel framework which has the benefits of fast,
% ``always-on'' autotuning, while being able to synchronise data with
% global repositories of knowledge which others may contribute to. The
% framework provides an interface for autotuning which is sufficiently
% generic to be easily re-purposed to target a range of optimisation
% parameters.

% To demonstrate the utility of this framework, I implemented a frontend
% for predicting the workgroup size of OpenCL kernels for SkelCL stencil
% codes. This optimisation space is complex, non linear, and critical
% for the performance of stencil kernels, with up to a $207.72\times$
% slowdown if an improper value is picked. Selecting the correct
% workgroup size is difficult --- requiring a knowledge of the kernel,
% dataset, and underlying architecture. The challenge is increased even
% more so by inconsistencies in the underlying system which cause some
% workgroup sizes to fail completely. Of the 269813 combinations of
% workgroup size, device, program, and dataset tested; only a
% \emph{single} workgroup size was valid for all test cases, and
% achieved only 24\% of the available performance. The value selected by
% human experts was invalid for 2.6\% of test cases. Autotuning in this
% space requires a system which is resilient these challenges, and
% several techniques were implemented to address them.

% Runtime performance of autotuned stencil kernels is very promising,
% achieving an average 90\% of the available performance with only a 3ms
% autotuning overhead. Even ignoring the cases for which the human
% expert selected workgroup size is invalid, this provides a
% $1.33\times$ speedup, or a $5.57\times$ speedup over the best
% performance that can be achieved using static tuning. Classification
% performance is comparable when predicting workgroup sizes for both
% unseen programs and unseen devices. I believe that the combination of
% performance improvements and the collaborative nature of OmniTune
% makes for a compelling case for the use of autotuning as a key
% component for enabling performant, high level parallel programming.


% \subsection{Critical Analysis}

% This subsection contains a critical analysis of the work presented in
% previous sections.

% \paragraph{OmniTune Framework}

% The purpose of the OmniTune framework is to provide a generic
% interface for runtime autotuning. This is demonstrated through the
% implementation of a SkelCL frontend; however, to truly evaluate the
% ease of use of this framework, it would have been preferable to
% implement one or more additional autotuning frontends, to target
% different optimisation spaces. This could expose any leakages in the
% abstractions between the SkelCL-specific and generic autotuning
% components.


% \paragraph{Synthetic Benchmarks}

% The OmniTune SkelCL frontend provides a template substitution engine
% for generating synthetic stencil benchmarks. The implementation of
% this generator is rigidly tied to the SkelCL stencil format. It would
% be preferred if this template engine was made more flexible, to
% support generation of arbitrary test programs. Additionally, due to
% time constraints, I did not have the opportunity to explore how the
% number of synthetic benchmarks in machine learning test data sets
% affects classification performance.

% One possible use of the synthetic stencil benchmark generator could be
% for creating minimal test cases of refused OpenCL parameters so that
% bug reports could be filed with the relevant implementation
% vendor. However, this would have added a great level of complexity to
% the the generator, as it would have to isolate and remove the
% dependency on SkelCL to generate minimal programs, requiring
% significant implementation work.


% \paragraph{Use of Machine Learning}

% The evaluation of OmniTune in this thesis uses multiple classifiers
% and regressors to predict workgroup sizes. The behaviour of these
% classifiers and regressors is provided by the Weka data mining
% suite. Many of these classifiers have parameters which affect their
% prediction behaviour. The quality of the evaluation could have been
% improved by exploring the effects that changing the values of these
% parameters has on the OmniTune classification performance. It would
% also have been informative to dedicate a portion of the evaluation to
% feature engineering, evaluating the information gain of each feature
% and exploring the effects of feature transformations on classification
% performance.


% \paragraph{Evaluation Methodology}

% The evaluation compares autotuning performance against the best
% possible performance that can be achieved using static tuning, a
% simple heuristic to tune workgroup size on a per-device basis, and
% against the workgroup size chosen by human experts. It would have been
% beneficial to also include a comparison of the performance of these
% autotuned stencils against hand-crafted equivalent programs in pure
% OpenCL, without using the SkelCL framework. This would allow a direct
% comparison between the performance of stencil kernels using high level
% and low level abstractions, but could not be completed due to time
% constraints and difficulties in acquiring suitable comparison
% benchmarks and datasets.


% \subsection{Future Work}

% Future work can be divided into two categories: continued development
% of OmniTune, and extending the behaviour of the SkelCL autotuner.

% The cost of offline training with OmniTune could be reduced by
% exploring the use of adaptive sampling plans, such as presented
% in~\cite{Leather2009}. This could reduce the number of runtime samples
% required to distinguish good from bad optimisation parameter values.

% Algorithm~\ref{alg:autotune-hybrid} proposes the behaviour of a hybrid
% approach to selecting the workgroup size of iterative SkelCL
% stencils. This approach attempts to exploit the advantages of all of
% the techniques presented in this thesis. First, runtime regression is
% used to predict the minimum runtime and a candidate workgroup
% size. If, after evaluating this workgroup size, the predicted runtime
% turned out to be inaccurate, then a prediction is made using speedup
% regression. Such a hybrid approach would enable online tuning through
% the continued acquisition of runtime and speedup performance, which
% would compliment the collaborative aspirations of OmniTune, and the
% existing server-remote infrastructure.

% Other skeleton optimisation parameters could be autotuned by SkelCL,
% including higher level optimisations such as the selection of border
% region loading strategy, or selecting the optimal execution device(s)
% for multi-device systems. Optimisation parameters of additional
% skeletons could be autotuned, or the interaction of multiple related
% optimisation parameters could be explored. Power consumption could be
% used as an additional optimisation cotarget.

% \begin{algorithm}
% \begin{algorithmic}[1]
% \Require kernel features $k$, hardware features $h$, dataset features $d$.
% \Ensure workgroup size $w$

% \State $r \leftarrow \underset{w \in W_{legal}(s)}{\min} f(k,h,d,w)$
% \Comment Predict minimum runtime.
% \State $w \leftarrow \underset{w \in W_{legal}(s)}{\argmin} f(k,h,d,w)$
% \Comment Workgroup size for $r$.
% \State $t_r \leftarrow$ measure runtime of program with $w$
% \State \textsc{Submit}$\left( f(s), w, t_r \right)$
% \Comment Submit observed runtime
% \If{$t_r \approx r$}
%   \State \textbf{return} $w$
% \Comment Predicted runtime is accurate.
% \Else
%    \State $W \leftarrow \left\{ w | w < W_{\max}(s) \right\}$
%    \State converged $\leftarrow$ false
%    \State $w_b \leftarrow$ baseline parameter
%    \State $t_b \leftarrow$ measure runtime of runtime of program with
%    $w_b$
%    \State \textsc{Submit}$\left( f(s), w_b, t_b \right)$
%    \Comment Submit observed runtime
%    \While{not converged}
%      \State $s \leftarrow \underset{w \in W}{\max} g(k,h,d,w)$
%      \Comment Predict best speedup.
%      \State $w \leftarrow \underset{w \in W}{\argmax} g(k,h,d,w)$
%      \Comment Workgroup size for $s$.
%      \State $t \leftarrow$ measure runtime of program with $s$
%      \State \textsc{Submit}$\left( f(s), w, t \right)$
%      \Comment Submit observed runtime
%      \State $s_w \leftarrow t_t / t$
%      \State \textsc{Submit}$\left( f(s), w, s_w \right)$
%      \Comment Submit observed speedup
%      \If{$s_w \approx s$}
%        \State converged = true
%        \Comment Predicted speedup is accurate.
%      \Else
%        \State $W = W - \{ w \}$
%      \EndIf
%    \EndWhile
%    \State \textbf{return} $w$
% \EndIf
% \end{algorithmic}
% \caption{%
%   Selecting workgroup size using a combination of classifiers and
%   regressors.%
% }
% \label{alg:autotune-hybrid}
% \end{algorithm}



%
% \appendix
% \section{Appendix Title}
%
% This is the text of the appendix, if you need one.
%

\acks

% We thank Michel Steuwer XXXX

This work was supported by grant EP/L01503X/1 for the University of
Edinburgh School of Informatics Centre for Doctoral Training in
Pervasive Parallelism (\url{http://pervasiveparallelism.inf.ed.ac.uk/}) from
the UK Engineering and Physical Sciences Research Council (EPSRC).

% We recommend abbrvnat bibliography style.

\label{bibliography}
\printbibliography


\end{document}
