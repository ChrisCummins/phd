\chapter{Conclusions}
\label{chap:conclusions}

This thesis new techniques for the generation and optimisation of programs using machine learning, thereby lowering the cost to construct compilers. In particular, Chapter~\ref{chap:clgen} presented a new methodology for generating executable benchmarks to address the problem of benchmark scarcity. Then Chapter~\ref{chap:deepsmith} extended this technique to compiler fuzzing, thereby addressing the compiler validation problem. Finally, Chapter~\ref{chap:deeptune}

Section~\ref{sec:conclusions-contributions} summarises the main contributions of this thesis, Section~\ref{sec:conclusions-critical-analysis} presents a critical analysis of this work, and future work is described in Section~\ref{sec:conclusions-future-work}.


\section{Contributions}
\label{sec:conclusions-contributions}

The problems addressed in this thesis are well established. This section summarises the main contributions of this thesis with respect to these problems.


\subsection{A Solution for Benchmark Scarcity}

There is a shortage of benchmarks, forcing compiler developers to work with a sparse sampling of the program space. Chapter~\ref{chap:clgen} develops a novel generator for compiler benchmarks, capable of generating an unbounded number of benchmarks. The usefulness of the generated benchmarks is evaluated on a state-of-the-art learned optimisation heuristic, finding that the additional exploration of the program space provided by the generated benchmarks improves performance by $1.27\times$.

Compared to previous works~\cite{Chiu2015}, the generation of benchmarks is entirely automatic, requiring no expert tuning or direction. Only a corpus of example programs is needed to guide the distributions of generated programs. Despite no a priori knowledge of the language, the generator is capable of producing executable benchmarks of such quality that professional software developers cannot distinguish code generated by it from handwritten code.

The approach, in generating an unbounded number of runnable programs, enables a finer grained exploration of the compiler optimisation feature space than was previously possible, without the development costs previously associated with benchmark generation. This simplifies the construction of compilers by enabling performance models to be learned from automatically generated data.

% My hope for this work is to demonstrate a proof of concept for an exciting new avenue of program generation, and that the full release of CLgen will expedite discovery in other domains. An OpenCL fuzzer is being used in industry (Codeplay). That I am aware of, it is being actively ported to Java, golang, and simulink programming languages.


\subsection{A Low Cost and Effective Compiler Fuzzer}

Chapter~\ref{sec:deepsmith} extends the technique of Chapter~\ref{sec:clgen} to the domain of compiler test case generation. Like in the domain of benchmarking, the state-of-the-art in compiler test case generation is an enormous engineering undertaking~\cite{Yang2011,Lidbury2015a}. The technique presented in the chapter presents an enormous reduction in developer effort compared to the state-of-the-art grammar-based approach. The technique presented in this thesis can be implemented in as few as 500 lines of code. The state-of-the-art approach is over 50,000 lines of code~\cite{Yang2011}. This $100\times$ reduction in code size is complemented by improved portability of the implementation, with only parts of the stack being specific to the input language of the compiler being tested. The remainder being language agnostic.

The portability of the approach is demonstrated by extending the generator from OpenCL to Solidity in only 12 lines of code. By contrast, extending a state-of-the-art generator from C to OpenCL required over 8000 lines of code~\cite{Lidbury2015a}.

Despite its simplicity, the proposed technique is effective. To date, 67 new bugs have been identified and reported in OpenCL compilers. Many of the bugs identified could not be exposed by state-of-the-art approaches due to the limitations in the expressiveness of grammar-based approaches. The expressiveness of the generated test cases is limited only by the code that has been uploaded to GitHub; this led to unintentional outcomes such as exploiting compiler-specific features to expose a bug in compilers' error handling for intrinsics.

\todo[inline]{TODO: how does it relate to the message of the thesis.}


\subsection{Simplifying the Construction of Compiler Optimisations}

Constructing features for machine learning is time consuming and error prone. Additionally, in machine learning for compilers, the choice of features typically couples the learning system tightly with the implementation of the compiler. Chapter~\ref{chap:deeptune} proposes a technique to address both issues. Instead of a numerical feature representation of programs, the source code of the entire program is fed directly into the learning system, greatly simplifying the approach.

The technique is evaluated for two distinct optimisation problems, finding that in both cases, the approach is able to match or outperform the state-of-the-art approach using hand-crafted features, achieving speedups of $1.14\times$ and $1.05\times$. This is spite of using the same approach for both problems, without any specialising the structure of the learning system to the task being learned. In abstracting the structure of the solution from the problem, the approach enables the novel transfer of information learned for one task to the other. By enabling transfer learning, the performance of a predictive model improves by a further $1.06\times$, despite only being provided with information learned for a different optimisation task.

In bypassing the need to engineer features, the proposed technique simplifies the construction of optimisation heuristics through machine learning, while leading to higher performance in the heuristics themselves. Since compilers typically contain hundreds or even thousands of distinct optimisation heuristics, techniques like the one proposed that enable the sharing of information between tasks are prudent to the widespread adoption of machine learning in optimising compilers.


\section{Critical Analysis}
\label{sec:conclusions-critical-analysis}

This section contains a critical analysis of the work.


\subsection{Generative Models for Source Code}

Chapters~\ref{chap:clgen} and~\ref{chap:deepsmith} present generative models that enable the synthesis of more human-like programs than current state-of-the-art program generators, and without the expert guidance required by template based generators, but they have limitations. The technique of seeding the language models with the start of a function means that it cannot support user defined types, or calls to user-defined functions. In turn, this restricts the inputs that can be fed to generated programs. Currently only  scalars and arrays may be used as inputs, whereas 6 (2.3\%) of the OpenCL benchmark kernels identified in Table~\ref{tab:benchmarks} use irregular data types as inputs. This may be addressed through recursive program synthesis, whereby a call to a user-defined function or unrecognised type will trigger candidate functions and type definitions to be synthesised.

This work evaluates the use of recurrent neural networks for generating programs in the OpenCL and Solidity programming languages. Although quite dissimilar (once extends the C programming language, the other is derived from JavaScript), it is unclear whether the generative modelling approach will prove effective for all possible language syntaxes. Unlike a random grammar enumerator, the ability to generate programs of arbitrary syntaxes cannot be guaranteed.

By learning from a corpus of programs assembled from GitHub, the model induces the biases of programs on GitHub. The contents of the GitHub corpus used in this work were only lightly vetted to ensure that it did not contain programs that would later be used to evaluate the model. This did not preclude the model training on program that may not be considered \emph{representative} of true handwritten code. For example, inspecting the corpus revealed a small number of large, automatically generated programs. Additionally, test cases for an OpenCL static analysis tool were found that deliberately contain runtime defects. While the corpus was filtered to ensure that training programs were syntactically valid, no checks were made to ensure that programs had correct semantics.


\subsection{Rejection Sampling for Program Generation}

The techniques presented in this work sample recurrent neural networks on a per-token basis to generate programs. Once an entire sample has been generated, the sample can be checked to see if it is a valid program. If not, the entire sample is discarded. Although automatic, this \emph{rejection sampling} approach is wasteful. Grammar-based sampling approaches have been proposed that could increase the likelihood of generating a valid program through masked sampling~\cite{Dyer2016}. Of course, this would make the generator more complicated. Ultimately there is a trade off between implementation complexity and sampling efficiency. This work emphasises simplicity.

Moreover, rejection sampling results in a bias towards shorter programs. This is because the probability that a sample is a valid program is inversely proportional to its length. This may skew the distribution of generated programs away from the training programs. This issue, arising from rejection sampling, can coincidentally be alleviated through further rejection sampling. To correct the bias towards shorter programs, an additional filter could be placed on the output of the generative model that discards samples with a random probability inversely proportional to their length. By removing more short samples than long, the bias in the distribution can be corrected for.


\subsection{Characterisation of OpenCL Compiler Bugs}

In Chapter~\ref{chap:deepsmith}, \emph{DeepSmith} is proposed for generating compiler test cases, and compared against the state-of-the-art \emph{CLSmith}. For each approach, the number of bug-exposing test cases are reported. However, it is not possible to determine which generator identified more \emph{unique} bugs. To determine this, one would need to de-duplicate the counts by locating the exact bug-exposing property of each test case and correlating it with a compiler defect. There are two challenges preventing this: the first is the amount of compute required to perform automated test case reduction in many thousands of CLSmith programs; the second is the that in many cases it is not possible to identify the root cause of a compiler bug without access to its source code.

While it is not possible to compare the rate at which DeepSmith and CLSmith identify unique bugs, the properties of each approach can be used to partially characterise the bugs that can be found. DeepSmith is capable of exposing bugs that CLSmith cannot; for example, by generating plausible but malformed inputs to expose bugs in compiler error handling, or by generating programs with thread-dependent control-flow which CLSmith's static analyses prevent. However, CLSmith is also capable of exposing bugs that DeepSmith cannot. For example, CLSmith programs makes heavy use of structs, whereas DeepSmith does not support structs. As such I believe the approach presented in this work to complement the existing state-of-the-art; it is not intended to replace it.


\subsection{Driving arbitrary OpenCL kernels}

This thesis presents a technique for driving any OpenCL kernel that has scalar and array inputs. This host driver accepts as input an OpenCL kernel, which it then compiles, produces input data sets, and runs the compiled kernel using the data sets. The host driver generates data sets from uniform random distributions, as do many OpenCL benchmark suites. For cases where non-uniform inputs are required (e.g. profile-directed feedback), an alternate methodology for generating inputs must be adopted.


% \subsection{Augmenting Benchmark Training Data}

% \todo[inline]{Usefulness of benchmarks is evaluated only when used as auxiliary training data, not as the sole training set.}

% for benchmarking, in cases where you know the domain, grammar-based approaches have their uses. The approach presented in this thesis is guided by the corpus, and any deficiencies in the corpus will show up in the output.


\subsection{Modelling Program Semantics}

Chapter~\ref{chap:deeptune} feeds a sequence of program tokens into a recurrent neural network to predict an optimisation decision that should be made on it. By using the text serialised representation of a program (its source code), this makes the approach vulnerable to code order. The text inputs used to evaluate the approach are single kernels. It is not clear how the approach will respond to multi-procedure inputs, where the order that procedures are declared may have a large impact on the pattern of activations that it produces in the recurrent neural network.

% Using text as the representation, like statistical feature extractors, is vulnerable to dead code insertion.

% Similarly, the sequential representation is vulnerable to dead code

The higher-dimensional representation of source code text over numerical features means that larger data sets are required to combat sparsity in training data. For example, the evaluation in Section~\ref{sec:deeptune-eval} uses sequences padded to 1024 tokens, providing a 1024-dimension input representation. This provides a much more sparse space representation than the 4-dimension feature space used by the state-of-the-art approach.

A common criticism of machine learning systems is that they are \emph{black boxes}. When the system fails to produce the desired result, there is no obvious method to correct the system so as to prevent similar errors. Still, in traditional machine learning it may be possible to correct problems by adjusting the features. Deep learning, with its typically low-level input representation, does not always have this option. For the approach presented in this thesis, there is no meaningful way to improve the model based on analysis of failure cases.


\section{Future Work}
\label{sec:future-work}

This section briefly outlines some of the avenues for future research that this thesis creates.

\subsection{Guided Program Synthesis to Minimise Benchmarking Costs}

This thesis presents a technique for the \emph{unguided} synthesis of compiler benchmarks. Using the technique may provide a fine grained exploration of the space of representative programs. For some use cases, a more efficient use of data can be achieved through \emph{directed} program generation.

One approach could be to employ a rejection filter that tests for the presence of a property of interest. Another approach would be to train the generative model simultaneously for both the structure of programs (as is done in this work), along with a representation of the properties of interest (such as a feature vector). At sample time, the target feature vector could be used as input to steer the program generation. A third approach would use the learned language model not to generate programs, but to guide an existing program generator, such as CSmith.

If successful, such a technique enable the exploration of larger feature spaces than is currently possible by efficiently navigating the generation of benchmarks to map out the decision surface.


\subsection{Neural Model Selection through Adversarial Games}

Section~\ref{sec:clgen-qualitative-evaluation} uses \emph{Turing tests} to evaluate the quality of synthetic code. The task presented to human participants was to identify whether a series of code snippets were written by hand or machine. This was used to evaluate whether or not the model produced human-like output. In future work, this approach could be extended to aid in the challenging task of model selection by instead presenting the participants with pairs of samples side by side, and asking the participant to select the sample which is \emph{more} human-like. If the two samples were both generated by different configurations of generative model, this would provide a means to compare generative models on the otherwise hard-to-assess quality of ``humanness''. The selection of the best model from a pool of candidates can thus be turned into a series of games. Each game pits a single sample from a pair of generative models head-to-head with a human selecting the winner, and an ELO rating can be used to assign scores and matches. The limitation of the work would likely be the availability of human participants.


\subsection{Learning Representations for Dynamic Program Inputs}

Chapter~\ref{chap:deeptune} presents an approach for learning optimisation heuristics from the raw representation of a program, but in the presence of dynamic properties, traditional feature extraction must be used. For example, feeding in the size of input data sets. In future work, this approach could be extended to also account for dynamic properties. Unlike with program source code, it is not clear what the raw representation of program inputs may be, perhaps the sequence of bytes that the program reads and writes, but the value of being able to model runtime properties at a low level would enable optimisations to be learned that cannot currently\todo{WTF do you mean here Chris}.


\subsection{Towards General-Purpose Program Comprehension}

The techniques presented in this thesis apply recurrent neural networks to the task of modelling the syntax and semantics of programming languages. For each task, such as program generation or optimisation, an artificial neural networks is trained from scratch. Chapter~\ref{chap:deeptune} explores the use of transfer learning to seed an artificial neural network with information from another task. Future work could explore this idea further by iteratively training and retraining a single network across a wide range of tasks, with the goal of finding a common set of model parameters which work can be used as a base for each task.

An ambitious goal would be the development and distribution of a model architecture for \emph{general-purpose} program comprehension. Such a system would enable, with little to no effort, the model to be re-purposed for a variety of compiler tasks. This is analogous to the widespread distribution of pre-trained state-of-the-art models in the field of image recognition. If developing an image classifier, a user can start by retraining an existing model such as ResNet~\cite{He2016}, rather than constructing a model from scratch. This drastically simplifies the adoption of machine learning for image classification as the model architecture has been pre-selected and tuned, and reduces the amount of training data required.

A prerequisite for developing this system will be applying techniques such as those proposed in this thesis to a wide range of different compiler tasks. My hope in publicly releasing of all software developed in this work is to expedite discover in other domains.

% \todo[inline]{Unify the program generation and optimisation to make a closed system.}

% \todo[inline]{Graph-level representations with RNNS~\cite{Jin2018}. Graph surveys~\cite{Li2018a,Wu2018}.}

% \todo[inline]{Table of GitHub corpus size by programming language. There's room for machine learning in lots of languages! E.g. Haskell, Java, C/C++, Solidity, Python, OpenCL}
