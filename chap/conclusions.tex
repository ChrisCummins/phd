\chapter{Conclusions}
\label{chap:conclusions}

\diff{%
This thesis presents new techniques for the generation and optimisation of programs using machine learning, to address the challenges facing machine learning for compilers outlined in Section~\ref{sec:intro-challenges}. Chapter~\ref{chap:clgen} addresses the data scarcity issue (Section~\ref{subsec:challenge-scarcity}) by developing a methodology for the unguided generation of realistic benchmarks. Chapter~\ref{chap:deepsmith} addresses the adoption challenge (Section~\ref{subsec:challenge-adoption}) by extending the generative technique to the domain of compiler validation, developing an effective compiler fuzzer that is significantly simpler than state-of-the-art approaches. Finally, Chapter~\ref{chap:deeptune} explores techniques that circumvent the challenge of feature design (Section~\ref{subsec:challenge-features}) by removing the need to manually construct features for programs.%
}

The remainder of this chapter is structured as follows: Section~\ref{sec:conclusions-contributions} summarises the main contributions of this thesis, Section~\ref{sec:conclusions-critical-analysis} presents a critical analysis of this work, and  Section~\ref{sec:conclusions-future-work} describes future research directions.


\section{Contributions}
\label{sec:conclusions-contributions}

\diff{%
The challenges addressed in this thesis prevent the adoption of machine learning in compilers. This section summarises the main contributions of this thesis which will help to establish machine learning as a valuable tool for compiler construction.%
}


\subsection{A Solution for Benchmark Scarcity}

There is a shortage of benchmarks, forcing compiler developers to work with sparse samplings of the program space. This data scarcity challenge limits the quality of learned models. Chapter~\ref{chap:clgen} develops a novel generator for compiler benchmarks, capable of generating an unbounded number of training programs. The usefulness of the generated benchmarks is evaluated on a state-of-the-art learned optimisation heuristic, finding that the additional exploration of the program space provided by the generated benchmarks improves performance by $1.27\times$.

This is the first use of machine learning over handwritten code to generate benchmarks. Compared to previous works~\cite{Chiu2015}, this approach is entirely automatic, requiring no expert tuning or direction. Only a corpus of example programs is needed to guide the distributions of generated programs. Despite no a priori knowledge of the language, the generator is capable of producing executable benchmarks of such quality that professional software developers cannot distinguish code generated by it from handwritten code.

The approach, in generating an unbounded number of runnable programs, enables a finer grained exploration of the compiler optimisation feature space than was previously possible, without the development costs previously associated with benchmark generation. This simplifies the construction of compilers by enabling performance models to be learned from automatically generated data. The technique may also prove valuable to compiler feature designers, as the granular exploration of the feature space may expose deficiencies in the choice of features.

% My hope for this work is to demonstrate a proof of concept for an exciting new avenue of program generation, and that the full release of CLgen will expedite discovery in other domains. An OpenCL fuzzer is being used in industry (Codeplay). That I am aware of, it is being actively ported to Java, golang, and simulink programming languages.


\subsection{Low-cost and Effective Compiler Fuzzing}

\diff{For machine learning techniques to be widely adopted in compilers, they must be present significant advantages of existing formal methods, without sacrificing correctness.} Chapter~\ref{chap:deepsmith} extends the application of recurrent neural networks to the domain of compiler testing. The state-of-the-art in compiler test case generation is a significant undertaking, comprising over 50,000 lines of handwritten code~\cite{Yang2011,Lidbury2015a}. The technique presented in this thesis presents an enormous reduction in developer effort compared to the state-of-the-art grammar-based approach. It is implemented in as few as 500 lines of code. This $100\times$ reduction in code size is complemented by improved portability of the implementation, with only parts of the stack being specific to the input language of the compiler being tested. The remainder being language-agnostic.

The portability of the approach is demonstrated by extending the generator from OpenCL to Solidity in only 12 lines of code. By contrast, extending a state-of-the-art generator from C to OpenCL required over 8000 lines of code~\cite{Lidbury2015a}.

Despite its simplicity, the proposed technique is effective.  To date, 67 new bugs have been identified and reported in OpenCL compilers. Many of the bugs identified could not be exposed by state-of-the-art approaches due to the limitations in the expressiveness of grammar-based approaches. The expressiveness of the generated test cases is limited only by the code that has been uploaded to GitHub; this led to unintentional outcomes such as exploiting compiler-specific features to expose bugs in the error handling of compilers' intrinsics.


\subsection{Automatic Compiler Optimisation Tuning}

\diff{A significant challenge facing machine learning techniques is the design of features.} Constructing program features is time-consuming and error-prone. Additionally, the choice of features typically couples the learning system tightly with the compiler implementation. This means that new features must be computed and the model retrained with every change to the compiler. Chapter~\ref{chap:deeptune} proposes a technique to address both issues. Instead of extracting numerical representations of programs, the source code of the entire program is fed directly into the learning system. This is a simpler approach that decouples the learning system from the compiler's internal representation.

The technique is evaluated for two distinct optimisation problems, finding that in both cases, the approach is able to match or outperform the state-of-the-art approach using hand-crafted features, achieving speedups of $1.14\times$ and $1.05\times$. This is in spite of using the same model parameters for both problems, without any specialising of the structure of the learning system to the task being learned. In abstracting the structure of the solution from the problem, the approach enables the novel transfer of information learned for one task to the other. By enabling transfer learning, the performance of a predictive model improves by a further $1.06\times$, despite only being provided with information learned for a different optimisation task.

In bypassing the need to engineer features, the proposed technique simplifies the construction of optimisation heuristics through machine learning, while leading to higher performance in the heuristics themselves. Since compilers typically contain hundreds or even thousands of distinct optimisation heuristics, techniques that enable the sharing of information between tasks, like the one proposed in this work, are prudent to the practical development of machine learning in optimising compilers.

\diff{%
Together, these three contributions indicate the exciting potential for new lines of research in program generation and optimisation, enabled by breakthrough results in the effectiveness of deep learning for modelling programs. The creates the potential for dramatically simplifying compiler construction, while improving the quality of compilers.%
}

\section{Critical Analysis}
\label{sec:conclusions-critical-analysis}

This section contains a critical analysis of the techniques presented in this work.


\subsection{Generative Models for Source Code}

Chapters~\ref{chap:clgen} and~\ref{chap:deepsmith} develop generative models that enable the synthesis of more human-like programs than current state-of-the-art program generators, and without the expert guidance required by template-based generators, but they have limitations. The technique of seeding the language models with the start of a function means that user-defined types or calls to user-defined functions are not supported. In turn, this restricts the inputs that can be fed to generated programs. Currently, only scalars and arrays may be used as inputs, whereas 6 (2.3\%) of the OpenCL benchmark kernels identified in Table~\ref{tab:cgo17-benchmarks} use irregular data types as inputs. This may be addressed through recursive program synthesis, whereby a call to a user-defined function or unrecognised type will trigger candidate functions and type definitions to be synthesised.

This work evaluates the use of recurrent neural networks for generating programs in the OpenCL and Solidity programming languages. Although the languages are dissimilar (one extends the C programming language, the other is derived from JavaScript), it is unclear whether the generative modelling approach will prove effective for all possible grammars. Unlike approaches which generate programs by enumerating randomly from a specification of the programming language grammar, the ability to generate programs of arbitrary syntaxes cannot be guaranteed.

By learning from a corpus of programs assembled from GitHub, the model induces the biases of programs on GitHub. This makes the implicit assumption that code uploaded to GitHub is representative of the real-world usage of a programming language. The contents of the GitHub corpus used in this work were only lightly vetted to ensure that it did not contain programs that would later be used to evaluate the model. This did not preclude the model training on programs that may not be considered \emph{representative} of true handwritten code. For example, inspecting the corpus revealed a small number of large, automatically generated programs which may bias the generator. Additionally, test cases for an OpenCL static analysis tool were found that deliberately contain runtime defects. While the corpus was filtered to ensure that training programs were syntactically valid, no checks were made to ensure that programs used for training had correct semantics.


\subsection{Rejection Sampling for Program Generation}

The techniques presented in this work sample recurrent neural networks on a per-token basis to generate programs. Once an entire sample has been generated, the sample is checked to see if it is a valid program. If not, the entire sample is discarded. Although automatic, this \emph{rejection sampling} approach is wasteful. Grammar-based sampling approaches have been proposed that could increase the likelihood of generating a valid program through masked sampling~\cite{Dyer2016}. Of course, this would make the generator more complicated. Ultimately there is a trade-off between implementation complexity and sampling efficiency. This work emphasises simplicity.

Moreover, rejection sampling results in a bias towards shorter programs. This is because, on average, the probability that a sample is a valid program decreases with each additional token. This skews the distribution of generated programs away from the training programs. This issue, arising from rejection sampling, can coincidentally be alleviated through further rejection sampling. To correct the bias towards shorter programs, an additional filter could be placed on the output of the generative model that discards samples with a random probability inversely proportional to their length. By removing more short samples than long, the bias in the distribution is corrected, albeit at the cost of fewer accepted samples.


\subsection{Characterisation of OpenCL Compiler Bugs}

Chapter~\ref{chap:deepsmith} presents \emph{DeepSmith}, a tool for generating compiler test cases, and compares it against the state-of-the-art \emph{CLSmith}. For each approach, the number of bug-exposing test cases is reported. However, it is not possible to determine which generator identified more \emph{unique} bugs. To determine this, one would need to de-duplicate the counts by locating the exact bug-exposing property of each test case and correlating it with a compiler defect. There are two challenges preventing this: the first is the amount of compute required to perform automated test case reduction in many thousands of CLSmith programs; the second is the that in the general case it is not possible to identify the root cause of a compiler bug without access to its source code.

While it is not possible to compare the rate at which DeepSmith and CLSmith identify unique bugs, the properties of each approach allow partial characterisation of the bugs that can be found. DeepSmith is capable of exposing bugs that CLSmith cannot; for example, by generating plausible but malformed inputs to expose bugs in compiler error handling, or by generating programs with thread-dependent control-flow which CLSmith's static analyses prevent.

Where de-duplication of underlying bugs is possible (such as by comparing compiler stack traces), DeepSmith matches or exceeds CLSmith's findings; however, CLSmith is also capable of exposing bugs that DeepSmith cannot. For example, CLSmith programs make heavy use of structs, which DeepSmith does not support. As such I believe the approach presented in this work to be complementary to the prior art. The functionality of DeepSmith and CLSmith overlap, but neither is a superset of the other.


\subsection{Driving arbitrary OpenCL kernels}

This thesis presents a technique for driving arbitrary OpenCL kernels, provided they have regular scalar or array inputs. This host driver accepts as input an OpenCL kernel, which it then compiles, produces input data sets, and runs the compiled kernel using the data sets. The host driver generates data sets from uniform random distributions, as do many OpenCL benchmark suites. For cases where non-uniform inputs are required (e.g. profile-directed feedback), an alternate methodology for generating inputs must be adopted.


% \subsection{Augmenting Benchmark Training Data}

% \todo[inline]{Usefulness of benchmarks is evaluated only when used as auxiliary training data, not as the sole training set.}

% for benchmarking, in cases where you know the domain, grammar-based approaches have their uses. The approach presented in this thesis is guided by the corpus, and any deficiencies in the corpus will show up in the output.


\subsection{Modelling Program Semantics from Syntactic Sequences}

Chapter~\ref{chap:deeptune} feeds a sequence of program tokens into a recurrent neural network to predict an optimisation decision that should be made on it. By treating the serialised representation of a program (its source code) as the sequence of syntactic tokens, the technique is vulnerable to changes in code order, since $p(y|[\bm{x}^{(1)}, \bm{x}^{(2)}, \bm{x}^{(3)}]) \ne p(y|[\bm{x}^{(3)}, \bm{x}^{(2)}, \bm{x}^{(1)}])$. The text inputs used to evaluate the approach are single kernels. It is not clear how the approach will respond to multi-procedure inputs, where the order that procedures are declared may have a large impact on the pattern of activations produced in the recurrent neural network.

% Using text as the representation, like statistical feature extractors, is vulnerable to dead code insertion.

% Similarly, the sequential representation is vulnerable to dead code

% The higher-dimensional representation of source code text over numerical features means that larger data sets are required to combat sparsity in training data. For example, in Chapter~\ref{chap:deeptune}, one-hot vocabulary sequences are padded to 1024 tokens, providing a much more sparse representation than the 4-dimension feature space used by the state-of-the-art approach for equivalent size data sets.

A common criticism of machine learning systems is that they are \emph{black boxes}. When the system fails to produce the desired result, there is no obvious method to correct the system so as to prevent similar errors. Still, in traditional machine learning, it may be possible to correct problems by adjusting the features. In an absence of features, there are fewer meaningful ways to improve a model based on analysis of failure cases.


\section{Future Work}
\label{sec:conclusions-future-work}

This section outlines some avenues for future research enabled by this thesis.

\subsection{Guided Program Synthesis to Minimise Benchmarking Cost}

This thesis presents a technique for the \emph{unguided} synthesis of compiler benchmarks. Using the technique may provide a fine-grained exploration of the space of representative programs. For some use cases, more efficient use of data will be achieved through \emph{directed} program generation.

One approach to direct program generation could be to employ a rejection filter that tests for the presence of a property of interest and rejects programs that do not satisfy this property. Another approach would be to train the generative model simultaneously for both the structure of programs (as is done in this work), along with a representation of the properties of interest (such as a feature vector). At sample time, the feature values of the desired program could be used as input to steer the program generation. A third approach would use the learned language model not to generate programs, but to guide an existing program generator by biasing the weights of grammar productions.

If successful, such a technique would enable the exploration of larger feature spaces than is currently possible by efficiently navigating the generation of benchmarks to map out the decision surface. \diff{Additional insight into the behaviour of a model and its failure cases would be enabled by steering synthesis towards the parts of the space where the model has the lowest confidence, or parts of the space where the model frequently makes wrong predictions.}


\subsection{Neural Model Selection through Adversarial Games}

Section~\ref{sec:clgen-qualitative-evaluation} employs \emph{Turing tests} to evaluate the quality of synthetic code. The task presented to human participants was to identify whether a series of code snippets were written by hand or machine. This was used to evaluate whether or not the model produced human-like output. In future work, this approach could be extended to aid in the challenging task of model selection by instead presenting the participants with pairs of samples side by side, and asking the participant to select the sample which is \emph{more} human-like. If the two samples were both generated by different configurations of a generative model, this would provide a means to compare generative models on the otherwise hard-to-assess quality of ``humanness''. The selection of the best model from a pool of candidates is thus turned into a series of zero-sum games, where each game pits a single sample from a pair of generative models head-to-head with a human selecting the winner, and an Elo rating can be used to assign scores and pair matches. The limiting factor of this approach would likely be the availability of human participants.

\diff{%
Generative Adversarial Networks (GANs)~\cite{Goodfellow2014} employ a similar adversarial approach, but use a second artificial neural network as a discriminative adversary. The generator and discriminator networks are trained concurrently; the generator is trained to maximise the probability of failure in the discriminator, the discriminator is trained to minimise this probability. This approach is a good fit for the domain of program generation, though filtering of the generator outputs to check for program correctness may be required to prevent the generator training to a local maxima where the output is hard to distinguish from the test set, but rarely contains meaningful program semantics.%
}


\subsection{Learning Representations for Dynamic Program Inputs}

Chapter~\ref{chap:deeptune} presents an approach for learning optimisation heuristics from the raw representation of a program, but in the presence of dynamic properties, traditional feature extraction must be used. For example, feeding in the size of input data sets. In future work, this approach could be extended to also account for dynamic properties. Unlike with program source code, it is not clear what the raw representation of program inputs may be as there is no equivalent human-interpretable representation of program data. One approach could be to model the sequence of bytes that the program reads and writes, though this may introduce a high overhead for runtime instrumentation~\cite{Gad2014}.


\subsection{Towards General-Purpose Program Comprehension}

The techniques presented in this thesis apply deep learning techniques to the task of modelling the behaviour of programs. For each task, be it program generation or optimisation, artificial neural networks are trained from scratch. Chapter~\ref{chap:deeptune} explores the use of transfer learning to seed an artificial neural network with information from another task. Future work could explore this idea further by iteratively training and retraining a single network across a wide range of tasks, with the goal of finding a common set of model parameters to be used as an effective base for each task.

An ambitious goal would be the development and distribution of a model architecture for \emph{general-purpose} program comprehension. Such a system would enable, with little effort, a single model to be re-purposed for a variety of compiler tasks. This is analogous to the widespread distribution of pre-trained state-of-the-art models in the field of image recognition. If developing an image classifier, a user may start by retraining an existing model such as ResNet~\cite{He2016}, rather than constructing a model from scratch. This drastically simplifies the adoption of machine learning for image classification as the model architecture has been pre-selected and tuned, and reduces the amount of training data required.

A prerequisite for developing this system will be applying techniques such as those proposed in this thesis to a wide range of different compiler tasks. My hope in publicly releasing all of the software developed during the course of my research is to enable and expedite this discovery in other domains.

% \todo[inline]{Unify the program generation and optimisation to make a closed system.}

% \todo[inline]{Graph-level representations with RNNS~\cite{Jin2018}. Graph surveys~\cite{Li2018a,Wu2018}.}

% \todo[inline]{Table of GitHub corpus size by programming language. There's room for machine learning in lots of languages! E.g. Haskell, Java, C/C++, Solidity, Python, OpenCL}
