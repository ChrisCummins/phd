\chapter{Introduction}

\diff{%
There has been an unprecedented increase in the scale and quantity of data-intensive workloads, with the energy consumption of the ICT industry estimated to rise to 20\% of global output by 2030~\cite{Andrae2019}. Fundamental shifts in both hardware and software are required to meet the demands of the transition to \emph{big data}~\cite{Hashem2015,Gandomi2015}.

Compilers play an essential role in extracting software efficiency by transforming the input code to a set of machine instructions that best utilises the available resources of the target architecture. Modern compilers are multi-million dollar projects comprising millions of lines of code from hundreds of developers, shown in Table~\ref{tab:compiler-costs}. Each architecture supported by a compiler requires extensive hand tuning by experts to extract good performance. Modern compilers are too complex to be fully understood by a single developer, yet optimising each component requires knowledge of the full compiler and the interaction of its components.%
}

\begin{table}
  \centering
  \input{tab/compiler-costs}
  \caption[Development history and costs of popular open-source compilers]{%
    \diff{%
    Development history, logical lines of code (LOC), and estimated cost for 10 popular open-source compilers. Estimated costs are calculated using a COCOMO model~\cite{David2001} with average 2019 US software developer salaries~\cite{Glassdoor2019}.%
    }
  }
  \label{tab:compiler-costs}
\end{table}

% In hardware, performance needs have long outstripped what can be provided by single processors, leading to a broad spectrum of parallel and increasingly heterogeneous architectures. These range from re-purposing existing hardware such as Graphics Processor Units (GPUs) to developing entirely novel application-specific chips~\cite{Misra2010,Jouppi2017}. While some compiler logic can be shared across architectures, the optimisation decisions to extract the best performance from specific hardware cannot. Currently, each architecture requires extensive hand tuning by experts to extract good performance.

% In software, the shift towards parallelism and heterogeneity has created a \emph{programmability challenge}. Parallel programming poses a far greater burdern on developers than traditional single-threaded development; it is often not clear how best to partition work across the available parallel resources and doing so easily introduces bugs. One of the most popular approaches to mitigating the programmability challenge is through the development and widespread adoption of libraries of high-level abstractions. High-level abstractions greatly simplify parallel programming by providing complex parallel communication and coordination logic, allowing users to plug-in only the business logic required to solve a problem. Still, there is a wide range of approaches to implementing such libraries. For example, two of the most popular high-level libraries for parallelising deep learning workloads --- TensorFlow~\cite{Abadi} and PyTorch~\cite{Paszke2017} --- use opposing data-flow and imperative programming styles, respectively. Optimising such libraries provides new challenges to the compiler --- the challenge of code analyses in the face of parallelised higher-order functions can defeat many optimisations.

\diff{%
The high cost of compiler development is unable to adapt to the rapidly changing hardware landscape arising from GPUs, FPGAs~\cite{Misra2010,Cong2016a}, and ASICs~\cite{Misra2010,Jouppi2017}. When compilers are unable to keep up with the pace of change, this leads to wasted energy, slow performance, and buggy software. For the trend towards data-intensive workloads and heterogeneous devices to continue, new techniques are required to reduce the cost of compiler construction. This thesis aims to address the widening gap between the potential and achieved efficiency of software by developing new tools to simplify and improve compiler construction.

The remainder of this chapter describes the application of machine learning to compiler construction, followed by the problems with this approach. Then the contributions of this thesis are detailed, followed by a description of the overall structure of the document.%
}


\section{Machine Learning for Compilers}

Machine learning, the study of algorithms and systems capable of learning from data without being explicitly programmed, has been successfully applied across a broad range of fields and disciplines. Within compilers, there are many tasks for which machine learning may prove useful.

A common case where machine learning aids in compiler construction is in the labour-intensive process of optimisation heuristic construction. For example, suppose a developer is tasked with tuning the loop unrolling heuristic of a compiler\footnote{Loop unrolling is a code transformation in which the body of a loop is duplicated so that fewer iterations of the loop must be executed. The idea is to reduce runtime by executing fewer loop control instructions, at the expense of an increasing the size of the executable binary.}. There is a multitude of factors that may influence the decision of whether to unroll a loop such as the size of the loop body, the number of loop iterations, and the number of registers required to execute the loop body. Determining which of these factors are most significant, and on what values the heuristic should differ, is an unwieldy task. Further to that, the outcome of the heuristic will depend not just on the program being compiled, but on properties of the target hardware, such as the number of registers and the size of the instruction cache. In the face of these challenges, developing good heuristics is a huge undertaking, and it is unlikely that the developer will be able to craft a heuristic capable of extracting the best performance in all situations.

Instead, the developer may cast construction of a loop unrolling heuristic as a machine learning problem. Rather than expertly crafting a heuristic through intuition and manual experimentation, the idea is to use a learning algorithm to derive a heuristic from empirical data of the performance of loops under different configurations of unrolling. To do this, the developer runs suites of benchmark programs repeatedly using different unrolling decisions to determine the best decision for each case, then combine this with a numerical representation of each program. A machine learning system then models the correlations between these numerical program representations --- \emph{features} --- and the unrolling decisions that should be made. Using machine learning for this task reduces the need for domain expertise compared to the expert-driven approach, and is achieved with less effort by the developer.

The appeal of machine learning is that it provides techniques to automatically understand the structure of data and how that structure relates to a specific goal. For unseen data which has similar structures to the training data, this enables accurate predictions to be made; all without the need for expert domain knowledge. In essence, machine learning negates the need for problem-domain expertise in cases where there is a ready supply of empirical observations.

The applicability of machine learning to a wide range of tasks in compiler construction has led to an established research direction. In previous studies machine learning has been shown to simplify the construction of compiler optimisations~\cite{Ashouri2018,Wang2018}, often leading to higher quality heuristics that outperform those constructed by human experts. With the increasing demand for aggressively optimising compilers across a range of heterogeneous hardware, it would appear that machine learning could provide a much-needed relief on the burden of compiler developers.

Yet, the integration of machine learning and compilers has remained a largely academic pursuit, with little progress towards adoption by industry. The following section summarises three key outstanding problems in applying machine learning to compilers that limit widespread adoption.


\newpage
\section{Challenges in Machine Learning for Compilers}
\label{sec:intro-challenges}

Machine learning techniques offer reduced development costs and improved runtime performance compared to expert approaches, yet there are three significant challenges that must be overcome to realise these goals:

\subsection{Scarcity of data}
\label{subsec:challenge-scarcity}

In machine learning, a model is trained based on past observations to predict the values for future inputs. In order to be able to generalise well to unseen observations, plentiful training data must be provided, with a fine-grained overview of the feature space. In the case of compilers, training data is derived from benchmark programs, meaning that many benchmarks are needed to produce sufficient observations for training. Typical machine learning experiments outside of the compilation field train over thousands or millions of observations. However, there are typically only a few dozen common benchmarks available.

The small number of available benchmarks limits the quality of learned models as they have sparse training data. The problem is worsened by the exponential increase in feature space size with the addition of new features. Each additional feature makes the sparsity of training examples more pronounced, increasing the number of observations required.

To address this issue, there must be a sizeable increase in the availability of benchmarks for machine learning. Previously, researchers sought to provide this by randomly instantiating from hand-crafted benchmark templates~\cite{Chiu2015}, but this is a challenging approach --- the generator must be biased in such a way that the generated programs draw from a similar distribution to \emph{real} programs so as to be useful for learning. It is not clear if such an approach could ever achieve parity with handwritten programs.

\subsection{Model and feature design}
\label{subsec:challenge-features}

Machine learning algorithms learn to correlate a set of explanatory variables, known as \emph{features}, with a target value. These features must be chosen so as to be discriminative for the target value.

Choosing the features to characterise a program so as to be discriminative for machine learning  depends on the desired task to be learned, the type of model, and the environment from which training data was collected, e.g. the hardware and machine configuration. Many problems in compilers do not map directly to numeric attributes, so systems for extracting numeric representations from non-numeric inputs must be developed. For example, one might extract instruction counts from the input source code. Knowing which attributes to extract to represent a program is a hard problem, there is no one-size-fits-all approach that works for all cases. Further, features may only be rendered suitable for learning by transforming the initial values such as through scaling or normalising. As a result, feature design is often an incremental process of trial and experimentation, and there are few clear signals when this iterative process is ``done''.

\diff{%
\subsection{Adoption of machine learning practices}
\label{subsec:challenge-adoption}

The 

TODO. Machine learning systems are probabilistic.%

For machine learning techniques to be widely adopted in compilers, they must be made significantly easier and cheaper to develop. The aim of this thesis is to research and develop machine learning techniques that simplify and lower the cost of compiler construction.
}

% TODO: Can we squeeze in a paragraph about testing?

\newpage
\section{Contributions}

This thesis presents machine learning-based techniques to simplify and accelerate compiler construction. The key contributions of this thesis are:

\begin{itemize}
  \item \diff{The first application of deep learning over source codes to synthesise compilable, executable benchmarks. The proposed approach automatically enhances the predictive power of a state-of-the-art predictive model, improving the performance of heterogeneous workloads by $1.27\times$. Further, the additional benchmarks expose limitations in the feature design of the model which, after correcting, further increases performance by $4.30\times$. This addresses the scarcity of data challenge (Section~\ref{subsec:challenge-scarcity}) by enabling the generation of an unbounded number of benchmarks for training machine learning models.}
  \item \diff{A novel, automatic, and fast approach for the generation of expressive random programs for compiler fuzzing. The developed system \emph{infers} programming language syntax, structure, and use from real-world examples, not through an expert-defined grammar. The system needs two orders of magnitude less code than the state-of-the-art. In modelling real handwritten code, the test cases are more interpretable than other approaches; the average test case size is two orders of magnitude smaller than state-of-the-art. An extensive testing campaign across 10 OpenCL compilers reveals a similar number of bugs as the state-of-the-art, but also finds bugs which prior work cannot, covering more components of the compiler. This addresses the machine learning adoption challenge (Section~\ref{subsec:challenge-adoption}) by offering a far simpler method for developing compiler fuzzers, and fast heuristic techniques are proposed to enable probabilistic machine learning systems in a domain which previously required formal approaches.}
	\item \diff{A methodology for building compiler heuristics without the need for program feature engineering. In an evaluation of the technique it is found to outperform existing prior  state-of-the-art predictive models by 14\% and 12\% in two challenging GPGPU compiler optimisation domains. The proposed approach enables the first application of \emph{transfer learning} to compiler optimisations, improving heuristics by reusing training information across different optimisation problems, even if they are unrelated. This addresses the model and feature design challenge (Section~\ref{subsec:challenge-features}) by removing the need to develop numeric representations of code, and enabling a single model design to be applied to multiple problems.}
\end{itemize}

\newpage
\section{Structure}

This thesis is organised as follows:

\textbf{Chapter~\ref{chap:background}} provides background. It defines terminology and describes the machine learning and evaluation techniques used in this work.

\textbf{Chapter~\ref{chap:related-work}} surveys the relevant literature, divided into three categories: first program generation, then program optimisation, finally deep learning for programming languages.

\textbf{Chapter~\ref{chap:clgen}} describes a novel technique for generating an unbounded number of executable benchmarks to augment the training data of a predictive model. A qualitative evaluation of the generated programs is presented, followed by a quantitative evaluation using a state-of-the-art OpenCL optimisation heuristic.

\textbf{Chapter~\ref{chap:deepsmith}} extends the generator presented in Chapter~\ref{chap:clgen} to the domain of compiler validation, presenting a low-cost technique for the inference of compiler fuzzers. Fast heuristic approaches are presented for false-positive prevention using probabilistic testcase generators. An extensive testing campaign of OpenCL compilers is described, resulting in 67 bug reports.

\textbf{Chapter~\ref{chap:deeptune}} introduces a novel methodology for constructing optimising compiler heuristics without the need for code features. It presents two case studies of the technique: the first for learning a heterogeneous device mapping heuristic, the second for learning OpenCL thread coarsening.

\textbf{Chapter~\ref{chap:conclusions}} summarises the overall findings of the thesis, provides a critical review, and outlines potential avenues for future research.


\section{Summary}

\diff{%
The promise of machine learning techniques is reduced development cost and improved performance of compilers. Achieving this goal requires three significant challenges to be overcome: the data scarcity challenge, the feature design challenge, and the adoption challenge. Subsequent chapters describe novel techniques to address these issues.%
}
