\chapter{Introduction}

There has been an unprecedented increase in the scale and quantity of data-intensive workloads. Fundamental shifts in both hardware and software are required to meet the demands of the transition to \emph{big data}.

In hardware, performance needs have long outstripped what can be provided by single processors, leading to a broad spectrum of parallel and increasingly heterogeneous architectures. These range from re-purposing existing hardware such as Graphics Processor Units (GPUs) to developing entirely novel application-specific chips~\cite{Misra2010,Jouppi2017}. While some compiler logic can be shared across architectures, the optimisation decisions to extract the best performance from specific hardware cannot. Each architecture requires extensive hand tuning by experts to extract good performance.

In software, the shift towards parallelism and heterogeneity has created a \emph{programmability challenge}. Parallel programming poses far greater challenges than traditional single-threaded development; it is often not clear how best to partition work across the available parallel resources and doing so easily introduces bugs. One of the most popular approaches to mitigating the programmability challenge is through the development and widespread adoption of high-level abstractions. High-level abstractions and libraries can greatly simplify parallel programming by providing complex parallel communication and coordination logic, allowing users to plug-in only the business logic required to solve a problem. Still, there is a wide range of approaches to implementing such libraries. For example, two of the most popular high-level libraries for parallelising deep learning workloads --- TensorFlow~\cite{Abadi} and PyTorch~\cite{Paszke2017} --- use opposing data-flow and imperative programming styles, respectively. Optimising such libraries provides new challenges to the compiler --- the challenge of code analyses in the face of parallelised higher-order functions can defeat many optimisations.

The combined burden of increased hardware and software diversity has resulted in compilers that are too complex to be fully understood by a single developer, and too cumbersome to keep up with the required pace of change. This results in slow performance, wasted energy, and buggy software. For the trend towards data-intensive workloads and heterogeneous devices to continue, new techniques are required to reduce the cost of compiler construction. The remainder of this chapter describes the application of machine learning to this issue, followed by the problems with this approach. Then the contributions of this thesis are detailed, followed by a description of the overall structure of the document.


\section{Machine Learning for Compilers}

Machine learning, the study of algorithms and systems capable of learning from data without being explicitly programmed, has been successfully applied across a broad range of fields and disciplines. Within compilers, there are many tasks for which machine learning may prove useful.

A common case where machine learning aids in compiler construction is in the labour-intensive process of optimisation heuristic construction. For example, suppose a developer is tasked with tuning the loop unrolling heuristic of a compiler\footnote{Loop unrolling is a code transformation in which the body of a loop is duplicated so that fewer iterations of the loop must be executed. The idea is to reduce runtime by executing fewer loop control instructions, at the expense of an increasing the size of the executable binary.}. There is a multitude of factors that may influence the decision of whether to unroll a loop such as the size of the loop body, the number of loop iterations, and the number of registers required to execute the loop body. Determining which of these factors are most significant, and on what values the heuristic should differ, is an unwieldy task. Further to that, the outcome of the heuristic will depend not just on the program being compiled, but on properties of the target hardware, such as the number of registers and the size of the instruction cache. In the face of these challenges, developing good heuristics is a huge undertaking, and it is unlikely that the developer will be able to craft a heuristic capable of extracting the best performance in all situations.

Instead, the developer may cast construction of a loop unrolling heuristic as a machine learning problem. Rather than expertly crafting a heuristic through intuition and manual experimentation, the idea is to use a learning algorithm to derive a heuristic from empirical data of the performance of loops under different configurations of unrolling. To do this, the developer would run a suite of benchmark programs repeatedly using different unrolling decisions to determine the best decision for each case, then combine this with a numerical representation of each program. A machine learning system would then model the correlations between these numerical representations --- \emph{features} --- and the unrolling decisions that should be made. Using machine learning for this task reduces the need for domain expertise compared to the expert-driven approach, and is achieved with less effort by the developer.

The appeal of machine learning is that it provides techniques to automatically understand the structure of data and how that structure relates to a specific goal, enabling predictions to be made on unseen data; all without the need for expert domain knowledge. In essence, machine learning negates the need for domain expertise in cases where there is a ready supply of empirical observations.

The applicability of machine learning to a wide range of tasks in compiler construction has led to an established research field. In previous studies machine learning has been shown to simplify the construction of compiler optimisations, often leading to higher quality heuristics that outperform those constructed by human experts. With the increasing demand for aggressively optimising compilers across a range of heterogeneous hardware, it would appear that machine learning could provide a much-needed relief on the burden of compiler developers.

Yet, the integration of machine learning and compilers has remained a largely academic pursuit, with little progress towards adoption by industry. The following section speculates as to the cause by summarising some of the outstanding problems in applying machine learning to compilers.


\section{Challenges in Machine Learning for Compilers}

\diff{TODO: s/problem/challenge/}

Machine learning techniques offer reduced costs and improved performance compared to expert approaches, yet there are challenges preventing widespread adoption. There are two significant problems that must be overcome:

\paragraph*{Scarcity of data} In machine learning, a model is trained based on past observations to predict the values for future inputs. In order to be able to generalise well to unseen observations, plentiful training data should be provided, with a fine-grained overview of the feature space. In the case of compilers, training data is derived from benchmark programs, meaning that many benchmarks are needed to produce sufficient observations for training. Typical machine learning experiments outside of the compilation field train over thousands or millions of observations. However, there are typically only a few dozen common benchmarks available.

The small number of available benchmarks limits the quality of learned models as they have very sparse training data. The problem is worsened by the exponential increase in feature space size with the addition of new features. Each additional feature makes the sparsity of training examples more pronounced, increasing the number of observations required.

To address the issue, there must be a sizeable increase in the availability of benchmarks for machine learning. Previously, researchers sought to provide this by randomly instantiating from hand-crafted benchmark templates, but this is a challenging approach --- the generator must be biased in such a way that the generated programs draw from a similar distribution to \emph{real} programs so as to be useful for learning. It is not clear if such an approach could ever achieve parity with real programs.

\paragraph*{Model and feature design} Machine learning algorithms learn to correlate a set of \emph{explanatory variables} with a target value. These explanatory variables, known as features, must be chosen so as to be discriminative for the target value.

Choosing the features to summarise a program so as to be discriminative for machine learning is a challenging task that depends on the thing being learned, and the environment from which training data was collected, e.g. the hardware and machine configuration.

Many problems in compilers do not map directly to numeric attributes, so systems for extracting numeric representations from non-numeric inputs must be developed. For example, one might extract instruction counts from the input source code. Knowing which attributes to extract to represent a program is a hard problem, there is no one-size-fits-all approach that works for all cases. Feature design is often an incremental process of trial and experimentation, and there are few clear signals when the iterative process is ``done''.

If machine learning is to be widely adopted in compilers, it must be made significantly easier and cheaper. The aim of this thesis is to develop machine learning techniques that simplify and lower the cost of compiler construction.

% TODO: Can we squeeze in a paragraph about testing?


\section{Contributions}

This thesis presents machine learning-based techniques to simplify and accelerate compiler construction. The key contributions of this thesis are:

\begin{itemize}
  \item The first application of deep learning over source codes to synthesise compilable, executable benchmarks. The approach automatically enhances the predictive power of a state-of-the-art predictive model, improving the performance of heterogeneous workloads by $1.27\times$. Further, the additional benchmarks expose limitations in the feature design of the model which, after correcting, further increases performance by $4.30\times$.
  \item A novel, automatic, and fast approach for the generation of expressive random programs for compiler fuzzing. The system \emph{infers} programming language syntax, structure, and use from real-world examples, not through an expert-defined grammar. The system needs two orders of magnitude less code than the state-of-the-art and takes less than a day to train. In modelling real handwritten code, the test cases are more interpretable than other approaches. The average test case size is two orders of magnitude smaller than state-of-the-art, without any expensive reduction process.
  \item An extensive evaluation campaign of the compiler fuzzing approach using 10 OpenCL compilers and 1000 hours of automated testing. The campaign uncovers a similar number of bugs as the state-of-the-art, but also finds bugs which prior work cannot, covering more components of the compiler.
	\item A methodology for building compiler heuristics without the need for program feature engineering. In an evaluation of the technique, it is found to outperform existing state-of-the-art predictive models by 14\% and 12\% in two challenging GPGPU compiler optimisation domains.
	\item The first application of \emph{transfer learning} to compiler optimisations, improving heuristics by reusing training information across different optimisation problems, even if they are unrelated.
\end{itemize}

\newpage
\section{Structure}

This thesis is organised as follows:

\textbf{Chapter~\ref{chap:background}} provides background. It defines terminology and describes the machine learning and evaluation techniques used in this work.

\textbf{Chapter~\ref{chap:related-work}} surveys the relevant literature, divided into three categories: first program generation, then program optimisation, finally deep learning for programming languages.

\textbf{Chapter~\ref{chap:clgen}} describes a novel technique for generating an unbounded number of executable benchmarks to augment the training data of a predictive model. A qualitative evaluation of the generated programs is presented, followed by a quantitative evaluation using a state-of-the-art OpenCL optimisation heuristic.

\textbf{Chapter~\ref{chap:deepsmith}} extends the generator presented in Chapter~\ref{chap:clgen} to the domain of compiler validation, presenting a low-cost technique for the inference of compiler fuzzers. It describes an extensive testing campaign of OpenCL compilers, resulting in 67 bug reports.

\textbf{Chapter~\ref{chap:deeptune}} introduces a novel methodology for constructing optimising compiler heuristics without the need for code features. It presents two case studies of the technique: the first for learning a heterogeneous device mapping heuristic, the second for learning OpenCL thread coarsening.

\textbf{Chapter~\ref{chap:conclusions}} summarises the overall findings of the thesis, provides a critical review, and outlines potential avenues for future research.


\section{Summary}

This introductory chapter has outlined the use of machine learning for reducing the cost of compiler construction and two significant issues preventing its widespread adoption: the scarcity of data and the challenge of designing features. Subsequent chapters describe novel techniques to address both issues.
