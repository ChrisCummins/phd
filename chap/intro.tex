\chapter{Introduction}

There has been an unprecedented increase in the scale and quantity of data intensive workloads. Fundamental shifts in both hardware and software are required to meet the demands of the transition to \emph{big data}.

In hardware, performance needs have long outstripped what can be provided by single processors, leading to a broad spectrum of heterogeneous architectures. These range from re-purposing existing Graphics Processor Units (GPUs) to even developing application-specific integrated circuits to offload numeric tasks~\cite{Misra2010,Jouppi2017}. While much of the compiler logic can be reused across architectures, the optimisations that are required to extract the best performance from specific hardware cannot. Each architecture requires extensive hand tuning by experts to extract good performance.

In software, the shift towards parallelism and heterogeneity has created a \emph{programmability challenge}. Parallel programming is significantly more challenging than traditional single threaded development; there are many more opportunities to introduce bugs in software, and Amdahl's law can make extracting performance much more challenging. One of the most popular approaches to mitigating the programmability challenge is through the development and widespread adoption of high level abstractions. High level abstractions and libraries can greatly simplify parallel programming by providing the complex parallel communication and coordination logic, allowing users to plug-in only the business logic required to solve a problem. Still, there is a wide range of approaches to implementing such libraries. One example is high level libraries for performing data intensive numeric workloads, two of the most popular examples of which --- TensorFlow~\cite{Abadi} and PyTorch~\cite{Paszke2017} --- use opposing data-flow and imperative programming styles, respectively. Optimising such libraries provides new challenges to the compiler --- the challenge of code analyses in the face of parallelised higher order functions can defeat many optimisations.

The combined burden of increased hardware and software diversity has resulted in compilers that are beyond the capabilities of a single expert, and too cumbersome to keep up with the pace of change. This results in slow performance, wasted energy, and buggy software. For the trend towards data-intensive workloads and heterogeneous devices to continue, new techniques are required to reduce the cost of compiler construction.


\section{Machine Learning for Compilers}

Machine learning has been successfully applied across a broad range of fields and disciplines. Within compilers, there are many tasks which require domain expertise for which machine learning may prove useful.

For example, suppose a developer is tasked with tuning the loop unrolling heuristic of a compiler\footnote{Loop unrolling is a code transformation in which the body of a loop is duplicated so that fewer iterations of the loop must be executed. The idea is to reduce runtime by executing fewer loop control instructions, at the expense of an increased binary size.}. There are a multitude of factors that may influence the decision of whether to unroll a loop --- the size of the loop body, the number of loop iterations, the number of registers required to execute the loop body, etc. Determining which of these factors are significant for unrolling, and on what values the heuristic should differ, is an unwieldy task. Further to that, the outcome of the heuristic will depend not just on the program being compiled, but on properties of the target hardware, such as the size of the instruction cache, the number of registers, etc. In the face of these challenges, developing heuristics is a huge undertaking, and it is unlikely that the developer will be able to craft a heuristic capable of extracting the best performance in all situations.

Instead, the developer may cast construction of a loop unrolling heuristic as a machine learning problem. The idea is, rather than expertly crafting a heuristic through intuition and experimentation, to use a learning algorithm to build the heuristic based on empirical data of the performance of loops both with and without unrolling. To do this, the developer would run a suite of benchmark programs both with and without loop unrolling to determine which option is better in each case, then combine this with a numerical representation of each program, which a machine learning system would use to model the correlations between the numerical representation --- \emph{features} --- and the unrolling decision that should be made. When approached in this manner, the task of constructing the loop unrolling heuristic no longer requires domain expertise and can be achieved with much lower effort by the developer.

The applicability of machine learning to a wide range of tasks in compiler construction has led to an established research field. In previous studies machine learning has been shown to simplify the construction of compiler optimisations, often leading to higher quality heuristics that outperform those constructed by human experts. With the increasing demand for aggressively optimising compilers across a range of heterogeneous hardware, it would appear that machine learning could provide a much needed relief on the burden of compiler developers.

The appeal of machine learning is that it provides techniques to automatically understand the structure of data and how that structure relates to a specific goal, enabling predictions to be made on unseen data; all without the need for expert domain knowledge. In essence, machine learning can negate the need for domain expertise in cases where there is a ready supply of empirical observations.

Yet, the integration of machine learning and compilers has remained a largely academic pursuit, with little progress being made of adoption within industry. The following section speculates as to the cause by summarising some of the outstanding problems in applying machine learning to compilers. The remainder of this chapter details the contributions of this thesis, followed by the overall structure of the document.


\section{The Problem}

Machine learning techniques offer reduced costs and improved performance compared to expert approaches, yet there are challenges preventing widespread adoption. There are two significant problems that must be overcome:

\paragraph*{Scarcity of data} In machine learning, a model is trained based on past observations to predict the values for unseen data points. In order to be able to generalise well to unseen points, plentiful training data should be provided, with a fine-grained overview of the feature space. For compilers, training data is usually derived from programs, meaning that many programs --- benchmarks --- are needed to produce sufficient observations for training. Typical machine learning experiments outside of the compilation field train over thousands or millions of observations. However, there are typically only a few dozen common benchmarks available.

The small number of available benchmarks limits the quality of learned models as they have very sparse training data. The problem is worsened exponentially with the dimensionality of the feature space. Each additional feature worsens the sparsity of training examples, requiring more observations.

To address the issue, there must be a sizeable increase in the availability of benchmarks for machine learning. Previously, researchers sought to provide this through the use of random grammar based program generators, but constructing such a generator is a challenging task --- the random generation must be biased in such a way that the generated programs draw have a similar distribution to \emph{real} programs so as to be useful for learning. This would be an enormous undertaking, and it is not clear if such an approach would ever achieve parity with real programs.

\paragraph*{Model and Feature design} Machine learning algorithms learn to correlate a set of \emph{explanatory variables} with a target value. These explanatory variables, known as features, must be chosen so as to be discriminative for the target value.

Choosing the features to summarise a program so as to be discriminative for machine learning is a challenging task that depends on the thing being learned, and the environment from which training data was collected, e.g. the hardware and machine configuration.

Many problems in compilers do not map directly to numeric attributes, requiring the extraction of a numeric representation from a non-numeric input. For example, extracting instruction counts from source code. Knowing which attributes to extract to represent a program is not easy, there is no one-size-fits-all approach that works for all cases. Feature design is often an incremental process of trial and experimentation, but there are few clear signals when the process is ``done''.

If machine learning is to be widely adopted in compilers, it must be made significantly easier and cheaper. The aim of this thesis is to develop machine learning techniques that simplify and lower the cost of compiler construction.


\section{Contributions}

This thesis presents machine learning-based techniques to simplify and accelerate compiler construction. The key contributions of this thesis are:

\begin{itemize}
  \item The first application of deep learning over source codes to synthesise compilable, executable benchmarks. The approach automatically improve the performance of a state-of-the-art predictive model by $1.27\times$, and expose limitations in the feature design of the model which, after correcting, further increases performance by $4.30\times$.
  \item A novel, automatic, and fast approach for the generation of expressive random programs for compiler fuzzing. The system \emph{infers} programming language syntax, structure, and use from real-world examples, not through an expert-defined grammar. The system needs two orders of magnitude less code than the state-of–the-art, and takes less than a day to train. In modelling real handwritten code, the test cases are more interpretable than other approaches. Average test case size is two orders of magnitude smaller than state-of-the-art, without any expensive reduction process.
  \item An extensive evaluation campaign of the compiler fuzzing approach using 10 OpenCL compilers and 1000 hours of automated testing. The campaign uncovers a similar number of bugs as the state-of–the-art, but also finds bugs which prior work cannot, covering more components of the compiler.
	\item A methodology for building compiler heuristics without any need for feature engineering. In an evaluation of the technique, it is found to outperform existing state-of-the-art predictive models by 14\% and 12\% in two challenging GPGPU compiler optimisation domains.
	\item The first application of \emph{transfer learning} to compiler optimisations, improving heuristics by reusing training information across different optimisation problems, even if they are unrelated.
\end{itemize}


\section{Structure}

This thesis is organised as follows:

\textbf{Chapter~\ref{chap:background}} provides background. It defines terminology and describes the machine learning and evaluation methodologies used in this work, along with an overview of heterogeneous programming.

\textbf{Chapter~\ref{chap:related-work}} surveys the relevant literature, divided into three categories: first program generation, then program optimisation, finally deep learning for programming languages.

\textbf{Chapter~\ref{chap:clgen}} describes a novel technique for generating an unbounded number of executable benchmarks to augment the training data of a predictive model. A qualitative evaluation of the generated programs is presented, followed by an quantitative evaluation using a state-of-the-art OpenCL optimisation heuristic.

\textbf{Chapter~\ref{chap:deepsmith}} extends the generator presented in Chapter~\ref{chap:clgen} to the domain of compiler validation, presenting a low cost technique for the inference of compiler fuzzers. It describes an extensive testing campaign of OpenCL compilers, resulting in 67 bug reports.

\textbf{Chapter~\ref{chap:deeptune}} introduces a novel methodology for constructing optimising compiler heuristics without the need for code features. It describes two case studies of the technique: the first for learning a heterogeneous device mapping heuristic, the second for learning OpenCL thread coarsening.

\textbf{Chapter~\ref{chap:conclusions}} summarises the overall findings, provides a critical review, and outlines potential avenues for future research.


\section{Summary}

This introductory chapter has outlined the use of machine learning for compilers and two significant issues preventing its widespread adoption: the scarcity of data, and the challenge of designing features. Subsequent chapters describe novel approaches to address both issues.
