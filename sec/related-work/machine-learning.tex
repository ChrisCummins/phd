\section{Deep Learning for Programming Languages}

Deep learning is a nascent branch of machine learning in which deep or multi-level graphs of processing layers are used to detect patterns in natural data~\cite{Buduma2015,LeCun2015}. It is proving especially successful for its ability to ability to process natural data in its raw form. This overcomes the traditionally laborious and time-consuming practise of engineering feature extractors to process raw data into an internal representation or feature vector. Deep learning has successfully discovered structures in high-dimensional data, and is responsible for many breakthrough achievements in machine learning, including human parity in conversational speech recognition~\cite{Xiong2016}; professional level performance in video games~\cite{Mnih2015}; and autonomous vehicle control~\cite{Lozano-Perez2012}.

In past work I used the Long Short-Term Memory (LSTM) architecture of Recurrent Neural Network (RNN)~\cite{Sundermeyer2012,Mikolov2015} to generate sequences of OpenCL code~\cite{Cummins2017a}. The LSTM network architecture comprises recurrent layers of \emph{memory cells}, each consisting of an input, output, and forget gate, and an output layer providing normalised probability values from a 1-of-K coded vocabulary~\cite{Graves,Graves2013}. Although this is the first application of deep learning for generating executable programs, RNNs have been successfully applied to a variety of other generative tasks, including image captioning~\cite{Vinyals}, colourising black and white photographs~\cite{Zhang2016}, artistic style~\cite{Gatys2015}, and image generation~\cite{Gregor2014}.

The proficiency of LSTMs for sequence modelling is demonstrated in~\cite{Sutskever2014}. \citeauthor{Sutskever2014} apply two LSTM networks to translate first a sequence into a fixed length vector, then to decode the vector into an output sequence. This architecture achieves state-of-the-art performance in machine translation. The authors find that reversing the order of the input sequences markedly improves translation performance by introducing new short term dependencies between input and output sequences. Such sequence transformations should be considered for the purpose of program generation.

The application of language modelling for generating executable programs is novel. In training on large corpuses of hand-written code, the language model learns the human biases which are present in common codes~\cite{Caliskan-islam2016}. While such human-induced biases can prove controversial in social domains~\cite{Bolukbasi2016,Joseph2017}, this enables the generation of programs which, unlike other approaches to program generation, are representative of real workloads.

Neural networks are computationally expensive, though their implementations can be generic and parallelised. Library implementations are available in Torch~\cite{Collobert2011}, Caffe~\cite{Jia2014}, and TensorFlow~\cite{Abadi}. The increasing size and depth of computation graphs in deep learning has challenged the ability to compute results in reasonable time. Possible methods for reducing computational overhead involve fusing operations across layers in the graph using domain specific languages~\cite{Truong2016,Ashari2015a,Potter2015}; decoupling interfaces between layers using small networks to synthesise learning gradients during training~\cite{Jaderberg2016a}; and specialising hardware for computing data parallel workloads using FPGAs and ASICs~\cite{Misra2010}.


% \subsubsection{Software engineering}
Machine learning has been applied to source code to aid software engineering.  Naturalize employs techniques developed in the natural language processing domain to model coding conventions~\cite{Allamanis2014a}. JSNice leverages probabilistic graphical models to predict program properties such as identifier names for JavaScript~\cite{Raychev}. \citeauthor{Zhang2015a} use deep learning to generate example code for APIs as responses to natural language queries~\cite{Zhang2015a}. \citeauthor{Allamanis2016} use attentional neural networks to generate summaries of source code~\cite{Allamanis2016}. \citeauthor{Wong2013} mines Q\&A site StackOverflow to automatically generate code comments~\cite{Wong2013}. \citeauthor{Raychev2014} use statistical models to provide contextual code completion~\cite{Raychev2014}.

There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyse software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}; however, no work so far has exploited mined source code for program generation.


% \subsection{Iterative compilation}

Iterative compilation is the method of performance tuning in which a program is compiled and profiled using multiple different configurations of optimisations in order to find the configuration which maximises performance. One of the the first formalised publications of the technique appeared in \citeyear{Bodin1998} by \citeauthor{Bodin1998}~\cite{din1998}.  Iterative compilation has since been demonstrated to be a highly effective form of empirical performance tuning for selecting compiler optimisations.

An enumeration of the optimisation space of Intel Thread Building Blocks in~\cite{Contreras2008} shows that runtime knowledge of the available parallel hardware can have a significant impact on program performance. \citeauthor{Collins2012} exploit this in~\cite{Collins2012}, first using Principle Components Analysis to reduce the dimensionality of the optimisation space, followed by a search of parameter values to improve program performance by a factor of $1.6\times$ over values chosen by a human expert. In~\cite{Collins2013}, they extend this using static feature extraction and nearest neighbour classification to further prune the search space, achieving an average 89\% of the oracle performance after evaluating 45 parameters.

Frameworks for iterative compilation offer mechanisms for abstracting the iterative compilation process from the optimisation space. \emph{OpenTuner} presents a generic framework for optimisation space exploration~\cite{Ansel2013}. \emph{CLTune} is a generic auto-tuner for OpenCL kernels~\cite{Nugteren2015}. Both frameworks implement \emph{search}, however, the huge number of possible compiler optimisations makes such a search expensive to perform for every new configuration of program, architecture and dataset.

% \subsubsection{Machine learning}
Machine learning has been used to guide iterative compilation and predict optimisations for code. \citeauthor{Stephenson2003} use ``meta optimisation'' to tune compiler heuristics through an evolutionary algorithm to automate the search of the optimisation space~\cite{Stephenson2003}. \citeauthor{Fursin2011} continued this with Milepost GCC, the first machine learning-enabled self-tuning compiler~\cite{Fursin2011}. A survey of machine learning heuristics quality concludes that the automatic \emph{generation} of self-tuning heuristics is an ongoing research challenge that offers the greatest generalisation benefits~\cite{Burke2013}.

\citeauthor{Dastgeer2011} developed a machine learning based auto-tuner for the SkePU skeleton library in~\cite{Dastgeer2011}. Training data is used to predict the optimal execution device (i.e.\ CPU, GPU) for a given program by predicting execution time and memory copy overhead based on problem size. The auto-tuner only supports vector operations, and there is limited cross-architecture evaluation. In~\cite{Dastgeer2015a}, the authors extend SkePU to improve the data consistency and transfer overhead of container types, reporting up to a $33.4\times$ speedup over the previous implementation.

\citeauthor{Ogilvie2015} use active learning to reduce the cost of iterative compilation by searching for points in the optimisation space which are close to decision boundaries~\cite{Ogilvie2015}. This reduces the cost of training compared to a random search. \citeauthor{Wahib2015a} use machine learning to automate the selection of GPU kernel transformations~\cite{Wahib2015a}.

PetaBricks is a language and compiler for algorithmic choice~\cite{Ansel2009a}. Users provide multiple implementations of algorithms, optimised for different parameters or use cases. This creates a search space of possible execution paths for a given program. This has been combined with autotuning techniques for enabling optimised multigrid programs~\cite{Chan2009}, with the wider ambition that these autotuning techniques may be applied to all algorithmic choice programs~\cite{Ansel2014}. While this helps produce efficient programs, it places a great burden on the developer, requiring them to provide enough contrasting implementations to make a search of the optimisation space fruitful.

In~\cite{Saclay2010,Memon2013,Fursin2014}, \citeauthor{Fursin2014} advocate a ``big data'' driven approach to autotuning, arguing that the challenges facing widespread adoption of autotuning and machine learning methodologies can be attributed to: a lack of common, diverse benchmarks and data sets; a lack of common experimental methodology; problems with continuously changing hardware and software stacks; and the difficulty to validate techniques due to a lack of sharing in publications. They propose a system for addressing these concerns, the Collective Mind knowledge system, which provides a modular infrastructure for sharing autotuning performance data and related artefacts across the internet.


% \subsubsection{Dynamic optimisers}
Iterative compilation typically involves searching the optimisation space offline --- dynamic optimisers perform this optimisation space exploration at runtime, allowing optimisations tailored to dynamic feature values. This is a challenging task, as a random search of an optimisation space may result in many configurations with sub-optimal performance. In a real world system, evaluating many sub-optimal configurations will cause significant slowdowns to a program. A resulting requirement of dynamic optimisers is that convergence time towards optimal parameters is minimised.

Existing dynamic optimisation research has typically taken a low level approach to performing optimisations. Dynamo is a dynamic optimiser which performs binary level transformations of programs using information gathered from runtime profiling and tracing~\cite{Bala2000}. While this provides the ability to respond to dynamic features, it restricts the range of optimisations that can be applied to binary transformations. These low level transformations cannot match the performance gains that higher level parameter tuning produces.

% \subsubsection{Superoptimisers}
In~\cite{Massalin1987}, the smallest possible program which performs a specific function is found through an iterative enumeration of the entire instruction set. Starting with a program of a single instruction, the superoptimiser tests to see if any possible instruction passes a set of conformity tests. If not, the program length is increased by a single instruction and the process repeats. The exponential growth in the size of the search space is far too expensive for all but the smallest of hot paths, typically less than 13 instructions. The optimiser is limited to register to register memory transfers, with no support for pointers, a limitation which is addressed in~\cite{Joshi2002}. Denali is a superoptimiser which uses constraint satisfaction and rewrite rules to generate programs which are \emph{provably} optimal, instead of searching for the optimal configuration through empirical testing. Denali first translates a low level machine code into guarded multi-assignment form, then uses a matching algorithm to build a graph of all of a set of logical axioms which match parts of the graph, before using boolean satisfiability to disprove the conjecture that a program cannot be written in $n$ instructions. If the conjecture cannot be disproved, the size of $n$ is increased and the process repeats.

% \subsubsection{GPUs}
Performant GPGPU programs require careful attention from the developer to properly manage data layout in DRAM, caching, diverging control flow, and thread communication. The performance of programs depends heavily on fully utilising zero-overhead thread scheduling, memory bandwidth, and thread grouping. \citeauthor{Ryoo2008a} illustrate the importance of these factors by demonstrating speedups of up to $432\times$ for matrix multiplication in CUDA by appropriate use of tiling and loop unrolling~\cite{Ryoo2008a}. The importance of proper exploitation of local shared memory and synchronisation costs is explored in~\cite{Lee2010}.

In~\cite{Chen2014}, data locality optimisations are automated using a description of the hardware and a memory-placement-agnostic compiler. The authors demonstrate speedups of up to $2.08\times$, although at the cost of requiring accurate memory hierarchy descriptor files for all targeted hardware. The descriptor files must be hand generated, requiring expert knowledge of the underlying hardware in order to properly exploit memory locality.

Data locality for nested parallel patterns is explored in~\cite{Lee}. The authors use an automatic mapping strategy for nested parallel skeletons on GPUs, which uses a custom intermediate representation and a CUDA code generator, achieving $1.24\times$ speedup over hand optimised code on 7 of 8 Rodinia benchmarks.

Reduction of the GPGPU optimisation space is demonstrated in~\cite{Ryoo2008}, using the common subset of optimal configurations across a set of training examples. This technique reduces the search space by 98\%, although it does not guarantee that for a new program, the reduced search space will include the optimal configuration.

\citeauthor{Magni2014} demonstrated that thread coarsening of OpenCL kernels can lead to speedups in program performance between $1.11\times$ and $1.33\times$ in~\cite{Magni2014}. The authors achieve this using a machine learning model to predict optimal thread coarsening factors based on the static features of kernels, and an LLVM function pass to perform the required code transformations.

A framework for the automatic generation of OpenCL kernels from high-level programming concepts is described in~\cite{Steuwer2015}. A set of rewrite rules is used to transform high-level expressions to OpenCL code, creating a space of possible implementations. This approach is ideologically similar to that of PetaBricks, in that optimisations are made through algorithmic choice, although in this case the transformations are performed automatically at the compiler level. The authors report performance on a par with that of hand written OpenCL kernels.

% \subsubsection{Stencils}
Stencil codes have a variety of computationally expensive uses from fluid dynamics to quantum mechanics. Efficient, tuned stencil kernels are highly sought after, with early work in \citeyear{Bolz2003} by \citeauthor{Bolz2003} demonstrating the capability of GPUs for massively parallel stencil operations~\cite{Bolz2003}. In the resulting years, stencil codes have received much attention from the performance tuning research community.

\citeauthor{Ganapathi2009} demonstrated early attempts at autotuning multi-core stencil codes in~\cite{Ganapathi2009}, drawing upon the successes of statistical machine learning techniques in the compiler community. They present an auto-tuner which can achieve performance up to 18\% better than that of a human expert. From a space of 10 million configurations, they evaluate the performance of a randomly selected 1500 combinations, using Kernel Canonical Correlation Analysis to build correlations between tunable parameter values and measured performance targets. Performance targets are L1 cache misses, TLB misses, cycles per thread, and power consumption. The use of KCAA restricts the scalability of their system as the complexity of model building grows exponentially with the number of features. In their evaluation, the system requires two hours of compute time to build the KCAA model for only 400 seconds of benchmark data. They present a compelling argument for the use of energy efficiency as an optimisation target in addition to runtime, citing that it was the power wall that lead to the multi-core revolution in the first place. Their choice of only 2 benchmarks and 2 platforms makes the evaluation of their auto-tuner somewhat limited.

\citeauthor{Berkeley2009} targeted 3D stencils code performance in~\cite{Berkeley2009}. Stencils are decomposed into core blocks, sufficiently small to avoid last level cache capacity misses. These are then further decomposed to thread blocks, designed to exploit common locality threads may have within a shared cache or local memory. Thread blocks are divided into register blocks to take advantage of data level parallelism provided by the available registers. Data allocation is optimised on NUMA systems. The performance evaluation considers speedups of various optimisations with and without consideration for host/device transfer overhead.

\citeauthor{Kamil2010} present an autotuning framework in~\cite{Kamil2010} which accepts as input a Fortran 95 stencil expression and generates tuned shared-memory parallel implementations in Fortan, C, or CUDA. The system uses an IR to explore autotuning transformations, enumerating a subset of the optimisation space and recording only a single execution time for each configuration, reporting the fastest. They demonstrate their system on 4 architectures using 3 benchmarks, with speedups of up to $22\times$ compared to serial implementations. The CUDA code generator does not optimise for the GPU memory hierarchy, using only global memory. As demonstrated in this thesis, improper utilisation of local memory can hinder program performance by two orders of magnitude. There is no directed search or cross-program learning.

In~\cite{Zhang2013a}, \citeauthor{Zhang2013a} present a code generator and auto-tuner for 3D Jacobi stencil codes. Using a DSL to express kernel functions, the code generator performs substitution from one of two CUDA templates to create programs for execution on GPUs. GPU programs are parameterised and tuned for block size, block dimensions, and whether input data is stored in read only texture memory. This creates an optimisation space of up to 200 configurations. In an evaluation of 4 benchmarks, the authors report performance that is comparable with previous implementations of iterative Jacobi stencils on GPUs~\cite{Holewinski2012, Phillips2010}. The dominating parameter is shown to be block dimensions, followed by block size, then read only memory. The DSL presented in the paper is limited to expressing only Jacobi Stencils applications. Their auto-tuner requires a full enumeration of the parameter space for each program, which may be impractical for the needs of general purpose stencil computing. Previous work (Appendix~\ref{app:adapt}) overcomes this drawback by learning parameter values which transfer to unseen stencils, without the need for an expensive tuning phase for each program and architecture.

In~\cite{Christen2011}, \citeauthor{Christen2011} presents a DSL for expressing stencil codes, a C code generator, and an auto-tuner for exploring the optimisation space of blocking and vectorisation strategies. The DSL supports stencil operations on arbitrarily high-dimensional grids. The auto-tuner performs either an exhaustive, multi-run Powell search, Nelder Mead, or evolutionary search to find optimal parameter values. They evaluate their system on two CPUS and one GPU using 6 benchmarks. The authors do not present a ratio of the available performance that their system achieves, or how the performance of optimisations vary across benchmarks or devices.

A stencil grid can be decomposed into smaller subsections so that multiple GPUs can operate on each subsection independently. This requires a small overlapping region where each subsection meets --- the halo region --- to be shared between devices. For iterative stencils, values in the halo region must be synchronised between devices after each iteration, leading to costly communication overheads. One possible optimisation is to deliberately increase the size of the halo region, allowing each device to compute updated values for the halo region, instead of requiring a synchronisation of shared state. This reduces the communication costs between GPUs, at the expense of introducing redundant computation. Tuning the size of this halo region is the goal of PARTANS~\cite{Lutz2013}, an autotuning framework for multi-GPU stencil computations. \citeauthor{Lutz2013} explore the effect of varying the size of the halo regions using six benchmark applications, finding that the optimal halo size depends on the size of the grid, the number of partitions, and the connection mechanism (i.e.\ PCI express). The authors present an auto-tuner which determines problem decomposition and swapping strategy offline, and performs an online search for the optimal halo size. The selection of overlapping halo region size compliments the selection of work-group size which is the subject of previous work (Appendix~\ref{app:adapt}).

In applications of machine learning for iterative compilation, a limiting factor of the effectiveness of learned models is the number of benchmarks used. The development of automatic program generation alleviates this problem by allowing an unbounded number of programs to enumerate the feature space at an increasingly granular scale.


% \subsection{Learning Representations}

Recently, deep neural networks~\cite{LeCun2015} have been shown to be a powerful tool for feature engineering in various tasks including image recognition~\cite{Krizhevsky2012,He2016} and audio processing~\cite{Lee2009b}. In the field of compiler optimisation, no work so far has applied deep neural networks for program feature generation and selection. Our work is the first to do so.

% \subsection{Generative Modeling}

Generative models for video~\cite{Srivastava2015}.

%% Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu, Y. (2016). Exploring the Limits of Language Modeling. arXiv:1602.02410. Retrieved from http://arxiv.org/abs/1602.02410%5Cnhttp://www.arxiv.org/pdf/1602.02410.pdf
Limits of language modelling~\cite{Jozefowicz2016a}.

%% Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. In NIPS.
Sequence to sequence learning, NIPS paper~\cite{Sutskever2014}.

%% Wang, H., Raj, B., & Xing, E. P. (2017). On the Origin of Deep Learning, 1–81. Retrieved from http://arxiv.org/abs/1702.07800
Deep Learning is an emerging branch of machine learning, offering breakthrough domains in a number of domains~\cite{Wang2017}.

Automatic inference from raw inputs in other fields. Review of representation learning~\cite{Bengio2013}. Object detection, image classification, text classification~\cite{Conneau2016}, and even to the optimisations themselves~\cite{Andrychowicz2016a}. We are the first to apply this approach to compilers.

End to end training of robotic control policies~\cite{Levine2016}.



% \subsection{Transfer Learning}

%% Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). CNN Features off-the-shelf: an Astounding Baseline for Recognition. In CVPRW. https://doi.org/10.1109/CVPRW.2014.131
Exploring relevance of inferred features in model hidden layers~\cite{Yosinski2014}. Transfer learning is common in other domains, e.g. vision~\cite{Razavian2014,Oquab2014}. Often using the hidden layers as fixed feature extractors (by using the output of one sub-model as input to a new model), rather than using the same model structure and transferring weights (as we did).

% Terence, P., & Vinju, J. (2016). Towards a Universal Code Formatter through Machine Learning. In SLE.
\todo[inline]{ML code formatter~\cite{Terence2016}.}

% Allamanis, M. (2018). The Adverse Effects of Code Duplication in Machine Learning Models of Code. ArXiv:1812.06469. Retrieved from https://github.com/Microsoft/dpu-utils.
\todo[inline]{Code duplication~\cite{Allamanis}.}

% Chen, Z., Kommrusch, S., Tufano, M., Pouchet, L., Poshyvanyk, D., & Monperrus, M. (2018). SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair. ArXiv:1901.01808. Retrieved from http://arxiv.org/abs/1901.01808
\todo[inline]{Program repair~\cite{Chen2018}.}

% Hata, H., Shihab, E., & Neubig, G. (n.d.). Learning to Generate Corrective Patches using Neural Machine Translation. ArXiv:1812.07170.
\todo[inline]{Corrective patches~\cite{Hata}.}

\subsection{Embeddings}

% Ben-nun, T., Jakobovits, A. S., & Hoefler, T. (2018). Neural Code Comprehension: A Learnable Representation of Code Semantics. In NeurIPS.
\todo[inline]{Neural Code Comprehension~\cite{Ben-nun2018} builds on the experiments in Chapter~\ref{chap:deeptune} of this thesis, using a novel XFG representation. Assembled from LLVM bytecode so language portable.}

% Yin, P., Neubig, G., Allamanis, M., Brockschmidt, M., & Gaunt, A. L. (2018). Learning to Represent Edits. ArXiv:1810.13337. https://doi.org/10.1080/02688697.2016.1244252
\todo[inline]{Learning to represent edits and diffs~\cite{Yin2018}~\cite{Tufano2019}.}

% Wang, K., Singh, R., & Su, Z. (2017). Dynamic Neural Program Embeddings for Program Repair. ArXiv:1711.07163.
\todo[inline]{Program embeddings~\cite{Wang2017d}.}

% Henkel, J., Lahiri, S. K., Liblit, B., & Reps, T. (2018). Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces. In FSE. https://doi.org/10.1145/3236024.3236085
\todo[inline]{Embeddings from AST~\cite{Henkel2018}.}

\subsection{Testing}

% Liu, X., Li, X., Prajapati, R., & Wu, D. (2019). DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing. In AAAI.
\todo[inline]{DeepFuzz~\cite{Liu2019}.}

% Nasrabadi, M. Z., Parsa, S., & Kalaee, A. (2018). Neural Fuzzing: A Neural Approach to Generate Test Data for File Format Fuzzing. ArXiv:1812.09961. Retrieved from https://arxiv.org/pdf/1812.09961.pdf
\todo[inline]{Fuzzing file formats~\cite{Nasrabadi}.}

\subsection{Mining Source Code}

There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyse software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}, for example code generation~\cite{Zhang2015a}, code summarisation~\cite{Allamanis2016}, comment generation~\cite{Wong2013}, and code completion~\cite{Raychev2014}. However, no work so far has exploited mined source code for benchmark generation. This work is the first to do so.

In recent years, machine learning techniques have been employed to model and learn from program source code on various tasks. These include mining coding conventions~\cite{Allamanis2014a} and idioms~\cite{Allamanis2014}, API example code~\cite{Zhang2015a} and pseudo-code generation~\cite{Oda2015}, and benchmark generation~\cite{Cummins2017a}. Our work is the first attempt to extend the already challenging task of modelling distributions over source code to learning distributions over source code with respect to code optimisations.

In a recent work~\cite{Bunel2017a}, Bunel \emph{et al.} formulate code super-optimisation as a stochastic search problem to learn the distribution of different code transformations and expected performance improvement. As acknowledged by the authors, their approach can be improved by having temporal information of the code structures.
