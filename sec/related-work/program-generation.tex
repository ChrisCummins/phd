\section{Program Generation}
\label{sec:related-work-program-generation}

Goal-directed program generators have been used for a variety of domains, including generating linear transforms~\cite{Voronenko2009}, MapReduce programs~\cite{Smith}, and data structure implementations~\cite{Loncaric2016}.

% PROBABILISTIC PROGRAMMING

% Bielik, P., Raychev, V., & Vechev, M. (2016). PHOG: Probabilistic Model for Code. In ICML. https://doi.org/10.1126/science.7.179.764
\todo[inline]{Probabilistic model for code~\cite{Bielik2016}.}

% Gaunt, A. L., Kushman, N., Brockschmidt, M., Kohli, P., Tarlow, D., Singh, R., & Taylor, J. (2016). TerpreT: A Probabilistic Programming Language for Program Induction. ArXiv:1608.04428.
\todo[inline]{Probabilistic programming~\cite{Gaunt2016}.}

% Toffola, L. D., Pradel, M., & Gross, T. R. (2018). Synthesizing programs that expose performance bottlenecks. In CGO. Retrieved from https://dl.acm.org/citation.cfm?id=3168830
\todo[inline]{Synthesis to expose performance bottlenecks~\cite{Toffola2018}.}


\subsection{Compiler Validation}

The random generation of test cases is a well established approach to the compiler validation problem. Prior approaches are surveyed in~\cite{Kossatchev2005,Boujarwah1997} and empirically contrasted in~\cite{Chen2014a}. The main question of interest is in how to efficiently generate codes which trigger bugs. There are two main approaches: \emph{program generation}, where inputs are synthesised from scratch; and \emph{program mutation}, where existing codes are modified so as to identify anomalous behaviour.


\subsubsection{Differential Testing}

In the foundational work on differential testing for compilers, McKeeman \emph{et al.\ }present generators capable of enumerating programs of a range of qualities, from random ASCII sequences to C model conforming programs~\cite{McKeeman1998}. Subsequent works have presented increasingly complex generators which improve in some metric of interest, generally expressiveness or probability of correctness. CSmith~\cite{Yang2011} is a widely known and effective generator which enumerates programs by pairing infrequently combined language features. In doing so, it produces correct programs with clearly defined behaviour but very unlikely functionality, increasing the chances of triggering a bug. Achieving this required extensive engineering work, most of it not portable across languages, and ignoring some language features. Subsequent generators influenced by CSmith, like Orange3~\cite{Nagai2013}, focus on features and bug types beyond the scope of CSmith, arithmetic bugs in the case of Orange3. Glade~\cite{Bastani2017} derives a grammar from a corpus of example programs. The derived grammar is enumerated to produce new programs, though unlike our approach, no distribution is learned over the grammar; program enumeration is uniformly random.

Random program generation is an effective method for software testing. Grammar-based \emph{fuzz testers} have been developed for C~\cite{Yang2012} and OpenCL~\cite{Lidbury2015a}. Programs generated by grammar-based approaches are often  unlike real handwritten code, and are typically very large. As such, once a bug has been identified, test case reduction~\cite{Regehr2012a} is required to minimise the size of the program and isolate the code of interest. A mutation-based approach for differential testing the Java Virtual Machine is demonstrated in~\cite{Chena}.

% She, D., Pei, K., Epstein, D., Yang, J., Ray, B., & Jana, S. (2018). NEUZZ: Efficient Fuzzing with Neural Program Learning. ArXiv:1807.05620.
\todo[inline]{Fuzzing through ML~\cite{She2018}.}

% Mathis, B., Kampmann, A., Gopinath, R., HÃ¶schele, M., Mera, M., & Zeller, A. (2019). Parser-Directed Fuzzing. In PLDI.
\todo[inline]{PLDI'19 fuzzing paper~\cite{Mathis2019}.}

\subsubsection{Program Mutation}

Equivalence Modulo Inputs (EMI) testing~\cite{Le2013a,Sun2016a} follows a different approach to test case generation. Starting with existing code, it inserts or deletes statements that will not be executed, so functionality should remain the same. If it is affected, it is due to a compiler bug. While a powerful technique able to find hard to detect bugs, it relies on having a very large number of programs to mutate. As such, it still requires an external code generator. Similarly to CSmith, EMI favours very long test programs. LangFuzz~\cite{Holler2012} also uses mutation but does this by inserting code segments which have previously exposed bugs. This increases the chances of discovering vulnerabilities in scripting language engines. Skeletal program enumeration~\cite{Zhang2017a} again works by transforming existing code. It identifies algorithmic patterns in short pieces of code and enumerates all the possible permutations of variable usage. Compared to all these, our fuzzing approach is low cost, easy to develop, portable, capable of detecting a wide range of errors, and focusing by design on bugs that are more likely to be encountered in a production scenario.

\subsubsection{Machine Learning}

There is an increasing interest in applying machine learning to software
testing. Most similar to our work is Learn\&fuzz~\cite{Godefroid2017}, in which
an LSTM network is trained over a corpus of PDF files to generate test inputs
for the Microsoft Edge renderer, yielding one bug. Unlike compiler testing, PDF
test cases require no inputs and no pre-processing of the training corpus.
Skyfire~\cite{Wang2017c} learns a probabilistic context-sensitive grammar over a
corpus of programs to generate input seeds for mutation testing. The generated
seeds are shown to improve the code coverage of AFL~\cite{Zalewski} when fuzzing
XSLT and XML engines, though the seeds are not directly used as test cases.
Machine learning has also been applied to other areas such as improving bug
finding static analysers~\cite{Heo2017,Koc2017}, repairing
programs~\cite{Koukoutos2017a,White}, prioritising test
programs~\cite{Chen2017}, identifying buffer overruns~\cite{Choi2016}, and
processing bug reports~\cite{Lam2016,Huo2016}. To the best of our knowledge, no
work so far has succeeded in finding compiler bugs by exploiting the learned
syntax of mined source code for test case generation. Ours is the first to do
so.

\subsection{Performance Characterization}
\label{subsec:training-with-synthetic-benchmarks}

\emph{GENESIS}~\cite{Chiu2015} is a language for generating synthetic training programs. The essence of the approach is to construct a probabilistic grammar with embedded semantic actions that defines a language of possible programs. New programs may be created by sampling the grammar and, through setting probabilities on the grammar productions, the sampling is biased towards producing programs from one part of the space or another. This technique is potentially completely general, since a grammar can theoretically be constructed to match any desired program domain. However, despite being theoretically possible, it is not easy to construct grammars which are both suitably general and also produce programs that are in any way similar to human written programs. It has been shown to be successful over a highly restricted space of stencil benchmarks with little control flow or program variability~\cite{Garvey2015b,Falch2015,Cummins2015}. But, it is not clear how much effort it will take, or even if it is possible for human experts to define grammars capable of producing human like programs in more complex domains. By contrast, learning from a corpus provides \emph{general-purpose} program generation over unknown domains, in which the statistical distribution of generated programs is automatically inferred from real world code.

% Joshi, A. M., Eeckhout, L., & John, L. K. (2008). The Return of Synthetic Benchmarks. In SPEC Benchmark Workshop.
\todo[inline]{Synthetic benchmarks~\cite{Joshi}.}

% Brockschmidt, M., Allamanis, M., Gaunt, A. L., & Polozov, O. (2018). Generative Code Modeling with Graphs. Retrieved from http://arxiv.org/abs/1805.08490
\todo[inline]{Generative code modelling with graphs~\cite{Brockschmidt2018}.}

The use of synthetic benchmarks for providing empirical performance
evaluations dates back to as early as 1974~\cite{Curnow1976}. The
\emph{automatic generation} of such synthetic benchmarks is a more
recent innovation, serving the purpose initially of stress-testing
increasingly complex software systems for behaviour validation and
automatic bug detection~\cite{Verplaetse2000,Godefroid2008}. A range
of techniques have been developed for these purposes, ranging from
applying random mutations to a known dataset to generate test stimuli,
to so-called ``white-box fuzz testing'' which analyses program traces
to explore the space of a program's control flow. CSmith is one such
tool which generates randomised C source programs for the purpose of
automatically detecting compiler bugs~\cite{Yang2012}.

A method for the automatic generation of synthetic benchmarks for the
purpose of \emph{performance} tuning is presented
in~\cite{Chiu2015}. \citeauthor{Chiu2015} use template substitution
over a user-defined range of values to generate training programs with
a statistically controlled range of features. A Perl preprocessor
generates output source codes from an input description using a custom
input language Genesis. Genesis is more flexible than the system
presented in this thesis, supporting substitution of arbitrary
sources. The authors describe an application of their tool for
generating OpenCL stencil kernels, but do not report any performance
results.

\emph{GENESIS}~\cite{Chiu2015} is a language for generating synthetic training programs. Users annotate template programs with statistical distributions over features, which are instantiated to generate statistically controlled permutations of templates. Template based approaches provide domain-specific solutions for a constrained feature and program space, for example, generating permutations of Stencil codes~\cite{Garvey2015b,Cummins2015a}. Our approach provides \emph{general-purpose} program generation over unknown domains, in which the statistical distribution of generated programs is automatically inferred from real world code.

Random program generation is an effective method for software testing. Grammar-based \emph{fuzz testers} have been developed for C~\cite{Yang2012} and OpenCL~\cite{Lidbury2015a}. A mutation-based approach for the Java Virtual Machine is demonstrated in~\cite{Chena}. Goal-directed program generators have been used for a variety of domains, including generating linear transforms~\cite{Voronenko2009}, MapReduce programs~\cite{Smith}, and data structure implementations~\cite{Loncaric2016}. Program synthesis from input/output examples is used for simple algorithms in~\cite{Zaremba2015a}, string manipulation in~\cite{Gulwani2011}, and geometry constructions in~\cite{Gulwani2012}.

Machine learning has been applied to source code to aid software engineering. Naturalize employs techniques developed in the natural language processing domain to model coding conventions~\cite{Allamanis2014a}. JSNice leverages probabilistic graphical models to predict program properties such as identifier names for JavaScript~\cite{Raychev}.


%\subsection{Parallelism}
%
%Programming languages have taken a variety of approaches in adopting parallelism. In C, a threading model was retrofitted using the \textit{POSIX pthreads} standard~\cite{Sura2005}. \citeauthor{Boehm2005} contends this approach in~\cite{Boehm2005}, describing issues in which a compiler developed independently of threading issues cannot be guaranteed to produce correct code. These issues were circumvented in C++11 by the introduction of a concurrency model~\cite{Boehm2008}, based off the success of the Java memory model~\cite{Bash2015a}. C++ and Java are two of the most popular imperative programming languages, and in both cases, the languages specify memory models which guarantee sequential consistency across threads, subject to some restrictions. A dialect of the Java programming language, Titanium, extends the Java feature set, adding multidimensional arrays, immutable classes, and a PGAS programming model~\cite{Yelick1998}. In contrast, Deterministic Parallel Java adds a type and effect system to in order to provide a provably sound language core~\cite{Bocchino2009}.
%
%
%%
%%% \subsubsection{Limits of static analysis}
%%Sequential consistency in shared-memory parallel programming~\cite{Krishnamurthy1995,Shasha1988,Sura2005}. Sequential consistency and caches~\cite{Goodman}.
%%
%%Sequential consistency for Java incurs a 10\% slowdown~\cite{Sura2005}.
%%
%
%%
%%Parallelism in the modern functional languages: Haskell, Clojure and Erlang~\cite{Pierro2012}.
%%
%%Scala actors~\cite{Haller2009a}, and~\cite{Haller2012}.
%%
%%
%%\subsubsection{Library level parallelism} Optimising MapReduce for multi-core processors~\cite{Kaashoek2010}.
%%\todo{OpenMP, MPI}
%
%
%\subsubsection{Automatic parallelisation} The goal of automatic parallelisation is to transform arbitrary sequential code into parallelised code. This is a well studied subject, with the typical approach being to perform these code transformations at the compilation stage. In \citeauthor{Banerjee1993}'s thorough review~\cite{Banerjee1993} of the subject, they outline the key challenges of automatic parallelisation: determining whether sequential code can be legally transformed for parallel execution; and identifying the transformation which will provide the highest performance improvement for a given piece of code. Both challenges are hard to tackle. For the former, the difficulties lie in performing accurate analysis of code behaviour. Obtaining accurate dynamic dependency analysis at compile time is an unsolved problem, as is resolving pointers and points-to analysis~\cite{Atkin-granville2013, Hind2001,Ghiya2001}.
%
%The result of these challenges is that reliably performant, automatic parallelisation of arbitrary programs remains a far reaching goal; however, there are noteworthy variations on which have been able to achieve some measure of success. One such example is speculative parallelism, which circumvents the issue of having incomplete dependency information by speculatively executing code regions in parallel while performing dependency tests at runtime, with the possibility to fall back to ``safe'' sequential execution if correctness guarantees are not met~\cite{Prabhu2010,Trachsel2010}.  In~\cite{Jimborean2014}, \citeauthor{Jimborean2014} present a system which combines polyhedral transformations of user code with binary algorithmic skeleton implementations for speculative parallelisation, reporting speedups over sequential code of up to $15.62\times$ on a 24 core processor.
%
%Annotation-driven parallelism takes a similar approach. The user annotates their code to provide ``hints'' to the compiler, which can then perform parallelising transformations. A popular example of this is OpenMP, which uses compiler pragmas to mark code sections for parallel or vectorised execution~\cite{Dagum1998}. Previous work has demonstrated code generators for translating OpenMP to OpenCL~\cite{Grewe2013} and CUDA~\cite{Lee2009}. Again, annotation-driven parallelism suffers from placing a burden on the developer to identify the potential areas for parallelism, and lacks the structure that algorithmic skeletons provide.


%
%
%\subsubsection{Debugging}
%General purpose, platform-specific debuggers: Intel Debugger for Linux~\cite{Blair-chappell}.
%
%
%% \subsubsection{Debugging Parallel Frameworks and Libraries}
%
%Allinea DDT~\cite{K2010}, a scalable parallel debugger.
%
%TotalView~\cite{Cownie2000}, an OpenMP debugger.
%
%Jumbune~\cite{ImpetusTechnologies}, an open source debugger for Hadoop.
%
%The parallel pipeline library FlumeJava~\cite{Chambers2010} provides debugging support through regular tools using a sequential execution mode,r and routing of exceptions and output from remote workers back to a central host.
%
%
%% \subsubsection{Debugging GPGPU programs}
%
%CUDA-GDB~\cite{NVIDIA2016}, Intel OpenCL Debugger for Linux
%OS~\cite{IntelCorporation2016}.
%
%Oclgrind~\cite{Price2015}.
%
%GPUverify~\cite{Betts2012}.
%
%
%% \subsubsection{Debugging Algorithmic Skeletons}
%SkIE~\cite{Bacci1999} is based on a coordination language, but provides advanced features such as debugging tools, performance analysis, visualization, and graphical user interface. Instead of directly using the coordination language, programmers interact with a graphical tool, where parallel modules based on skeletons can be composed
%
%SKiPPER~\cite{Serot1999} supports sequential execution for debugging.
%
%
%\subsubsection{Profiling}
%Profiling sequential programs~\cite{Ball1994}.
%
%% \subsubsection{Profiling parallel programs}
%Profiling parallel programs using KOJAK~\cite{Mohr2003}.
%
%


%\subsection{Performance Benchmarking}
%
%Rodinia~\cite{Che2009,Che2010},
%NPB (SNU~\cite{Seo2011}),
%Parboil~\cite{Stratton2012},
%PolyBench~\cite{Grauer-Gray2012},
%SHOC~\cite{Danalis2010}.
%
%Evaluating GPGPU benchmarks~\cite{Ryoo2015}.
%
%Benchmarking parallel computing systems~\cite{Belli2015}.
%
%\subsubsection{Methodology} Use geometric mean for speedups~\cite{Fleming1986}. Statistical rigour\cite{Georges2007}. Execution times and variance~\cite{Box}.
%
%\subsection{Self-Tuning Software}
%



\todo[inline]{Integrate CGO'17 related work section}

Our work lies at the intersections of a number of areas: program generation, benchmark characterisation, and language modelling and learning from source code. There is no existing work which is similar to ours, in respect to learning from large corpuses of source code for benchmark generation.

\emph{GENESIS}~\cite{Chiu2015} is a language for generating synthetic training programs. Users annotate template programs with statistical distributions over features, which are instantiated to generate statistically controlled permutations of templates. Template based approaches provide domain-specific solutions for a constrained feature and program space, for example, generating permutations of Stencil codes~\cite{Garvey2015b,Cummins2015a}. Our approach provides \emph{general-purpose} program generation over unknown domains, in which the statistical distribution of generated programs is automatically inferred from real world code.

Random program generation is an effective method for software testing. Grammar-based \emph{fuzz testers} have been developed for C~\cite{Yang2012} and OpenCL~\cite{Lidbury2015a}. A mutation-based approach for the Java Virtual Machine is demonstrated in~\cite{Chena}. Goal-directed program generators have been used for a variety of domains, including generating linear transforms~\cite{Voronenko2009}, MapReduce programs~\cite{Smith}, and data structure implementations~\cite{Loncaric2016}. Program synthesis from input/output examples is used for simple algorithms in~\cite{Zaremba2015a}, string manipulation in~\cite{Gulwani2011}, and geometry constructions in~\cite{Gulwani2012}.

Machine learning has been applied to source code to aid software engineering. Naturalize employs techniques developed in the natural language processing domain to model coding conventions~\cite{Allamanis2014a}. JSNice leverages probabilistic graphical models to predict program properties such as identifier names for JavaScript~\cite{Raychev}.

There is an increasing interest in mining source code repositories at large scale~\cite{Allamanis2013a,White2015a,Bird2009}. Previous studies have involved data mining of GitHub to analyse software engineering practices~\cite{Wu2014,Guzman2014,Baishakhi2014a,Vasilescu2015}, for example code generation~\cite{Zhang2015a}, code summarisation~\cite{Allamanis2016}, comment generation~\cite{Wong2013}, and code completion~\cite{Raychev2014}. However, no work so far has exploited mined source code for benchmark generation. This work is the first to do so.


\todo[inline]{Integrate PACT'17 Related work section}


Machine learning has emerged as a viable means in automatically constructing heuristics for code optimisation~\cite{Wang2010,Kulkarni2012,Muralidharan2016,Ogilvie2017,Ren,Cummins2016}. Its great advantage is that it can adapt to changing hardware platforms as it has no a priori assumptions about their behaviour. The success of machine learning based code optimisation has required having a set of high-quality features that can capture the important characteristics of the target program. Given that there is an infinite number of these potential features, finding the right set of features is a non-trivial, time-consuming task.

Various forms of program features have been used in compiler-based machine learning. These include static code structures~\cite{Jiang2010} and runtime information such as system load~\cite{Wen2015} and performance counters~\cite{Dubach2009}. In compiler research, the feature sets used for predictive models are often provided without explanation and rarely is the quality of those features evaluated. More commonly, an initial large, high dimensional candidate feature space is pruned via feature selection~\cite{Stephenson2005,Taylor}, or projected into a lower dimensional space~\cite{Collins2013,Dubach2007}. FEAST employs a range of existing feature selection methods to select useful candidate features~\cite{Ting2016}. Unlike these approaches, DeepTune extracts features and reduces the dimensionality of the feature space completely internally and without expert guidance.

Park \emph{et al.} present a unique graph-based approach for feature representations~\cite{Park2012}. They use a Support Vector Machine where the kernel is based on a graph similarity metric. Their technique still requires hand coded features at the basic block level, but thereafter, graph similarity against each of the training programs takes the place of global features. Being a kernel method, it requires that training data graphs be shipped with the compiler, which may not scale as the size of the training data grows with the number of instances, and some training programs may be very large. Finally, their graph matching metric is expensive, requiring $O(n^3)$ to compare against each training example. By contrast, our method does not need any hand built static code features, and the deployment memory footprint is constant and prediction time is linear in the length of the program, regardless of the size of the training set.

A few methods have been proposed to automatically generate features from the compiler's intermediate representation~\cite{Namolaru2010a,Leather2014}. These approaches closely tie the implementation of the predictive model to the compiler IR, which means changes to the IR will require modifications to the model. The work of \cite{Leather2014} uses genetic programming to search for features, and required a huge grammar to be written, some 160kB in length. Although much of this can be created from templates, selecting the right range of capabilities and search space bias is non trivial and up to the expert. The work of \cite{Namolaru2010a} expresses the space of features via logic programming over relations that represent information from the IRs. It greedily searches for expressions that represent good features. However, their approach relies on expert selected relations, combinators and constraints to work. For both approaches, the search time may be significant.

Cavazos \emph{et al.\ }present a reaction-based predictive model for software-hardware co-design~\cite{Cavazos2006}. Their approach profiles the target program using several carefully selected compiler options to see how program runtime changes under these options for a given micro-architecture setting. They then use the program ``reactions'' to predict the best available application speedup. While their approach does not use static code features, developers must carefully select a few settings from a large number of candidate options for profiling, because poorly chosen options can significantly affect the quality of the model. Moreover, the program must be run several times before optimisation,while our technique does not require the program to be profiled.

In recent years, machine learning techniques have been employed to model and learn from program source code on various tasks. These include mining coding conventions~\cite{Allamanis2014a} and idioms~\cite{Allamanis2014}, API example code~\cite{Zhang2015a} and pseudo-code generation~\cite{Oda2015}, and benchmark generation~\cite{Cummins2017a}. Our work is the first attempt to extend the already challenging task of modelling distributions over source code to learning distributions over source code with respect to code optimisations.

Recently, deep neural networks have been shown to be a powerful tool for feature engineering in various tasks including image recognition~\cite{Krizhevsky2012,He2016} and audio processing~\cite{Lee2009b}. No work so far has applied deep neural networks for program feature generation. Our work is the first to do so.
