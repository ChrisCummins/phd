\section{Program Optimisation}
\label{sec:related-work-optimisation}

Modern compilers are complex, typically containing dozens or hundreds of independent optimisation passes. Determining which optimisation passes to apply, and in what order, is a challenge that depends on a variety of factors from the properties of the program being compiled to the target hardware. Current state-of-practise is for compilers to use a fixed ordering of optimisations, and for each optimisation to contain a heuristic to determine when to use it and with what parameters. Such heuristics require expert design at the expense of great effort and compiler expertise. Still, they rarely are capable of extracting all of the available performance.

% Georgiou, K., Blackmore, C., Xavier-de-Souza, S., & Eder, K. (2018). Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption. In SCOPES.
Extracting maximum performance in a binary is not simply a case of enabling more optimisations, but in identifying which, out of a set of candidate optimisations, will provide the best performance for the current case. A recent study by \citeauthor{Georgiou2018}~\cite{Georgiou2018} illustrates the scale of the challenge. Using two modern releases of the industry-standard LLVM compiler, they obtain an average 3.9\% performance improvement across 71 benchmarks on embedded processors by selectively \emph{disabling} optimisations enabled at the standard \texttt{-O2} optimisation level.

% Ryoo, S., Rodrigues, C. I., Baghsorkhi, S. S., Stone, S. S., Kirk, D. B., & Hwu, W. W. (2008). Optimization principles and application performance evaluation of a multithreaded GPU using CUDA. In PPoPP.
Selecting the right optimisations is critical. In some domains, the margin of performance to be gained is significant. For example, \citeauthor{Ryoo2008a}~\cite{Ryoo2008a} find speedups of up to $432\times$ through the appropriate selection and use of tiling and loop unrolling optimisations on a GPU matrix multiplication implementation.

Given the challenges of heuristics and analytical methods for extracting performance, researchers have turned to empirical methods, reviewed in this section.


\subsection{Iterative Compilation and Auto-tuning}

Iterative compilation is the method of performance tuning in which a program is compiled and profiled using multiple optimisation configurations to find the configuration which maximises performance. Unlike analytical methods which attempt to predict the parameters that produce good performance, iterative compilation is empirical. A set of candidate configurations are selected, and for each, the program is compiled and profiled. The configuration that minimises the value of a suitable cost function (such as runtime) is selected. Pioneered by \citeauthor{Bodin1998}, the technique was initially demonstrated to find good configurations in the non-linear three-dimensional optimisation space of a matrix multiplication benchmark~\cite{Bodin1998}. By exhaustively enumerating the optimisation space they were able to find the global minima of the cost function; however, the authors state that in practise this may not be possible. In cases where an exhaustive enumeration of the optimisation space is infeasible, the process can be cast as a search problem.

While conceptually simple, the empirical nature of iterative compilation yields good results. Iterative compilation has since been demonstrated to be a highly effective form of empirical performance tuning for selecting compiler optimisations.
% Chen, Y., Huang, Y., Eeckhout, L., Fursin, G., Peng, L., Temam, O., & Wu, C. (2010). Evaluating Iterative Optimization Across 1000 Data Sets. In PLDI.
In a large scale evaluation across 1000 data sets, \citeauthor{Chen2010}~\cite{Chen2010} found iterative compilation to yield speedups in GCC over the highest optimisation level (\texttt{-O3}) of up $2.23\times$.

The greatest challenge of iterative compilation is the exponential blow up of optimisation space size with the addition of independent optimisations. The hundreds of discrete optimising transformations found in modern compilers renders an exhaustive search of the optimisation space infeasible. This has driven the development of methods for reducing the cost of evaluating configurations. These methods reduce evaluation costs either by pruning the size of the optimisation space and performing a random or exhaustive enumeration, or by guiding a directed search to traverse the optimisation space while evaluating fewer points.


\subsubsection{Pruning the Iterative Compilation Search Space}

% Triantafyllis, S., & August, D. I. (2003). Compiler Optimization-Space Exploration. In CGO. IEEE.
\citeauthor{Triantafyllis2003}~\cite{Triantafyllis2003} propose using feedback during the evaluation of configurations to prune the optimisation space. This is combined with a fast static performance estimator to obviate the need to run each configuration of a program. \citeauthor{Pan2006}~\cite{Pan2006} formalise the iterative compilation problem as: given a set of compiler optimisation options $\left\{ F_1, F_2, \ldots, F_n \right\}$, find the combination that minimises the program execution time efficiently, without \emph{a priori} knowledge of the optimisations and their interactions. Their technique, \emph{Combined Elimination}, iteratively prunes the search space, reducing the tuning time to 57\% of the closest alternative. Posing the problem as a subset search negates the challenge of optimisation \emph{ordering}, though this challenge has been the focus of other work~\cite{Kulkarni2012,Purini2013}.

% Ryoo, S., Rodrigues, C. I., Stone, S. S., Baghsorkhi, S. S., Ueng, S., Stratton, J. A., & Hwu, W. W. (2008). Program optimization space pruning for a multithreaded GPU. In CGO. IEEE. https://doi.org/10.1145/1356058.1356084
\citeauthor{Ryoo2008} prune the optimisation space for GPGPU workloads using the common subset of optimal configurations across a set of training examples. This technique reduces the search space by 98\%~\cite{Ryoo2008}. There is no guarantee that for a new program, the reduced search space will include the optimal configuration.

% Purini, S., & Jain, L. (2013). Finding Good Optimization Sequences Covering Program Space. TACO.
\citeauthor{Purini2013} prune the \emph{solution} space for optimisation orderings. Rather than attempting to find a universally optimal sequence of optimisations, they identify a \emph{set} of good optimisation sequences that is small enough for each new program to be tried with all sequences in the set. They find that a sequence set size of 10 yields 13\% speedups on PolyBench and MiBench programs~\cite{Purini2013}. Although this does not reduce the cost of finding the set of good sequences, the process need only be performed once per platform, so the cost may be amortised by reusing the same sequence set.

Frameworks for iterative compilation offer mechanisms to abstract the iterative compilation process from the optimisation space. This can lower the cost of adopting iterative compilation techniques by reusing the logic to search optimisation spaces. Examples include \emph{OpenTuner}~\cite{Ansel2013} which provides ensemble search techniques and \emph{CLTune}~\cite{Nugteren2015} for tuning OpenCL kernels.

A complementary approach to search space pruning is knowledge sharing. The idea is that, since most software is shared across many users, leverage this by sharing knowledge of the optimisation space between users, rather than having each redundantly perform their own exploration of the optimisation space from scratch. Such ``big data'' approaches to auto-tuning have been variously proposed as
% Fursin, G., & Temam, O. (2010). Collective Optimization: A Practical Collaborative Approach. TACO, 7(4).
\emph{Collective Optimization}~\cite{Saclay2010},
% Memon, A. W., & Fursin, G. (2013). Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing. In PARCO.
\emph{Crowdtuning}~\cite{Memon2013},
% Fursin, G., Miceli, R., Lokhmotov, A., Gerndt, M., Baboulin, M., Malony, A. D., ... Del Vento, D. (2014). Collective Mind: Towards practical and collaborative auto-tuning. Scientific Programming, 22(4).
and \emph{Collective Mind}~\cite{Fursin2014}.
\citeauthor{Fursin2014} argue that the challenges facing widespread adoption of iterative compilation techniques can be attributed to: a lack of common, diverse benchmarks and data sets; a lack of common experimental methodology; problems with continuously changing hardware and software stacks; and the difficulty to validate techniques due to a lack of sharing in publications. They propose a system for addressing these concerns which provides a modular infrastructure for sharing iterative compilation performance data and related artefacts across the internet~\cite{Fursin2014}.
% Cummins, C., Petoumenos, P., Steuwer, M., & Leather, H. (2016). Towards Collaborative Performance Tuning of Algorithmic Skeletons. In HLPGPU.
In past work~\cite{Cummins2016}, a domain specific implementation of knowledge sharing was used to accelerate tuning of stencil codes on GPUs by sharing iterative compilation data between users across the internet.

Another challenge facing iterative compilation is that results are not portable. Any change to the combination of program, input data, and hardware may impact the optimisation space, requiring a new iterative compilation process to start form scratch. This challenge can be mitigated using online iterative compilation.


\subsubsection{Online Iterative Compilation}

The expensive optimisation space exploration required by iterative compilation has spurred development of online iterative compilation that attempts to negate this ``training'' phase by interleaving the exploration of the optimisation space with regular program use. This is a challenging task, as a random search of an optimisation space may result in many configurations with performance far from optimal. In a real world system, evaluating many sub-optimal configurations can cause a significant slowdown of the program. Thus a requirement of dynamic optimisers is that convergence time towards optimal parameters is minimised, and that \emph{exploration} and \emph{exploitation} are balanced so as to maintain an acceptable quality of service for the user.

% Tartara, M., & Crespi Reghizzi, S. (2013). Continuous learning of compiler heuristics. TACO, 9(4). https://doi.org/10.1145/2400682.2400705
\citeauthor{Tartara2013}~\cite{Tartara2013} propose a technique for \emph{long-term learning} of compiler heuristics without an initial training phase. They treat the continued optimisation of a program over its lifetime as an evolutionary process with goal of finding the best set of compiler heuristics for a given binary.

% Ansel, J., & Reilly, U. O. (2012). SiblingRivalry: Online Autotuning Through Local Competitions. In CASES. ACM. https://doi.org/10.1145/2380403.2380425
\citeauthor{Ansel2012}~\cite{Ansel2012} present an adversarial approach to online evolutionary performance tuning. At runtime, the available parallel resources of a device are divided between two partitions. A different configuration of the application is then executed simultaneously on each partition. The configuration used for one of the configuration is chosen to be ``safe'', the other, experimental. The configuration which yields the best performance is retained as the ``safe'' choice for future iterations, and the process repeats.

% Mpeis, P., Petoumenos, P., & Leather, H. (2016). Iterative compilation on mobile devices. In ADAPT.
\citeauthor{Mpeis2015}~\cite{Mpeis2015} present a technique for online iterative compilation on mobile devices. They capture slices of user behaviour on a device during use, which are then replayed offline for iterative compilation. This has the advantage of specialising the performance tuning of software to the behaviour of the individual user.

Related to online iterative compilation is dynamic optimisation. \emph{Dynamo}~\cite{Bala2000} is a dynamic optimiser which performs binary level transformations of programs using information gathered from runtime profiling and tracing. This provides the ability for the program to respond to changes in dynamic features at runtime using low-level binary transformations.


\subsubsection{Algorithmic Choice \& Rewriting}

Complementary to iterative compilation is \emph{algorithmic choice}. Like iterative compilation, the goal is to find the configuration of a program that maximises performances. However, whereas iterative compilation selects compiler optimisations to produce different configurations, algorithmic choice selects between permutations of semantically equivalent algorithms, typically explicitly provided by the user.

\emph{PetaBricks}~\cite{Ansel2009a} is a language and compiler for algorithmic choice. Users provide multiple implementations of algorithms, optimised for different parameters or use cases. This creates a search space of possible execution paths for a given program. This has been combined with auto-tuning techniques for enabling optimised multigrid programs~\cite{Chan2009}, with the wider ambition that these auto-tuning techniques may be applied to all algorithmic choice programs~\cite{Ansel2014}. While this helps produce efficient programs, it places the burden of producing each algorithmic permutation on the developer, requiring them to provide enough contrasting implementations to make a search of the optimisation space fruitful.

% Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., & Amarasinghe, S. (2013). Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines. In PLDI. https://doi.org/10.1016/0024-3205(81)90116-8
\emph{Halide}~\cite{Ragan-Kelley2013} alleviates the burden of algorithmic rewriting by providing a high level domain-specific language that allows users to express pipelines of stencil computations succinctly.
% Steuwer, M., & Dubach, C. (2017). Lift: A Functional Data-Parallel IR for High-Performance GPU Code Generation. In CGO. IEEE.
The \emph{Lift} framework~\cite{Steuwer2017} uses a set of semantic-preserving rewrite rules to transform high-level Halide expressions to candidate low-level implementations, creating a space of possible implementations.


\subsubsection{Super-optimisation}

When extreme form of iterative compilation is \emph{super-optimisation}. Like with algorithmic choice, the process performs higher-level algorithmic rewrites than compiler transformations. However, the process begins with no description of the algorithm, instead it typically requires only a handful of input output examples. Super-optimisation strives to find the \emph{globally} optimal implementation of an algorithm. The term super-optimisation is a reference to the misnaming of compiler \emph{optimisation}, where finding the true \emph{optimal} is considered an unrealistic goal given the time and resource constraints of a compiler.

Pioneered by \citeauthor{Massalin1987}, the smallest possible subroutine which performs a specific function is found through a brute force enumeration of the x86 instruction set. Starting with a program of a single instruction, the super-optimiser tests to see if any possible instruction passes a set of conformity tests. If not, the program length is increased by a single instruction and the process repeats. The optimiser is limited to register to register memory transfers, with no support for pointers, a limitation which is addressed in~\cite{Joshi2002}. \emph{Denali} is a super-optimiser which uses constraint satisfaction and rewrite rules to generate programs which are \emph{provably} optimal, instead of searching for the optimal configuration through empirical testing. Denali first translates a low level machine code into guarded multi-assignment form, then uses a matching algorithm to build a graph of all of a set of logical axioms which match parts of the graph before using boolean satisfiability to disprove the conjecture that a program cannot be written in $n$ instructions. If the conjecture cannot be disproved, the size of $n$ is increased and the process repeats.
% Bunel, R., Desmaison, A., Kumar, M. P., & Torr, P. H. S. (2017). Learning to Superoptimize Programs. In ICLR.
\citeauthor{Bunel2017a}~\cite{Bunel2017a} formulate code super-optimisation as a stochastic search problem to learn the distribution of different code transformations and expected performance improvement. % As acknowledged by the authors, their approach can be improved by having temporal information of the code structures.

As with iterative compilation, the main problem is in pruning and efficiently navigating the search space. In practise, \citeauthor{Massalin1987}~\cite{Massalin1987} found their system to scale only to subroutines typically fewer than 13 instructions. As such, researchers have turned to machine learning techniques as a means to alleviate the cost of empirical evaluation.


\subsection{Machine Learning for Compiler Optimisations}
\label{subsec:related-work-machine-learning-optimisation}

Machine learning has emerged as a viable means for automatically constructing heuristics for code optimisation. Its great advantage is that it can adapt to changes in the compiler and hardware environment as it has no a priori assumptions about their behaviour.
% Ashouri, A. H., Killian, W., Cavazos, J., Palermo, G., & Silvano, C. (2018). A Survey on Compiler Autotuning using Machine Learning. CSUR, 51(5). https://doi.org/10.1145/3197978
This section provides a brief overview of the field. A more comprehensive review is provided in the surveys of \citeauthor{Ashouri2018}~\cite{Ashouri2018} and
% Wang, Z., & O’Boyle, M. (2018). Machine learning in Compiler Optimization. Proceedings of the IEEE, 1(23).
\citeauthor{Zhang2018c}~\cite{Zhang2018c}.

% Agakov, F., Bonilla, E., Cavazos, J., Franke, B., Fursin, G., O’Boyle, M., … Williams, C. K. I. (2006). Using Machine Learning to Focus Iterative Optimization. In CGO. IEEE. https://doi.org/10.1109/CGO.2006.37
Pioneered by \citeauthor{Agakov}, the idea is to use iterative compilation to evaluate a collection of training programs offline and gather explanatory variables describing the features of the programs. The program features and the optimisation decisions which yield the greatest performance are combined and a model is learned. This model can then be used to make predictions on unseen programs by extracting the features describing the program. In~\cite{Agakov} machine learning is used to guide the iterative compilation search.
% Stephenson, M., Martin, M., & Reilly, U. O. (2003). Meta Optimization: Improving Compiler Heuristics with Machine Learning. In PLDI.
In~\cite{Stephenson2003}, ``meta optimisation'' is used to tune compiler heuristics through an evolutionary algorithm to automate the search of the optimisation space.
% Stephenson, M., Martin, M., & Reilly, U. O. (2003). Meta Optimization: Improving Compiler Heuristics with Machine Learning. In PLDI.
The phase-ordering problem is formulated as a Markov process in~\cite{Kulkarni2012} and tackled using neuro-evolution to construct a neural network that predicts beneficial optimisation orderings given a characterisation of the state of code being optimised.
% Ashouri, A. H., Bignoli, A., Palermo, G., Silvano, C., Kulkarni, S., & Cavazos, J. (2017). MiCOMP: Mitigating the Compiler Phase-ordering Problem Using Optimization Sub-sequences and Machine Learning. TACO.
A later approach to the phase-ordering problem clusters optimisations and uses machine learning to predict the speedup of a sequence of all optimisation clusters~\cite{Ashouri2017}.

% Cummins, C., Petoumenos, P., Steuwer, M., & Leather, H. (2015). Autotuning OpenCL Workgroup Size for Stencil Patterns. In ADAPT. Retrieved from http://arxiv.org/abs/1511.02490
%
% Lutz, T., Fensch, C., & Cole, M. (2013). PARTANS: An Autotuning Framework for Stencil Computation on Multi-GPU Systems. TACO, 9(4).
In past work~\cite{Cummins2016a} and in~\cite{Lutz2013}, domain-specific  machine learning systems are used to optimise stencil computations on GPUs. Restricting the domain of optimisations to a single class of algorithm can simplify the problem by limiting the variance in the function being estimated.

% Ganapathi, A., Datta, K., Fox, A., & Patterson, D. (2009). A Case for Machine Learning to Optimize Multicore Performance. In HotPar.
\citeauthor{Ganapathi2009}~\cite{Ganapathi2009} tackle multi-core stencil code optimisation, drawing upon the successes of statistical machine learning techniques in the compiler community. They present an auto-tuner which can achieve performance up to 18\% better than that of a human expert. From a space of 10 million configurations, they evaluate the performance of a randomly selected 1500 combinations and use Kernel Canonical Correlation Analysis to build correlations between tunable parameter values and measured performance targets. Performance targets are L1 cache misses, TLB misses, cycles per thread, and power consumption. The use of KCAA restricts the scalability of their system as the complexity of model building grows exponentially with the number of features. In their evaluation, the system requires two hours of compute time to build the KCAA model for only 400 seconds of benchmark data.

% Dastgeer, U., Enmyren, J., & Kessler, C. W. (2011). Auto-tuning SkePU: a Multi-Backend Skeleton Programming Framework for Multi-GPU Systems. In IWMSE. ACM.
A domain-specific machine learning based auto-tuner is presented for the SkePU library in~\cite{Dastgeer2011b}. SkePU is a C++ template library for data-parallel computations on GPUs. The auto-tuner predicts optimal device mapping (i.e.\ CPU, GPU) for a given program by predicting execution time and memory copy overhead based on problem size. Similarly, in this thesis machine learning is used to predict optimal heterogeneous device mapping, though the system is capable of making predictions for arbitrary GPU programs, it is not bound to a single template library.
% Moren, K., & Gohringer, D. (2018). Automatic Mapping for OpenCL-Programs on CPU/GPU Heterogeneous Platforms. In ICCS. https://doi.org/10.1007/978-3-319-93701-4
\citeauthor{Moren2018} also tackle the task of mapping arbitrary OpenCL kernels to CPU/GPU using dynamic features extracted from the kernel at runtime~\cite{Moren2018}.

% TODO:
% \todo[inline]{These three papers:
% % Marco, V. S., Taylor, B., Porter, B., & Wang, Z. (2017). Improving Spark Application Throughput Via Memory Aware Task Co-location: A Mixture of Experts Approach. In Middleware. Retrieved from http://arxiv.org/abs/1710.00610
% \cite{Marco2017}

% % Zhang, P., Fang, J., Tang, T., Yang, C., & Wang, Z. (2018). Auto-Tuning streamed applications on intel xeon phi. IPDPS. https://doi.org/10.1109/IPDPS.2018.00061
% \cite{Zhang2018d}

% % Taylor, B., Marco, V. S., & Wang, Z. (2017). Adaptive Optimization for OpenCL Programs on Embedded Heterogeneous Systems. ACM SIGPLAN Notices, 52(4). https://doi.org/10.1145/3140582.3081040
% \cite{Taylor2017}}

% Fursin, G., Kashnikov, Y., Memon, A. W., Chamski, Z., Temam, O., Namolaru, M., … O’Boyle, M. (2011). Milepost GCC: Machine Learning Enabled Self-tuning Compiler. IJPP, 39(3).
\emph{Milepost GCC}~\cite{Fursin2011} is the first practical attempt to embed machine learning into a production compiler. It adds an interface for extracting program features and controlling optimisation passes, combined with a knowledge sharing system to distribute training data over the internet. The embedded interface exposes candidate features which may be used to apply machine learning to an optimisation in GCC, however it does not address the issue of feature selection.

% Ogilvie, W. F., Petoumenos, P., Wang, Z., & Leather, H. (2017). Minimizing the cost of iterative compilation with active learning. CGO.
\citeauthor{Ogilvie2017}~\cite{Ogilvie2017} use active learning to reduce the cost of iterative compilation by searching for points in the optimisation space which are close to decision boundaries. This reduces the cost of training compared to a random search. The approach compliments the techniques presented in this thesis, potentially allowing more efficient use of training data.

Besides compilers, there are a broad range of applications for machine learning in improving software performance.
% Kraska, T., Beutel, A., Chi, E. H., Dean, J., & Polyzotis, N. (2017). The Case for Learned Index Structures. ArXiv:1712.01208.
Surprising applications include the us of machine learning to replace conventional as hash functions in key-value stores. \citeauthor{Kraska2017}~\cite{Kraska2017} find that replacing a cache-optimised B-Tree-Index implementation with a deep learning model yields up to 70\% speedup with a $10\times$ reduction in memory on real workloads.
% Krishnan, S., Yang, Z., Goldberg, K., Hellerstein, J. M., & Stoica, I. (2018). Learning to Optimize Join Queries With Deep Reinforcement Learning. ArXiv:1808.03196.
\citeauthor{Krishnan2018}~\cite{Krishnan2018} use deep reinforcement learning to optimise SQL join query implementations. %
When applying machine learning in a new domain, the challenge is often in finding a suitable program representation to use as the features.


\subsubsection{Representing programs with features}

The success of machine learning based code optimisation requires having a set of high-quality features that can capture the important characteristics of programs. Given that there is an infinite number of these potential features, finding the right set of features is a non-trivial, time-consuming task.

Various forms of features have been used to summarise programs.
% Dubach, C., Jones, T. M., Bonilla, E. V., Fursin, G., & O’Boyle, M. (2009). Portable Compiler Optimisation Across Embedded Programs and Microarchitectures using Machine Learning. In MICRO. ACM.
\citeauthor{Dubach2009} characterise programs using performance counters~\cite{Dubach2009}.
% Jiang, Y., Zhang, Z. Z., Tian, K., Mao, F., Gethers, M., Shen, X., & Gao, Y. (2010). Exploiting Statistical Correlations for Proactive Prediction of Program Behaviors. CGO. https://doi.org/10.1145/1772954.1772989
\citeauthor{Jiang2010} extract program-level behaviours such as loop trip counts and the size of input files~\cite{Jiang2010}.
% Berral, J. L., Goiri, Í., Nou, R., Julià, F., Guitart, J., Gavaldà, R., & Torres, J. (2010). Towards energy-aware scheduling in data centers using machine learning. In e-Energy. ACM.
\citeauthor{Berral2010a} use additional runtime information such as system load~\cite{Berral2010a}.

In compiler research, the feature sets used for predictive models are often provided without explanation and rarely is the quality of those features evaluated. More commonly, an initial large, high dimensional candidate feature space is pruned via feature selection, or projected into a lower dimensional space.
% Stephenson, M., & Amarasinghe, S. (2005). Predicting Unroll Factors Using Supervised Classification. In CGO. IEEE.
\citeauthor{Stephenson2005}~\cite{Stephenson2005} propose two approaches to select the most useful features from 38 candidates: the first using a Mutual Information Score to rank features, the second using a greedy feature selection.
% Collins, A., Fensch, C., Leather, H., & Cole, M. (2013). MaSiF: Machine Learning Guided Auto-tuning of Parallel Skeletons. In HiPC. IEEE. https://doi.org/10.1109/HiPC.2013.6799098
\citeauthor{Collins2013}~\cite{Collins2013} use Principle Component Analysis (PCA) to reduce a four-dimensional feature space to two dimensions, reducing the size of the space to 0.05\%.
% Dubach, C., Cavazos, J., Franke, B., Fursin, G., O’Boyle, M., & Temam, O. (2007). Fast Compiler Optimisation Evaluation Using Code-Feature Based Performance Prediction. In CF. ACM.
\citeauthor{Dubach2007}~\cite{Dubach2007} also use PCA to reduce the dimensionality of their feature space, but determine and use the number of components that account for 95\% of the total variance. In their case, 5 components.
% Ting, P., Tu, C., Chen, P., Lo, Y., & Cheng, S. (2016). FEAST: An Automated Feature Selection Framework for Compilation Tasks. ArXiv:1610.09543.
\emph{FEAST}~\cite{Ting2016} employs a range of existing feature selection methods to select useful candidate features.

This thesis presents a technique to construct machine learning compiler heuristics without the need for program features. Prior works have sought to reduce the cost of feature design.
% Park, E., Cavazos, J., & Alvarez, M. A. (2012). Using Graph-Based Program Characterization for Predictive Modeling. In CGO. IEEE.
\citeauthor{Park2012}~\cite{Park2012} present a unique graph-based approach for feature representations. They use a Support Vector Machine (SVM) where the kernel is based on graph similarity metric. Their technique still requires hand coded features at the basic block level, but thereafter, graph similarity against each of the training programs takes the place of global features. Being a kernel method, it requires that training data graphs be shipped with the compiler, which may not scale as the size of the training data grows with the number of instances, and some training programs may be very large. Finally, their graph matching metric is expensive, requiring $O(n^3)$ to compare against each training example. The techniques presented in this thesis do not need any hand built static code features, and the deployment memory footprint is constant and prediction time is linear in the length of the program, regardless of the size of the training set.

A few methods have been proposed to automatically generate features from the compiler's intermediate representation. These approaches closely tie the implementation of the predictive model to the compiler IR, which means changes to the IR will require modifications to the model.
% Leather, H., Bonilla, E., & O’Boyle, M. (2014). Automatic Feature Generation for Machine Learning Based Optimizing Compilation. TACO, 11(1).
\citeauthor{Leather2014}~\cite{Leather2014} uses genetic programming to search for features, requiring a huge grammar to be written, some 160kB in length. Although much of this can be created from templates, selecting the right range of capabilities and search space bias is non trivial and up to the expert.
% Namolaru, M., Cohen, A., Fursin, G., Zaks, A., & Freund, A. (2010). Practical Aggregation of Semantical Program Properties for Machine Learning Based Optimization. In CASES.
\citeauthor{Namolaru2010a}~\cite{Namolaru2010a} express the space of features via logic programming over relations that represent information from the IRs. They greedily search for expressions that represent good features. However, this approach relies on expert selected relations, combinators and constraints to work. For both approaches, the search time may be significant.

% Cavazos, J., Dubach, C., Agakov, F., Bonilla, E., O’Boyle, M., Fursin, G., & Temam, O. (2006). Automatic Performance Model Construction for the Fast Software Exploration of New Hardware Designs. In CASES.
\citeauthor{Cavazos2006}~\cite{Cavazos2006} present a reaction-based predictive model for software-hardware co-design. Their approach profiles the target program using several carefully selected compiler options to see how program runtime changes under these options for a given micro-architecture setting. They then use the program ``reactions'' to predict the best available application speedup. While their approach does not use static code features, developers must carefully select a few settings from a large number of candidate options for profiling, because poorly chosen options can significantly affect the quality of the model. Moreover, the program must be run several times before optimisation, while the technique presented in this thesis does not require the program to be profiled.

Compared to these approaches, the techniques presented in this thesis are entirely automatic and require no expert involvement. In the field of compiler optimisations, no work so far has developed deep learning methodologies for program feature generation and selection. This work is the first to do so.


\subsubsection{Representing programs with embeddings}

This thesis presents deep learning methodologies for learning over programs, inspired by natural language processing. With these techniques, a program source code is tokenised into a vocabulary of words, and the words mapped into a real-valued \emph{embedding} space.
% Allamanis, M., Barr, E. T., Devanbu, P., & Sutton, C. (2017). A Survey of Machine Learning for Big Code and Naturalness. ArXiv:1709.06182.
There are many choices in how to construct the vocabulary and embedding. \citeauthor{Chen2019} review some of the proposed techniques~\cite{Chen2019}.
% Babii, H., Janes, A., & Robbes, R. (2019). Modeling Vocabulary for Big Code Machine Learning. ArXiv:1904.01873.
\citeauthor{Babii} explore the impact that choices in vocabulary have on time to convergence of software language models~\cite{Babii}.

% Cvitkovic, M., Singh, B., & Anandkumar, A. (2018). Deep Learning On Code with an Unbounded Vocabulary. Machine Learning, 4.
The techniques in this thesis use a hybrid character/token-level vocabulary to tokenise source code. This is to prevent the blow-up in vocabulary size that occurs from using a purely token-based vocabulary. \citeauthor{Cvitkovic2018a} propose modelling vocabulary elements as nodes in a graph and then processing the graph using Graph Neural Networks; this enables learning over an unbounded vocabulary~\cite{Cvitkovic2018a}.

% Mou, L., Li, G., Zhang, L., Wang, T., & Jin, Z. (2016). Convolutional Neural Networks over Tree Structures for Programming Language Processing. In AAAI.
\citeauthor{Mou2016} derive an embedding space from the tokens in the source code of a program~\cite{Mou2016}.
% Wang, K., Singh, R., & Su, Z. (2017). Dynamic Neural Program Embeddings for Program Repair. ArXiv:1711.07163.
\citeauthor{Wang2017d} propose an embedding space extracted from program traces, rather than the syntactic structure of the program~\cite{Wang2017d}.
% Henkel, J., Lahiri, S. K., Liblit, B., & Reps, T. (2018). Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces. In FSE.
\citeauthor{Henkel2018} use symbolic execution to abstract the program traces. Embeddings are then learned from these abstracted symbolic traces~\cite{Henkel2018}.
% Yin, P., Neubig, G., Allamanis, M., Brockschmidt, M., & Gaunt, A. L. (2018). Learning to Represent Edits. ArXiv:1810.13337.
%
% Tufano, M., Pantiuchina, J., Watson, C., Bavota, G., & Poshyvanyk, D. (2019). On Learning Meaningful Code Changes via Neural Machine Translation. ArXiv:1901.09102.
\citeauthor{Yin2018}~\cite{Yin2018} and \citeauthor{Tufano2019}~\cite{Tufano2019} present techniques for learning representations of code edits.

% Ben-nun, T., Jakobovits, A. S., & Hoefler, T. (2018). Neural Code Comprehension: A Learnable Representation of Code Semantics. In NeurIPS.
\emph{Neural Code Comprehension}~\cite{Ben-nun2018} builds on the experiments in Chapter~\ref{chap:deeptune} of this thesis to develop embeddings derived from a novel \emph{Contextual Flow Graph} (XFG) representation which contains the union of both data and control flow graphs. The embeddings are trained using a skip-gram model~\cite{Mikolov2013a}, using a vocabulary derived from LLVM byte-code. This enables the same embeddings to be re-used for any programming language for which there exists a front-end to LLVM.

% TODO: \todo[inline]{There are plenty of other \_2vec papers to touch on}
