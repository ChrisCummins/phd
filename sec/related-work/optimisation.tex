\section{Program Optimisation}
\label{sec:related-work-optimisation}

Modern compilers are complex, frequently containing hundreds of optimisation passes. Determining which optimisation passes should be applied, and in what order, is a challenge that depends on a variety of factors from the properties of the program being compiled to the target hardware. Current state-of-practise is for compilers to use a fixed ordering of optimisations, and for each optimisation to contain a heuristic to determine when with what parameters it should be used, and under what circumstances. Such heuristics are typically developed by hand at the expense of great effort and compiler expertise.

Unfortunately, the complex interactions between hardware and software often cannot be captured by heuristic and analytical methods, resulting in performance being left on the table.

The challenges:

% Georgiou, K., Blackmore, C., Xavier-de-Souza, S., & Eder, K. (2018). Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption. In SCOPES.
\todo[inline]{Selecting which optimisations to use is not easy. Interestingly, using \emph{less} flags than -O2 can improve performance~\cite{Georgiou2018}.}

\todo[inline]{Selecting the right parameters is critical. E.g. \citeauthor{Ryoo2008a} find speedups of up to $432\times$ for a CUDA matrix multiplication implementation by appropriate use of tiling and loop unrolling~\cite{Ryoo2008a}.}


\subsection{Iterative Compilation \& Auto-tuning}

Iterative compilation is the method of performance tuning in which a program is compiled and profiled using multiple different configurations of optimisations in order to find the configuration which maximises performance. Unlike analytical methods which attempt to predict the parameters that produce good performance, iterative compilation is empirical. A set of candidate configurations are selected, and for each, the program is compiled and profiled. The configuration that maximises some metric for reward (such as speedup) is selected. In one of the first works of the field, \citeauthor{Bodin1998} demonstrated the utility of the approach at finding good configurations in the non-linear three-parameter optimisation space of a matrix multiplication benchmark~\cite{Bodin1998}.

While conceptually simple, the empirical nature of iterative compilation yields good results. Iterative compilation has since been demonstrated to be a highly effective form of empirical performance tuning for selecting compiler optimisations.

Frameworks for iterative compilation offer mechanisms for abstracting the iterative compilation process from the optimisation space. \emph{OpenTuner} presents a generic framework for optimisation space exploration~\cite{Ansel2013}. \emph{CLTune} is a generic auto-tuner for OpenCL kernels~\cite{Nugteren2015}. Both frameworks implement \emph{search}, however, the huge number of possible compiler optimisations makes such a search expensive to perform for every new configuration of program, architecture and dataset.

One of the challenges of iterative compilation is the exponential blow up of optimisation space size when considering many independent parameters. Many compilers contain dozens or hundreds of discrete optimisation transformations, rendering an exhaustive search of the optimisation space infeasible. This has driven the development of methods for reducing the cost of evaluating configurations. These methods reduce evaluation costs either by pruning the size of the optimisation space, or by guiding a directed search to traverse a subset of the space.


\subsubsection{Pruning the Iterative Compilation Search Space}

\todo[inline]{\citeauthor{Triantafyllis2003} uses feedback during evaluation of configurations to prune the optimisation space~\cite{Triantafyllis2003}.}

\citeauthor{Pan2006} formalise the problem in \cite{Pan2006} as: given a set of compiler optimization options $\left\{ F_1, F_2, \ldots, F_n \right\}$, find the combination that minimizes the program execution time efficiently, without a priori knowledge of the optimisations and their interactions. Their technique, \emph{Combined Elimination}, iteratively prunes the search space, reducing the tuning time to 57\% of the closest alternative. Posing the problem as a subset search negates the challenge of optimisation \emph{ordering}, though this challenge has been the focus of other work~\cite{Kulkarni2012,Purini2013}.

The \emph{solution} space for optimisation orderings is pruned in~\cite{Purini2013}. Rather than attempting to find a universally optimal sequence of optimisations, \citeauthor{Purini2013} identify a \emph{set} of good optimisation sequences that is small enough that each new program can be tried with all sequences in the set. They find that a sequence set size of 10 yields 13\% speedups on PolyBench and MiBench programs. Although determining the elements of the good sequence set may be expensive, this process need only be performed once, offline.

A complementary approach to search space pruning is knowledge sharing. Since most software is shared across many users, leverage this by having dividing the optimisation work across users, rather than each redundantly performing their own performance tuning. A ``big data'' approach to auto-tuning has been variously proposed as
% Fursin, G., & Temam, O. (2010). Collective Optimization: A Practical Collaborative Approach. TACO, 7(4).
\emph{Collective Optimization} in~\cite{Saclay2010},
% Memon, A. W., & Fursin, G. (2013). Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing. In PARCO.
\emph{Crowdtuning} in~\cite{Memon2013},
% Fursin, G., Miceli, R., Lokhmotov, A., Gerndt, M., Baboulin, M., Malony, A. D., â€¦ Del Vento, D. (2014). Collective Mind: Towards practical and collaborative auto-tuning. Scientific Programming, 22(4).
and \emph{Collective Mind} in~\cite{Fursin2014}.%
\citeauthor{Fursin2014} argue that the challenges facing widespread adoption of autotuning and machine learning methodologies can be attributed to: a lack of common, diverse benchmarks and data sets; a lack of common experimental methodology; problems with continuously changing hardware and software stacks; and the difficulty to validate techniques due to a lack of sharing in publications. They propose a system for addressing these concerns, the Collective Mind knowledge system, which provides a modular infrastructure for sharing autotuning performance data and related artefacts across the internet~\cite{Fursin2014}.
% Cummins, C., Petoumenos, P., Steuwer, M., & Leather, H. (2016). Towards Collaborative Performance Tuning of Algorithmic Skeletons. In HLPGPU.
In past work, a domain specific implementation of knowledge sharing was used to accelerate tuning of stencil codes on GPUs~\cite{Cummins2016}.

Reduction of the GPGPU optimisation space is demonstrated in~\cite{Ryoo2008}, using the common subset of optimal configurations across a set of training examples. This technique reduces the search space by 98\%, although it does not guarantee that for a new program, the reduced search space will include the optimal configuration.

Iterative compilation does not learn --- for every new program, we must start from scratch. This can be mitigated using online compilation.


\subsubsection{Online iterative compilation}

\todo{rewrite} Expensive offline training phase required by iterative compilation has spurred development of dynamic optimisers which perform this optimisation space exploration at runtime. Dynamic optimiser allow programs to respond to dynamic features ``online''. This is a challenging task, as a random search of an optimisation space may result in configurations with vastly sub-optimal performance. In a real world system, evaluating many sub-optimal configurations will cause a significant slowdown of the program. Thus a requirement of dynamic optimisers is that convergence time towards optimal parameters is minimised.

% Tartara, M., & Crespi Reghizzi, S. (2013). Continuous learning of compiler heuristics. TACO, 9(4). https://doi.org/10.1145/2400682.2400705
\todo[inline]{\citeauthor{Tartara2013} propse a technique for \emph{long-term learning} of compiler heuristics without an initial training phase~\cite{Tartara2013}.}

\todo{rewrite} Dynamo is a dynamic optimiser which performs binary level transformations of programs using information gathered from runtime profiling and tracing~\cite{Bala2000}. While this provides the ability to respond to dynamic features, it restricts the range of optimisations that can be applied to binary transformations. These low level transformations cannot match the performance gains that higher level parameter tuning produces.

% Mpeis, P., Petoumenos, P., & Leather, H. (2016). Iterative compilation on mobile devices. In ADAPT.
\todo[inline]{A technique for online iterative compilation on mobile devices is presented in~\cite{Mpeis2015}. The technique captures slices of user behaviour of a device, which is then replayed offline for iterative compilation. This has the advantage of specializing the performance tuning of software to the beahviour of the individual user.}

% Ansel, J., & Reilly, U. O. (2012). SiblingRivalry: Online Autotuning Through Local Competitions. In CASES. ACM. https://doi.org/10.1145/2380403.2380425
\todo[inline]{An approach to online tuning of parallel programs is presented in~\cite{Ansel2012} which partitions the available parallel resources of a device in to two partitions and then executes two different configurations simultaneously using each partition. The configuration used for one of the configuration is guaranteed to be ``safe'', and the performance ...}


\subsubsection{Multi-compilation \& Algorithmic Choice}

Algorithmic choice is an offshoot of iterative compilation that is still actively pursued:

PetaBricks is a language and compiler for algorithmic choice~\cite{Ansel2009a}. Users provide multiple implementations of algorithms, optimised for different parameters or use cases. This creates a search space of possible execution paths for a given program. This has been combined with autotuning techniques for enabling optimised multigrid programs~\cite{Chan2009}, with the wider ambition that these autotuning techniques may be applied to all algorithmic choice programs~\cite{Ansel2014}. While this helps produce efficient programs, it places a great burden on the developer, requiring them to provide enough contrasting implementations to make a search of the optimisation space fruitful.


\subsubsection{Super-optimisation \& Rewrite rules}

A tangent to iterative compilation is the development of so-called ``super-optimisers''. In~\cite{Massalin1987}, the smallest possible program which performs a specific function is found through a brute force enumeration of the entire instruction set. Starting with a program of a single instruction, the super-optimiser tests to see if any possible instruction passes a set of conformity tests. If not, the program length is increased by a single instruction and the process repeats. The exponential growth in the size of the search space is far too expensive for all but the smallest of hot paths, typically less than 13 instructions. The optimiser is limited to register to register memory transfers, with no support for pointers, a limitation which is addressed in~\cite{Joshi2002}. Denali is a super-optimiser which uses constraint satisfaction and rewrite rules to generate programs which are \emph{provably} optimal, instead of searching for the optimal configuration through empirical testing. Denali first translates a low level machine code into guarded multi-assignment form, then uses a matching algorithm to build a graph of all of a set of logical axioms which match parts of the graph, before using boolean satisfiability to disprove the conjecture that a program cannot be written in $n$ instructions. If the conjecture cannot be disproved, the size of $n$ is increased and the process repeats.

% Christen, M., Schenk, O., & Burkhart, H. (2011). PATUS: A Code Generation and Autotuning Framework for Parallel Iterative Stencil Computations on Modern Microarchitectures. In PDPS. IEEE. https://doi.org/10.1109/IPDPS.2011.70
\todo[inline]{Starting from a high level specification for a stencil operation, this system produces an implementation~\cite{Christen2011}.}

A framework for the automatic generation of OpenCL kernels from high-level programming concepts is described in~\cite{Steuwer2015}. A set of rewrite rules is used to transform high-level expressions to OpenCL code, creating a space of possible implementations. This approach is ideologically similar to that of PetaBricks, in that optimisations are made through algorithmic choice, although in this case the transformations are performed automatically at the compiler level. The authors report performance on a par with that of hand written OpenCL kernels.



\subsection{Machine Learning}

Machine learning has emerged as a viable means in automatically constructing heuristics for code optimisation. Its great advantage is that it can adapt to changing hardware platforms as it has no a priori assumptions about their behaviour.

% Ashouri, A. H., Killian, W., Cavazos, J., Palermo, G., & Silvano, C. (2018). A Survey on Compiler Autotuning using Machine Learning. CSUR, 51(5). https://doi.org/10.1145/3197978
\todo[inline]{Autotuning through ML survey~\cite{Ashouri2018}.}

% Wang, Z., & Oâ€™Boyle, M. (2018). Machine learning in Compiler Optimization. Proceedings of the IEEE, 1(23).
\todo[inline]{Survey of ML for compiler opts~\cite{Zhang2018c}.}

% Stephenson, M., Martin, M., & Reilly, U. O. (2003). Meta Optimization: Improving Compiler Heuristics with Machine Learning. In PLDI.
In~\cite{Stephenson2003}, ``meta optimisation'' is used to tune compiler heuristics through an evolutionary algorithm to automate the search of the optimisation space.

% Stephenson, M., Martin, M., & Reilly, U. O. (2003). Meta Optimization: Improving Compiler Heuristics with Machine Learning. In PLDI.
\todo[inline]{The phase-ordering problem formulated as a Markov process in~\cite{Kulkarni2012} and tackled using neuro-evolution to construct a neural network that predicts beneficial optimization orderings given a characterization of the state of code being optimized.}

% Ashouri, A. H., Bignoli, A., Palermo, G., Silvano, C., Kulkarni, S., & Cavazos, J. (2017). MiCOMP: Mitigating the Compiler Phase-ordering Problem Using Optimization Sub-sequences and Machine Learning. TACO.
\todo[inline]{Another approach to the phase-ordering problem clusers optimisations and uses machine learning to predict the speedup of a sequenece of all optimisation clusters~\cite{Ashouri2017}.}

% Dastgeer, U., Enmyren, J., & Kessler, C. W. (2011). Auto-tuning SkePU: a Multi-Backend Skeleton Programming Framework for Multi-GPU Systems. In IWMSE. ACM.
\todo[inline]{A domain-specific machine learning based auto-tuner is presented for the SkePU library in~\cite{Dastgeer2011b}. SkePU is a C++ template library for data-parallel computations on GPUs. The auto-tuner predicts optimal device mapping (i.e.\ CPU, GPU) for a given program by predicting execution time and memory copy overhead based on problem size. Similarly, in this thesis a machine learning auto-tuner is used to predict optimal device mapping, though the auto-tuner is capable of making predictions for any arbitrary GPU program, it is not bound to a single template library.}

% Moren, K., & Gohringer, D. (2018). Automatic Mapping for OpenCL-Programs on CPU/GPU Heterogeneous Platforms. In ICCS. https://doi.org/10.1007/978-3-319-93701-4
\todo[inline]{CPU/GPU mapping using dynamic features~\cite{Shi2018}.}

% Cummins, C., Petoumenos, P., Steuwer, M., & Leather, H. (2015). Autotuning OpenCL Workgroup Size for Stencil Patterns. In ADAPT. Retrieved from http://arxiv.org/abs/1511.02490
%
% Lutz, T., Fensch, C., & Cole, M. (2013). PARTANS: An Autotuning Framework for Stencil Computation on Multi-GPU Systems. TACO, 9(4).
\todo[inline]{In past work~\cite{Cummins2016a} and in~\cite{Lutz2013}, domain-specific autotuning is used to optimise stencil computations on GPUs. Restricting the domain of optimisations to a single class of algorithm simplifies the problem.}

% Ganapathi, A., Datta, K., Fox, A., & Patterson, D. (2009). A Case for Machine Learning to Optimize Multicore Performance. In HotPar.
\citeauthor{Ganapathi2009} demonstrated early attempts at autotuning multi-core stencil codes in~\cite{Ganapathi2009}, drawing upon the successes of statistical machine learning techniques in the compiler community, as discussed in Section~\ref{sec:iterative-compilation}. They present an auto-tuner which can achieve performance up to 18\% better than that of a human expert. From a space of 10 million configurations, they evaluate the performance of a randomly selected 1500 combinations, using Kernel Canonical Correlation Analysis to build correlations between tunable parameter values and measured performance targets. Performance targets are L1 cache misses, TLB misses, cycles per thread, and power consumption. The use of KCAA restricts the scalability of their system as the complexity of model building grows exponentially with the number of features. In their evaluation, the system requires two hours of compute time to build the KCAA model for only 400 seconds of benchmark data. They present a compelling argument for the use of energy efficiency as an optimisation target in addition to runtime, citing that it was the power wall that lead to the multi-core revolution in the first place. Their choice of only 2 benchmarks and 2 platforms makes the evaluation of their auto-tuner somewhat limited.

% Fursin, G., Kashnikov, Y., Memon, A. W., Chamski, Z., Temam, O., Namolaru, M., â€¦ Oâ€™Boyle, M. (2011). Milepost GCC: Machine Learning Enabled Self-tuning Compiler. IJPP, 39(3).
\emph{Milepost GCC} is a machine learning-enabled self-tuning compiler~\cite{Fursin2011}.

% Ogilvie, W. F., Petoumenos, P., Wang, Z., & Leather, H. (2017). Minimizing the cost of iterative compilation with active learning. CGO.
\todo[inline]{\citeauthor{Ogilvie2017} use active learning to reduce the cost of iterative compilation by searching for points in the optimisation space which are close to decision boundaries~\cite{Ogilvie2017}. Complimentary to work presented in this thesis. This reduces the cost of training compared to a random search.}

The uses for machine learning in improving software performance are diverse:

% Kraska, T., Beutel, A., Chi, E. H., Dean, J., & Polyzotis, N. (2017). The Case for Learned Index Structures. ArXiv:1712.01208.
\todo[inline]{Surprisingly, \citeauthor{Kraska2017} find that replacing a cache-optimised B-Tree-Index implementation yields up to 70\% speedup with a $10\times$ reduction in memory~\cite{Kraska2017}.}

% Krishnan, S., Yang, Z., Goldberg, K., Hellerstein, J. M., & Stoica, I. (2018). Learning to Optimize Join Queries With Deep Reinforcement Learning. ArXiv:1808.03196.
\todo[inline]{SQL join queries are optimised using deep reinforcement learning in~\cite{Krishnan2018}.}


The limitation in applying ML to other software tasks is often in finding a suitable representation.


\subsubsection{Representing programs with features}

The success of machine learning based code optimisation has requires having a set of high-quality features that can capture the important characteristics of the target program. Given that there is an infinite number of these potential features, finding the right set of features is a non-trivial, time-consuming task. In the field of compiler optimisation, no work so far has applied deep neural networks for program feature generation and selection. This work is the first to do so.

\todo[inline]{Automatic feature \emph{selection} has been done before. The technique proposed in this thesis is for feature \emph{generation}, i.e. extracting a novel representation from text.}

Various forms of program features have been used in compiler-based machine learning. These include static code structures~\cite{Jiang2010} and runtime information such as system load~\cite{Wen2015} and performance counters~\cite{Dubach2009}. In compiler research, the feature sets used for predictive models are often provided without explanation and rarely is the quality of those features evaluated. More commonly, an initial large, high dimensional candidate feature space is pruned via feature selection~\cite{Stephenson2005}, or projected into a lower dimensional space~\cite{Collins2013,Dubach2007}. FEAST employs a range of existing feature selection methods to select useful candidate features~\cite{Ting2016}. Unlike these approaches, DeepTune extracts features and reduces the dimensionality of the feature space completely internally and without expert guidance.

Park \emph{et al.} present a unique graph-based approach for feature representations~\cite{Park2012}. They use a Support Vector Machine where the kernel is based on graph similarity metric. Their technique still requires hand coded features at the basic block level, but thereafter, graph similarity against each of the training programs takes the place of global features. Being a kernel method, it requires that training data graphs be shipped with the compiler, which may not scale as the size of the training data grows with the number of instances, and some training programs may be very large. Finally, their graph matching metric is expensive, requiring $O(n^3)$ to compare against each training example. By contrast, our method does not need any hand built static code features, and the deployment memory footprint is constant and prediction time is linear in the length of the program, regardless of the size of the training set.

A few methods have been proposed to automatically generate features from the compiler's intermediate representation~\cite{Namolaru2010a,Leather2014}. These approaches closely tie the implementation of the predictive model to the compiler IR, which means changes to the IR will require modifications to the model. The work of \cite{Leather2014} uses genetic programming to search for features, and required a huge grammar to be written, some 160kB in length. Although much of this can be created from templates, selecting the right range of capabilities and search space bias is non trivial and up to the expert. The work of \cite{Namolaru2010a} expresses the space of features via logic programming over relations that represent information from the IRs. It greedily searches for expressions that represent good features. However, their approach relies on expert selected relations, combinators and constraints to work. For both approaches, the search time may be significant.

Cavazos \emph{et al.\ }present a reaction-based predictive model for software-hardware co-design~\cite{Cavazos2006}. Their approach profiles the target program using several carefully selected compiler options to see how program runtime changes under these options for a given micro-architecture setting. They then use the program ``reactions'' to predict the best available application speedup. While their approach does not use static code features, developers must carefully select a few settings from a large number of candidate options for profiling, because poorly chosen options can significantly affect the quality of the model. Moreover, the program must be run several times before optimisation, while our technique does not require the program to be profiled.


\subsubsection{Representing programs with embeddings}

% Ben-nun, T., Jakobovits, A. S., & Hoefler, T. (2018). Neural Code Comprehension: A Learnable Representation of Code Semantics. In NeurIPS.
\todo[inline]{Neural Code Comprehension~\cite{Ben-nun2018} builds on the experiments in Chapter~\ref{chap:deeptune} of this thesis, using a novel XFG representation. Assembled from LLVM byte-code so language portable.}

% Ben-nun, T., Jakobovits, A. S., & Hoefler, T. (2018). Neural Code Comprehension: A Learnable Representation of Code Semantics. In NeurIPS.
\todo[inline]{Neural Code Comprehension~\cite{Ben-nun2018} builds on the experiments in Chapter~\ref{chap:deeptune} of this thesis, using a novel XFG representation. Assembled from LLVM byte-code so language portable.}

\todo[inline]{The work presented in this thesis uses a hybrid character/token-level language model to present source code. This is to prevent the blow-up in vocabulary size that occurs from using a purely token-based vocabulary. \citeauthor{Cvitkovic2018a} propose learning using an unbounded vocabulary in~\cite{Cvitkovic2018a}.}

% Babii, H., Janes, A., & Robbes, R. (2019). Modeling Vocabulary for Big Code Machine Learning. ArXiv:1904.01873.
\todo[inline]{\citeauthor{Babii} explore the impact that choices in vocabulary have on time to convergence of software language models~\cite{Babii}}

% Wang, K., Singh, R., & Su, Z. (2017). Dynamic Neural Program Embeddings for Program Repair. ArXiv:1711.07163.
\todo[inline]{Program embeddings~\cite{Wang2017d}.}



\subsubsection{Transfer Learning}

% Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). CNN Features off-the-shelf: an Astounding Baseline for Recognition. In CVPRW. https://doi.org/10.1109/CVPRW.2014.131
Exploring relevance of inferred features in model hidden layers~\cite{Yosinski2014}. Transfer learning is common in other domains, e.g. vision~\cite{Razavian2014,Oquab2014}. Often using the hidden layers as fixed feature extractors (by using the output of one sub-model as input to a new model), rather than using the same model structure and transferring weights (as we did).
