\section{Summary}%
\label{sec:clgen-conclusion}

The quality of predictive models is bound by the quantity and quality of programs used for training, yet there are typically only a few dozen common benchmarks available for experiments. This chapter presents a novel tool which is the first of its kind --- an entirely probabilistic program generator capable of producing an unbounded number of human-like programs. The approach applies deep learning over a huge corpus of publicly available code from GitHub to automatically infer the semantics and practical usage of a programming language. The tool generateSynthetics programs which to trained eyes are indistinguishable from handwritten code. The approach is tested using a state-of-the-art predictive model, improving its performance by a factor of $1.27\times$. Synthetic benchmarks automatically exposed weaknesses in the feature set which, when corrected, further improved the performance by $4.30\times$.

Given the ability of generative models for performance characterisation, it is natural to hypothesise that a generator for unbounded programs may be used for compiler validation. Compared to benchmarking, compiler validation places very different requirements for how the benchmarks are used. In the following Chapter, the generative model is extended to the challenging domain of compiler test case generation.
