\section{Summary}%
\label{sec:clgen-conclusion}

The quality of predictive models is bound by the quantity and quality of programs used for training, yet there is typically only a few dozen common benchmarks available for experiments. This chapter present a novel tool which is the first of it's kind --- an entirely probabilistic program generator capable of generating an unbounded number of human like programs. The approach applies deep learning over a huge corpus of publicly available code from GitHub to automatically infer the semantics and practical usage of a programming language. The tool generates programs which to trained eyes are indistinguishable from hand-written code. The approach is tested using a state-of-the-art predictive model, improving its performance by a factor of $1.27\times$. Synthetic benchmarks automatically exposed weaknesses in the feature set which, when corrected, further improved the performance by $4.30\times$.
