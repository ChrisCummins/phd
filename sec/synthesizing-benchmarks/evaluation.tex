\section{Qualitative Evaluation of Generated Programs}
\label{sec:clgen-qualitative-evaluation}

This section evaluates the quality of programs synthesised by CLgen by their likeness to hand-written code.

\subsection{Methodology}

Judging whether a piece of code has been written by a human is a challenging task for a machine, so I adopted a methodology from machine learning research based on the \emph{Turing Test}~\cite{Gao2015a,Zhang2016,Vinyals}. If the output of CLgen is human-like code, it reasons then that a human judge will be unable to distinguish it from hand-written code.

A double blind test was devised in which 15 volunteer OpenCL developers from industry and academia were shown 10 OpenCL kernels each. Participants were tasked with judging whether, for each kernel, they believed it to have been written by hand or by machine. Kernels were randomly selected for each participant from two equal sized pools of synthetically generated and hand-written code from GitHub. The samples from GitHub were vetted to ensure that they were indeed hand-written and not generated by machine or template (such vetting is a manual process and was not applied during the assembly of the model training corpus). The code rewriting process was applied to all kernels to remove comments and ensure uniform identifier naming. The participants were divided into two groups, with 10 of them receiving code generated by CLgen, and 5 of them acting as a control group, receiving code generated by CLSmith~\cite{Lidbury2015a}, a program generator for differential testing\footnote{An online version of this test is available at \emph{http://humanorrobot.uk/}.}.

\subsection{Experimental Results}

Each participant's answers was scored. The average score of the control group is 96\% (stdev.\ 9\%), an unsurprising outcome as programs generated using the CLSmith grammar for testing have multiple ``tells''; for example, their only input is a single \texttt{ulong} pointer. There were no false positives (synthetic code labelled human) for CLSmith, only false negatives (human code labelled synthetic). With CLgen synthesised programs, the average score was 52\% (stdev.\ 17\%), and the ratio of errors was even. This suggests that CLgen code is indistinguishable from hand-written programs, with human judges scoring no better than random chance.