\section{Qualitative Evaluation of Generated Programs}
\label{sec:clgen-qualitative-evaluation}

This section evaluates the quality of programs synthesised by CLgen by their likeness to handwritten code.

\subsection{Methodology}

Judging whether a piece of code has been written by a human is a challenging task for a machine, so a methodology was adopted from artificial intelligence research based on the \emph{Turing Test}~\cite{Gao2015a,Zhang2016,Vinyals}. If the output of CLgen is human-like code, it reasons that a human judge will be unable to distinguish it from handwritten code.

A double-blind test was devised in which 15 volunteer OpenCL developers from industry and academia were shown 10 OpenCL kernels each. Participants were tasked with judging whether, for each kernel, they believed it to have been written by hand or by machine. Kernels were randomly selected for each participant from two equal sized pools of synthetically generated and handwritten code from GitHub. The samples from GitHub were vetted to ensure that they were indeed handwritten and not generated by machine or template (such vetting is a manual process and was not applied during the assembly of the model training corpus). The code rewriting process was applied to all kernels to remove comments and ensure uniform identifier naming. The participants were divided into two groups of 10 and 5 members, with the larger group reviewing synthetic code generated CLgen. The smaller group acted as a control group, reviewing synthetic code generated by CLSmith~\cite{Lidbury2015a}, an OpenCL program generator for differential testing\footnote{An online version of this test is available at \emph{http://humanorrobot.uk/}.}.

\subsection{Experimental Results}

Each participant's answers were scored. The average score of the control group is 96\% (stdev.\ 9\%), an unsurprising outcome as programs generated using CLSmith for testing have multiple ``tells''; for example, they make much heavier use of \texttt{struct}s than is typical, they use unusual combinations of programming language features, and their only input is a single \texttt{ulong} pointer. There were no false positives (synthetic code labelled human) for CLSmith, only false negatives (human code labelled synthetic).

With CLgen synthesised programs, the average score was 52\% (stdev.\ 17\%), and the ratio of errors was even. This suggests that CLgen code is indistinguishable from handwritten programs, with human judges scoring no better than random chance.
