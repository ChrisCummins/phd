\section{Related Work}\label{sec:rw}

Machine learning has emerged as a viable means in automatically constructing heuristics for code optimization~\cite{Wang2010,Kulkarni2012,Muralidharan2016,Ogilvie2017,Ren,Cummins2016}. Its great advantage is that it can adapt to changing hardware platforms as it has no a priori assumptions about their behavior. The success of machine learning based code optimization has required having a set of high-quality features that can capture the important characteristics of the target program. Given that there is an infinite number of these potential features, finding the right set of features is a non-trivial, time-consuming task.

Various forms of program features have been used in compiler-based machine learning. These include static code structures~\cite{Jiang2010} and runtime information such as system load~\cite{Wen2015} and performance counters~\cite{Dubach2009}. In compiler research, the feature sets used for predictive models are often provided without explanation and rarely is the quality of those features evaluated. More commonly, an initial large, high dimensional candidate feature space is pruned via feature selection~\cite{Stephenson2005,Taylor}, or projected into a lower dimensional space~\cite{Collins2013,Dubach2007}. FEAST employs a range of existing feature selection methods to select useful candidate features~\cite{Ting2016}. Unlike these approaches, DeepTune extracts features and reduces the dimensionality of the feature space completely internally and without expert guidance.

Park \emph{et al.} present a unique graph-based approach for feature representations~\cite{Park2012}. They use a Support Vector Machine where the kernel is based on a graph similarity metric. Their technique still requires hand coded features at the basic block level, but thereafter, graph similarity against each of the training programs takes the place of global features. Being a kernel method, it requires that training data graphs be shipped with the compiler, which may not scale as the size of the training data grows with the number of instances, and some training programs may be very large. Finally, their graph matching metric is expensive, requiring $O(n^3)$ to compare against each training example. By contrast, our method does not need any hand built static code features, and the deployment memory footprint is constant and prediction time is linear in the length of the program, regardless of the size of the training set.

A few methods have been proposed to automatically generate features from the compiler's intermediate representation~\cite{Namolaru2010a,Leather2014}. These approaches closely tie the implementation of the predictive model to the compiler IR, which means changes to the IR will require modifications to the model. The work of \cite{Leather2014} uses genetic programming to search for features, and required a huge grammar to be written, some 160kB in length. Although much of this can be created from templates, selecting the right range of capabilities and search space bias is non trivial and up to the expert. The work of \cite{Namolaru2010a} expresses the space of features via logic programming over relations that represent information from the IRs. It greedily searches for expressions that represent good features. However, their approach relies on expert selected relations, combinators and constraints to work. For both approaches, the search time may be significant.

Cavazos \emph{et al.\ }present a reaction-based predictive model for software-hardware co-design~\cite{Cavazos2006}. Their approach profiles the targetprogram using several carefully selected compiler options to see how programruntime changes under these options for a given micro-architecture setting. Theythen use the program ``reactions'' to predict the best available applicationspeedup. While their approach does not use static code features, developers mustcarefully select a few settings from a large number of candidate options forprofiling, because poorly chosen options can significantly affect the quality ofthe model. Moreover, the program must be run several times before optimization,while our technique does not require the program to be profiled.

In recent years, machine learning techniques have been employed to model and learn from program source code on various tasks. These include mining coding conventions~\cite{Allamanis2014a} and idioms~\cite{Allamanis2014}, API example code~\cite{Zhang2015a} and pseudo-code generation~\cite{Oda2015}, and benchmark generation~\cite{Cummins2017a}. Our work is the first attempt to extend the already challenging task of modeling distributions over source code to learning distributions over source code with respect to code optimizations.

Recently, deep neural networks have been shown to be a powerful tool for feature engineering in various tasks including image recognition~\cite{Krizhevsky2012,He2016} and audio processing~\cite{Lee2009b}. No work so far has applied deep neural networks for program feature generation. Our work is the first to do so.
