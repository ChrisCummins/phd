% -*- ispell-local-dictionary: "en_US" -*-
\input{preamble}

\begin{document}

\markboth{C. Cummins et al.}{Collaborative Autotuning of Algorithmic Skeletons for CPUs and GPUs}

\title{Collaborative Autotuning of Algorithmic Skeletons for GPUs and CPUs}

\author{CHRIS CUMMINS, PAVLOS PETOUMENOS, MICHEL STEUWER, and HUGH LEATHER
\affil{University of Edinburgh}}

\begin{abstract}
  The physical limitations of microprocessor design have forced the industry towards increasingly heterogeneous designs to extract performance, pressuring developers to offload traditionally CPU based workloads to the GPU. Currently this requires developers to use programming languages such as OpenCL or CUDA which provides a low level model with little abstraction above the hardware. Programming at this level requires expert knowledge of both the domain and the target hardware, and achieving high performance requires laborious hand tuning of each program. This has led to a growing disparity between the availability of parallelism in modern hardware and the ability for application developers to exploit it.

The goal of this work is to bring the performance of hand tuned heterogeneous code to high level programming by incorporating autotuning into \textit{Algorithmic Skeletons}. Algorithmic Skeletons simplify parallel programming by providing reusable, high-level, patterns of computation. However, achieving performant skeleton implementations is a difficult task; skeleton authors must attempt to anticipate and tune for a wide range of architectures and use cases. This results in implementations that target the general case and cannot provide the performance advantages to be gained from tuning low level optimization parameters for individual programs and architectures. Autotuning combined with machine learning offers promising performance benefits by tailoring parameter values to individual cases, but the high cost of training and the ad-hoc nature of existing autotuners limits the practicality of autotuning for real world programming.

To address these issues we introduce \textit{OmniTune} --- an extensible and open source framework for dynamically autotuning Algorithmic Skeletons at program runtime. Embedding autotuning at the skeletal level overcomes the limitations of ad-hoc autotuners whilst allowing \textit{learning} of the optimization spaces across entire computational patterns. OmniTune provides a lightweight interface for autotuning using machine learning, with training data distributed across devices. We provide a demonstration of OmniTune by autotuning OpenCL workgroup sizes in the Algorithmic Skeleton library \textit{SkelCL}. We evaluate three methodologies for predicting workgroup sizes in an empirical study of 429 combinations of architecture, kernel, and dataset, comparing an average of 629 different workgroup sizes for each. We find that autotuning provides a median $3.79\times$ speedup over the best possible fixed workgroup size, achieving 94\% of the maximum performance. Unlike state of the art stencil autotuners, OmniTune achieves this without requiring any exploration phase for unseen programs, requires no modifications to user code, and introduces only a 2.5ms overhead to perform parameter prediction.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10011007.10010940.10011003.10011002</concept_id>
        <concept_desc>Software and its engineering~Software performance</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10011007.10011006.10011041</concept_id>
        <concept_desc>Software and its engineering~Compilers</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
    <concept>
        <concept_id>10011007.10011006.10011041.10011047</concept_id>
        <concept_desc>Software and its engineering~Source code generation</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software performance}
\ccsdesc[300]{Software and its engineering~Compilers}
\ccsdesc[300]{Software and its engineering~Source code generation}

%
% End generated code
%

\keywords{%
  Adaptive Optimization,
  Algorithmic Skeletons,
  Autotuning,
  GPGPU,
  Machine Learning,
  OpenCL,
  Stencils,
  Workgroup Size%
}

\acmformat{%
  Chris Cummins, Pavlos Petoumenos, Michel Steuwer, and Hugh Leather,
  2015. Collaborative Autotuning of Algorithmic Skeletons for GPUs and
  CPUs.%
}

\begin{bottomstuff}
  Corresponding Author: C. Cummins, University of Edinburgh;
  email: \url{c.cummins@ed.ac.uk}.\\*
  This work was supported by the UK Engineering and Physical Sciences
  Research Council under grants EP/L01503X/1 for the University of
  Edinburgh School of Informatics Centre for Doctoral Training in
  Pervasive Parallelism
  (\url{http://pervasiveparallelism.inf.ed.ac.uk/}), EP/H044752/1
  (ALEA), and EP/M015793/1 (DIVIDEND).
\end{bottomstuff}

\maketitle


\section{Introduction}\label{sec:introduction}

General purpose programming with GPUs offers huge throughput for data parallel workloads, but this comes at the cost of increased code complexity. Programming for GPUs requires application developers to master a low level programming model (such as provided by CUDA or OpenCL) and architecture (SIMD with a multi-level memory hierarchy). As a result, GPGPU programming is often beyond the scope of everyday development, requiring a committed approach and a much greater investment of a developer's time than traditional parallel programming~\cite{Fuller2011}.

\textit{Algorithmic Skeletons} offer a solution to this problem by raising the level of abstraction above that of coordinating parallel resources. Algorithmic Skeletons allow application developers to structure their problem-solving logic sequentially, which frees programmers from the challenges of coordinating parallel resources~\cite{Cole2004,Gonzalez2010}. This has proven to be a promising solution to the \textit{programmability challenge} of GPU parallelism, and has led to a rising number of Algorithmic Skeleton frameworks which support GPU hardware~\cite{%
  Enmyren2010,% SkePU
  Ernsting2012,% Muesli
  Marques2013,%
  Steuwer2011,% SkelCL
  Aldinucci2014% FastFlow
}.

The challenge facing Algorithmic Skeletons is that of \textit{performance portability}. By raising the level of abstraction, Algorithmic Skeletons necessarily hide low level optimization parameters which influence the behavior of parallel programs; for example, the group size of threads on many-core SIMD architectures. The performance of parallel programs is sensitive to the values of these parameters, and when tuning to maximize performance, one size does not fit all. The suitability of parameter values depends on the program implementation, the target hardware, and even the dataset that is operated upon.

Iterative compilation and autotuning have been shown to help in these cases by automating the selection of parameter values to match individual execution environments~\cite{%
  Bodin1998,% Iterative compilation in a non-linear optimisation space
  Chen,% Evaluating Iterative Optimization Across 1000 Data Sets
  Choi2010% Model-driven autotuning of sparse matrix-vector multiply on GPUs
}. However, these approaches are typically ad-hoc and require a lengthy exploration phase to converge on good parameter values.

We believe that by embedding autotuning at the \textit{skeletal} level, it is possible to achieve performance with algorithmic skeletons that is competitive with --- and in some cases, exceeds --- that of hand tuned parallel implementations, without requiring any modifications to user code. There are two key motivations for our hypothesis: first, autotuning at the skeletal level provides implicit information about the nature of the computation being tuned, as each skeleton has a well defined interface and pattern of execution; secondly, the autotuning logic can be entirely encapsulated within the skeleton implementation, allowing training data to be shared across multiple systems. In this work, we implement and evaluate this proposed approach to autotuning. Our key contributions are:
%
\begin{itemize}
\item The design and implementation of a generic toolset for
  autotuning: \emph{OmniTune} is a novel and extensible framework for
  collaboratively autotuning Algorithmic Skeletons across the life
  cycle of programs. OmniTune abstracts autotuning logic, requiring
  minimal modifications to Algorithmic Skeleton
  implementations. Training data for machine learning is distributed
  across systems to allow the collaborative training of optimization
  spaces.
\item A practical demonstration of OmniTune using \textit{SkelCL}, an
  established Algorithmic Skeleton library for CPU and multi-GPU
  parallelism. We implement three methodologies for selecting OpenCL
  workgroup sizes for the SkelCL Stencil skeleton --- the first, using
  classifiers to predict optimal workgroup sizes. The second and third
  methodologies employ the novel use of regressors for performing
  classification by predicting the runtime of kernels and the relative
  performance of different workgroup sizes, respectively.
\item An exhaustive evaluation of the OpenCL workgroup size
  optimization across 429 combinations of stencil programs, devices,
  and datasets. We combine procedurally generated and real world
  stencil benchmarks to evaluate the effectiveness of our autotuning
  technique, demonstrating that OmniTune provides a meedian
  $3.79\times$ speedup over the best statically selected workgroup
  size, achieving 94\% of the maximum performance.
\end{itemize}

\section{Motivation}

\begin{figure}
\centering
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_1}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-1}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_2}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-2}
    \end{subfigure}
    \caption{%
      The performance of different workgroup sizes for the same stencil
      program on two different devices: (\subref{fig:motivation-1})
      Intel CPU, (\subref{fig:motivation-2}) NVIDIA GPU. Selecting an
      appropriate workgroup size depends on the execution device.%
    }
    \label{fig:motivation-arch}
  \end{minipage}%
}%
\hspace{2.5mm}
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_3}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-3}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=1.0\columnwidth]{img/motivation_4}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:motivation-4}
    \end{subfigure}
    \caption{%
      The performance of different workgroup sizes for the same device
      executing two different stencil programs. Selecting an appropriate
      workgroup size depends on the program.%
    }
    \label{fig:motivation-prog}
  \end{minipage}%
}
\end{figure}

By way of motivating the development of autotuning for Algorithmic Skeletons, we provide a brief examination of the performance impact the OpenCL workgroup size parameter has on the performance of Stencil skeletons in SkelCL (see Section~\ref{sec:omnitune-skelcl} for a full explanation). SkelCL uses OpenCL to parallelise skeleton operations on GPUs and CPUs across many threads~\cite{Steuwer2011}. In OpenCL, multiple threads are grouped into \emph{workgroups}. The shape and size of these workgroups is known to have a big impact on performance. For the SkelCL stencil skeleton, the selection of workgroup size presents a two dimensional parameter space of rows ($w_r$) and columns ($w_c$). Measuring and plotting the runtime of SkelCL stencil kernels using different workgroup sizes allows us to compare the performance of different workgroup sizes for different combinations of architecture and program. Figure~\ref{fig:motivation-arch} shows this performance comparison for a single stencil program on two different devices, demonstrating that a good choice of workgroup size is device dependent. The optimization space of the same stencil kernel on different devices is radically different: not only does the optimal workgroup size change between devices, but the performance of suboptimal workgroup sizes is also dissimilar. The optimization space of~\ref{fig:motivation-1} has a grid-like structure, with clear performance advantages of workgroup sizes at multiples of 8 for $w_c$. A developer specifically targeting this device would learn to select workgroup sizes following this pattern. This domain specific knowledge clearly does not transfer to the device shown in~\ref{fig:motivation-2}.

In Figure~~\ref{fig:motivation-prog}, we compare the performance of two different stencil programs on the \emph{same} device, showing that workgroup size choice is also program dependent. In each of these four examples, the optimal workgroup size changes, as does the relative performance of suboptimal parameters. The average speedup of the best over the worst workgroup size is $37.0\times$, and the average performance that can be achieved using a single fixed workgroup size is only 63\% of the maximum performance achieved using a different workgroup size for each situation.

SkelCL uses a fixed workgroup size by default. Since both the execution device and the user-provided stencil code are not known until runtime, dynamic selection of workgroup size can provide clear performance benefits. To the best of our knowledge, there is currently no such generic system meeting our requirements for lightweight runtime machine learning autotuning with distributed training sets.


\section{The OmniTune Framework}\label{sec:autotune}

\begin{figure}
\centering
\includegraphics[width=.6\columnwidth]{img/omnitune-system-overview.pdf}
\caption{%
  OmniTune system architecture, showing the separate components and
  the many to one relationship between client applications and
  servers, and servers to remotes.%
  % \vspace{-2em}%
}
\label{fig:omnitune-system-overview}
\end{figure}

OmniTune is our novel open source\footnote{OmniTune source code available
  at: \url{http://chriscummins.cc/s/omnitune}.}, framework for
distributed autotuning of Algorithmic Skeletons using machine
learning. It is an extensible, generic platform for developing
autotuning solutions, aiming to reduce both the engineering effort
required to target new optimization parameters, and the time to
deployment on new systems.

OmniTune emphasizes collaborative learning of optimization spaces. A
client-server architecture with clearly delineated separation of
concerns minimizes the code footprint in client applications, and enables
quick re-purposing for new autotuning targets. OmniTune provides a
lightweight interface for communication between each of the
components, and aims to strike a balance between offering a fully
featured environment for quickly implementing autotuning, while
providing enough flexibility to cater to a wide range of use
cases. First, we describe the overall structure of OmniTune and the
rationale for the design, followed by the interfaces and steps
necessary to integrate OmniTune with a Skeleton library.


\subsection{System Architecture}

OmniTune is built around a three tier client-server architecture,
shown in Figure~\ref{fig:omnitune-system-overview}. The applications
which are to be autotuned are the \emph{clients}. These clients
communicate with a system-wide \emph{server}, which handles autotuning
requests. The server communicates and caches data sourced from a
\emph{remote} server, which maintains a global store of all autotuning
data. There is a many to one relationship between clients, servers,
and remotes, such that a single remote may handle connections to
multiple servers, which in turn may accept connections from multiple
clients. This design has two primary advantages: the first is that it
decouples the autotuning logic from that of the client program,
allowing developers to easily repurpose the autotuning framework to
target additional optimization parameters without a significant
development overhead for the target applications; the second advantage
is that this enables collective tuning, in which training data
gathered from a range of devices can be accessed and added to by any
OmniTune server.

Common implementations of autotuning in the literature either embed
the autotuning logic within each target application
(e.g.~\cite{Chen2014}), or take a standalone approach in which the
autotuner is a separate program which must be externally invoked by
the user (e.g.~\cite{Lutz2013}). Embedding the autotuner within each
target application has the advantage of providing ``always-on''
behavior, but is infeasible for complex systems in which the cost of
building machine learning models must be added to each program
run. The standalone approach separates the autotuning logic, at the
expense of adding one additional step to the build process. The
client-server approach taken in OmniTune aims to combine the
advantages of both techniques by implementing autotuning \emph{as a
  service}, in which a standalone autotuning server performs the heavy
lifting of managing training data and machine learning models, with a
minimal set of lightweight communication logic to be embedded in
target applications.

The OmniTune framework is implemented as a set of Python classes which are extended to target specific parameters. The generic implementation of OmniTune's server and remote components consists of 8987 lines of Python and MySQL code. No predefined client logic is provided, since that is use case dependent (see Section~\ref{sec:omnitune-skelcl} for details of a concrete implementation for SkelCL). Inter-process communication between client programs and the server uses the D-Bus protocol. D-Bus is cross-platform, and bindings are available for most major programming languages, allowing flexibility for use with a range of clients. Communication between servers and remotes uses TCP/IP (we used an Amazon Web Services instance for testing).


\subsection{Autotuning Behavior}

The goal of machine learning enabled autotuning is to build models
from empirical performance data of past programs to select parameter
values for new \emph{unseen} programs. Instead of an iterative search
for good parameters, parameter values are \emph{predicted} based on
models built from measured performance and \emph{features}
(explanatory variables) of past values. The data used to build such
models is called training data. OmniTune supports autotuning using a
separate offline training phase, online training, or a mixture of
both. For each autotuning-capable machine, an OmniTune server hosts
the autotuning logic. On launch, the server requests the latest
training data from the remote, which it uses to build the relevant
models for performing parameter value prediction. If additional
training data is gathered by the server, this can be uploaded and
synchronized with the remote.

While the data types of the autotuning interface are
application-specific (e.g. a binary flag or one or more numeric
values), the general pattern is that a client application will request
parameter values from an OmniTune server by sending it a set of
explanatory variables. The server will then use machine learning
models to form a prediction for the optimal parameter values and
return these. Crucially, there is a mechanism provided for the client
to \emph{refuse} parameter values. This functionality is provided for
cases where the predicted parameter values are in some way invalid and
do not lead to a valid program.

The server contains a library of machine learning tools to perform parameter prediction, interfacing with the popular datamining software suite Weka\footnote{Weka 3 available at:  \url{http://www.cs.waikato.ac.nz/ml/weka/}}. The provided tools include classifiers, regressors, and a selection of meta-learning algorithms.

OmniTune servers may perform additional feature extraction of
explanatory variables supplied by incoming client requests. The reason
for performing feature extraction on the server as opposed to on the
client side is that this allows the results of expensive operations
(for example, analyzing source code of target applications) to be
cached for use across the lifespan of client applications. The
contents of these local caches are periodically and asynchronously
synced with the remote to maintain a global store of lookup tables for
expensive operations.


\subsection{Interfaces}

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{img/omnitune-system-flow.pdf}
\caption{%
  Predicting parameter values and collecting training data with
  OmniTune.%
}
\label{fig:omnitune-system-flow}
\end{figure}

Key design elements of OmniTune are the interfaces exposed by the
server and remote components.

\paragraph{Client-Server} An OmniTune server exposes a public
interface over D-Bus with four operations. Client applications invoke
these methods to request parameter values, submit new training
observations, and refuse suggested parameters:
%
\begin{itemize}
\item \textsc{Request}$(x) \to p$\\*Given explanatory variables $x$,
  request the parameter values $p$ which are expected to provide
  maximum performance.
\item \textsc{RequestTraining}$(x) \to p$\\*Given explanatory
  variables $x$, allow the server to select parameter values $p$ for
  evaluating their fitness.
\item \textsc{Submit}$(x, p, y)$\\*Submit an observed measurement of
  fitness $y$ for parameter values $p$, given explanatory variables
  $x$.
\item \textsc{Refuse}$(x, p)$\\*Refuse parameter values $p$, given a
  set of explanatory variables $x$. Once refused, those parameters are
  blacklisted and will not be returned by any subsequent calls to
  \textsc{Request()} or \textsc{RequestTraining()} for the same
  explanatory variables $x$.
\end{itemize}
%
% This set of operations enables the core functionality of an
% autotuner, while providing flexibility for the client to control how
% and when training data is collected.

\paragraph{Server-Remote} The role of the remote is to provide
bookkeeping of training data for machine learning. Remotes allow
shared access to data from multiple servers using a transactional
communication pattern, supported by two methods:
%
\begin{itemize}
\item \textsc{Push}$(\bf{x}, \bf{p}, \bf{y})$\\*Asynchronously submit
  training data as three lists: explanatory variables $\bf{x}$,
  parameter values $\bf{p}$, and observed outcomes $\bf{y}$.
\item \textsc{Pull}$() \to (\bf{x}, \bf{p}, \bf{y})$\\*Request
  training data as three lists: explanatory variables $\bf{x}$,
  parameter values $\bf{p}$, and observed outcomes $\bf{y}$.
\end{itemize}


\subsection{Extensibility}

To extend OmniTune to target an optimization parameter, a developer
extends the Python server class to implement response handlers for the four
public interface operations, and then inserts client code into the
target application to call these operations. The implementation of
these response handlers and invoking client code determines the type
of autotuning methods supported. Figure~\ref{fig:omnitune-system-flow}
shows the flow diagram for an example OmniTune implementation. The
call to \textsc{RequestTraining()} is matched with a response call of
\textsc{Submit()}, showing the client recording a training
observation. In the next Section, we will detail the steps required to
apply OmniTune to SkelCL.


\section{Integration of OmniTune with SkelCL}\label{sec:omnitune-skelcl}

In this section we demonstrate the practicality of OmniTune by integrating the framework into an established open source\footnote{SkelCL source code available at:  \url{https://github.com/skelcl/skelcl}} Algorithmic Skeleton library. Introduced in~\cite{Steuwer2011}, SkelCL enables users to easily harness the power of GPUs and CPUs for data parallel computing, offering a set of OpenCL implementations of data parallel skeletons in a C++ library. Skeletons are parameterised with user functions which are compiled into OpenCL kernels for execution on GPUs and CPUs.

\subsection{The Stencil Skeleton}

\begin{figure}
  \centering
  \begin{subfigure}[h]{.55\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{img/lena-stencil}
  \end{subfigure}
  \begin{subfigure}[h]{.4\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{stencil}
  \end{subfigure}
  \caption{%
    Application of a Gaussian blur stencil operation to an image: the
    input pixels are decomposed into workgroups, consisting of
    $w_r \times w_c$ elements. Each element is mapped to a
    work-item. Each work-item operates on its corresponding element
    and a surrounding border region. In a Gaussian blur, pixel values
    are interpolated with neighboring pixels, producing a smoothed
    effect.%
  }
  \label{fig:stencil-img}
\end{figure}

Stencils are patterns of computation which operate on uniform grids of
data, where the value of each grid element (cell) is updated based on
its current value and the value of one or more neighboring elements,
called the \emph{border region}. Figure~\ref{fig:stencil-img} shows
the use of a stencil to apply a Gaussian blur to an image. SkelCL
provides a 2D stencil skeleton which allows users to provide a
function which updates a cell's value, while SkelCL orchestrates the
parallel execution of this function across all
cells~\cite{Steuwer2014a}. The border region is described by a
\emph{stencil shape}, which defines an $i \times j$ rectangular region
around each cell which is used to update the cell value. Stencil
shapes may be asymmetrical, and are defined in terms of the number of
cells in the border region to the north, east, south, and west of each
cell $(S_n, S_e, S_s, S_w)$. Given a function $f$, a stencil shape $S$, and an $n \times m$
matrix with elements $x_{ij}$:
%
\begin{equation}
\scriptsize
% \begin{split}
\stencil \left( f, S,
\begin{bmatrix}
  x_{11} & \cdots & x_{1m} \\
  \vdots & \ddots & \vdots \\
  x_{n1} & \cdots & x_{nm}
\end{bmatrix} \right)
\to
\begin{bmatrix}
  z_{11} & \cdots & z_{1m} \\
  \vdots & \ddots & \vdots \\
  z_{n1} & \cdots & z_{nm}
\end{bmatrix}
% \end{split}
\end{equation}
%
where:
%
\begin{equation}
\scriptsize
z_{ij} = f \left(
\begin{bmatrix}
  x_{i-S_n,j-S_w} & \cdots & x_{i-S_n,j+S_e} \\
  \vdots & \ddots & \vdots \\
  x_{i+S_s,j-S_w} & \cdots & x_{i+S_s,j+S_e}
\end{bmatrix} \right)
\end{equation}
%
For border region elements outside the bounds of the matrix, values
are substituted from either a predefined padding value, or the value
of the nearest element within the matrix, depending on user
preference. A popular usage of Stencil codes is for iterative problem
solving, whereby a stencil operation is repeated over a range of
discrete time steps $0 \le t \le t_{max}$, and $t \in \mathbb{N}$. An
iterative stencil operation $g$ accepts a customizing function $f$, a
Stencil shape $S$, and a matrix $M$ with initial values
$M_{init}$. The value of an iterative stencil can be defined
recursively as:
%
\begin{equation}
\scriptsize
g(f, S, M, t) =
\begin{cases}
  \stencil \left( f, S, g(f, S, M, t-1) \right),& \text{if } t \geq 1\\
  M_{init}, & \text{otherwise}
\end{cases}
\end{equation}
%
Examples of iterative stencils include cellular automata and partial
differential equation solvers.

In the implementation of the SkelCL stencil skeleton, each element in
the matrix is mapped to a unique thread (known as a \emph{work item}
in OpenCL) which applies the user-specified function. The work items
are then divided into \emph{workgroups} for execution on the target
hardware. Each work-item reads the value of its corresponding matrix
element and the surrounding elements defined by the border
region. Since the border regions of neighboring elements overlap, the
value of all elements within a workgroup are copied into a
\emph{tile}, allocated as a contiguous region of the fast, but small
local memory. As local memory access times are much faster than that
of global device memory, this greatly reduces the latency of the
border region memory accesses performed by each work item. Changing
the size of workgroups thus directly affects the amount of local memory
required for each workgroup, and in turn affects the number of
workgroups which may be simultaneously active on the device. While the
user defines the data size and type, the shape of the border region,
and the function being applied to each element, it is the
responsibility of the SkelCL stencil implementation to select an
appropriate workgroup size to use.


\subsection{Optimization Parameters}\label{subsec:op-params}

SkelCL stencil kernels are parameterised by a workgroup size $w$,
which consists of two integer values to denote the number of rows and
columns in a workgroup. The space of optimization parameter values is
subject to hard constraints, and these constraints cannot conveniently
be statically determined. Contributing factors are architectural
limitations, kernel constraints, and parameters which are refused for
other reasons.  Each OpenCL device imposes a maximum workgroup size
which can be statically checked. These are defined by architectural
limitations of how code is mapped to the underlying execution
hardware. Typical values are powers of two, e.g.\ 1024, 4096, 8192. At
runtime, once an OpenCL program has been compiled to a kernel, users
can query the maximum workgroup size supported by that particular
kernel dynamically. This value cannot be obtained statically as
there is no mechanism to determine the maximum workgroup size for a
given source code and device without first compiling it, which in
OpenCL does not occur until runtime.

Factors which affect a kernel's maximum workgroup size include the
number of registers required for a kernel, and the available number of
SIMD execution units for each type of instructions in a kernel. In
addition to satisfying the constraints of the device and kernel, not
all points in the workgroup size optimization space are guaranteed to
provide working programs. A \emph{refused parameter} is a workgroup
size which satisfies the kernel and architectural constraints, yet
causes a \texttt{CL\_OUT\_OF\_RESOURCES} error to be thrown when the
kernel is enqueued. Note that in many OpenCL implementations, this
error type acts as a generic placeholder and may not necessarily
indicate that the underlying cause of the error was due to finite
resources constraints. We define a \emph{legal} workgroup size as one
which, for a given \emph{scenario} $s$ (a combination of program,
device, and dataset), satisfies the architectural and kernel
constraints, and is not refused. The subset of all possible workgroup
sizes $W_{legal}(s) \subset W$ that are legal for a given scenario $s$
is then:
%
\begin{equation}
  W_{legal}(s) = \left\{w | w \in W, w < W_{\max}(s) \right\} - W_{refused}(s)
\end{equation}
%
Where $W_{\max}(s)$ can be determined at runtime prior to the kernels
execution, but the set $W_{refused}(s)$ can only be determined
experimentally.

The \emph{oracle} workgroup size $\Omega(s) \in W_{legal}(s)$ of a scenario $s$ is the $w$ value which provides the lowest mean runtime. The relative performance $p(s,w)$ of a particular workgroup size against the maximum available performance for that scenario, within the range $0 \le p(s,w) \le 1$, is the ratio of the runtime of a program with workgroup size $w$ over the oracle workgroup size $\Omega(s)$. For a given workgroup size, the average performance $\bar{p}(w)$ across a set of scenarios $S$ can be found using the geometric mean of performance relative to the oracle:
%
\begin{equation}
  \bar{p}(w) =
  \left(
    \prod_{s \in S} p(s, w)
  \right)^{1/|S|}
\end{equation}
%
The \emph{baseline} workgroup size $\bar{w}$ is the value which
provides the best average case performance across a set of
scenarios. Such a baseline value represents the \emph{best} possible
performance which can be achieved using a single, statically chosen
workgroup size. By defining $W_{safe} \in W$ as the intersection of
legal workgroup sizes, the baseline can be found using:
%
\begin{align}
W_{safe} &= \cap \left\{ W_{legal}(s) | s \in S \right\}\\
\bar{w} &= \argmax_{w \in W_{safe}} \bar{p}(w)
\end{align}


\subsection{Machine Learning}

This section presents three contrasting methods for predicting
workgroup sizes of \emph{unseen} scenarios.


\subsubsection{Predicting Oracle Workgroup Sizes}

\begin{algorithm}[t]
  \SetAlgoLined
  \KwData{scenario $s$}
  \KwResult{workgroup size $w$}
  \SetKwRepeat{Do}{do}{while}

  $w \leftarrow \text{classify}(f(s))$\;
  $W \leftarrow \left\{ w | w < W_{\max}(s) \right\} - W_{refused}(s)$\;
  \While{$w \not\in W_{legal}(s)$}{%
    $W = W - \{w\}$\;
    \eIf{random}{%
      $w \leftarrow $ random selection $w \in W$\;
    }{%
      $d_{min} \leftarrow \infty$; $w_{closest} \leftarrow \text{null}$\;
      \For{$c \in W$}{
        $d \leftarrow \sqrt{\left(c_r - w_r\right)^2 + \left(c_c - w_c\right)^2}$\;
        \If{$d < d_{min}$}{%
          $d_{min} \leftarrow d$\;
          $w_{closest} \leftarrow c$\;
        }
      }
      $w \leftarrow w_{closest}$\;
    }
  }
\caption{Prediction using classifiers}
\label{alg:autotune-classification}
\end{algorithm}

The first approach is detailed in
Algorithm~\ref{alg:autotune-classification}. By considering the set of
possible workgroup sizes as a hypothesis space, we train a classifier
to predict, for a given set of features, the \emph{oracle} workgroup
size. The oracle workgroup size $\Omega(s)$ is the workgroup size
which provides the lowest mean runtime $t(s,w)$ for a scenario $s$:
%
\begin{equation}
  \Omega(s) = \argmin_{w \in W_{legal}(s)} t(s,w)
\end{equation}
%
Training a classifier for this purpose requires pairs of stencil
features $f(s)$ to be labeled with their oracle workgroup size for a
set of training scenarios $S_{training}$:
%
\begin{equation}
  D_{training} = \left\{ \left(f(s), \Omega(s)\right) | s \in S_{training} \right\}
\end{equation}
%
After training, the classifier predicts workgroup sizes for unseen
scenarios from the set of oracle workgroup sizes from the training
set. This is a common and intuitive approach to autotuning, in that a
classifier predicts the best parameter value based on what worked well
for the training data. However, given the constrained space of
workgroup sizes, this presents the problem that future scenarios may
have different sets of legal workgroup sizes to that of the training
data, i.e.:
%
\begin{equation}
  \bigcup_{\forall s \in S_{future}} W_{legal}(s) \nsubseteq \left\{ \Omega(s) | s \in S_{training} \right\}
\end{equation}
%
This results in an autotuner which may predict workgroup sizes that
are not legal for all scenarios, either because they exceed
$W_{\max}(s)$, or because parameters are refused,
$w \in W_{refused}(s)$. For these cases, we evaluate the effectiveness
of three \emph{fallback strategies}:
%
\begin{enumerate}
\item \emph{Baseline} --- select the workgroup size which provides the
  highest average case performance from the set of safe workgroup sizes.
\item \emph{Random} --- select a random workgroup size which is
  expected from prior observations to be legal.
\item \emph{Nearest Neighbor} --- select the workgroup size which
  from prior observations is expected to be legal, and has the lowest
  Euclidian distance to the prediction.
\end{enumerate}


\subsubsection{Predicting Kernel Runtimes}

\begin{algorithm}[t]
  \SetAlgoLined
  \KwData{scenario $s$, regressor $R(x, w)$, fitness function $\Delta(x)$}
  \KwResult{workgroup size $w$}
  \SetKwRepeat{Do}{do}{while}

  $W \leftarrow \left\{ w | w < W_{\max}(s) \right\} - W_{refused}(s)$\;
  % \Comment Candidates.
  \Do{$w \not\in W_{legal}(s)$}{%
    $w \leftarrow \underset{w \in W}{\argmax} \Delta(R(f(s), w))$\;
    % \Comment Select best candidate.
    $W \leftarrow W - \left\{ w \right\}$\;
    % \Comment Remove candidate from selection.
  }
\caption{Prediction using regressors}
\label{alg:autotune-regression}
\end{algorithm}

A problem of predicting oracle workgroup sizes is that, for each
training instance, an exhaustive search of the optimization space must
be performed in order to find the oracle workgroup size. An
alternative approach is to instead predict the expected \emph{runtime}
of a kernel given a specific workgroup size. Given training data
consisting of $(f(s),w,t)$ tuples, where $f(s)$ are scenario features,
$w$ is the workgroup size, and $t$ is the observed runtime, we train a
regressor $R(f(s), w)$ to predict the runtime of scenario and
workgroup size combinations. The selected workgroup size
$\bar{\Omega}(s)$ is then the workgroup size from a pool of candidates
which minimizes the output of the
regressor. Algorithm~\ref{alg:autotune-regression} formalizes this
approach of autotuning with regressors. A fitness function $\Delta(x)$
computes the reciprocal of the predicted runtime so as to favor
shorter over longer runtimes. Note that the algorithm is self
correcting in the presence of refused parameters --- if a workgroup
size is refused, it is removed from the candidate pool, and the next
best candidate is chosen. This removes the need for fallback
handlers. Importantly, this technique allows for training on data for
which the oracle workgroup size is unknown, and has the secondary
advantage that this allows for an additional training instance to be
gathered every time a kernel is evaluated.


\subsubsection{Predicting Relative Performance}

Accurately predicting the runtime of arbitrary code is a difficult
problem. It may instead be more effective to predict the relative
performance of two different workgroup sizes for the same kernel. To
do this, we predict the \emph{speedup} of a workgroup size over a
baseline. This baseline is the workgroup size which provides the best
average case performance across all scenarios and is known to be
safe. Such a baseline value represents the \emph{best} possible
performance which can be achieved using a single, fixed workgroup
size. As when predicting runtimes, this approach performs
classification using regressors
(Algorithm~\ref{alg:autotune-regression}). We train a regressor
$R(f(s), w)$ to predict the relative performance of workgroup size $w$
over a baseline parameter for scenario $s$. The fitness function
returns output of the regressor, so the selected workgroup size
$\bar{\Omega}(s)$ is the workgroup size from a pool of candidates
which is predicted to provide the best relative performance. This has
the same advantageous properties as predicting runtimes, but by
training using relative performance, we negate the challenges of
predicting dynamic code behavior.


\section{Experimental Setup}

To evaluate the performance of the presented autotuning techniques, an
exhaustive enumeration of the workgroup size optimization space for
429 combinations of architecture, program, and dataset was performed.

Table~\ref{tab:hw} describes the experimental platforms and OpenCL devices used. Each platform was unloaded, frequency governors disabled, and benchmark processes set to the highest scheduling priority. Datasets and programs were stored in an in-memory file system.

In addition to synthetic stencil codes, six stencil benchmarks taken from four reference implementations of standard stencil applications from the fields of image processing, cellular automata, and partial differential equation solvers are used: Canny Edge Detection, Conway's Game of Life, Heat Equation, and Gaussian Blur. For each program, datasets of size $512\times512$, $1024\times1024$, $2048\times2048$, and $4096\times4096$ were used.

All runtimes were recorded with millisecond precision using OpenCL's
Profiling API to record the kernel execution time. The workgroup size
space was enumerated for each combination of $w_r$ and $w_c$ values in
multiples of 2, up to the maximum workgroup size. A minimum of 30
samples were recorded for each scenario and workgroup size.

Program behavior is validated by comparing program output against a
gold standard output collected by executing each of the real-world
benchmarks programs using the baseline workgroup size. The output of
real-world benchmarks with other workgroup sizes is compared to this
gold standard output to test for correct program execution.

Five different classification algorithms are used to predict oracle
workgroup sizes, chosen for their contrasting properties: Naive Bayes,
SMO, Logistic Regression, J48 Decision tree, and Random
Forest~\cite{Han2011}. For regression, a Random Forest with regression
trees is used, chosen because of its efficient handling of large
feature sets compared to linear models~\cite{Breiman1999}. The
autotuning system is implemented in Python as a system daemon. SkelCL
stencil programs request workgroup sizes from this daemon, which
performs feature extraction and classification.

\begin{table}
\tiny
\centering
\begin{tabular}{l R{1.1cm} | l R{1.1cm} r R{1.2cm} R{1.2cm} R{1.2cm}}
\toprule
Host & Host Memory &  OpenCL Device &  Compute units & Frequency & Local Memory & Global Cache & Global Memory \\
\midrule
Intel i5-2430M & 8 GB  & CPU              &              4 &   2400 Hz &        32 KB &       256 KB &       7937 MB \\
Intel i5-4570  & 8 GB  & CPU              &              4 &   3200 Hz &        32 KB &       256 KB &       7901 MB \\
Intel i7-3820  & 8 GB  & CPU              &              8 &   1200 Hz &        32 KB &       256 KB &       7944 MB \\
Intel i7-3820  & 8 GB  & AMD Tahiti 7970  &             32 &   1000 Hz &        32 KB &        16 KB &       2959 MB \\
Intel i7-3820  & 8 GB  & Nvidia GTX 590   &              1 &   1215 Hz &        48 KB &       256 KB &       1536 MB \\
Intel i7-2600K & 16 GB & Nvidia GTX 690   &              8 &   1019 Hz &        48 KB &       128 KB &       2048 MB \\
Intel i7-2600  & 8 GB  & Nvidia GTX TITAN &             14 &    980 Hz &        48 KB &       224 KB &       6144 MB \\
\bottomrule
\end{tabular}
\caption{Specification of experimental platforms and OpenCL devices.}
\label{tab:hw}
\end{table}


\section{Performance Results}\label{sec:evaluation}

This section describes the performance results of enumerating the workgroup size optimization space. The effectiveness of autotuning techniques for exploiting this space are examined in Section~\ref{sec:autotuning}. The experimental results consist of measured runtimes for a set of \emph{test cases}, where a test case $\tau_i$ consists of a scenario, workgroup size pair $\tau_i = (s_i,w_i)$, and is associated with a \emph{sample} of observed runtimes of the program. A total of 269813 test cases were evaluated, which represents an exhaustive enumeration of the workgroup size optimization space for 429 scenarios. For each scenario, runtimes for an average of 629 (max 7260) unique workgroup sizes were measured. The average sample size for each test case is 83 (min 33, total 16917118).

The workgroup size optimization space is non-linear and complex, as shown in Figure~\ref{fig:oracle-param-space}, which plots the distribution of optimal workgroup sizes. Across the 429 scenarios, there are 135 distinct optimal workgroup sizes (31.5\%). The average speedup of the oracle workgroup size over the worst workgroup size for each scenario is $15.14\times$ (min $1.03\times$, max $207.72\times$).

Of the 8504 unique workgroup sizes tested, 11.4\% were refused in one or more test cases, with an average of 5.5\% test cases leading to refused parameters. There are certain patterns to the refused parameters: for example, workgroup sizes which contain $w_c$ and $w_r$ values which are multiples of eight are less frequently refused, since eight is a common width of SIMD vector operations~\cite{IntelCorporation2012}. However, a refused parameter is an obvious inconvenience to the user, as one would expect that any workgroup size within the specified maximum should generate a working program, if not a performant one.


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/oracle_param_space}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:oracle-param-space}
  \end{subfigure}
  ~%
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/coverage_space}
    \vspace{-1.5em} % Shrink vertical padding
    \caption{}
    \label{fig:coverage}
  \end{subfigure}
  \vspace{-.5em}
  \caption{%
    Log frequency counts for: (\subref{fig:oracle-wgsizes})
    optimality, and (\subref{fig:coverage}) legality for a subset of
    the aggregated workgroup size optimization space,
    $w_c \le 100, w_r \le 100$. The space of oracle workgroup size
    frequencies is highly irregular and uneven, with a most frequently
    optimal workgroup size of $w_{(64 \times 4)}$. Legality
    frequencies are highest for smaller row and column counts (where
    $w < W_{\max}(s) \forall s \in S$), and $w_c$ and $w_r$ values
    which are multiples of 8.%
  }
\label{fig:oracle-wgsizes}
\end{figure}

Experimental results suggest that the problem is vendor --- or at
least device --- specific. Figure~\ref{fig:refused-params} shows the
ratio of refused test cases, grouped by device. We see many more
refused parameters for test cases on Intel CPU devices than any other
type, while no workgroup sizes were refused by the AMD GPU. The exact
underlying cause for these refused parameters is unknown, but can
likely by explained by inconsistencies or errors in specific OpenCL
driver implementations. Note that the ratio of refused parameters
decreases across the three generations of Nvidia GPUs: GTX 590 (2011),
GTX 690 (2012), and GTX TITAN (2013). For now, it is imperative that
any autotuning system is capable of adapting to these refused
parameters by suggesting alternatives when they occur.

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{img/num_params_oracle.pdf}
\caption{%
  Accuracy compared to the oracle as a function of the number of
  unique workgroup sizes. The greatest accuracy that can be achieved
  using a single statically chosen workgroup size is 15\%. Achieving
  50\% oracle accuracy requires a minimum of 14 distinct workgroup
  sizes.%
}
\label{fig:oracle-accuracy}
\end{figure}

The baseline parameter is the workgroup size providing the best
overall performance while being legal for all scenarios. Because of
refused parameters, only a \emph{single} workgroup size
$w_{(4 \times 4)}$ from the set of experimental results is found to
have a legality of 100\%, suggesting that an adaptive approach to
setting workgroup size is necessary not just for the sake of
maximizing performance, but also for guaranteeing program
execution. The utility of the baseline parameter is that it represents
the best performance that can be achieved through static tuning of the
workgroup size parameter; however, compared to the oracle workgroup
size for each scenario, the baseline parameter achieves only 24\% of
the optimal performance.


We find that the \emph{human expert} selected workgroup size is
invalid for 2.6\% of scenarios, as it is refused by 11 test cases. By
device, these are: 3 on the GTX 690, 6 on the i5-2430M, and 2 on the
i5-4570. For the purpose of comparing performance against human
experts, we will ignore these test cases, but it demonstrates the
utility of autotuning not just for maximizing performance, but
ensuring program reliability. For the scenarios where the human expert
workgroup size \emph{is} legal, it achieves an impressive geometric
mean of 79.2\% of the oracle performance. The average speedup of
oracle workgroup sizes over the workgroup size selected by a human
expert is $1.37\times$ (min $1.0\times$, max $5.17\times$).

The utility of the \emph{baseline} workgroup size is that it
represents the best performance that can be achieved through static
tuning. The baseline workgroup size achieves only 24\% of the maximum
performance. Figures~\ref{fig:performance-wgsizes}
and~\ref{fig:performances} show box plots for the performance of all
workgroup sizes using different groupings: ratio of maximum workgroup
size, kernel, device, and dataset. The plots show the median
performance, interquartile range, and outliers. What is evident is
both the large range of workgroup size performances (i.e. the high
performance upper bounds), and the lack of obvious correlations
between any of the groupings and performance.

\begin{figure}
\centering
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_max_wgsize}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-max-wgsize}
    \end{subfigure}
    \\
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_max_c}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-wg-c}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_max_r}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-wg-r}
    \end{subfigure}
    \caption{%
      Comparing performance of workgroup sizes relative to the oracle as
      a function of: (\subref{fig:performance-max-wgsize})~maximum legal
      size, (\subref{fig:performance-wg-c})~number of columns, and
      (\subref{fig:performance-wg-r})~number of rows. Each workgroup
      size is normalized to the maximum allowed for that scenario,
      $W_{\max}(s)$. There is no clear correlation between workgroup
      size and performance.%
    }
    \label{fig:performance-wgsizes}
  \end{minipage}%
}%
\hspace{2.5mm}
\adjustbox{valign=t}{%
  \begin{minipage}{.48\textwidth}
    \centering
    \begin{subfigure}[h]{\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_kernels.pdf}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-kernels}
    \end{subfigure}
    \\
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_devices.pdf}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-devices}
    \end{subfigure}
    ~%
    \begin{subfigure}[h]{.48\columnwidth}
      \centering
      \includegraphics[width=\columnwidth]{img/performance_datasets.pdf}
      \vspace{-1.5em} % Shrink vertical padding
      \caption{}
      \label{fig:performance-datasets}
    \end{subfigure}
    \caption{%
      Performance relative to the oracle of workgroup sizes across all
      test cases, grouped by:
      (\subref{fig:performance-kernels})~kernels,
      (\subref{fig:performance-devices})~devices, and
      (\subref{fig:performance-datasets})~datasets. The performance
      impact is not consistent across kernels, devices, or datasets. The
      Intel i7-3820 has the lowest performance gains from tuning
      workgroup size.%
    }
    \label{fig:performances}
  \end{minipage}%
}
\end{figure}


\begin{figure}
  \centering
  \vspace{-1em}
  \includegraphics[width=.4\columnwidth]{refused_params_by_device}
  \vspace{-1em}
  \caption{%
    The ratio of test cases with refused workgroup sizes, grouped by
    OpenCL device ID. No parameters were refused by the AMD device.
    \vspace{-1em}%
  }
\label{fig:refused-params}
\end{figure}


\section{Autotuning Evaluation}\label{sec:autotuning}

In this section we evaluate the effectiveness of the three proposed
autotuning techniques for predicting performant workgroup sizes. For
each autotuning technique, we partition the experimental data into
training and testing sets. Three strategies for partitioning the data
are used: the first is a 10-fold cross-validation; the second is to
divide the data such that only data collected from synthetic
benchmarks are used for training and only data collected from the
real-world benchmarks are used for testing; the third strategy is to
create leave-one-out partitions for each unique device, kernel, and
dataset. For each combination of autotuning technique and testing
dataset, we evaluate each of the workgroup sizes predicted for the
testing data using the following metrics:
%
\begin{itemize}
\item time (real) --- the time taken to make the autotuning
  prediction. This includes classification time and any communication
  overheads.
\item accuracy (binary) --- whether the predicted workgroup size is
  the true oracle, $w = \Omega(s)$.
\item validity (binary) --- whether the predicted workgroup size
  satisfies the workgroup size constraints constraints,
  $w < W_{\max}(s)$.
\item refused (binary) --- whether the predicted workgroup size is
  refused, $w \in W_{refused}(s)$.
\item performance (real) --- the performance of the predicted
  workgroup size relative to the oracle for that scenario.
\item speedup (real) --- the relative performance of the predicted
  workgroup size relative to the baseline workgroup size
  $w_{(4 \times 4)}$.
\end{itemize}
%
The \emph{validty} and \emph{refused} metrics measure how often
fallback strategies are required to select a legal workgroup size
$w \in W_{legal}(s)$. This is only required for the classification
approach to autotuning, since the process of selecting workgroup sizes
using regressors respects workgroup size constraints.


\subsection{Predicting Oracle Workgroup Size}

Figure~\ref{fig:class-syn} shows the results when classifiers are
trained using data from synthetic benchmarks and tested using
real-world benchmarks. With the exception of the ZeroR, a dummy
classifier which ``predicts'' only the baseline workgroup size
$w_{\left( 4 \times 4 \right)}$, the other classifiers achieve good
speedups over the baseline, ranging from $4.61\times$ to $5.05\times$
when averaged across all test sets. The differences in speedups
between classifiers is not significant, with the exception of
SimpleLogistic, which performs poorly when trained with synthetic
benchmarks and tested against real-world programs. This suggests the
model over-fitting to features of the synthetic benchmarks which are
not shared by the real-world tests. Of the three fallback handlers,
\textsc{NearestNeighbour} provides the best performance, indicating
that it successfully exploits structure in the optimization space.


\subsection{Predicting with Regressors}

Figure~\ref{fig:regression-class} shows a summary of results for
autotuning using regressors to predict kernel runtimes
(\ref{fig:runtime-class-xval}) and speedups
(\ref{fig:speedup-class-xval}). Of the two regression techniques,
predicting the \emph{speedup} of workgroup sizes is much more
successful than predicting the \emph{runtime}. This is most likely
caused by the inherent difficulty in predicting the runtime of
arbitrary code, where dynamic factors such as flow control and loop
bounds are not captured by the instruction counts which are used as
features for the machine learning models. The average speedup achieved
by predicting runtimes is $4.14\times$. For predicting speedups, the
average is $5.57\times$, the highest of all of the autotuning
techniques.


\subsection{Autotuning Overheads}

Comparing the classification times of Figures~\ref{fig:class-syn}
and~\ref{fig:regression-class} shows that the prediction overhead of
regressors is significantly greater than classifiers. This is because,
while a classifier makes a single prediction, the number of
predictions required of a regressor grows with the size of
$W_{\max}(s)$, since classification with regression requires making
predictions for all $w \in \left\{ w | w < W_{\max}(s) \right\}$. The
fastest classifier is the J48 decision tree, due to the it's
simplicity --- it can be implemented as a sequence of nested
\texttt{if} and \texttt{else} statements. The measured overhead of
autotuning using this classifier is 2.5ms, of which only 0.3ms is
required for classification using Weka, although an optimized decision
tree implementation could reduce this further. The remaining 2.2ms is
required for feature extraction and the inter-process round trip
between the OmniTune server and client.

\begin{figure}
\centering
\includegraphics[width=.55\columnwidth]{classification-syn-real}
\vspace{-.7em}
\caption{%
  Autotuning performance using classifiers and synthetic benchmarks. Each
  classifier is trained on data collected from synthetic stencil
  applications, and tested for prediction quality using data from 6
  real-world benchmarks. Each of the different values correspond to a
  different data partitioning strategy, e.g.\ cross-kernel
  partitioning, 10-fold validation, etc. 95\% confidence intervals are
  shown where appropriate.
}
\label{fig:class-syn}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[h]{.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{runtime-class-xval}
\vspace{-2em}
\caption{}
\label{fig:runtime-class-xval}
\end{subfigure}
\begin{subfigure}[h]{.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{speedup-class-xval}
\vspace{-2em}
\caption{}
\label{fig:speedup-class-xval}
\end{subfigure}
\vspace{-.5em}
\caption{%
  Autotuning performance for each type of test dataset using
  regressors to predict: (\subref{fig:runtime-class-xval}) kernel
  runtimes, and (\subref{fig:speedup-class-xval}) relative performance
  of workgroup sizes.%
}
\label{fig:regression-class}
\end{figure}


\subsection{Comparison with Human Expert}

\begin{figure}
\centering
\includegraphics[width=.75\columnwidth]{speedup-distributions}
\vspace{-1.2em}
\caption{%
  Violin plot of speedups over \emph{human expert}, ignoring cases
  where the workgroup size selected by human experts is
  invalid. Classifiers are using \textsc{NearestNeighbour} fallback
  handlers. Horizontal dashed lines show the median, Q1, and
  Q3. Kernel Density Estimates show the distribution of results. The
  speedup axis is fixed to the range 0--2.5 to highlight the IQRs,
  which results in some outlier speedups > 2.5 being clipped.%
}
\label{fig:speedup-distributions}
\end{figure}

In the original implementation of the SkelCL stencil
pattern~\cite{Steuwer2014a}, a workgroup size of $w_{(32 \times 4)}$
was selected in an evaluation of 4 stencil operations on a Tesla S1070
system. In our evaluation of 429 combinations of kernel, architecture,
and dataset, we found that this workgroup size is refused by 2.6\% of
scenarios, making it unsuitable for use as a baseline. However, if we
remove the scenarios for which $w_{(32 \times 4)}$ is \emph{not} a
legal workgroup size, we can directly compare the performance against
the autotuning predictions.

Figure~\ref{fig:speedup-distributions} plots the distributions and
Interquartile Range (IQR) of all speedups over the human expert
parameter for each autotuning technique. The distributions show
consistent classification results for the five classification
techniques, with the speedup at Q1 for all classifiers being
$\ge 1.0\times$. The IQR for all classifiers is $< 0.5$, but there are
outliers with speedups both well below $1.0\times$ and well above
$2.0\times$. In contrast, the speedups achieved using regressors to
predict runtimes have a lower range, but also a lower median and a
larger IQR. Clearly, this approach is the least effective of the
evaluated autotuning techniques. Using regressors to predict relative
performance is more successful, achieving the highest median speedup
of all the techniques of $1.33\times$ over the human expert.


\subsection{OmniTune Extensibility}

The client-server architecture OmniTune neatly separates the
autotuning logic from the target application. This makes adjusting the
autotuning methodology a simple process. To demonstrate this, we
changed the machine learning algorithm from a J48 decision tree to a
Naive Bayes classifier, and duplicated the evaluation. This required
only a single line of source code in the OmniTune server extension to
be changed. Figure~\ref{fig:class-hmaps} visualizes the differences in
autotuning predictions when changing between these two
classifiers. While the average performances of the two classifiers is
comparable, the distribution of predictions is not. For example, the
Naive Bayes classifier predicted the human expert selected workgroup
size of $w_{(32 \times 4)}$ more frequently than it was optimal, while
the decision tree predicted it less frequently. Selection of machine
learning algorithms has a large impact on the effectiveness of
autotuning, and the OmniTune client-server design allows for low cost
experimenting with different approaches. In future work we will
investigate meta-tuning techniques for selecting autotuning
algorithms.

\begin{figure}
\centering
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_1}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-1}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_2}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-2}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_3}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-3}
\end{subfigure}
\\
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/heatmap_5}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-4}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/reg_runtime_heatmap}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-5}
\end{subfigure}
~%
\begin{subfigure}[t]{0.28\columnwidth}
\centering
\includegraphics[width=\columnwidth]{img/reg_speedup_heatmap}
\vspace{-1.5em} % Shrink vertical padding
\caption{}
\label{fig:class-hmaps-6}
\end{subfigure}
\caption[Classification error heatmaps]{%
  Heatmaps of classification errors for 10-fold cross-validation,
  showing a subset of the optimization space. The shading in each
  cells indicates if it is predicted less frequently (blue), ore more
  frequently (red) than it is optimal. Color gradients are normalized
  across plots.%
}
\label{fig:class-hmaps}
\end{figure}


\subsection{Summary}

In this section we have explored the performance impact of the
workgroup size optimization space, and the effectiveness of autotuning
using OmniTune to exploit this. By comparing the relative performance
of an average of 629 workgroup sizes for each of 429 scenarios, the
following conclusions can be drawn:

%
\begin{itemize}
\item The performance gap between the best and workgroup sizes for a
  particular combination of hardware, software, and dataset is up to
  $207.72\times$.
\item Not all workgroup sizes are legal, and the space of legal
  workgroup sizes cannot statically be determined. Adaptive tuning is
  required to ensure reliable performance.
\item Statically tuning workgroup size fails to extract the potential
  performance across a range of programs, architectures, and
  datasets. The best statically chosen workgroup size achieves only
  26\% of the optimal performance.
\item Workgroup size prediction using a decision tree achieves an
  average 90\% of the optimal performance.
\item Autotuning provides performance portability across programs,
  devices, and datasets. The performance of predicted workgroup sizes
  for unseen devices is within 8\% of the performance for known
  devices.
\end{itemize}


\section{Related Work}\label{sec:related}

Early work in autotuning applied iterative search techniques to the
space of compiler optimisations~\cite{Bodin1998,Kisuki}. Since then,
machine learning techniques have been successfully employed to reduce
the cost of iterative
compilation~\cite{Agakov,Stephenson2003,Fursin2011}. However,
optimizing for GPU hardware presents different challenges to that of
CPUs. Zero-overhead thread scheduling, memory bandwidth, and thread
grouping optimisations were applied to CUDA matrix multiplication
in~\cite{Ryoo2008a}, enabling speedups of up to $432\times$. The
importance of proper exploitation of local shared memory and
synchronization costs is explored
in~\cite{Lee2010}. In~\cite{Chen2014}, data locality optimisations are
automated using a description of the hardware and a
memory-placement-agnostic compiler. A machine learning model to
predict optimal thread coarsening factors of OpenCL kernels is
presented in~\cite{Magni2014}, demonstrating speedups between
$1.11\times$ and $1.33\times$.

The autotuning frameworks OpenTuner~\cite{Ansel2013} and CLTune~\cite{Nugteren2015} use iterative search techniques to explore optimization spaces. In OpenTuner, using ensemble search techniques; in CLTune, using simulated annealing or particle swarms. Since OpenTuner and CLTune do not \emph{learn} optimization spaces as OmniTune does, performance data is not shared across devices. This means that the search for good parameter values must be repeated for each new device and program to be autotuned. In contrast, our approach combines machine learning with distributed training sets so that these repeated explorations are avoided. \cite{Fursin2015} advocate a ``big data'' driven approach to compiler optimization, where datasets, benchmarks, and metadata about experimental setups are shared in a global repository. This work is complementary to our own, where we have demonstrated the utility of sharing training data for cross-device autotuning. The design of OmniTune is highly optimised for minimal latency autotuning, requiring us to use the client-server architecture to cache local copies of training data, keeping autotuning latency at 2.5ms.

For autotuning stencil codes, \cite{Ganapathi2009} drew upon the
successes of statistical machine learning techniques in the compiler
community, using Kernel Canonical Correlation Analysis to build
correlations between stencil features and optimization
parameters. Their use of KCCA restricts the scalability of their
system, as the complexity of model building grows exponentially with
the number of features. In~\cite{Garvey2015b}, synthetically generated
OpenCL stencils are autotuned for a single GPU using a random forest
classifier to predict memory loading strategy, followed by a heuristic
and an exhaustive search of additional parameter spaces. They evaluate
their system on a single GPU, cross-device performance.

A code generator and autotuner for 3D Jacobi stencil codes is
presented in~\cite{Zhang2013a}, although their approach requires a
full enumeration of the parameter space for each new program, and has
no cross-program learning. A DSL and CUDA code generator for stencils
is presented in~\cite{Kamil2010}. Unlike the SkelCL stencil pattern,
the generated stencil codes do not exploit fast local device
memory. Autotuning memory transfers for multi-GPU stencils is explored
in~\cite{Lutz2013} using a combination of offline tuning and runtime
parameter search. A comparison of tuning for CPU and GPU architectures
is made in~\cite{Christen2011}, although the authors do not compare
differences between GPUs. \cite{Falch2015} use Neural Networks to
autotune three hand-parameter OpenCL benchmarks. Their approach does
not account for invalid parameter values, which causes their autotuner
to predict invalid configurations for some benchmarks.

The automatic generation of synthetic benchmarks using parameterised
template substitution is presented in~\cite{Chiu2015}. The authors
describe an application of their tool for generating OpenCL stencil
kernels for machine learning, but do not report any performance
results.


\section{Conclusions and Future Work}\label{sec:conclusions}

As the trend towards increasingly programmable heterogeneous
architectures continues, the need for industrial quality, high level
abstractions to manage such parallelism will continue to
grow. Autotuning proves to be a valuable aid for achieving these
goals, provided that the burden of development and collecting
performance data is lifted from the user. The system presented in this
paper aims to solve this issue by providing a generic interface for
implementing machine learning-enabled autotuning.

OmniTune is a novel framework for autotuning which has the benefits of
a fast, ``always-on'' interface for client applications, while being
able to synchronize data with global repositories of knowledge which
are built up across devices. To demonstrate the utility of this
framework, we implemented a frontend for predicting the workgroup size
of OpenCL kernels for SkelCL stencil codes. This optimization space is
complex, non linear, and critical for the performance of stencil
kernels.  Selecting the correct workgroup size is difficult ---
requiring a knowledge of the kernel, dataset, and underlying
architecture. The autotuning techniques proposed in this paper achieve
up to 94\% of the maximum performance, providing median speedups of
$3.79\times$ over static tuning and $1.33\times$ over a human expert, while providing robust fallbacks in the presence of unexpected behavior of OpenCL driver implementations.

Of the three techniques proposed, predicting the relative performances
of workgroup sizes using regressors provides the greatest performance
improvements, whilst predicting the oracle workgroup size using
decision tree classifiers adds the lowest runtime overhead. This
presents a trade-off between classification time and training time
which could be explored in future work using a hybrid technique.

In future work we will further explore the transition towards online
machine learning, developing methods for the parallel exploration of
optimization spaces across multiple devices. This will be combined
with the use of adaptive sampling plans to minimize the number of
observations required to distinguish bad from good parameter values.

% TODO: High level patterns as core language features.
% \cite{%
% Stuart2011,% MapReduce on GPUs
% Sander2012,% AMD Bolt C++
% JaredHoberock2014a% ParallelSTL for C++
% }


% bibliography
\newpage % TODO: remove this page break
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{refs}


\end{document}
