\chapter{Introduction}

There has been an unprecedented increase in the scale and quantity of data-intensive workloads, with the energy consumption of the ICT industry estimated to rise to 20\% of global output by 2030~\cite{Andrae2019}. Fundamental shifts in both hardware and software are required to meet the demands of the transition to \emph{big data}~\cite{Hashem2015,Gandomi2015}.

Compilers play an essential role in extracting software efficiency by transforming the input code to a set of machine instructions that best utilises the available resources of the target architecture. Modern compilers are multi-million dollar projects comprising millions of lines of code from hundreds of developers, shown in Table~\ref{tab:compiler-costs}. Each architecture supported by a compiler requires extensive hand tuning by experts to extract good performance. Modern compilers are too complex to be fully understood by a single developer, yet optimising a single component requires knowledge of the full compiler and the interaction of its components.%

\begin{table}
  \centering
  \input{tab/compiler-costs}
  \caption[Development history and costs of popular open-source compilers]{%
    Development history, logical lines of code (LOC), and estimated cost of 10 popular open-source compiler projects. Estimated costs are calculated using a COCOMO model~\cite{David2001} with average 2019 US software developer salaries~\cite{Glassdoor2019}.%
  }
  \label{tab:compiler-costs}
\end{table}

Due to their high cost of development, compilers are unable to adapt to the rapidly changing hardware landscape arising from GPUs, FPGAs~\cite{Misra2010,Cong2016a}, and \linebreak ASICs~\cite{Misra2010,Jouppi2017}. When compilers are unable to keep up with the pace of change, this leads to wasted energy, slow performance, and buggy software. For the trend towards data-intensive workloads and heterogeneous devices to continue, new techniques are required to reduce the cost of compiler construction. This thesis aims to address the widening gap between the potential and achieved efficiency of software by developing new tools to simplify and improve compiler construction.

This thesis demonstrates the use of machine learning for lowering the cost of compiler construction. The remainder of this chapter describes the application of machine learning to compilers, followed by its challenges. Then the contributions of this thesis are detailed, followed by a description of the overall structure of the document.


\section{Machine Learning for Compilers}

Machine learning, the study of algorithms and systems capable of learning from data without being explicitly programmed, has been successfully applied across a broad range of fields and disciplines. Within compilers, there are many tasks for which machine learning may prove useful.

A common case where machine learning aids in compiler construction is in the labour-intensive process of optimisation heuristic construction. For example, suppose a developer is tasked with tuning the loop unrolling heuristic of a compiler\footnote{Loop unrolling is a code transformation in which the body of a loop is duplicated so that fewer iterations of the loop must be executed. The idea is to reduce runtime by executing fewer loop control instructions, at the expense of increasing the size of the executable binary.}. There is a multitude of factors that may influence the decision of whether to unroll a loop such as the size of the loop body, the number of loop iterations, and the number of registers required to execute the loop body. Determining which of these factors are most significant, and on what values the heuristic should differ, is an unwieldy task. The problem is worsened since the quality of the heuristic depends not just on the program being compiled, but on properties of the target hardware, such as the number of registers and the size of the instruction cache. In the face of these challenges, developing good heuristics is a huge undertaking, and it is unlikely that a developer will be able to craft a heuristic capable of extracting the best performance in all situations.

Instead, a developer may cast construction of the loop unrolling heuristic as a machine learning problem. Rather than expertly crafting a heuristic through intuition and manual experimentation, the idea is to use a learning algorithm to derive a heuristic from empirical data of the performance of loops under different configurations of unrolling. To do this, the developer profiles suites of benchmark programs repeatedly using different unrolling decisions to determine the best decision for each case, then combines this with a numerical representation of each program. A machine learning system then models the correlations between these numerical program representations --- \emph{features} --- and the unrolling decisions that should be made. Using machine learning for this task reduces the need for domain expertise compared to the expert-driven approach, requiring less effort from the developer. Unlike expert-driven approaches, the system can easily be adapted to new architectures and changes in the compiler simply by repeating the data collection process and constructing a new model.

The appeal of machine learning is that it provides techniques to automatically understand the structure of data and how that structure relates to a specific goal. For unseen data which has similar structures to the training data, this enables accurate predictions to be made; all without the need for expert domain knowledge. In essence, machine learning negates the need for problem-domain expertise in cases where there is a ready supply of empirical observations.

The applicability of machine learning to a wide range of tasks in compiler construction has led to an established research direction. In previous studies machine learning has been shown to simplify the construction of compiler optimisations~\cite{Ashouri2018,Wang2018}, often leading to higher quality heuristics that outperform those constructed by human experts. With the increasing demand for aggressively-optimising compilers across a range of heterogeneous hardware, it would appear that machine learning could provide a much-needed relief on the burden of compiler developers.

Yet, the integration of machine learning and compilers has remained a largely academic pursuit, with little progress towards adoption by industry. The following section summarises three key outstanding problems in applying machine learning to compilers.


\newpage
\section{Challenges in Machine Learning for Compilers}
\label{sec:intro-challenges}

Machine learning techniques offer reduced compiler development costs and improved runtime performance compared to expert approaches, yet there are three significant challenges that must be overcome to realise these goals:

\subsection{Scarcity of Data}
\label{subsec:challenge-scarcity}

In machine learning, a model is trained based on past observations to predict the values for future inputs. In order to be able to generalise well to unseen observations, plentiful training data must be provided, with a fine-grained overview of the feature space. In the case of compilers, training data is derived from benchmark programs, meaning that many benchmarks are needed to produce sufficient observations for training. Typical machine learning experiments outside of the compilation field train over thousands or millions of observations. However, there are typically only a few dozen common benchmarks available.

The small number of available benchmarks limits the quality of learned models as they have sparse training data. The problem is worsened by the exponential increase in feature space size with the addition of new features. Each additional feature makes the sparsity of training examples more pronounced, increasing the number of observations required.

To address this issue, there must be a sizeable increase in the availability of benchmarks for machine learning. Previously, researchers sought to provide this by randomly instantiating from hand-crafted benchmark templates~\cite{Chiu2015}, but this is a challenging approach --- the generator must be biased in such a way that the generated programs draw from a similar distribution to \emph{real} programs so as to be useful for learning. It is not clear if such an approach could ever achieve parity with handwritten programs.

\subsection{Model and Feature Design}
\label{subsec:challenge-features}

Machine learning algorithms learn to correlate a set of explanatory variables, known as \emph{features}, with a target value. Learning the correlation between features and target value requires that features must be chosen so as to be discriminative for the target value. Features that are not discriminative will prevent a machine learning system from being able to achieve the desired task.

Choosing a set of features to characterise a program so as to be discriminative for machine learning  depends on the desired task to be learned, the type of model, and the environment from which training data was collected, e.g. the hardware and machine configuration. Many problems in compilers do not map directly to numeric attributes, so systems for extracting numeric representations from non-numeric inputs must be developed. For example, one might extract instruction counts from the input source code. Knowing which attributes to extract to represent a program is a hard problem; there is no one-size-fits-all approach that works for all cases. Further, features may only be rendered suitable for learning by transforming the initial values, such as through scaling or normalising. As a result, feature design is often an incremental process of trial and experimentation, and there are few clear signals when this iterative process is ``done''.


\subsection{Adoption of Machine Learning Practices}
\label{subsec:challenge-adoption}

Machine learning systems are fundamentally probabilistic. This stands at odds with established compiler construction practice, which uses formal algorithms and systems to minimise the risk of errors and provide correctness. Given the high cost of developing compilers (Table~\ref{tab:compiler-costs}), there is a significant effort and expertise invested in existing approaches. For the benefits of machine learning techniques for compilers to be realised, machine learning must provide significant benefits over existing approaches, without degrading the quality of the compiler.

Techniques must be developed for probabilistic machine learning systems to coexist with formal approaches. One approach is to limit machine learning only to use cases where correctness guarantees will not be violated, such as in replacing hand-crafted optimisation heuristics. This mitigates the risk of probabilistic systems introducing errors, but limits the applicability of the technique to only certain parts of the compiler. For other use cases with more stringent requirements for behaviour, for example when generating inputs to test compilers, techniques must be developed that enable probabilistic methods to be used without introducing errors or invalid behaviour.

For machine learning techniques to be widely adopted in compilers, they must be made significantly easier and cheaper to develop. The aim of this thesis is to research and develop machine learning techniques that simplify and lower the cost of compiler construction.


\newpage
\section{Contributions}

This thesis presents machine learning-based techniques to simplify and accelerate compiler construction. The key contributions of this thesis are:

\begin{itemize}
  \item The first application of deep learning over source codes to synthesise compilable, executable benchmarks. The proposed approach automatically enhances the predictive power of a state-of-the-art predictive model, improving the performance of heterogeneous workloads by $1.27\times$. Further, the additional benchmarks expose limitations in the feature design of the model which, after correcting, further increases performance by $4.30\times$. This addresses the scarcity of data challenge (Section~\ref{subsec:challenge-scarcity}) by enabling the generation of an unbounded quantity of training data for use in machine learning models.
  \item A novel, automatic, and fast approach for the generation of expressive random programs for compiler fuzzing. The proposed system \emph{infers} programming language syntax, structure, and semantics from real-world examples, not through an expert-defined grammar. The system needs two orders of magnitude less code than the state-of-the-art. In modelling real handwritten code, the test cases are more interpretable than other approaches; the average test case size is two orders of magnitude smaller than the state-of-the-art. An extensive testing campaign reveals 67 new bugs in commercial and open source OpenCL compilers, exposing many bugs which prior work cannot, by covering more components of the compiler. This addresses the machine learning adoption challenge (Section~\ref{subsec:challenge-adoption}) by demonstrating a far simpler method for developing compiler fuzzers, and proposing fast heuristic techniques that enable probabilistic machine learning systems in a domain which previously required formal approaches.
  \item A methodology for building compiler heuristics without the need for program feature engineering. In an evaluation of the technique it is found to outperform existing prior  state-of-the-art predictive models by 14\% and 12\% in two challenging GPGPU compiler optimisation domains. The proposed approach enables the first application of \emph{transfer learning} to compiler optimisations, improving heuristics by reusing training information across different optimisation problems, even if they are unrelated. This addresses the model and feature design challenge (Section~\ref{subsec:challenge-features}) by circumventing the need for numeric representations of code, and enables a single model design to be applied to multiple problems.
\end{itemize}

\newpage
\section{Publications}

This thesis is in part based on ideas and results which have been described in previous publications. The system for generating benchmarks which provides the basis of Chapter~\ref{chap:clgen} is described in:
\begin{enumerate}
  \item \emph{Cummins, C., Petoumenos, P., Zang, W., \& Leather, H. (2017). Synthesizing Benchmarks for Predictive Modeling. In CGO. IEEE.}~\cite{Cummins2017a}.
\end{enumerate}

\noindent
Chapter~\ref{chap:deepsmith} describes an approach to compiler testing which was previously published in:
\begin{enumerate}
  \setcounter{enumi}{1}
  \item \emph{Cummins, C., Petoumenos, P., Murray, A., \& Leather, H. (2018). Compiler Fuzzing through Deep Learning. In ISSTA.}~\cite{Cummins2017a}.
  \item \emph{Cummins, C., Petoumenos, P., Murray, A., \& Leather, H. (2018). DeepSmith: Compiler Fuzzing through Deep Learning. In ACACES.~\cite{Cummins2018a}}
\end{enumerate}

\noindent
The \emph{DeepTune} system proposed in Chapter~\ref{chap:deeptune} was first published in:
\begin{enumerate}
  \setcounter{enumi}{3}
  \item \emph{Cummins, C., Petoumenos, P., Wang, Z., \& Leather, H. (2017). End-to-end Deep Learning of Optimization Heuristics. In PACT. IEEE.}~\cite{Cummins2017b}.
\end{enumerate}

The experimental results in this thesis are reproductions of those in the above publications. What differentiates this work from prior publications is the addition of background material (Chapter~\ref{chap:background}) and a literature review (Chapter~\ref{chap:related-work}) which offers a more comprehensive overview of the relevant fields, and includes references to new works that have been published after the above publications, including works based on those publications.


\section{Structure}

This thesis is organised as follows:

\textbf{Chapter~\ref{chap:background}} provides background. It defines terminology and describes the machine learning and evaluation techniques used in this work.

\textbf{Chapter~\ref{chap:related-work}} surveys the relevant literature, divided into three categories: first program generation, then program optimisation, finally deep learning for programming languages.

\textbf{Chapter~\ref{chap:clgen}} describes a novel technique for generating an unbounded number of executable benchmarks to augment the training data of a predictive model. A qualitative evaluation of the generated programs is presented, followed by a quantitative evaluation using a state-of-the-art OpenCL optimisation heuristic.

\textbf{Chapter~\ref{chap:deepsmith}} extends the generator presented in Chapter~\ref{chap:clgen} to the domain of compiler validation, presenting a low-cost technique for the inference of compiler fuzzers. Fast heuristic approaches are presented for false-positive prevention using probabilistic test case generators. An extensive testing campaign of OpenCL compilers is described, resulting in 67 bug reports.

\textbf{Chapter~\ref{chap:deeptune}} introduces a novel methodology for constructing optimising compiler heuristics without the need for code features. It presents two case studies of the technique: the first for learning a heterogeneous device mapping heuristic, the second for learning OpenCL thread coarsening.

\textbf{Chapter~\ref{chap:conclusions}} summarises the overall findings of the thesis, provides a critical review, and outlines potential avenues for future research.

\section{Summary}

The promise of machine learning techniques is reduced compiler development cost and improved performance. Achieving these goals requires overcoming three significant challenges: the data scarcity challenge, the feature design challenge, and the adoption challenge. The next two chapters discuss technical background knowledge and related work. Subsequent chapters describe novel techniques which address the three challenges.
