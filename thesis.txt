






tablexcolor







































refs.bib











L[1]>
0ptm#1
C[1]>
0ptm#1
R[1]>
0ptm#1








  basicstyle=,
  numbers=left,
  xleftmargin=1em,
  framexleftmargin=2.5em,
  framexrightmargin=-2em,
  escapeinside=@@,
  frame=b,
  breaklines=true,
  postbreak=,
  captionpos=b,
  autogobble=true 






















[OpenCL]C[ANSI]C
morekeywords=__kernel,kernel,__local,local,__global,global,
    __constant,constant,__private,private,
    __read_only,read_only,__write_only,write_only,
    char2,char3,char4,char8,char16,
    uchar2,uchar3,uchar4,uchar8,uchar16,
    short2,short3,short4,short8,short16,
    ushort2,ushort3,ushort4,ushort8,ushort16,
    int2,int3,int4,int8,int16,
    uint2,uint3,uint4,uint8,uint16,
    long2,long3,long4,long8,long16,
    ulong2,ulong3,ulong4,ulong8,ulong16,
    float2,float3,float4,float8,float16,
    image2d_t,image3d_t,sampler_t,event_t,size_t,
    bool2,bool3,bool4,bool8,bool16,
    half2,half3,half4,half8,half16,
    quad,quad2,quad3,quad4,quad8,quad16,
    complex,imaginary,barrier,








*argmin
*argmax
*Gain





[scale=0.2](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;

































statistics











[subfigure]labelformat=parens







Program Generation and Optimisation through Recurrent Neural Networks
Chris Cummins

Compilers are a fundamental technology that are difficult and time consuming to construct. Modern optimising compilers comprise thousands of interacting components, with the demands of data intensive workloads requiring ever-more aggressive optimisations. In addition, the rapid transition to heterogeneous parallelism requires compilers to support a diverse range of hardware, which has left compiler developers struggling to keep up. The cost of this is software that has poor performance and more bugs. What is needed are techniques that radically reduce the cost of compiler construction.

This thesis presents deep learning methodologies to simplify compiler construction. First, a generative model for source code is developed, capable of producing executable programs derived from language models trained on open source corpora. Unlike prior approaches, the generative model presented in this thesis is inferred entirely from example code, greatly reducing the cost of development. It requires no grammar or prior knowledge of the language being modelled, yet is capable of producing code of such quality that professional software developers cannot distinguish generated from handwritten. Secondly, this thesis explores the use of recurrent neural networks for code comprehension through learning optimisation heuristics directly on raw source code.

The effectiveness of programs generated using this approach is investigated in two orthogonal domains. In the first, generated programs are used as benchmarks to supplement the training data of predictive models for compiler optimisations. The additional fine-grained exploration of the feature space that training on an additional 1000 generated programs provides yields a  speedup of a state-of-the-art predictive model. In addition, the extra information automatically exposes weaknesses in the feature design which, when corrected, yields a further  improvement in performance.

The second domain for which automatic program generation is applied is compiler validation. The generative model is extended and used to enable compiler fuzzing. Compared to a state-of-the-art fuzzer, the proposed approach presents an enormous reduction in developer effort, requiring  fewer lines of code to implement, and is capable of generating an expressive range of tests that expose bugs that the state-of-the-art cannot. In a testing campaign of 10 OpenCL compilers, 67 new bugs are identified and reported, many of which are now fixed.

Finally, this thesis presents a new methodology for constructing compiler heuristics, which significantly reduces the cost of applying machine learning to compiler heuristics. Unlike state-of-the-art approaches in which program features have to be expertly engineered and selected, the proposed approach uses recurrent neural networks which learn directly over the textual representation of program code. Doing so yields 
and  performance improvements in state-of-the-art predictive models. Additionally, by using the same neural network structure for different optimisation problems, this enables the novel transfer of information between optimisation problems.







  
  
  [inline]Crisis, solution, happiness



Compilers are an essential ingredient in computer systems. A compiler translates the ideas and algorithms written by humans into instructions that machines can act on. In performing the translation, compilers have many different choices in how to generate the instructions. These choices may affect the performance or energy efficiency of a program. Each of the choices was put there by the engineers that built the compiler.

Compilers are frighteningly complex, they are too large to fit inside the mind of a single human, and the number of available choices during compilation is enormous - many more than the number of atoms in the observable universe.

This thesis presents new techniques to simplify compilers and make them easier to build. The main idea is, rather than have compiler builders expertly craft all of the choices, to let compilers learn for themselves.
  Acknowledgements placeholder.
  I declare that this thesis was composed by myself, that the work contained herein is my own except where explicitly stated otherwise in the text, and that this work has not been submitted for any other degree or professional qualification except as specified. Some of the material used in this thesis has been published in the following papers:

	Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather ``Synthesizing Benchmarks for Predictive Modeling''. In Proceedings of the International Symposium on Code Generation and Optimization (CGO), 2017.
	Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather ``End-to-end Deep Learning of Optimization Heuristics''. In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), 2017.
	Chris Cummins, Pavlos Petoumenos, Alastair Murray, and Hugh Leather ``Compiler Fuzzing through Deep Learning''. In Proceedings of the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA), 2018.

(Chris Cummins)

  Dedication placeholder.

  
  
  
  
  
  


  Introduction

There has been an unprecedented increase in the scale and quantity of data intensive workloads. Fundamental shifts in both hardware and software are required to meet the demands of the transition to big data.

In hardware, performance needs have long outstripped what can be provided by single processors, leading to a broad spectrum of heterogeneous architectures. These range from re-purposing existing Graphics Processor Units (GPUs) to offload numeric computations, to adapting Field-programmable Gate Arrays (FPGAs), and even developing highly specialised ASICs to perform numeric tasks ). While much of the compiler logic can be reused across architectures, the optimisations that are required to extract the best performance from specific hardware cannot. Each architecture requires extensive hand tuning by experts to extract good performance.

In software, the shift towards parallelism and heterogeneity has created a programmability challenge. Parallel programming is significantly more challenging than traditional single threaded development; there are many more opportunities to introduce bugs in software, and Amdahl's law can make extracting performance much more challenging. One of the most popular approaches to mitigating the programmability challenge is through the development and widespread adoption of high level abstractions. High level abstractions and libraries can greatly simplify parallel programming by providing the complex parallel communication and coordination logic, allowing users to plug-in only the business logic required to solve a problem. Still, there is a wide range of approaches to implementing such libraries. One example is high level libraries for performing data intensive numeric workloads, two of the most popular examples of which --- TensorFlow  and PyTorch  --- use opposing dataflow and imperative programming styles, respectively. Optimising such libraries provides new challenges to the compiler --- the challenge of code analyses in the face of highly parallel abstract code can defeat many optimisations.

The combined burden of increased hardware and software diversity has resulted in compilers that are too complex for the expert to fully reason about, and too large to keep up with the pace of change. This results in low performance, wasted energy, and buggy software. For the trend towards larger workloads and heterogeneous devices to continue, new techniques are required to reduce the cost of compiler construction.


Machine Learning for Compilers

Machine learning has been successfully applied across a broad range of fields and disciplines. In recent years this has been accelerated by the development of deep learning techniques. The appeal of machine learning is that it provides techniques to automatically understand the structure of data and how that structure relates to a specific goal, enabling predictions to be made on unseen data; all without the need for expert domain knowledge. In essence, machine learning can negate the need for domain expertise in cases where there is a ready supply of empirical observations.

Within compilers, there are many tasks which require domain expertise that are eligible candidates for machine learning. Examples of which include learning models to control optimisation heuristics, and generating representative inputs to differential test the compiler. As such, the use of machine learning to aid in compiler construction is an established research pursuit. In many studies machine learning has been shown to simplify the construction of compiler optimisations, often leading to higher quality heuristics that outperform those constructed by human experts. With the increasing demand for aggressively optimising compilers across a range of heterogeneous hardware, it would appear that machine learning could provide a much needed relief on the burden of compiler developers.

Yet, the integration of machine learning to compilers has remained a largely academic pursuit, with little progress being made of adoption within industry. The following section speculates as to the cause by summarising some of the outstanding problems in applying machine learning to compilers. The remainder of this chapter then details the contributions of this thesis, followed by the overall structure of the document.


The Problem

Machine learning techniques can offer reduced costs and improved performance compared to expert approaches, yet there are challenges preventing widespread adoption. There are two significant problems that must be overcome:

Scarcity of data In machine learning techniques, a model is trained based on past observations to predict the values for unseen data points. In order to be able to generalise well to unseen points, plentiful training data should be provided, with a fine-grained overview of the feature space. Typical machine learning experiments outside of the compilation field train over thousands or millions of observations. In machine learning for compilers, however, there are typically only a few dozen common benchmarks available.

The small number of common benchmarks limits the quality of learned models as they have very sparse training data. The problem is worsened exponentially with the dimensionality of the feature space. Complex heuristics often have high-dimensional feature spaces, and each additional dimension worsens the sparsity of training examples.

To address the issue, there must be a sizeable increase in the set of common benchmarks, or programs to perform machine learning over. Previously, researchers have sought to provide these through the use of random grammar based generation of programs, but this is a challenging task --- the generated programs must be similar to that of real programs so as to be useful to the learning algorithm, and biasing the generation towards these types of programs is hard to do in the general case.

Model and Feature design Machine learning algorithms learn to correlate a set of explanatory variables with a target value. These explanatory variables, known as features, must be chosen so as to be discriminative for the target value.

Choosing the features to summarise a program so as to be discriminative for machine learning is a challenging task that depends on the thing being learned, and the environment from which training data was collected, e.g. the hardware and machine configuration.

Many problems in compilers do not map directly to numeric attributes, requiring the extraction of a numeric representation from a non-numeric input. For example, extracting instruction counts from source code. Knowing which attributes to extract to represent a program is not easy, and developers are typically faced with the challenge of having to laborious select the right combination features from a large candidate set.

If machine learning is to be widely adopted in compilers, it must be made significantly easier and cheaper. The aim of this thesis is to reduce the cost of compiler construction through developing low cost machine learning techniques to build and maintain compilers.


Contributions

This thesis presents machine learning-based techniques to simplify and accelerate compiler construction. The key contributions of this thesis are:


  the first application of deep learning over source codes to synthesise compilable, executable benchmarks. The approach automatically improve the performance of a state-of-the-art predictive model by , and expose limitations in the feature design of the model which, after correcting, further increases performance by .
  a novel, automatic, and fast approach for the generation of expressive random programs for compiler fuzzing. The system infers programming language syntax, structure, and use from real-world examples, not through an expert-defined grammar. The system needs two orders of magnitude less code than the state-of–the-art, and takes less than a day to train. In modelling real handwritten code, the test cases are more interpretable than other approaches. Average test case size is two orders of magnitude smaller than state-of-the-art, without any expensive reduction process;
  an extensive evaluation campaign of the compiler fuzzing approach using 10 OpenCL compilers and 1000 hours of automated testing. The campaign uncovers a similar number of bugs as the state-of–the-art, but also finds bugs which prior work cannot, covering more components of the compiler;
	a methodology for building compiler heuristics without any need for feature engineering. In an evaluation of the technique, it is found to outperform existing state-of-the-art predictive models by 14 and 12 in two challenging GPGPU compiler optimisation domains;
	the first application of transfer learning to compiler optimisations, improving heuristics by reusing training information across different optimisation problems, even if they are unrelated.



Structure

This thesis is organised as follows:

Chapter  provides background. It defines terminology and describes the machine learning and evaluation methodologies used in this work, along with an overview of heterogeneous programming.

Chapter  surveys the relevant literature, divided into three categories: first program generation, then program optimisation, finally deep learning for programming languages.

Chapter  describes a novel technique for generating an unbounded number of executable benchmarks to augment the training data of a predictive model. A qualitative evaluation of the generated programs is presented, followed by an quantitative evaluation using a state-of-the-art OpenCL optimization heuristic.

Chapter  extends the generator presented in Chapter  to the domain of compiler validation, presenting a low cost technique for the inference of compiler fuzzers. It describes an extensive testing campaign of OpenCL compilers, resulting in 67 bug reports.

Chapter  introduces a novel methodology for constructing optimising compiler heuristics without the need for code features. It describes two case studies of the technique: the first for learning a heterogeneous device mapping heuristic, the second for learning OpenCL thread coarsening.

Chapter  summarises the overall findings, provides a critical review, and outlines potential avenues for future research.


Summary

This introductory chapter has outlined the use of machine learning for compilers and two significant issues preventing its widespread adoption: the scarcity of data, and the challenge of designing features. Subsequent chapters describe novel approaches to address both issues.

  Background


Introduction

This chapter provides a brief and non-exhaustive overview of the techniques and theory used in this thesis. First Section  defines terminology. Then Section  describes the machine learning techniques used in this thesis. Section  provides an overview of heterogeneous programming. Section  describes the statistical tools and methodologies used in this thesis. Lastly Section  concludes.


Terminology


Machine learning refers to a family of statistical models and algorithms used to infer a function for future values given past example values.

Deep learning is a loosely defined class of machine learning methods built on artificial neural networks. It is easily confused with deep neural networks, which is a specific deep learning technique employing artificial neural networks with one or more hidden layers. In this thesis, deep learning refers to the use of Recurrent Neural Networks.








Machine Learning


Machine learning techniques are used in this thesis for classification and sequence-to-sequence modelling. Classification is the task of predicting the correct category --- or class --- for a new instance, based on ``labelled'' training data, i.e. instances whose categories are known. The properties describing instances are called features. Sequence modelling is the task of capturing the underling probability distribution describing a sequence of values. This section briefly describes the classification and sequential modelling techniques used in this thesis.












Artificial Neural Networks

Artificial Neural Networks comprise a network of artificial neurons and weighted connections between them to map input variables to a response. Artificial neural networks are powerful universal function approximators, capable of learning any bounded continuous function to arbitrary precision .

Feed-forward network Figure  shows the architecture of a feed-forward artificial neural network. Each node represents an artificial neuron. The neurons are grouped into layers. The signal of an artificial neuron at a given layer is connected to the input of each artificial neuron in the next layer. For a training pair , the first layer of artificial neurons receive , and the final layer values are compared against expected values . Intermediate layers for which there are no ground truth values are known as hidden layers. Figure  depicts a three layered network with three input values, a single hidden layer with four artificial neurons, and two outputs. This type of feed-forward multi-layered artificial neural networks are known as Multilayer Perceptrons (MLPs).










The signal of an artificial neural network is the activation. For a given layer , the activations  are a function of the activations of the previous layer , an activation function , the connection weights , and biases :






Activation function

The activation function  is a non-linear differentiable function used to calculate the activation of an artificial neuron given a value . A non-linear function is required to enable the ``stacking'' of artificial neuron layers to approximate non-linear functions. Common activation functions are the logistic function (Sigmoid) and Rectified Linear Unit (ReLU), and their variants Hyperbolic Tangent (tanh) and Leaky ReLU:Consider plots of activation functions for input range [-5,5]


  Sigmoid  & (z) = 11 + e^-z 

  ReLU  & (z) = max(z, 0) 

  Tanh  & (z) = e^z - e^-ze^z + e^-z

  Leaky ReLU  & (z) =
    
      0.01z, & if  z < 0

      z, & otherwise
    


Sigmoid activations are bounded in the range .


For hidden layers, a disadvantage of sigmoid activation is that the output isn't zero centred. To address this, the hyperbolic tangent (tanh) activation may be used, which is a scaled sigmoid function with values in the range .

Logistic function-based activations suffer from a squashing effect with large positive and negative values, and they are dense activations in which every artificial neuron contributes to the output value. The ReLU activation function addresses both issues, with an unbounded range . If initialized with random weights in the range , 50 of the network will not fire, so that activations are sparse.

The Leaky ReLU variation addresses the issue of large adjustments to parameters during training leading to neurons that are not activated for any input, becoming ``dead''. Leaky ReLU prevents artificial neurons becoming unresponsive to variations in input by introducing a small slope for negative values.

Typically a separate activation function is used for the output layer of an artificial neural network. For multi-class classification, softmax may be used, which, for  classes produces a vector:





Where .


Backpropagation and Gradient Descent

The most widely used technique to train artificial neural networks is backpropagation . Typically, the artificial neuron parameters are initialised with small random values. During training, a mini-batch of  observations is propagated through the network in a feed forward stage. The final outputs of the network  are then compared against the true values  and used to compute an error . The appropriate error function depends on the task. For a classification task with  classes, categorical cross-entropy may be used:





For each layer , the average error of the mini-batch  is backpropagated through the network to update the connection weight and bias parameters based on a learning rate :


J &= 1B _i=1^B L^(i)(y^(i), y^(i))

W^[] &= W^[] - JW^[]

b^[] &= b^[] - Jb^[]


[inline]Calculating derivatives through chain rule











Regularization Techniques

Neural networks are vulnerable to over-fitting, whereby the parameters of the model become too specialised on the training values and fail to generalise to unseen data. Many regularization techniques have been adopted to mitigate the risk of over-fitting.


Dropout is a regularisation technique in which a parameter in the range  is used to determine a proportion of artificial neurons that are removed. Randomly removing neurons helps training by preventing complex co-adaptations on training values .







Recurrent Neural Networks

A recurrent neural network (RNN)  is an artificial neural network in which the connections between artificial neurons form a cycle, enabling the processing of arbitrary size sequences by updating its hidden state. Figure  depicts an RNN, where  represents point  in a sequence of inputs . The hidden states  and predicted output  are updated at each step using:




















  h^(t) &= ( U x^(t) + W h^(t-1) + b_h ) 

  y^(t) &= ( V h^(t) + b_y )


Where  and  are non-linear activation functions,  and  are bias vectors, and , , and  are matrices representing the hidden-to-hidden, input-to-hidden, and hidden-to-output weights respectively.

The recurrent structure of RNNs enables the modelling of patterns in data with a temporal domain, such as text or numerical time series. Whereas a feed-forward ANN estimates a conditional distribution based on an instantaneous input , an RNN estimates a distribution conditioned on prior observations .

Ordinary backpropagation may be used on an RNN by unfolding the computation graph over time, such in Figure fig:rnn-unfolded. By enables the propagation of errors through time in the same manner as through layers, known as Backpropagation Through Time (BPTT) .


RNNs are universal, in that any function computable by a Turing machine can be computed by an RNN of a finite size . In practise, a significant obstacle to the performance of RNNs is the diminishing ability to learn connections between values over long sequences. This is caused by the exponential diminishing and enlarging of gradients as they are propagated through the recurrence relation. This issue is known as the

vanishing gradients problem .







Long Short-Term Memory

The long short-term memory (LSTM)  is an RNN architecture designed to overcome the vanishing gradients problem. The LSTM augments the design of an RNN with the addition of a cell which stores information, and three gates which control the flow of information into and out of the cell.

Figure  depicts the structure of a single LSTM cell. I addition to the recurrent connections for hidden states, an additional cell state  vector is used. The flow of signal through the cell is controlled by three sigmoid activated gates: the forget gate , input gate , and output gate . At each time step, the values of the gate vectors are updated using:











  f^(t) &= ( W_f x^(t) + U_f h^(t-1) + b_f ) 

  i^(t) &= ( W_i x^(t) + U_i h^(t-1) + b_i ) 

  o^(t) &= ( W_o x^(t) + U_o h^(t-1) + b_o ) 



Where  denotes element-wise product, and weight matrices  and , and bias vectors  are subscripted for the activation being calculated:  input gate,  output gate,  forget gate, or  cell state. The cell state and hidden state are then updated using:


  c^(t) &= f^(t) c^(t-1) + i^(t) tanh ( W_c x^(t) + U_c h^(t-1) + b_c ) 

  h^(t) &= o^(t) tanh ( c ^(t) )


The gated cell enables LSTMs to build connections over very long sequences in data. The LSTMs architecture (and its many variants ) have been responsible breakthrough results in a number of areas, for example machine translation , speech recognition , and weather prediction .



Decision Trees

Decision trees are an intuitive classifier whereby a tree structure of decision nodes are used to predict the class for a given set of features. Decision trees are built using binary recursive partitioning: by creating a decision node for the feature which provides the highest gain, creating new child nodes for each possible outcome, splitting dividing the data amongst these child nodes, and continuing recursively. To find the gain of an feature, we first compute the entropy of the data set. Given a set of data points , and  and  are the number of positive and negative examples in :





The gain of a feature  is found using:





Decision trees are popular and low overhead form of classification. Implementations can be as simple and efficient as a set of nested if/else statements. However, care must be taken to avoid over-fitting to training data.


ZeroR

A ``ZeroR'' classifier is a model whose output is always the mode of the training data classes, irrespective of the input features. For example, given training data with the labels , the model will predict  for all future values. A ZeroR model represents the simplest approach to classification, and is useful for providing a baseline to evaluate the performance of more complex classifiers, since a ZeroR classifier has no power of prediction.















Model Evaluation Techniques



Training, validation, test data

To evaluate a machine learning model, data should be divided into disjoint training and testing partitions. For a set of  pairs of input and output observations , let  be an indexing function that indicates the partition to which the observation  is allocated by the randomization. The model is then trained with the th part of the data removed, yielding fitted function . The quality of the model  is then evaluated using the loss between the expected values and unpredicted values in the unseen test set:





Employing a second indexing function  to divide the data not set aside for testing into disjoint training and validation sets enables the automatic tuning of model parameters. Given a model  parametrised by , the quality of the model evaluating using the parameters  that provide the smallest loss on the validation set can be found from a set of parameters  using:


  p* &= _p L(y_u, f_p^-(k u)(x_u)),  p P

  V(f_p*^-k) &= L(y_k, f_p*^-(k u)(x_k))


-Fold Cross-validation

Dividing the dataset into a fixed training set and a fixed test set can be problematic if it results in the test set being small. A small test set implies statistical uncertainty around the estimated average test error, making it difficult to compare models.

Ideally, sufficient data would exist to be set aside for testing. When not practically possible, cross-validation may be used. Cross-validations enables the use of all observations in a data set in the estimation of the mean test error, at the expense of increased computational cost through repeated evaluations.

K-Fold sets aside part of the available data to fit the model, and a different part to test it. Data is split into  roughly equal-sized parts.





When , this is known as leave-one-out cross-validation.

If observations are distributed amongst the  partitions such that the distribution of observed labels in all partitions is equal, this called stratified -fold cross-validation.


Programming Heterogeneous Systems


Heterogeneous systems are ones that contain multiple types of processor. Programming for heterogeneous systems is especially challenging. Typical approaches divide the system processors into a host processor and one or more additional accelerator processors. The host processor is responsible for coordinating and synchronizing work on the accelerator processors. This approach has been successfully used for General-purpose computing on graphics processing units (GPGPU), where the system CPU is the host and the GPUs are used accelerators . Much of the work in this thesis is evaluated using OpenCL, a popular language and framework for programming heterogeneous systems.


The OpenCL Programming Model

OpenCL is a vendor-independent open standard for programming heterogeneous systems, supporting CPUs, GPUs, and other parallel processors such as Field-Programmable Gate Arrays . It comprises a compute language based on a subset of the ISO C99 programming, and a set of APIs for controlling heterogeneous compute devices from a central host. Programs written for these devices are called kernels, and are compiled by platform-specific tool chains. At runtime, an OpenCL platform is selected and a context object is created which exposes access to each supported compute device through command queues. When a kernel is executed, each unit of computation is referred to as a work-item, and these work-items are grouped into work-groups. The sum of all work-group dimensions defines the global size. For GPUs, work-groups execute on the Single Instruction Multiple Data (SIMD) processing units in lock-step. This is very different from the behaviour of traditional CPUs, and can cause severe performance penalties in the presence of flow control, as work-items must be stalled across diverging flow paths.




OpenCL uses a hierarchical memory model, shown in Figure . The host and each OpenCL device has a single global memory address space. Each work-group has a local memory space, and each work-item has a region of private memory.










Work-groups cannot access the memory of neighbouring work-groups, nor can work-items access the private memory of other work-items. OpenCL provides synchronisation barriers to allow for communication between work-items within a single work-group via the local memory, but not global barriers. Memory transfers between the host and devices occurs between global memory regions. In the case of programming heterogeneous devices, these transfers must occur over the connection bus between the CPU and device (e.g. PCIe for discrete GPUs), which typically creates a performance bottleneck by introducing a performance overhead to transfer data to the device for processing, then back to the device afterwards. Direct transfers of data between devices is not supported, requiring an intermediate transfer to the host memory.












































































































Summary


This chapter details the machine learning techniques used in this thesis, and describes the domain for which they are applied, heterogeneous programming with OpenCL. The following chapter surveys the research literature relevant to this work.

  Related Work


Introduction

This chapter reviews research in areas relevant to this thesis. Section  reviews the literature of program generation, focusing on compiler testing and benchmarking. Then Section  reviews the literature of program optimisation, starting with empirical techniques, iterative compilation, and machine learning. Section  surveys the literature of relevant works in machine learning for programming languages. Finally Section  concludes.

Program Generation


The generation of artificial programs is a broad field with a wide range of applications. This section categorises the literature of two use cases that are relevant to this thesis: program generation for performance characterisation, and program generation for compiler validation.

Performance Characterisation


Benchmark suites serve a wide variety of uses from compiler optimisations to hardware design. The challenge in creating a benchmark suite is to create a diverse set of workloads that is both representative of the target real world use while providing an adequate coverage of the program space. Achieving either of these two goals is a challenging task, and efforts towards one goal can hamper the other. As a result there is no ``one size fits all'' benchmark suite.


An evaluation of GPGPU benchmark suites reveals there are important parts of the program space were left untested .

Goswami2010 evaluate 38 benchmark workloads, finding that Similarity Score, Scan of Large Arrays, and Parallel Reduction workloads show significantly different behaviour due to their large number of diverse kernels, but the remaining 35 workloads provide similar characteristics .

Xiong2013 demonstrate that workload behaviour is highly input dependent, and argue that benchmarks created for academic research cannot represent the cases of real world applications .

A review of big data benchmarks found many to be unrepresentative, and that current hardware designs, while optimized for existing benchmark suites, are inefficient for true workloads .

As well as covering the program space, benchmarks within suites should be diverse.

Ould-Ahmed-Vall2008 show that statistical models trained on 10 of SPEC CPU 2006 data is transferable to the remaining data .

Phansalkar2007 show that a subset of 14 SPEC CPU 2006 programs can yield most of the information of the entire suite , and

Draft2018 find that SPEC CPU 2017 contains workloads that can be safely removed without degrading coverage of the program space .

Researchers have turned to synthetic benchmarks to address the coverage and diversity challenges.

The use of synthetic benchmarks is not new, with an early example from Curnow1976 being used to evaluate the compute power of processors for scientific workloads .

Bell2005 pose the synthesis of synthetic benchmarks as a test case generation problem, using hardware counters to validate the similarity of synthesized benchmarks to a target workload .


A popular use of synthetic benchmark generation techniques is to aid microprocessor design. Joshi2008 use micro-architecture-independent characteristics such as basic block sizes and data footprint to summarize workloads. Their benchmark generator, BenchMaker, then generates a linear sequence of basic blocks and randomly populates them with assembly instructions to match the desired workload characteristics .

MicroProbe uses feedback-directed micro-benchmark generation to perform a systematic energy characterisation of an processor .

GENESIS  is a language for generating synthetic training programs. The essence of the approach is to construct a probabilistic grammar with embedded semantic actions that defines a language of possible programs. New programs may be created by sampling the grammar and, through setting probabilities on the grammar productions, the sampling is biased towards producing programs from one part of the space or another. This technique is potentially completely general, since a grammar can theoretically be constructed to match any desired program domain. However, despite being theoretically possible, it is not easy to construct grammars which are both suitably general and also produce programs that are in any way similar to human written programs. It has been shown to be successful over a highly restricted space of stencil benchmarks with little control flow or program variability . But, it is not clear how much effort it will take, or even if it is possible for human experts to define grammars capable of producing human like programs in more complex domains.

Interesting recent developments in synthetic benchmarking have combined elements from feedback-directed test case synthesis (reviewed in the next section) with synthetic benchmarking for the purpose of generating adversarial benchmarks that expose performance issues in systems.

Dhok2016 apply mutation techniques to an initial set of coverage-driven inputs to expose inefficiencies in loops .


SlowFuzz uses a resource-usage-guided evolutionary search to find inputs that expose poor algorithmic complexity that could be exploited by attackers to produce Denial-of-Service attacks . It considers the input to a program as a byte sequence and performs mutations to find the byte sequence within a fixed input size that maximizes slowdown.


Singularity uses an evolutionary search over the space of input patterns to find the input with worst case performance .


PerfSyn tackles the related problem of exposing performance bottlenecks from API usage. For a method under test, it starts with a minimal example input and applies a sequence of mutations that modify the original code .


PerfFuzz uses feedback-directed program mutation to generate programs which maximise execution counts at program locations .


Pedrosa2018 applies the adversarial benchmark approach to network functions. Their tool, CASTAN, takes as input the code for a network function and outputs packet sequences that trigger slow execution paths .


In contrast to prior works, the benchmark generation technique proposed in this thesis provides general-purpose program generation over unknown domains, in which the statistical distribution of generated programs is automatically inferred from a corpus of real world code. To the best of my knowledge, no prior work has tackled the problem of undirected benchmark generation from example code, as presented in this thesis.


Compiler Validation

Compilers are a fundamental trusted technology, and their correctness is critical. Errors in compilers can introduce security vulnerabilities and catastrophic runtime failures. Therefore, verifying the behaviour of a compiler is of utmost importance.

The complexity of optimizing compilers and programming languages renders formal verification of the entire compiler prohibitively expensive. Efforts have been made in this direction, for example CompCert , a formally verified compiler for the C programming language, but this comes at the cost of supporting only a subset of the language features and with lower performance compared to GCC. Still, even CompCert is not fully verified, and errors have been discovered in the unverified components of it .

Because of the difficulties of verification, compilers developers turn to validation, in which the behaviour of a compiler is validated using a set of input programs, or test cases. For each test case, the expected outcome (determined by the specification of the compiler) is compared against the observed outcome to verify that the compiler conforms to the specification, for those inputs. However, the absence of errors for does not prove that the compiler is free from errors unless all possible inputs are tested exhaustively, and the input space for compilers is huge. As such, hand designed suites of test programs, while important, are inadequate for covering such a large space and will not touch all parts of the compiler.

The random generation of programs to test compilers is a well established approach to the compiler validation problem. The main question of interest is in how to efficiently generate codes which trigger bugs. There are two main approaches: program generation, where inputs are synthesised from scratch; and program mutation, where existing codes are modified so as to identify anomalous behaviour.


Test Case Generation for Compilers

The idea of generating test cases for compilers is well established. The majority of test case generation approaches are based on a formal specification of the programming language syntax and grammar.

An early approach is presented by Hanford1970a, which randomly enumerates a grammar to produce an inexhaustible supply of new programs. While the generated programs are syntactically valid, they are meaningless, and cannot be executed. This limits their value only to testing the compiler front end .

Deeper testing of compiler components is enabled by generating both a syntactically correct program and a gold standard output that would be produced by a conforment compiler. The compiled program can then be executed and its output compared against this gold standard. Figure  shows the process. The challenge of this approach is in generating the gold standard output. These early approaches are surveyed by Boujarwah1997  and Kossatchev2005 .


















Differential testing, illustrated in Figure , accelerates testing by enabling many compilers to be tested at once. The advantage of differential testing over prior approaches is that it does not require a gold standard for the expected behaviour of a conformant compiler. As such, any well formed program may be used as a test. Even malformed inputs may be used to identify anomalies in the error handling logic of compilers. Lacking a gold standard for behaviour makes differential testing less sound than an oracle approach, though in practise the likelihood of the majority consensus being incorrect is extremely unlikely, and no work in the literature has reported such issues. Differential testing can be done across compilers, or using the same compiler with optimizations on or off (or a combination of the two). Chen2014a empirically contrasts the two methodologies in , along with a comparison to Equivalence Module Inputs (described in the next subsection).


In the foundational work on differential testing for compilers, McKeeman et al. present generators capable of enumerating programs of a range of qualities, from random ASCII sequences to C model conforming programs . Subsequent works have presented increasingly complex generators which improve in some metric of interest, generally expressiveness or probability of correctness.


CSmith  is a widely known and effective generator which enumerates programs by pairing infrequently combined language features. In doing so, it produces correct programs with clearly defined behaviour but very unlikely functionality, increasing the chances of triggering a bug. Achieving this required extensive engineering work, most of it not portable across languages, and ignoring some language features.

Lidbury2015a extend CSmith to the OpenCL programming language, a superficially simple task, yet this required 8 man-months of development and 8000 lines of code .

Subsequent generators influenced by CSmith, like Orange3 , focus on features and bug types beyond the scope of CSmith, arithmetic bugs in the case of Orange3.


Glade  derives a grammar from a corpus of example programs. The derived grammar is enumerated to produce new programs, though no distribution is learned over the grammar; program enumeration is uniformly random.

Programs generated by grammar-based approaches are often unlike real handwritten code, and are typically very large. As such, once a bug has been identified, test case reduction  is required to minimise the size of the program and isolate the code of interest. Automated test case reduction does not scale to the rate at which effective compilers find bug; sometimes taking hours for a single test case. An alternate method to generating test cases is to instead mutate a seed input.


Mutation and Feedback-directed Testing


Equivalence Modulo Inputs (EMI) testing, introduced by Le2013a  follows a different approach to test case generation. Starting with existing code, it inserts or deletes statements that will not be executed so that the functionality of the code is unchanged. If it is affected, it is due to a compiler bug. Hermes extends the initial EMI approach to permit the mutation of live code regions, not just dead code . This greatly increases the expressiveness of the generated programs.

LangFuzz also uses program mutation but does this by inserting code segments which have previously exposed bugs. This increases the chances of discovering vulnerabilities in scripting language engines .

Starting with a coverage-guided set of inputs, T-Fuzz uses dynamic tracing to detect input checks in programs and selectively removes them to expose defects .

Skeletal program enumeration again works by transforming existing code. It identifies algorithmic patterns in short pieces of code and enumerates all the possible permutations of variable usage .

pFuzzer targets input parsers, using dynamic tainting to produce a set of legal inputs that cover all conditions during parsing .

Coverage-directed mutation techniques have been for differential testing the Java Virtual Machine .

Machine learning has been used to guide test case mutation.

Cheng2019 construct neural networks to discover correlation between PDF test case and execution in the target program. The correlations are then leveraged to generate new paths in the target program .

NEUZZ learns a differentiable neural approximation of target program logic, then uses Stochastic Gradient Descent to guide program mutation .

Skyfire learns a probabilistic context-sensitive grammar over a corpus of programs to generate input seeds for mutation testing . The generated seeds are shown to improve the code coverage of AFL  when fuzzing XSLT and XML engines. The seeds are not directly used as test cases.

EMI and feedback-directed approaches rely on having a large number of seed programs to mutate. As such, it often still requires an external code generator. Similarly to CSmith, these methods tend to favour very long test programs.


Neural Program Generation


Recently, machine learning methods have been proposed for generating test cases. These differ from prior works that use machine learning to guide the generation of test cases.

Many methods based are based on the astonishing success of Recurrent Neural Networks (RNNs) at modelling sequential data . RNNs have been successfully applied to a variety of other generative tasks, including image captioning , colourising black and white photographs , artistic style , and image generation .


The proficiency of RNNs for sequence modelling is demonstrated in . Sutskever2014 apply two RNN networks to translate first a sequence into a fixed length vector, then to decode the vector into an output sequence. This architecture achieves state-of-the-art performance in machine translation. The authors find that reversing the order of the input sequences markedly improves translation performance by introducing new short term dependencies between input and output sequences.


Although nascent, the use of neural networks to generate programs is evolving rapidly. Neural Programmer was an early example at program generation through the latent representation of a neural network .

Most similar to the work presented in this thesis is Learnfuzz, in which an LSTM network is trained over a corpus of PDF files to generate test inputs for the Microsoft Edge renderer, yielding one bug . Unlike compiler testing, PDF test cases require no inputs and no pre-processing of the training corpus.


DeepFuzz  uses a sequence-to-sequence model to generate fragments of C programs that are inserted into GCC unit tests. The mutated unit tests are then used for differential testing. They propose a character-level model with three sampling methodologies: one where a sample is made for every character, one where no sampling is made (so the generated samples are determined entirely by the prefix), and a hybrid approach in which sampling occurs only on white-space. In their evaluation they uncover 8 new bugs in GCC, and that up to 82.63 of samples are syntactically valid.


IUST DeepFuzz is a neural file generator for file format fuzzing, trained and evaluated on a corpus of PDF files .

Brockschmidt2018 present a novel methodology for program generation in which a graph is used as the intermediate representation .

Machine learning has been used for other purposes in testing other than program generation, reviewed in Section .

Program Optimisation


Modern compilers are complex, typically containing dozens or hundreds of optimisation passes. Determining which optimisation passes to apply, and in what order, is a challenge that depends on a variety of factors from the properties of the program being compiled to the target hardware. Current state-of-practise is for compilers to use a fixed ordering of optimisations, and for each optimisation to contain a heuristic to determine when to use it and with what parameters. Such heuristics require expert design at the expense of great effort and compiler expertise. Still, they rarely are capable of achieving ideal performance.


Extracting maximum performance in a compiler is not as simple as enabling more optimisations, but in identifying which, out of a set of candidate optimisations, will provide the best performance for the current case. A recent study by Georgiou2018 illustrates the scale of the challenge. Taking two modern releases of LLVM, an industry-standard compiler, they obtained an average 3.9 performance improvement across 71 benchmarks on embedded processors by selectively disabling optimisations enabled at the standard -O2 optimisation level .

Selecting the right optimisations is critical. In some domains, the margin of performance to be gained is significant. For example, Ryoo2008a find speedups of up to  through the appropriate selection and use of tiling and loop unrolling optimisations on a GPU matrix multiplication implementation .

Given the challenges of heuristics and analytical methods to extract performance, researchers have turned to empirical methods. This section reviews proposed approaches for two popular empirical approaches, iterative compilation and auto-tuning.


Iterative Compilation and Auto-tuning

Iterative compilation is the method of performance tuning in which a program is compiled and profiled using multiple different configurations of optimisations in order to find the configuration which maximises performance. Unlike analytical methods which attempt to predict the parameters that produce good performance, iterative compilation is empirical. A set of candidate configurations are selected, and for each, the program is compiled and profiled. The configuration that minimises the value of a suitable cost function (such as runtime) is selected. Pioneered by Bodin1998, the technique was initially demonstrated to find good configurations in the non-linear three-dimensional optimisation space of a matrix multiplication benchmark . By exhaustively enumerating the optimisation space they were able to find the global minima of the cost function; however, the authors state that in practise this may not be possible. In cases where an exhaustive enumeration of the optimisation space is infeasible, the process can be cast as a search problem.

While conceptually simple, the empirical nature of iterative compilation yields good results. Iterative compilation has since been demonstrated to be a highly effective form of empirical performance tuning for selecting compiler optimisations.

In a large scale evaluation across 1000 data sets, Chen2010 found iterative compilation to yield speedups in GCC over the highest optimisation level (-O3) of up  .

Frameworks for iterative compilation offer mechanisms to abstract the iterative compilation process from the optimisation space. This can lower the cost to adopting iterative compilation techniques by reusing the logic to search optimisation spaces. Examples include OpenTuner  which provides ensemble search techniques and CLTune  for tuning OpenCL kernels.

The greatest challenge of iterative compilation is the exponential blow up of optimisation space size with the addition of independent optimisations. Many compilers contain dozens or hundreds of discrete optimisation transformations, rendering an exhaustive search of the optimisation space infeasible. This has driven the development of methods for reducing the cost of evaluating configurations. These methods reduce evaluation costs either by pruning the size of the optimisation space and performing a random or exhaustive enumeration, or by guiding a directed search to traverse the optimisation space while evaluating fewer points.


Pruning the Iterative Compilation Search Space


Triantafyllis2003 proposes using feedback during the evaluation of configurations to prune the optimisation space . This is combined with a fast static performance estimator to obviate the need to run each configuration of a program. Pan2006 formalise the iterative compilation problem as: given a set of compiler optimization options , find the combination that minimizes the program execution time efficiently, without a priori knowledge of the optimisations and their interactions. Their technique, Combined Elimination, iteratively prunes the search space, reducing the tuning time to 57 of the closest alternative . Posing the problem as a subset search negates the challenge of optimisation ordering, though this challenge has been the focus of other work .


Ryoo2008 prune the optimisation space for GPGPU workloads using the common subset of optimal configurations across a set of training examples. This technique reduces the search space by 98 . There is no guarantee that for a new program, the reduced search space will include the optimal configuration.


Purini2013 prune the solution space for optimisation orderings. Rather than attempting to find a universally optimal sequence of optimisations, they identify a set of good optimisation sequences that is small enough that each new program can be tried with all sequences in the set. They find that a sequence set size of 10 yields 13 speedups on PolyBench and MiBench programs . Although this does not reduce the cost of finding the set of good sequences, the process need only be performed once per platform, so the cost may be amortised by reusing the same set.

A complementary approach to search space pruning is knowledge sharing. The idea is that, since most software is shared across many users, leverage this by sharing knowledge of the optimisation space between users, rather than each user redundantly performing their own exploration of the optimisation space from scratch. Such ``big data'' approaches to auto-tuning has been variously proposed as

Collective Optimization ,

Crowdtuning ,

and Collective Mind .
Fursin2014 argue that the challenges facing widespread adoption of iterative compilation techniques can be attributed to: a lack of common, diverse benchmarks and data sets; a lack of common experimental methodology; problems with continuously changing hardware and software stacks; and the difficulty to validate techniques due to a lack of sharing in publications. They propose a system for addressing these concerns which provides a modular infrastructure for sharing iterative compilation performance data and related artefacts across the internet .

In past work, a domain specific implementation of knowledge sharing was used to accelerate tuning of stencil codes on GPUs by sharing iterative compilation data between users across the internet .

Another challenge facing iterative compilation is that results are not portable. Any change to the combination of program, input data, and hardware may impact the optimisation space, requiring a new iterative compilation process to start form scratch. This challenge can be mitigated using online compilation.


Online Iterative Compilation

The expensive optimisation space exploration required by iterative compilation has spurred development of dynamic optimisers that attempt to negate this ``training'' phase by interleaving the exploration of the optimisation space with regular program use. This is a challenging task, as a random search of an optimisation space may result in many configurations with performance far from optimal. In a real world system, evaluating many sub-optimal configurations can cause a significant slowdown of the program. Thus a requirement of dynamic optimisers is that convergence time towards optimal parameters is minimised, and that exploration and exploitation are balanced so as to maintain an acceptable quality of service for the user.


Tartara2013 propose a technique for long-term learning of compiler heuristics without an initial training phase. They treat the continued optimisation of a program over its lifetime as an evolutionary process with goal of finding the best set of compiler heuristics for a given binary .


Ansel2012 present an adversarial approach to online evolutionary performance tuning. At runtime, the available parallel resources of a device are divided in to two partitions. A different configuration of the application is then executed simultaneously on each partition. The configuration used for one of the configuration is chosen to be ``safe'', the other, experimental. The configuration which yields the best performance is retained as the ``safe'' choice for future iterations, and the process repeats.


Mpeis2015 present a technique for online iterative compilation on mobile devices. They capture slices of user behaviour on a device during use, which are then replayed offline for iterative compilation . This has the advantage of specializing the performance tuning of software to the behaviour of the individual user.

Related to online iterative compilation is dynamic optimisation. Dynamo is a dynamic optimiser which performs binary level transformations of programs using information gathered from runtime profiling and tracing . This provides the ability for the program to respond to changes in dynamic features at runtime using low-level binary transformations.


Algorithmic Choice  Rewriting

Complementary to iterative compilation is algorithmic choice. Like iterative compilation, the goal is to find the configuration of a program that maximises performances, however, whereas in iterative compilation selects the compiler transformations that produce the best configuration, in algorithmic choice the different configurations are explicitly provided by the user.

PetaBricks is a language and compiler for algorithmic choice . Users provide multiple implementations of algorithms, optimised for different parameters or use cases. This creates a search space of possible execution paths for a given program. This has been combined with auto-tuning techniques for enabling optimised multigrid programs , with the wider ambition that these auto-tuning techniques may be applied to all algorithmic choice programs . While this helps produce efficient programs, it places the burden of producing each algorithmic permutation on the developer, requiring them to provide enough contrasting implementations to make a search of the optimisation space fruitful.


Halide alleviates the burden of algorithmic rewriting by providing a high level domain-specific language that allows users to express pipelines of stencil computations succinctly .

The Lift framework then uses a set of semantic-preserving rewrite rules to transform the high-level expressions to candidate low-level implementations, creating a space of possible implementations .


Super-optimisation

A logical conclusion of iterative compilation is super-optimisation. Like with algorithmic choice, the process performs higher-level algorithmic rewrites than compiler transformations. However, the process begins with no description of the algorithm, not even a high-level representation. Super-optimisation strives to find the globally optimal implementation of an algorithm, typically only from a handful of input output examples. The term super-optimisation is a reference to the misnaming of compiler optimisation, where finding the true optimal is considered an unrealistic goal given the time and resource constraints of a compiler.

Pioneered by Massalin1987, the smallest possible program which performs a specific function is found through a brute force enumeration of the entire instruction set. Starting with a program of a single instruction, the super-optimiser tests to see if any possible instruction passes a set of conformity tests. If not, the program length is increased by a single instruction and the process repeats. The optimiser is limited to register to register memory transfers, with no support for pointers, a limitation which is addressed in . Denali is a super-optimiser which uses constraint satisfaction and rewrite rules to generate programs which are provably optimal, instead of searching for the optimal configuration through empirical testing. Denali first translates a low level machine code into guarded multi-assignment form, then uses a matching algorithm to build a graph of all of a set of logical axioms which match parts of the graph before using boolean satisfiability to disprove the conjecture that a program cannot be written in  instructions. If the conjecture cannot be disproved, the size of  is increased and the process repeats.

Bunel2017a formulate code super-optimisation as a stochastic search problem to learn the distribution of different code transformations and expected performance improvement . As acknowledged by the authors, their approach can be improved by having temporal information of the code structures.

As with iterative compilation, the main problem is in pruning and efficiently navigating the search space. In practise, Massalin1987 found their system to scale only to subroutines typically less than 13 instructions . As such, researchers have turned to machine learning techniques as a means to alleviate the cost of empirical evaluation.


Machine Learning for Compiler Optimisations


Machine learning has emerged as a viable means in automatically constructing heuristics for code optimisation. Its great advantage is that it can adapt to changes in the compiler and hardware environment as it has no a priori assumptions about their behaviour.

The machine learning for compiler literature has been recently reviewed by Ashouri2018  and

Zhang2018c .


Pioneered by Agakov, the idea is to iteratively evaluate a collection of training programs offline and gather explanatory variables describing the features of the programs. The program features and the optimisation decisions which yielded the greatest performance are combined and a model is learned. This model can then be used to make predictions on unseen programs by extracting the features describing the program. In  machine learning is used to guide the iterative compilation search.

In , ``meta optimisation'' is used to tune compiler heuristics through an evolutionary algorithm to automate the search of the optimisation space.

The phase-ordering problem is formulated as a Markov process in  and tackled using neuro-evolution to construct a neural network that predicts beneficial optimization orderings given a characterization of the state of code being optimized.

A later approach to the phase-ordering problem clusters optimisations and uses machine learning to predict the speedup of a sequence of all optimisation clusters .


Ganapathi2009 tackle multi-core stencil code optimisation in , drawing upon the successes of statistical machine learning techniques in the compiler community. They present an auto-tuner which can achieve performance up to 18 better than that of a human expert. From a space of 10 million configurations, they evaluate the performance of a randomly selected 1500 combinations and use Kernel Canonical Correlation Analysis to build correlations between tunable parameter values and measured performance targets. Performance targets are L1 cache misses, TLB misses, cycles per thread, and power consumption. The use of KCAA restricts the scalability of their system as the complexity of model building grows exponentially with the number of features. In their evaluation, the system requires two hours of compute time to build the KCAA model for only 400 seconds of benchmark data.




In past work  and in , domain-specific  machine learning systems are used to optimise stencil computations on GPUs. Restricting the domain of optimisations to a single class of algorithm can simplify the problem by limiting the variance in the function being estimated.

A domain-specific machine learning based auto-tuner is presented for the SkePU library in . SkePU is a C++ template library for data-parallel computations on GPUs. The auto-tuner predicts optimal device mapping (i.e. CPU, GPU) for a given program by predicting execution time and memory copy overhead based on problem size. Similarly, in this thesis a machine learning auto-tuner is used to predict optimal device mapping, though the auto-tuner is capable of making predictions for arbitrary GPU programs, it is not bound to a single template library.

Moren2018 also tackle the task of mapping arbitrary OpenCL kernels to CPU/GPU using dynamic features extracted from the kernel at runtime .

[inline]These three papers:










Milepost GCC is the first practical attempt to embed machine learning into a production compiler. It adds an interface for extracting program features and controlling optimisation passes, combined with a knowledge sharing system to distribute training data over the internet . The embedded interface exposes candidate features which may be used to apply machine learning to an optimisation in GCC, however it does not address the issue of feature selection.


Ogilvie2017 use active learning to reduce the cost of iterative compilation by searching for points in the optimisation space which are close to decision boundaries . This reduces the cost of training compared to a random search. The approach compliments the techniques presented in this thesis, potentially allowing more efficient use of training data.

Besides compilers, there are a broad range of applications for machine learning in improving software performance.

Even purposes as conventional as hash key functions have been the subject of machine learning. Kraska2017 find that replacing a cache-optimised B-Tree-Index implementation with a deep learning model yields up to 70 speedup with a  reduction in memory on some real workloads .

Krishnan2018 use deep reinforcement learning to optimise SQL join query implementations. 
When applying machine learning in a new domain, the challenge is often in finding a suitable program representation to use as the features.


Representing programs with features

The success of machine learning based code optimisation requires having a set of high-quality features that can capture the important characteristics of programs. Given that there is an infinite number of these potential features, finding the right set of features is a non-trivial, time-consuming task.

Various forms of features have been used to summarise programs.

Dubach2009 characterise programs using performance counters .

Jiang2010 extract program-level behaviours such as loop trip counts and the size of input files .

Berral2010a use additional runtime information such as system load .

In compiler research, the feature sets used for predictive models are often provided without explanation and rarely is the quality of those features evaluated. More commonly, an initial large, high dimensional candidate feature space is pruned via feature selection, or projected into a lower dimensional space.

Stephenson2005 propose two approaches to select the most useful features from 38 candidates: the first using a Mutual Information Score to rank features, the second using a greedy feature selection .

Collins2013 use Principle Component Analysis (PCA) to reduce a four-dimensional feature space to two dimensions, reducing the size of the space to 0.05.

Dubach2007 also use PCA to reduce the dimensionality of their feature space, but determine and use the number of components that account for 95 of the total variance. In their case, 5 components.

FEAST employs a range of existing feature selection methods to select useful candidate features .


Park2012 present a unique graph-based approach for feature representations . They use a Support Vector Machine (SVM) where the kernel is based on graph similarity metric. Their technique still requires hand coded features at the basic block level, but thereafter, graph similarity against each of the training programs takes the place of global features. Being a kernel method, it requires that training data graphs be shipped with the compiler, which may not scale as the size of the training data grows with the number of instances, and some training programs may be very large. Finally, their graph matching metric is expensive, requiring  to compare against each training example. The techniques presented in this thesis do not need any hand built static code features, and the deployment memory footprint is constant and prediction time is linear in the length of the program, regardless of the size of the training set.

A few methods have been proposed to automatically generate features from the compiler's intermediate representation. These approaches closely tie the implementation of the predictive model to the compiler IR, which means changes to the IR will require modifications to the model.

Leather2014 uses genetic programming to search for features, requiring a huge grammar to be written, some 160kB in length . Although much of this can be created from templates, selecting the right range of capabilities and search space bias is non trivial and up to the expert.

Namolaru2010a express the space of features via logic programming over relations that represent information from the IRs. They greedily search for expressions that represent good features. However, this approach relies on expert selected relations, combinators and constraints to work. For both approaches, the search time may be significant.


Cavazos2006 present a reaction-based predictive model for software-hardware co-design . Their approach profiles the target program using several carefully selected compiler options to see how program runtime changes under these options for a given micro-architecture setting. They then use the program ``reactions'' to predict the best available application speedup. While their approach does not use static code features, developers must carefully select a few settings from a large number of candidate options for profiling, because poorly chosen options can significantly affect the quality of the model. Moreover, the program must be run several times before optimisation, while the technique presented in this thesis does not require the program to be profiled.

Compared to these approaches, the techniques presented in this thesis are entirely automatic and require no expert involvement. In the field of compiler optimisations, no work so far has applied deep neural networks for program feature generation and selection. This work is the first to do so.


Representing programs with embeddings

This thesis presents deep learning methodologies for learning over programs, inspired by natural language processing. With these techniques, program source code is tokenized into a vocabulary of words, and the words mapped into a real-valued embedding space.

There are many choices in how to construct the vocabulary and embedding. Chen2019 review some of the proposed techniques 

Babii explore the impact that choices in vocabulary have on time to convergence of software language models .


The techniques in this thesis use a hybrid character/token-level vocabulary to tokenize source code. This is to prevent the blow-up in vocabulary size that occurs from using a purely token-based vocabulary. Cvitkovic2018a propose modelling vocabulary elements as nodes in a graph and then processing the graph using Graph Neural Networks; this enables learning over an unbounded vocabulary .


Mou2016 derive an embedding space from the tokens in the source code of a program .

Wang2017d propose an embedding space extracted from program traces, rather than the syntactic structure of the program .

Henkel2018 use symbolic execution to abstract the program traces. Embeddings are then learned from these abstracted symbolic traces .



This feels out of place Yin2018 and Tufano2019 present techniques for learning representations of code edits .


Neural Code Comprehension builds on the experiments in Chapter  of this thesis, using embeddings derived from a novel Contextual Flow Graph (XFG) representation which combines both data and control flow. The embeddings are assembled from LLVM byte-code, enabling support for any language for which there exists a front-end to LLVM .

[inline]There are plenty of other 2vec papers to touch on

Deep Learning for Programming Languages


Deep learning techniques for program generation and optimisation were reviewed in Section  and Section  respectively, but there have been many other applications of machine learning over programs.


Deep learning is a nascent branch of machine learning in which deep or multi-level systems of processing layers are used to detect patterns in natural data . The great advantage of deep learning over traditional techniques is its ability to process natural data in its raw form. This overcomes the traditionally laborious and time-consuming practise of engineering feature extractors to process raw data into an internal representation or feature vector. Deep learning has successfully discovered structures in high-dimensional data, and is responsible for many breakthrough achievements in machine learning, including human parity in conversational speech recognition ; professional level performance in video games ; and autonomous vehicle control . The use of deep learning techniques for software has long been a goal of research .


A Allamanis2017a survey by Allamanis2017a describes fast moving field of machine learning techniques for programming languages .

AutoComment mines the popular QA site StackOverflow to automatically generate code comments .

Naturalize employs techniques developed in the natural language processing domain to model coding conventions .

JSNice leverages probabilistic graphical models to predict program properties such as identifier names for JavaScript .

Allamanis2016 use attentional neural networks to generate summaries of source code .

Nero uses an encode-decoder architecture to predict method names in stripped binaries. The system takes as input a sequence of call sites from the execution of a binary and produces as output a predicted method name .

There is an increasing interest in mining source code repositories at large scale . Previous uses outside the field of machine learning have involved data mining of GitHub to analyse software engineering practices .

Allamanis raises concern about code duplicates in corpora of open-source programs used for machine learning . They find that corpora often contain a high percentage of duplicate or near-duplicate code. This impacts cases where the corpus is divided into training and test sets. Duplicate code appearing both in the training and test sets can lead to artificially high accuracies of models on the test set. The work in this thesis does not use open source corpus as a test set.

Machine learning has also been applied to other areas such as bug detection and static analysis.

Heo2017 present a machine-learning technique to tune static analysis to be selectively unsound, based on anomaly detection techniques .

Koc2017 present a classifier that attempts to predict whether a static analysis tool's error report is a false positive based on the program structures of previous reports that produced false error reports .

Lam2016 employ neural networks to relate keywords in bug reports to code tokens and terms in source files and documentation to accelerate bug localisation.

Wang2016c employ a Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). The features are then used for automatic defect detection .

Chen2017 train two models on compiler test cases, one to predict whether a test case will trigger a compiler bug, the other to predict the execution of the test program. The outputs of these two models is used to schedule test cases so as to maximise the potential for exposing bugs in the shortest amount of time .

DeepBugs combines binary classification of correct and incorrect code with semantic processing to name bugs .

Code2Inv uses reinforcement learning to learn loop invariants for program verification .


Machine learning has been applied to the task of automatic software repair. Monperrus2018 surveys the literature in the field .

DeepRepair using an encoder-decoder architecture to sort code fragments according to similarity of suspicious elements .

Vasic2019 train a model to jointly localise and repair variable-misuse bugs using multi-headed pointer networks .

SequenceR uses sequence-to-sequence learning to generate patches .

Getafix uses a hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns .


CodeBuff uses a carefully designed set of features to learn abstract code formatting rules from a representative corpus of programs .

Raychev2014 use statistical models to provide contextual code completion .

Zhang2015a use deep learning to generate example code for APIs as responses to natural language queries .

Oda2015 employ machine translation techniques to generate pseudo-code from source code .

Summary


This chapter has surveyed the relevant literature in the fields of program generation and optimisation. The next chapter presents a novel technique for generating executable programs using models trained on corpora of example programs.


  
Synthesising Benchmarks for Predictive Modelling
Introduction
[1-2]
[pages=-]2017-cgo.pdf
Conclusion
[1-2]

Synthesising Benchmarks for Predictive Modelling


Introduction

Predictive modelling using machine learning is an effective method for building compiler heuristics, but there is a shortage of benchmarks. Typical machine learning experiments outside of the compilation field train over thousands or millions of examples. In machine learning for compilers, however, there are typically only a few dozen common benchmarks available. This limits the quality of learned models, as they have very sparse training data for what are often high-dimensional feature spaces. What is needed is a way to generate an unbounded number of training programs that finely cover the feature space. At the same time the generated programs must be similar to the types of programs that human developers actually write, otherwise the learning will target the wrong parts of the feature space.

This chapter introduces CLgen, a generator for OpenCL benchmarks. Open source repositories are mined for program fragments which are used to automatically construct deep learning models for how humans write programs. The models are sampled to generate an unbounded number of runnable training programs. The quality of the programs is such that even human developers struggle to distinguish the generated programs from hand-written code. In this chapter, CLgen is used to automatically synthesise thousands of programs and show that learning over these improves the performance of a state-of-the-art predictive model by . In addition, the fine covering of the feature space automatically exposes weaknesses in the feature design which are invisible with the sparse training examples from existing benchmark suites. Correcting these weaknesses further increases performance by .

Predictive modelling is a well researched method for building optimisation heuristics that often exceed human experts and reduces development time . Figure  shows the process by which a predictive model is constructed. A set of training programs are identified that are expected to be representative of the application domain. The programs are compiled and executed with different parameter values for the target heuristic, to determine which are the best values for each training program. Each program is also summarised by a vector of features which describe the information that is expected to be important in predicting the best heuristic parameter values. These training examples of program features and desired heuristic values are used to create a machine learning model which, when given the features from a new, unseen program, can predict good heuristic values for it.







It is common for feature vectors to contain dozens of elements. This means that a large volume of training data is needed to have an adequate sampling over the feature space. Without it, the machine learned models can only capture the coarse characteristics of the heuristic, and new programs which do not lie near to training points may be wrongly predicted. The accuracy of the machine learned heuristic is thus limited by the sparsity of available training points.

There have been efforts to solve this problem using templates. The essence of the approach is to construct a probabilistic grammar with embedded semantic actions that defines a language of possible programs. New programs may be created by sampling the grammar and, through setting probabilities on the grammar productions, the sampling is biased towards producing programs from one part of the space or another. This technique is potentially completely general, since a grammar can theoretically be constructed to match any desired program domain. However, despite being theoretically possible, it is not easy to construct grammars which are both suitably general and also produce programs that are in any way similar to human written programs. It has been shown to be successful over a highly restricted space of stencil benchmarks with little control flow or program variability . But, it is not clear how much effort it will take, or even if it is possible for human experts to define grammars capable of producing human like programs in more complex domains.

The approach introduced in this chapter does not require an expert to define what human programs look like. Instead, the structure and likelihood of programs is automatically inferred over a huge corpus of open source projects. A probability distribution is constructed over sets of characters seen in human written code. This distribution is sampled to generate new random programs which, because the distribution models human written code, are indistinguishable from human code. These samples can be used to populate training data with an unbounded number of human like programs, covering the space far more finely than either existing benchmark suites or even the corpus of open source projects. The approach is enabled by two recent developments:

The first is the breakthrough effectiveness of deep learning for modelling complex structure in natural languages . Deep learning is capable not just of learning the macro syntactical and semantic structure of programs, but also the nuances of how humans typically write code. It is truly remarkable when one considers that it is given no prior knowledge of the syntax or semantics of the language.

The second is the increasing popularity of public and open platforms for hosting software projects and source code. This popularity provides the thousands of programming examples that are necessary to feed into the deep learning. These open source examples are not, sadly, as useful for directly learning the compiler heuristics since they are not presented in a uniform, runnable manner, nor do they typically have extractable test data. Preparing each of the thousands of open source projects to be directly applicable for learning compiler heuristics would be an insurmountable task. In addition to the program generator, CLgen, this chapter presents an accompanying host driver which generates data sets for, then executes and profiles synthesised programs.

In the course of evaluating the technique against prior work we discover that it is also useful for evaluating the quality of features. Since the program space is covered so much more finely than in the prior work, which only used standard benchmark suites, we are able to find multiple programs with identical feature values but different best heuristic values. This indicates that the features are not sufficiently discriminative and should be extended with more information to allow those programs to be separated. Doing this significantly increases the performance of the learned heuristic. This indicates a potential value of this technique for feature designers.

This chapter is organised as follows: first, motivation is provided for the use of benchmark generators in Section . Then Section  introduces CLgen, a generator for human like source code. Section  describes the driver for executing synthesised source code. CLgen is then evaluated; first through a qualitative evaluation comparing the output to handwritten code in Section , then quantitatively by extending the training set of a state-of-the-art machine learning optimisation heuristic. The setup of the quantitative experiments is described in Section , and the results in Section . Finally Section  concludes this chapter.
The Case for Benchmark Generators


This section makes the argument for synthetic benchmarks. Frequently used benchmark suites were identified in a survey of 25 research papers in the field of GPGPU performance tuning from four top tier conferences between 2013--2016: CGO, HiPC, PACT, and PPoPP. The average number of benchmarks used in each paper is 17, and a small pool of benchmarks suites account for the majority of results, illustrated in Figure . The performance of the state-of-the-art Grewe et   al.  predictive model was evaluated across each of the 7 most frequently used benchmark suites (accounting for 92 of results in the surveyed papers). The Grewe et   al. model predicts whether running a given OpenCL kernel on the GPU gives better performance than on the CPU. The full experimental setup is described in Section .









Table  summarises the results. The performance of a model trained on one benchmark suite and used to predict the mapping for another suite is generally very poor. The benchmark suite which provides the best results, NVIDIA SDK, achieves on average only 49 of the optimal performance. The worst case is when training with Parboil to predict the optimal mappings for Polybench, where the model achieves only 11.5 of the optimal performance. From this it is clear that heuristics learned on one benchmark suite fail to generalise across other suites.










This problem is caused both by the limited number of benchmarks contained in each suite, and the distribution of benchmarks within the feature space. Figure  shows the feature space of the Parboil benchmark suite, showing whether, for each benchmark, the model was able to correctly predict the appropriate optimisation. Principle Component Analysis is used to reduce the multi-dimensional feature space to aid visualisation.



















As can be seen in Figure , there is a dense cluster of neighbouring benchmarks, a smaller cluster of three benchmarks, and two outliers. The lack of neighbouring observations means that the model is unable to learn a good heuristic for the two outliers, which leads to them being incorrectly optimised. In Figure , I hand-selected benchmarks which are neighbouring in the feature space and retrained the model. The addition of these observations (and the information they provide about that part of the feature space) causes the two outliers to be correctly optimised. Such outliers can be found in all of the benchmark suites of Table .

These results highlight the significant effect that the number and distribution of training programs has on the quality of predictive models. Without good coverage of the feature space, any machine learning methodology is unlikely to produce high quality heuristics, suitable for general use on arbitrary real applications, or even applications from different benchmark suites. The novel approach described in the next section addresses this problem by generating an unbounded number of programs to cover the feature space with fine granularity.

CLgen: A System for Generating OpenCL Benchmarks


This section introduces CLgen, an undirected, general-purpose program synthesizer. It adopts and augments recent advanced techniques from deep learning to learn over massive code-bases. In contrast to existing grammar and template based approaches, CLgen is entirely probabilistic. The system learns to program using recurrent neural networks which model the semantics and usage of a huge corpus of code fragments in the target programming language.

Overview

Figure  provides an overview of the program synthesis and execution pipeline. CLgen learns the semantics and structure from over a million lines of hand-written code from GitHub, and synthesises programs through a process of iterative model sampling. A host driver, described in Section , executes the synthesised programs to gather performance data for use in predictive modelling. While the approach is demonstrated using OpenCL, it is language agnostic. This approach extends the state-of-the-art by providing a general-purpose solution for benchmark synthesis, leading to better and more accurate predictive models.

Section  describes the assembly of a language corpus, Section  describes the application of deep learning over this corpus, and Section  describes the process of synthesising programs.










An OpenCL Language Corpus


Deep learning requires large data sets . For the purpose of modelling a programming language, this means assembling a very large collection of real, hand-written source codes. OpenCL codes are assembled by mining public repositories on the popular code hosting site GitHub.

This is itself a challenging task since OpenCL is an embedded language, meaning device code is often difficult to untangle since GitHub does not presently recognise it as a searchable programming language. A search engine was developed which attempts to identify and download standalone OpenCL files through a process of file scraping and recursive header inlining. The result is a 2.8 million line data set of 8078 ``content files'' which potentially contain OpenCL code, originating from 793 GitHub repositories.

The raw data set extracted from GitHub is then pruned using a custom tool chain developed for rejection filtering and code rewriting, built on LLVM.


Rejection Filter


The rejection filter accepts as input a content file and returns whether or not it contains compilable, executable OpenCL code. To achieve this, it attempts to compile the input to NVIDIA PTX byte code and performs a static analysis to ensure a minimum static instruction count of three. Any inputs which do not compile or contain fewer than three instructions are discarded.

During initial development it became apparent that isolating the OpenCL device code leads to a higher-than-expected discard rate (that is, seemingly valid OpenCL files being rejected). Through analysing 148k lines of compilation errors, a large number of failures was discovered to be caused by undeclared identifiers, a result of isolating device code. 50 of undeclared identifier errors in the GitHub dataset were caused by only 60 unique identifiers. To address this, a shim header was developed which contains inferred values for common type definitions (e.g. FLOATT), and common constants (e.g. WGSIZE), shown in Listing .

[
  label=lst:opencl-shim-header,
  float,
  language=C,
  caption=
    [The shim header file for OpenCL]An overview of the shim header file, providing inferred type aliases and constants for OpenCL on GitHub.
  
]lst/opencl-shim-header

Injecting the shim decreases the discard rate from 40 to 32, responsible for an additional 88k lines of code in the final language corpus. The resulting data set is 2.0 million lines of compilable OpenCL source code.

Code Rewriter


Programming languages have few of the issues of semantic interpretation present in natural language, though there remains many sources of variance at the syntactic level. For example, the presence and content of comments in code, and the choice of identifying names given to variables. For the purposes of generative modelling, these ambiguities are considered to be non-functional variance. The code rewriter is a tool developed to normalise code of these variances so as to make code more amenable to machine learning. This is a three step process:


  The source is pre-processed to remove macros, conditional compilation, and source comments.
  Identifiers are rewritten to have a short but unique name based on their order of appearance, using the sequential series  for variables and  for functions. This process isolates the syntactic structure of the code, and unlike prior work , our rewrite method preserves program behaviour. Language built-ins (e.g. getglobalid, asin) are not rewritten.
  A variant of the Google C++ code style is enforced to ensure consistent use of braces, parentheses, and white space.


An example of the code rewriting process is shown in Listings  and . A side effect of this process is a reduction in code size, largely due to the removal of comments and excess white space. The final language corpus contains 1.3 million lines of transformed OpenCL, consisting of 9487 kernel functions. Identifier rewriting reduces the bag-of-words vocabulary size by 84.

[
  language=[OpenCL]C,
  label=lst:code-rewriting-before,
  float,
  caption=[Example OpenCL content file]An example OpenCL content file downloaded from GitHub prior to code rewriting.
]lst/clgen-rewrite-before

[
  language=[OpenCL]C,
  label=lst:code-rewrite-after,
  float,
  caption=[OpenCL content file after rewriting]The example OpenCL content file of Listing  after code rewriting. Conditional compilation has been removed, the variables and functions renamed, and a code style enforced.
]lst/clgen-rewrite-after

Learning OpenCL


Generating valid, executable program code is an ambitious and challenging goal for unsupervised machine learning. CLgen employs state-of-the-art deep language modelling techniques to achieve this task.

The Long Short-Term Memory (LSTM) architecture of Recurrent Neural Network  is used to learn a character-level language model over the corpus of OpenCL compute kernels. The LSTM network architecture comprises recurrent layers of memory cells, each consisting of an input, output, and forget gate, and an output layer providing normalised probability values from a 1-of-K coded vocabulary .

A 3-layer LSTM network is used with 2048 nodes per layer, implemented in Torch. This 17-million parameter model is trained using Stochastic Gradient Descent for 50 epochs, using an initial learning rate of 0.002, decaying by a factor of one half every 5 epochs. Training took three weeks on a single machine using an NVIDIA GTX Titan, with a final model size of 648MB. Training the network is a one-off cost, and can be parallelised across devices. The trained network can be deployed to lower-compute machines for use.

Synthesising Source Code


OpenCL compute kernels are synthesised by iteratively sampling the learned language model. Two modes for model sampling are supported: the first involves providing an argument specification, stating the data types and modifiers of all kernel arguments. When an argument specification is provided, the model synthesises kernels matching this signature. In the second sampling mode this argument specification is omitted, allowing the model to synthesise compute kernels of arbitrary signatures, dictated by the distribution of argument types within the language corpus.

In either mode a seed text is generated and model is sampled, character by character, until the end of the compute kernel is reached, or until a predetermined maximum number of characters is reached. Algorithm  illustrates this process. The same rejection filter described in Section  then either accepts or rejects the sample as a candidate synthetic benchmark. Listings , , and  show three examples of unique compute kernels generated in this manner from an argument specification of three single-precision floating-point arrays and a read-only signed integer. The quality of synthesised code is evaluated in Section .


[1]
LSTM model , maximum kernel length .
Completed sample string .
``kernel void A(const int a) ''Seed text
Initial code block depth
 to 
  Generate new character
  ``''
    Entered code block, increase depth
  ``''
    Exited code block, decrease depth
  
  Append new character
  
    breakExited function block, stop sampling
  




[Sampling a candidate kernel from a seed text]Sampling a candidate kernel from a seed text.



[
  float,
  label=lst:clgen-sample-a,
  caption=
    [Synthesised vector operation with branching and synchronisation]
    CLgen-synthesised vector operation with branching and synchronisation.
  ,
	language=[OpenCL]C
]lst/clgen-sample-a

[
  float,
  label=lst:clgen-sample-b,
  caption=
    [Synthesised zip operation]
    CLgen-synthesised zip operation which computes .
  ,
  language=[OpenCL]C
]lst/clgen-sample-b

[
  float,
  label=lst:clgen-sample-c,
  caption=
    [Synthesised partial reduction operation]
    CLgen-synthesised partial reduction over reinterpreted vector type.
  ,
  language=[OpenCL]C
]lst/clgen-sample-c

Benchmark Execution


A host driver is used to gather performance data from synthesised CLgen code. The driver accepts as input an OpenCL kernel, generates payloads of user-configurable sizes, and executes the kernel using the generated payloads, providing dynamic checking of kernel behaviour.

Generating Payloads

A payload encapsulates all of the arguments of an OpenCL compute kernel. After parsing the input kernel to derive argument types, a rule-based approach is used to generate synthetic payloads. For a given global size : host buffers of  elements are allocated and populated with random values for global pointer arguments, device-only buffers of  elements are allocated for local pointer arguments, integral arguments are given the value , and all other scalar arguments are given random values. Host to device data transfers are enqueued for all non-write-only global buffers, and all non-read-only global buffers are transferred back to the host after kernel execution.

Dynamic Checker

For the purpose of performance benchmarking the correctness of computed values is of little interest, but I define a class of programs as performing useful work if they predictably compute some result. A low-overhead runtime behaviour check is used to validate that a synthesised program does useful work based on the outcome of four executions of a tested program:


Create 4 equal size payloads , , ,
  , subject to restrictions: ,
  , .
Execute kernel  4 times: ,
  ,
  ,
  .
Assert that:
  
   and , else  has no
  output (for these inputs).
   and , else  is input insensitive t (for these inputs).
   and , else  is
  non-deterministic.
  


Equality checks for floating point values are performed with an appropriate epsilon to accommodate rounding errors, and a timeout threshold is also used to catch kernels which are non-terminating. The method is based on random differential testing , though I emphasise that this is not a general purpose approach and is tailored specifically for this use case. For example, I anticipate a false positive rate for kernels with subtle sources of non-determinism which more thorough methods may expose , however I deemed such methods unnecessary for the purpose of performance modelling.
Qualitative Evaluation of Generated Programs


This section evaluates the quality of programs synthesised by CLgen by their likeness to hand-written code.

Judging whether a piece of code has been written by a human is a challenging task for a machine, so I adopted a methodology from machine learning research based on the Turing Test . If the output of CLgen is human-like code, it reasons then that a human judge will be unable to distinguish it from hand-written code.

A double blind test was devised in which 15 volunteer OpenCL developers from industry and academia were shown 10 OpenCL kernels each. Participants were tasked with judging whether, for each kernel, they believed it to have been written by hand or by machine. Kernels were randomly selected for each participant from two equal sized pools of synthetically generated and hand-written code from GitHub. The samples from GitHub were vetted to ensure that they were indeed hand-written and not generated by machine or template (such vetting is a manual process and was not applied during the assembly of the model training corpus). The code rewriting process was applied to all kernels to remove comments and ensure uniform identifier naming. The participants were divided into two groups, with 10 of them receiving code generated by CLgen, and 5 of them acting as a control group, receiving code generated by CLSmith , a program generator for differential testing .

Each participant's answers was scored. The average score of the control group is 96 (stdev. 9), an unsurprising outcome as programs generated using the CLSmith grammar for testing have multiple ``tells''; for example, their only input is a single ulong pointer. There were no false positives (synthetic code labelled human) for CLSmith, only false negatives (human code labelled synthetic). With CLgen synthesised programs, the average score was 52 (stdev. 17), and the ratio of errors was even. This suggests that CLgen code is indistinguishable from hand-written programs, with human judges scoring no better than random chance.
Experimental Methodology


Experimental Setup

Predictive Model

To evaluate the efficacy of synthetic benchmarks for training, the predictive model of Grewe2013 is used . The predictive model is used to determine the optimal mapping of a given OpenCL kernel to either a GPU or CPU. It uses supervised learning to construct a decision tree with a combination of static and dynamic kernel features extracted from source code and the OpenCL runtime, detailed in Table .










Benchmarks

As in , the model is tested on the NAS Parallel Benchmarks (NPB) . The hand-optimised OpenCL implementation of Seo2011  is used. In  the authors augment the training set of the predictive model with 47 additional kernels taken from 4 GPGPU benchmark suites. To more fully sample the program space, a much larger collection of 142 programs is used, summarised in Table . These additional programs are taken from all 7 of the most frequently used benchmark suites identified in Section . None of these programs were used to train CLgen. 1,000 kernels were synthesised with CLgen to use as additional benchmarks.








Platforms 

Two 64-bit CPU-GPU systems are used to evaluate the approach, detailed in Table . One system has an AMD GPU and uses OpenSUSE 12.3; the other is equipped with an NVIDIA GPU and uses Ubuntu 16.04. Both platforms were unloaded.








Data sets

The NPB and Parboil benchmark suites are packaged with multiple data sets. We use all of the packaged data sets (5 per program in NPB, 1-4 per program in Parboil). For all other benchmarks, the default data sets are used. The CLgen host driver was configured to synthesise payloads between 128B-130MB, approximating that of the dataset sizes found in the benchmark programs.


Methodology

The same methodology is used as in . Each experiment is repeated five times and the average execution time is recorded. The execution time includes both device compute time and the data transfer overheads.

Models are evaluated using leave-one-out cross-validation. For each benchmark, a model is trained on data from all other benchmarks and used to predict the mapping for each kernel and dataset in the excluded program. The process is repeated with and without the addition of synthetic benchmarks in the training data. Only the real handwritten benchmarks are used to test model predictions, the synthetic benchmarks are not used.

Experimental Results


The effectiveness of synthetic benchmarks is evaluated on two heterogeneous systems. First the performance of a state-of-the-art predictive model  is compared with and without the addition of synthetic benchmarks, then synthetic benchmarks are shown expose weaknesses in the feature design and how these can be addressed to develop a better model. Finally we compare the ability of CLgen to explore the program feature space against a state-of-the-art program generator .

Performance Evaluation















Figure  shows speedups of the Grewe et al. predictive model over the NAS Parallel Benchmark suite with and without the addition of synthesised benchmarks for training. Speedups are calculated relative to the best single-device mapping for each experimental platform, which is CPU-only for AMD and GPU-only for NVIDIA. The fine grained coverage of the feature space which synthetic benchmarks provide improves performance dramatically for the NAS benchmarks. Across both systems, an average speedup of  is achieved with the addition of synthetic benchmarks, with prediction improvements over the baseline for 62.5 of benchmarks on AMD and 53.1 on NVIDIA.

The strongest performance improvements are on NVIDIA with the FT benchmark which suffers greatly under a single-device mapping. However, the performance on AMD for the same benchmark slightly degrades after adding the synthetic benchmarks. This issue is addressed in the next section.

Extending the Predictive Model


Feature designers are bound to select as features only properties which are significant for the sparse benchmarks they test on, which can limit a model's ability to generalise over a wider range of programs. This is found to be the case with the Grewe et al. model. The addition of automatically generated programs exposed two distinct cases where the model failed to generalise as a result of overspecialising to the NPB suite.

The first case is that the feature F3 is sparse on many programs. This is a result of the NPB implementation's heavy exploitation of local memory buffers and the method by which they combined features (speculatively, this may have been a necessary dimensionality reduction in the presence of sparse training programs). A simple countermeasure is taken to address this by extending the model to use the raw feature values in addition to the combined features.

The second case is that some of CLgen-generated programs had identical feature values as in the benchmark set, but had different behaviour (i.e. optimal mappings). Listing  shows one example of a CLgen benchmark which is indistinguishable in the feature space to one the of existing benchmarks --- AMD's Fast Walsh-Hadamard transform --- but with different behaviour. We found this to be caused by the lack of discriminatory features for branching, since the NPB programs are implemented in a manner which aggressively minimised branching. To counter this the predictive model was extended with an additional feature containing a static count of branching operations in a kernel.

[
  label=lst:zero-b,
  float,
  caption=
	  [CLgen program with same features as an AMD benchmark]
  	In the Grewe et al. feature space this CLgen program is indistinguishable from AMD's Fast Walsh–Hadamard transform benchmark, but has very different runtime behaviour and optimal device mapping. The addition of a branching feature fixes this.
  ,
  language=[OpenCL]C
]lst/zero-b

Figure  shows speedups of the extended model across all seven of the benchmark suites used in Section . Model performance, even on this tenfold increase of benchmarks, is good. There are three benchmarks on which the model performs poorly: MatrixMul, cutcp, and pathfinder. Each of those programs make heavy use of loops, which it is believed the static code features of the model fail to capture. This could be addressed by extracting dynamic instruction counts using profiling, but this is beyond the scope of this work. It is not the goal to perfect the predictive model, but to show the performance improvements associated with training on synthetic programs. To this extent, the goal is succeeded, achieving average speedups of  on AMD and  on NVIDIA across a very large test set.










Comparison of Source Features

As demonstrated in Section , the predictive quality of a model for a given point in the feature space is improved with the addition of observations from neighbouring points. By producing thousands of artificial programs modelled on the structure of real OpenCL programs, CLgen is able to consistently and automatically generate programs which are close in the feature space to the benchmarks that we are testing on.

To quantify this effect, the static code features of Table , plus the branching feature discussed in the previous subsection, are used to measure the number of CLgen kernels generated with the same feature values as those of the benchmarks we examined in the previous subsections. Only static code features are examined to allow comparison with the GitHub kernels for which we have no automated method to execute them and extract runtime features, and CLSmith generated programs.

Figure  plots the number of matches as a function of the number of kernels. Out of 10,000 unique CLgen kernels, more than a third have static feature values matching those of the benchmarks, providing on average 14 CLgen kernels for each benchmark. This confirms the original intuition: CLgen kernels, by emulating the way real humans write OpenCL programs, are concentrated in the same area of the feature space as real programs. Moreover, since the number of CLgen kernels that can be generated is unbounded, the exploration of the feature space can be continually refined. This is not the case for GitHub, where the number of kernels is finite. CLSmith rarely produces code similar to real-world OpenCL programs, with only 0.53 of the generated kernels have matching feature values with benchmark kernels. I conclude that the unique contribution of CLgen is its ability to generate many thousands of programs that are appropriate for predictive modelling.









Summary


The quality of predictive models is bound by the quantity and quality of programs used for training, yet there is typically only a few dozen common benchmarks available for experiments. This chapter present a novel tool which is the first of it's kind --- an entirely probabilistic program generator capable of generating an unbounded number of human like programs. The approach applies deep learning over a huge corpus of publicly available code from GitHub to automatically infer the semantics and practical usage of a programming language. The tool generates programs which to trained eyes are indistinguishable from hand-written code. The approach is tested using a state-of-the-art predictive model, improving its performance by a factor of . Synthetic benchmarks automatically exposed weaknesses in the feature set which, when corrected, further improved the performance by .



  
Compiler Fuzzing through Deep Learning
Introduction
[1-2]
[pages=-]2018-issta.pdf
Conclusion
[1-2]

Synthesising Test Cases for Compiler Validation


Introduction

Compilers should produce correct code for valid inputs, and meaningful errors for invalid inputs. Failure to do so can hinder software development or even cause catastrophic runtime errors. Still, properly testing compilers is hard. Modern optimising compilers are large and complex programs, and their input space is huge. Hand designed suites of test programs, while important, are inadequate for covering such a large space and will not touch all parts of the compiler.

Random test case generation --- fuzzing --- is a well established and effective method for identifying compiler bugs . When fuzzing, randomly generated valid or semi-valid inputs are fed to the compiler. Any kind of unexpected behaviour, including crashes, freezes, or wrong binaries, indicates a compiler bug. While crashes and freezes in the compiler are easy to detect, determining that binaries are correctly compiled is not generally possible without either developer provided validation for the particular program's behaviour or a gold standard compiler from which to create reference outputs. In the absence of those, Differential Testing  can be used. The generated code and a set of inputs form a test case which is compiled and executed on multiple testbeds. If the test case should have deterministic behaviour, but the output differs between testbeds, then a bug has been discovered.

Compiler fuzzing requires efficiently generating test cases that trigger compiler bugs. The state-of-the-art approach, CSmith , generates large random programs by defining and sampling a probabilistic grammar which covers a subset of the C programming language. Through this grammar, CSmith ensures that the generated code easily passes the compiler front-end and stresses the most complex part of the compiler, the middle-end. Complex static and dynamic analyses make sure that programs are free from undefined behaviour. The programs are then differentially tested.

While CSmith has been successfully used to identify hundreds of bugs in otherwise-robust  compilers, it and similar approaches have a significant drawback. They represent a huge undertaking and require a thorough understanding of the target programming language. CSmith was developed over the course of years, and consists of over 41k lines of handwritten C++ code. By tightly coupling the generation logic with the target programming language, each feature of the grammar must be painstakingly and expertly engineered for each new target language. For example, lifting CSmith from C to OpenCL  --- a superficially simple task --- took 9 months and an additional 8k lines of code. Given the difficulty of defining a new grammar, typically only a subset of the language is implemented.

This chapter introduces DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. DeepSmith is a fast, effective, and low effort approach to the generation of random programs for compiler fuzzing. The methodology uses recent advances in deep learning to automatically infer probabilistic models of how humans write code, instead of painstakingly defining a grammar to the same end. By training a deep neural network on a corpus of handwritten code, it is able to infer both the syntax and semantics of the programming language and the common constructs and patterns. The approach essentially frames the generation of random programs as a language modelling problem. This greatly simplifies and accelerates the process. The expressiveness of the generated programs is limited only by what is contained in the corpus, not the developer's expertise or available time. Such a corpus can readily be assembled from open source repositories. Once trained, the model is used to automatically generate tens of thousands of realistic programs. Finally, established differential testing methodologies are used on them to expose bugs in compilers.

In this chapter the approach is applied to the OpenCL programming language. In 1,000 hours of automated testing of commercial and open source compilers, bugs are discovered in all of them, and 67 bug reports are submitted. The generated test cases are on average two orders of magnitude smaller than the state-of-the-art, require  less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. The random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code.  This work primarily targets OpenCL, an open standard for programming heterogeneous systems, though the approach is largely language agnostic. OpenCL is chosen for three reasons: it is an emerging standard with the challenging promise of functional portability across a diverse range of heterogeneous hardware; OpenCL is compiled ``online'', meaning that even compiler crashes and freezes may not be discovered until a product is deployed to customers; and there is already a hand written random program generator for the language to compare against. With 18 lines of code the program generator is extended to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing.

This chapter is organised as follows:  Section  presents DeepSmith, a novel approach to compiler validation. Section  describes the experimental setup of an extensive evaluation of OpenCL compilers using DeepSmith. Section  evaluates the results of the experiment, with Section  containing preliminary results supporting DeepSmith's potential for multi-lingual compiler fuzzing. Section  provides concluding remarks for this chapter.
DeepSmith: Compiler Fuzzing through Deep Learning


DeepSmith  is an open source framework for compiler fuzzing. Figure  provides a high-level overview. In this work OpenCL is targeted, though the approach is language agnostic. This section describes the three key components: a generative model for random programs, a test harness, and voting heuristics for differential testing.










Generative Model

Generating test cases for compilers is hard because their inputs are highly structured. Producing text with the right structure requires expert knowledge and a significant engineering effort, which has to be repeated from scratch for each new language. Instead, the proposed approach frames the problem as an unsupervised machine learning task, employing state-of-the-art deep learning techniques to build models for how humans write programs. The approach is inspired by breakthrough results in modelling challenging and high dimensional data sets through unsupervised learning . Contrary to existing tools, this approach does not require expert knowledge of the target language and is only a few hundred lines of code.

Handwritten Programs

The generative model needs to be trained on a seed corpus of example programs. The assembly of this corpus is automated by mining 10k OpenCL kernels from open source repositories on GitHub. An oracle compiler (LLVM 3.9) is used to statically check each downloaded source file, discarding files that are not well-formed. The main purpose of this step is to remove the need to manually check that each file selected from GitHub does indeed contain OpenCL. A downside is that any training candidate which triggers a bug in the LLVM 3.9's front end will not be included. However, this did not prevent our system from uncovering errors in that compiler (Section ).

This corpus, exceeding one million lines of code, is used as a representative sample of OpenCL code from which a generative model can be derived.

Encoder

The textual representation of program codes must be encoded as numeric sequences for feeding as input to the machine learning model. Prior machine learning works have used character-level encodings, token-level encodings, or fixed length feature vectors. We extend the hybrid character/token-level encoding of , in which a programming language's keywords and common names are treated as individual tokens while the rest of the text is encoded on a character-level basis. This approach hits a balance between compressing the input text and keeping the number of tokens in the vocabulary low.

Semantic-preserving transformations are employed to simplify the training programs. First, each source file is preprocessed to expand macros and remove conditional compilation and comments. Then, all user-declared identifiers are renamed using an arbitrary, but consistent pattern based on their order of declaration:  for variables and  for functions. This ensures a consistent naming convention, without modifying program behaviour. Finally, a uniform code style is enforced to ensure consistent use of braces, parentheses, and white space. These rewriting simplifications give more opportunities for the model to learn the structure and deeper aspects of the language and speed up the learning. On the other hand, some bugs in the preprocessor or front-end might no longer be discoverable. For the purpose of fuzzing OpenCL compilers I reason that this is an acceptable trade-off. For languages where the corpus can be many orders of magnitude larger, for example, C or Java, models may be generated without these modifications.

Neural Network

The Long Short-Term Memory (LSTM) architecture of Recurrent Neural Network is used to model program code . In the LSTM architecture activations are learned with respect not just to their current inputs but to previous inputs in a sequence. In our case, this allows modelling the probability of a token appearing in the text given a history of previously seen tokens. Unlike previous recurrent networks, LSTMs employ a forget gate with a linear activation function, allowing them to avoid the vanishing gradients problem . This makes them effective at learning complex relationships over long sequences  which is important for modelling program code. In this approach, LSTM networks are employed to model the vocabulary distribution over the encoded corpus. Initial experiments using different model parameters revealed that a two layer LSTM network of 512 nodes per layer provided a good trade-off between the fidelity of the learned distribution and the size of the network, which limits the rate of training and inferenceThis should be quantified. The network is trained using Stochastic Gradient Descent for 50 epochs, with an initial learning rate of 0.002 and decaying by 5 every epoch. Training the model on the OpenCL corpus took 12 hours using a single NVIDIA Tesla P40. The model is given no prior knowledge of the structure or syntax of a programming language.

Program Generation

The trained network is sampled to generate new programs. The model is seeded with the start of a kernel (identified in OpenCL using the keywords kernel void), and sampled token-by-token. A ``bracket depth'' counter is incremented or decremented upon production of  or  tokens respectively, so that the end of the kernel can be detected and sampling halted. The generated sequence of tokens is then decoded back to text and used for compiler testing.


Test Harness

OpenCL is an embedded compute kernel language, requiring host code to compile, execute, and transfer data between the host and device. For the purpose of compiler fuzzing, this requires a test harness to run the generated OpenCL programs. At first, the test harness of CLSmith was used. The harness assumes a kernel with no input and a ulong buffer as its single argument where the result is written. Only 0.2 of the GitHub kernels share this structure. A more flexible harness was desired so as to test a more expressive range of programs, capable of supporting multi-argument kernels and generating data to use as inputs.

A new harness was developed which first determines the expected arguments from the function prototype and generates host data for them. At the moment, scalars and arrays of all OpenCL primitive and vector types are supported. For a kernel execution across  threads, buffers of size  are allocated for pointer arguments and populated with values ; scalar inputs are given value , since scalar integer arguments are frequently used in OpenCL for specifying buffer sizes.

The training programs from which the generative model is created are real programs, and as such do not share the argument type restrictions. The model, therefore, may generate correct programs for which the driver cannot create example inputs. In this case, a ``compile-only'' stub is used, which only compiles the kernel, without generating input data or executing the compiled kernel.

Unlike the generative model, this test harness is language-specific and the design stems from domain knowledge. Still, it is a relatively simple procedure, consisting of a few hundred lines of Python.

Test Harness Output Classes

Executing a test case on a testbed leads to one of seven possible outcomes, illustrated in Figure . A build failure occurs when online compilation of the OpenCL kernel fails, usually accompanied by an error diagnostic. A build crash or build timeout outcome occurs if the compiler crashes or fails to produce a binary within 60 seconds, respectively. For compile-only test cases, a pass is achieved if the compiler produces a binary. For test cases in which the kernel is executed, kernel execution leads to one of three potential outcomes: runtime crash if the program crashes, timeout if the kernel fails to terminate within 60 seconds, or pass if the kernel terminates gracefully and computes an output.











Voting Heuristics for Differential Testing

Established Differential Testing methodologies are employed to expose compiler defects. As in prior work, voting on the output of programs across compilers has been used to circumvent the oracle problem and detect miscompilations . However, this approach is extended to describe not only miscompilations, but also anomalous build failures and crashes.

When evaluating the outcomes of test cases, build crash () and build timeout () outcomes are of immediate interest, indicative of erroneous compiler behaviour (examples may be found in Section ). For all other outcomes, differential tests are required to confirm anomalous behaviour. We look for test cases where there is a majority outcome -- i.e. for which some fraction of the testbeds behave the same -- but some testbed deviates. The presence of the majority increases the likelihood that there is a `correct' behaviour for the test case. In this work, a majority fraction of  is used, where  is the number of testbeds.

An anomalous build failure () or anomalous runtime crash () occurs if, for a given test case, the majority of testbeds execute successfully, and a testbed yields a compilation error or runtime crash. An anomalous wrong-output () occurs if, for a given test case, the majority of testbeds execute successfully, producing the same output values, and a testbed yields a result which differs from this majority output. Anomalous wrong-output results are indicative of miscompilations, a particularly hard to detect class of bug in which the compiler silently emits wrong code. CSmith is designed specifically to target this class of bug.

False Positives for Anomalous Runtime Behaviour

Generated programs may contain undefined or non-deterministic behaviour which will incorrectly be labelled as anomalous. CSmith circumvents this problem by performing complex analyses during generation so as to minimise the chance of producing programs with undefined behaviour. Although similar analyses could be created as filters for DeepSmith, a simpler approach is taken, filtering only the few types of non-deterministic behaviour that have been actually observed to happen in practice.

Data races, out-of-bounds and uninitialised accesses are filtered using GPUverify  and Oclgrind . Some compiler warnings provide strong indication of non-deterministic behaviour (e.g. comparison between pointer and integer) -- these warnings are checked for and filtered accordingly.

Floating point operations in OpenCL can be imprecise, so code can produce different output on different testbeds. For this reason, CSmith and CLSmith do not support floating point operations. DeepSmith permits floating point operations but since it cannot apply differential testing on the outputs, it can detect all results except for the anomalous wrong-output results.

The last type of undefined behaviour we observed comes from division by zero and related mathematical functions which require non-zero values. A simple detection and filtering heuristic was applied -- the input values are changed and the output is checked to see if it remains anomalous. While theoretically insufficient, in practice no false positives have been found to remain.

Experimental Setup


In this section we describe the experimental parameters used.

OpenCL Systems

We conducted testing of 10 OpenCL systems, summarised in
Table . We covered a broad range of hardware: 3 GPUs, 4
CPUs, a co-processor, and an emulator. 7 of the compilers tested are commercial
products, 3 of them are open source. Our suite of systems includes both
combinations of different drivers for the same device, and different devices
using the same driver.












Testbeds

For each OpenCL system, we create two testbeds. In the first, the compiler is
run with optimisations disabled. In the second, optimisations are enabled. Each
testbed is then a triple, consisting of <device, driver, isoptimised>
settings. This mechanism gives 20 testbeds to evaluate.


Test Cases

For each generated program we create inputs as described in
Section . In addition, we need to choose the number of
threads to use. We generate two test cases, one using one thread, the other
using 2048 threads. A test case is then a triple, consisting of <program,
inputs, threads> settings.

Bug Search Time Allowance

DeepSmith and CLsmith are compared by allowing both to run for 48 hours on
each of the 20 testbeds. CLSmith used its default configuration. The total
runtime for a test case consists of the generation and execution time.

Evaluation




  
  
    kernel void A(global double* a) 
      int b = get_global_id(0);
      if (b < -1)
        a[b] = 1;
    
  




  
  
  kernel void A(global int* a, global int* b) 
    switch (get_global_id(0)) 
    case 0:
      a[get_global_id(0)]=b[get_global_id(0)+13];
      break;
    case 2:
      a[get_global_id(0)]=b[get_global_id(0)+11];
      break;
    case 6:
      a[get_global_id(0)]=b[get_global_id(0)+128];
    
    barrier(2);
  
  




  
  
    kernel void A() 
      __builtin_astype(d, uint4);
    
  




  
  
    kernel void A(global unsigned char* a, global unsigned char* b) 
      unsigned long c = get_global_id(0);
      d[0] = (mad24(f, (int)(a[0], b[get_global_id(0)])) 
    
  








  
  
    kernel void A(global ulong* a) 
      a[get_global_id(0)] = (ulong)(a + 1);
    
  















  
  
  
    kernel void A(global float* a, global float* b,
                  global float* c) 
      int d, e, f;
      d = get_local_id(0);
      for (int g = 0; g < 100000; g++)
        barrier(1);
    
  




  
  
    kernel void A(global unsigned char* a,
                  unsigned b) 
      a[get_global_id(0)] 
      barrier(1);
    
  















  
  
    kernel void A(global int* a) 
      int b = get_global_id(0);
      while (b < 512)  
    
  









  
  
    kernel void A(float4 a, global float4* b,
                  global float4* c, unsigned int d,
                  global double* e, global int2* f, 
                  global int4* g, constant int* h, 
                  constant int* i) 
      A(a, b, c, d, d, e, f, g, h);
    
  






  
  
    kernel void B(constant int2* a, global int* b) 
      A(b, a);
    
  




  
  
    kernel void A(global float4* a) 
      a[get_local_id(0) / 8][get_local_id(0)] = 
          get_local_id(0);
    
  




  
  
    kernel void A(global float* a, global float* b) 
      a[0] = max(a[c], b[2]);
    
  




  
  
    kernel void A(global int* a) 
      int b = get_global_id(0);
      a[b] = (6 * 32) + 4 * (32 / 32) + a;
    
  




  
  
    kernel void A(global float* a, local float* b, local float* c, int d, int e) 
      int f, g;
      int h = get_local_id(0);
      int i = get_local_id(1);
      int j = get_global_id(0);
      global char* k = c + f * g + f;
      if (f + 1 < h)
        b[f * d + g * h + g] = g * f;
    
  




  
  
    kernel void A(void) 
      global int* a;
      unsigned int* b;
      b = a[0];
      a[0] = b;
      a[0] = b;
      barrier(1);
      if (get_global_id(0) == 0)
        *a = 0;
      a[get_local_id(0)] = 0;
    
  




  
  
    kernel void A() 
      while (true) 
        barrier(1);
      
    
  




  
  
    kernel void A(global double* a, global double* b,
                  global double* c, int d, int e) 
      double f;
      int g = get_global_id(0);
      if (g < e - d - 1)
        c[g] = (((e) / d) 
    
  




  
  
    kernel void A(local int* a) 
      for (int b = 0; b < 100; b++)
        B(a);
    
  





  
  
    kernel void A(global float* a, global float* b, global float* c, const int d) 
      int e = get_global_id(0);
      if (e < d)
        c[e] = a[e] + b[e];
      b[e] = (char)(c[e] + d);
    
  




  
  
    kernel void A(global int* a) 
      a[0] = 1;
      a[-1] = 2;
      a[0] = 3;
    
  




  
  
      kernel void A(global int* a, global int* b) 
        int c[16];
        int d = get_global_id(0);
        a[d] = b[d] + c[d];
      
  




  
  
    kernel void A(global int* a, global int* b) 
      int c = (int)get_global_id(0);
      a[c] += b;
    
  




  
  
    kernel void A(global float* a, global float* b, global float* c, local float* d, unsigned int e, unsigned int f) 
      for (unsigned int g = get_local_id(0) + get_local_size(0); g < get_local_size(0); g += get_local_size(0))
        a[2 * get_local_id(0) + 1] = get_local_id(0);
    
  




  
  
    kernel void A(global int* a, global int* b, 
                  global int* c) 
      c[0] = (a[0] > b[0]) ? a[0] : 0;
      c[2] = (a[3] <= b[3]) ? a[4] : b[5];
      c[4] = (a[4] <= b[5]) ? a[7] : b[7];
      c[7] = (a[7] < b[0]) ? a[0] : (a[0] > b[1]);
    
  










  
  
    kernel void A(global int* a, global int* b, global int* c) 
      c[0] = (a[0] > 0) ? a[0] : b[1];
      c[1] = (a[1] <= b[1]) ? a[0] : b[0];
    
  




  
  
    kernel void A(read_only image2d_t a, 
                  global double2* b) 
      b[0] = get_global_id(0);
    
  




  
  
    kernel void A() 
      char a;
      int b;
      int const;
    
  




  
  
    kernel void A(global float4* a, global float4* b, global float4* c, global float4* d, global float4* e, float f) 
      unsigned int g = get_global_id(0);
      int h = get_global_size(0);
      constant sampler_t i = 0x0000  0x0004  0x0000;
      unsigned int j = g * (1 << ((h 
    
  




  
  
    kernel void A(global int* a) 
      local int b[2][3][4][5];
      if (get_global_id(0) == 0)
        a = b[0];
    
  




  
  
    kernel void A(global uchar4* a, const int b) 
      local int c[16];
      local float8* d = a + 133;
      atomic_cmpxchg(c, 10, 13);
    
  






  
  
    kernel void A() 
      while (true)
        barrier(1);
    
  

























  
  
    kernel void A(global int* a) 
      int b = get_global_id(0);
      while (b < *a)
        if (a[0] < 0)
          a[1] = b / b * get_local_id(0);
    
  




  
  
    kernel void A(int a, global int* b) 
      int c = get_global_id(0);
      int d = work_group_scan_inclusive_max(c);
      b[c] = c;
    
  




  
  
    kernel void A(global half* a, global int* b, global bool* c, int d, int e) 
      int f = get_global_id(0);
      int g = get_global_id(1) * e;
      if (f < e)
        a[f] = b[f];
    
  




  
  
    kernel void A() 
      local float a; A(a);
    
  




  
  
    kernel void A() 
      local int a[10];
      local int b[16][16];
      a[1024 + (2 * get_local_id(1) +
        get_local_id(0)) + get_local_id(0)] = 6;
      barrier(b);
    
  




  
  
    kernel void A(global float* a, global float* b,
                  global float* c, const int d) 
      for (unsigned int e = get_global_id(0);
           e < d; e += get_global_size(0))
        for (unsigned f = 0; f < d; ++f)
          e += a[f];
    
  





  
  
    kernel void A(global int* a, global int* b,
                  global int* c) 
      a[get_global_id(0)] = a[get_global_id(0)] > b;
    
  




  
  
    kernel void A(global int* a) 
      global int* b = ((void*)0);
      b[0] = a;
    
  




  
  
    void A()(global a*)()
  




  
  
    void A()void* a; uint4 b=0; b=(b>b)?a:a
  




  
  
    void A()double2 k; return (float4)(k,k,k,k)
  




  
  
    kernel void A(global half* a) 
      int b = get_global_id(0);
      a[b] = b * b;
    
  




  
  
    kernel void A(global float* a, global float* b,
                  const int c) 
      for (int d = 0; d < c; d++)
        for (d = 0; d < a; d += 32)
          b[d] = 0;
    
  




  
  
    kernel void A(global int* a, const int b, const int c) 
      int d; int e; int f = 0;
      for (int g = b >> 1; g > 1; g >>= 1) 
        barrier(1);
        for (int h = c; h < g; h += d) 
          int i = f * (2 * h + 1) - 1;
          int j = f * (2 * h + 2) - 1;
          a[j] += a[i];
        
      
    
  


This section reports on the results of DeepSmith testing of the 10 OpenCL systems from Table , in which each ran for 48 hours. Bugs were found in all the compilers tested --- every compiler crashed, and every compiler generated programs which either crash or silently compute the wrong result. To date, 67 bug reports have been submitted to compiler vendors. This section first contains a qualitative analysis of compile-time and runtime defects found, followed by a quantitative comparison of the approach against the state-of-the-art in OpenCL compiler fuzzing --- CLSmith . DeepSmith is able to identify a broad range of defects, many of which CLSmith cannot, for only a fraction of the engineering effort. Finally, this section contains a quantitative analysis of compiler robustness over time, using the compiler crash rate of every LLVM release in the past two years as a metric of compiler robustness. The findings show that progress is good, compilers are becoming more robust, yet the introduction of new features and regressions ensures that compiler validation remains a moving target.

Unless stated otherwise, DeepSmith code listings are presented verbatim, with only minor formatting changes applied to preserve space. No test case reduction, either manual or automatic, was needed.

For the remainder of this chapter, testbeds are identified using the OpenCL system number from Table , suffixed with , , or  to denote optimisations on, off, or either, respectively.

Compile-time Defects


OpenCL is typically compiled online, which amplifies the significance of detecting compile-time defects, as they may not be discovered until code has been shipped to customers. Numerous cases were found where DeepSmith kernels trigger a crash in the compiler (and as a result, the host process), or cause the compiler to loop indefinitely. In the testing time allotted 199 test cases were identified which trigger unreachable code failures, triggered 31 different compiler assertions, and produced 114 distinct stack traces from other compiler crashes.



















Semantic Analysis Failures

Compilers should produce meaningful diagnostics when inputs are invalid, yet dozens of compiler defects were discovered attributable to improper or missing error handling. Many generation and mutation based approaches to compiler validation have focused solely on testing under valid inputs. As such, this class of bugs may go undiscovered. Compared to these approaches, DeepSmith may contribute a significant improvement to generating plausibly-erroneous code over prior random-enumeration approaches.

The use of undeclared identifiers is a core error diagnostic which one would expect to be robust in a mature compiler. DeepSmith discovered cases in which the presence of undeclared identifiers causes the Testbeds  compiler to crash. For example, the undeclared identifier c in Figure  raises an assertion during semantic analysis of the AST when used as an array index.




























Type errors were an occasional cause of compile-time defect. Figure  induces a crash in NVIDIA compilers due to an implicit conversion between global to constant address qualifiers. Worse, Testbeds  was found to loop indefinitely on some kernels containing implicit conversions from a pointer to an integer, as shown in Figure . While spinning, the compiler would utilise 100 of the CPU and consume an increasing amount of host memory until the entire system memory is depleted and the process crashes.

Occasionally, incorrect program semantics will remain undetected until late in the compilation process. Both Figures  and  pass the type checker and semantic analysis, but trigger compiler assertions during code generation.

An interesting yet unintended by-product of having trained DeepSmith on thousands of real world examples is that the model learned to occasionally generate compiler-specific code, such as invoking compiler intrinsics. The quality of error handling on these builtins was found to vary wildly. For example, Figure  silently crashes 6 of the 10 compilers, which, to the best of my knowledge, makes DeepSmith the first random program generator to induce a defect through exploiting compiler-specific functionality.

Parser Failures

Parser development is a mature and well understood practice. Parser errors were discovered in several compilers. Each of the code samples in Figure  induce crash errors during parsing of compound statements in both Testbeds  and . For space, the listings have been hand-reduced to minimal code samples, which have been reported to Intel. Each reduction took around 6 edit-compile steps, taking less than 10 minutes. In total, 100 distinct programs have been generated which crash compilers during parsing.

























Compiler Hangs

As expected, some compile-time defects are optimisation sensitive. Testbed  hangs on large loop bounds, shown in Figure . All commercial Intel compilers  tested hang during optimisation of non-terminating loops (Figure ).

Testbeds  loop indefinitely during compilation of the simple OpenCL kernel in Figure .

Other errors

Some compilers are more permissive than others. Testbeds , ,  reject out-of-range literal values e.g. int i = 0xFFFFFFFFFFFFFFFFFFFFFFFF, whilst Testbeds , , , , and  interpret the literal as an unsigned long long and implicitly cast to an integer value of -1. Testbeds ,  emit no warning.

Testbeds , ,  rejected address space qualifiers on automatic variables, where all other testbeds successfully compiled and executed.

On Testbeds , the statement int n = mad24(a, (32), getglobalsize(0)); (a call to a maths builtin with mixed types) is rejected as ambiguous.


Runtime Defects

Prior work on compiler test case generation has focused on extensive stress-testing of compiler middle-ends to uncover miscompilations . CSmith, and by extension, CLSmith, specifically targets this class of bugs. Grammar based enumeration is highly effective at this task, yet is bounded by the expressiveness of the grammar. Here, examples are provided of bugs which cannot currently be discovered by CLSmith.


Thread-dependent Flow Control

A common pattern in OpenCL is to obtain the thread identity, often as an int, and to compare this against some fixed value to determine whether or not to complete a unit of work (46 of OpenCL kernels on GitHub use this ( int, if (tid < ) ) pattern). DeepSmith, having modelled the frequency with which this pattern occurs in real handwritten code, generates many permutations of this pattern. And in doing so, exposed a bug in the optimiser of Testbeds  and  which causes the if branch in Figure  to be erroneously executed when the kernel is compiled with optimisations enabled. This issue has been have reported to Intel. CLSmith does not permit the thread identity to modify control flow, rendering such productions impossible.
























Figure  shows a simple program in which thread identity determines the program output. This test case was found to sporadically crash Testbeds , an OpenCL device simulator and debugger. Upon reporting to the developers, the underlying cause was quickly diagnosed as a race condition in switch statement evaluation, and fixed within a week.


Kernel Inputs

CLSmith kernels accept a single buffer parameter into which each thread computes its result. This fixed prototype limits the ability to detect bugs which depend on input arguments. Figure  exposes a bug of this type. Testbeds  will silently miscompile ternary operators when the ternary operands consist of values stored in multiple different global buffers. CLSmith, with its fixed single input prototype, is unable to discover this bug.


Latent Compile-time Defects

Sometimes, invalid compiler inputs may go undetected, leading to runtime defects only upon program execution. Since CLSmith enumerates only well-formed programs, this class of bugs cannot be discovered.

Figure  exposes a bug in which a kernel containing an undefined symbol will successfully compile without warning on Testbeds , then crash the program when attempting to run the kernel. This issue has been reported to the developers and fixed.


Comparison to State-of-the-art

This section provides a quantitative comparison of the bug-finding capabilities of DeepSmith and CLSmith.


Results Overview

Tables  and  shows the results of 48 hours of consecutive testing for all Testbeds using CLSmith and DeepSmith, respectively. An average of 15k CLSmith and 91k DeepSmith test cases were evaluated on each Testbed, taking 12.1s and 1.90s per test case respectively. There are three significant factors providing the sixfold increase in testing throughput achieved by DeepSmith over CLSmith: test cases are faster to generate, test cases are less likely to timeout (execute for 60 seconds without termination), and the test cases which do not timeout execute faster.



















Figure a shows the generation and execution times of DeepSmith and CLSmith test cases, excluding timeouts . DeepSmith generation time grows linearly with program length, and is on average  faster than CLSmith. Test case execution is on average  faster than CLSmith.










The optimisation level generally does not affect testing throughput significantly, with the exception of Testbed . Optimisation of large structs is expensive on Testbed , and CLSmith test cases use global structs extensively. This is a known issue --- in  the authors omit large-scale testing on this device for this reason. The use of structs in handwritten OpenCL is comparatively rare --- only 7.1 of kernels on GitHub use them.


Comparison of Test Cases

The average CLSmith program is 1189 lines long (excluding headers). CLSmith test cases require reduction in order to expose the underlying bug. An automated approach to OpenCL test case reduction is presented in , though it requires on average 100 minutes for each test case using a parallelised implementation (and over 6 hours if this parallelisation is not available); the authors also suggest a final manual pass after automated reduction. In contrast, DeepSmith learned to program from humans, and humans do not typically write such large kernel functions. The average DeepSmith kernel is 20 lines long, which is interpretable without reduction, either manual or automatic.


Comparison of Results

Both testing systems found anomalous results of all types. In 48 hours of testing, CLSmith discovered compile-time crashes () in 8 of the 20 testbeds, DeepSmith crashed all of them. DeepSmith triggered 31 distinct compiler assertions, CLSmith 2. Both of the assertions triggered by CLSmith were also triggered by DeepSmith. DeepSmith also triggered 3 distinct unreachable! compile-time crashes, CLSmith triggered 0. The ratio of build failures is higher in the token-level generation of DeepSmith (51) than the grammar-based generation of CLSmith (26).

The Intel CPU Testbeds (, , , and ) would occasionally emit a stack trace upon crashing, identifying the failure point in a specific compiler pass. CLSmith triggered such crashes in 4 distinct passes. DeepSmith triggered crashes in 10 distinct passes, including 3 of the 4 in which CLSmith did. Figures  and  provide examples. Many of these crashes are optimisation sensitive, and are more likely to occur when optimisations are enabled. CLSmith was able to induce a crash in only one of the Intel testbeds with optimisations disabled. DeepSmith crashed all of the compilers with both optimisations enabled and disabled.














































CLSmith produced many results across 13 Testbeds. Given the large kernel size, it is unclear how many of those are infinite loops or simply a result of slow compilation of large kernels. The average size of CLSmith kernels is 1558 lines. Automated test case reduction --- in which thousands of permutations of a program are executed --- may be prohibitively expensive for test cases with very long runtimes. DeepSmith produced results across 11 Testbeds and with an average kernel size of 9 lines, allowing for rapid identification of the underlying problem.


















The integrated GPU Testbeds () frequently failed to compile CLSmith kernels, resulting in over 10k and results. Of the build crashes, 68 failed silently, and the remainder were caused by the same two compiler assertions for which DeepSmith generated 4 line test cases, shown in Figure . DeepSmith also triggered silent build crashes in Testbeds , and a further 8 distinct compiler assertions.

The 4719 results for CLSmith on Testbeds  and  are all a result of compilers rejecting empty declarations, (e.g. int;) which CLSmith occasionally emits. DeepSmith also generated these statements, but with a much lower probability, given that it is an unusual construct (0.6 of test cases, versus 7.0 of CLSmith test cases).

ComputeAorta (Testbeds ) defers kernel compilation so that it can perform optimisations dependent on runtime parameters. This may contribute to the relatively large number of results and few results of Testbeds . Only DeepSmith was able to expose compile-time defects in this compiler.

Over the course of testing, a combined  lines of CLSmith code was evaluated, compared to  lines of DeepSmith code. This provides CLSmith a greater potential to trigger miscompilations. CLSmith generated 33 programs with anomalous wrong-outputs. DeepSmith generated 30.


Compiler Stability Over Time


The Clang front-end to LLVM supports OpenCL, and is commonly used in OpenCL drivers. This in turn causes Clang-related defects to potentially affect multiple compilers, for example the one in Figure . To evaluate the impact of Clang, debug+assert builds of every LLVM release in the past 24 months were used to process 75,000 DeepSmith kernels through the Clang front-end (this includes the lexer, parser, and type checker, but not code generation).

Figure  shows that the crash rate of the Clang front-end is, for the most part, steadily decreasing over time. The number of failing compiler crashes decreased tenfold between 3.6.2 and 5.0.0. Table  shows the 7 distinct assertions triggered during this experiment. Assertion 1 (Uncorrected typos!) is raised on all compiler versions --- see Figure  for an example. The overall rate at which the assertion is triggered has decreased markedly, although there are slight increases between some releases. Notably, the current development trunk has the second lowest crash rate, but is joint first in terms of the number of unique assertions. Assertions 3 (Addr == 0  hasTargetSpecificAddressSpace()) and 4 (isScalarType()) were triggered by some kernels in the development trunk but not under any prior release. Bug reports have been submitted for each of the three assertions triggered in the development trunk, as well as for two distinct unreachables.



















The results emphasise that compiler validation is a moving target. Every change and feature addition has the potential to introduce regressions or new failure cases. Since LLVM will not release unless their compiler passes their own extensive test suites, this also reinforces the case for compiler fuzzing. DeepSmith provides an effective means for the generation of such fuzzers, at a fraction of the cost of existing techniques.


Extensibility of Language Model
























A large portion of the DeepSmith architecture is language-agnostic, requiring only a corpus, encoder, and harness for each new language. This potentially significantly lowers the barrier-to-entry compared with prior grammar-based fuzzers. This section reports on initial results in extending DeepSmith to the Solidity programming language. Solidity is the smart contract programming language of the Ethereum blockchain. At less than four years old, it lacks much of the tooling of more established programming languages. Yet, it is an important candidate for rigorous testing, as exploitable bugs may undermine the integrity of the blockchain and lead to fraudulent transactions.


Testing Methodology

The same methodology was applied to train the program generator as for OpenCL. A corpus of Solidity contracts was assembled from GitHub, recursively inlining imported modules where possible. The same tokeniser was used as for OpenCL, only changing the list of language keywords and builtins. Code style was enforced using clang-format. The model is trained in the same manner as OpenCL. No modification to either the language model or generator code was required. A simple compile-only test harness is used to drive the generated Solidity contracts.


Initial Results

The generator and harness loop was run for 12 hours on four testbeds: the Solidity reference compiler solc with optimisations on or off, and solc-js, which is an Emscripten compiled version of the solc compiler. Table  summarises the results. Numerous cases were found where the compiler silently crashes, and two distinct compiler assertions. The first is caused by missing error handling of language features (this issue is known to the developers). The source of the second assertion is the JavaScript runtime and is triggered only in the Emscripten version, suggesting an error in the automatic translation from LLVM to JavaScript.

Extending DeepSmith to a second programming required an additional 150 lines of code (18 lines for the generator and encoder, the remainder for the test harness) and took about a day. Given the re-usability of the core DeepSmith components, there is a diminishing cost with the addition of each new language. For example, the OpenCL encoder and re-writer, implemented using LLVM, could be adapted to C with minimal changes. Given the low cost of extensibility, these preliminary results indicate the utility of the approach for simplifying test case generation.

Summary


This chapter presents a novel framework for compiler fuzzing. By posing the generation of random programs as an unsupervised machine learning problem, the cost and human effort required to engineer a random program generator is drastically lowered. Large parts of the stack are programming language-agnostic, requiring only a corpus of example programs, an encoder, and a test harness to target a new language.

The approach is demonstrated by targeting the challenging many-core domain of OpenCL. The implementation, DeepSmith, has uncovered dozens of bugs in both commercial and open-source OpenCL compilers. DeepSmith exposed bugs in parts of the compiler where current approaches have not, for example in missing error handling. A preliminary exploration of the extensibility of our approach to other languages has been performed. DeepSmith test cases are small, two orders of magnitude shorter than the state-of-the-art, and easily interpretable.



  
End-to-end Deep Learning of Optimisation Heuristics
Introduction
[1-2]
[pages=-]2017-pact.pdf
Conclusion
[1-2]

End-to-end Deep Learning of Optimisation Heuristics


Introduction









There are countless scenarios during the compilation and execution of a parallel program where decisions must be made as to how, or if, a particular optimisation should be applied. Modern compilers and runtimes are rife with hand coded heuristics which perform this decision making. The performance of parallel programs is thus dependent on the quality of these heuristics.

Hand-written heuristics require expert knowledge, take a lot of time to construct, and in many cases lead to sub-optimal decisions. Researchers have focused on machine learning as a means to constructing high quality heuristics that often outperform their handcrafted equivalents . A predictive model is trained, using supervised machine learning, on empirical performance data and important quantifiable properties, or features, of representative programs. The model learns the correlation between these features and the optimisation decision that maximises performance. The learned correlations are used to predict the best optimisation decisions for new programs. Previous works in this area were able to build machine learning based heuristics with less effort, that outperform ones created manually experts .

Still, experts are not completely removed from the design process, which is shown in Figure . Selecting the appropriate features is a manual undertaking which requires a deep understanding of the system. The designer essentially decides which compile or runtime characteristics affect optimisation decisions and expresses them in ways that make it easy to model their relationship to performance. Failing to identify an important feature has a negative effect on the resulting heuristic. For example, Section  identified one such feature, causing performance to be 40 lower on average.

To make heuristic construction fast and cheap, humans must be taken out of the loop. While techniques for automatic feature generation from the compiler IR have been proposed in the past , they do not solve the problem in a practical way. They are deeply embedded into the compiler, require expert knowledge to guide the generation, have to be repeated from scratch for every new heuristic, and their search time can be prohibitive. The inciting motivation for this chapter is that such costly approaches are not necessary any more. Deep learning techniques have shown astounding successes in identifying complex patterns and relationships in images , audio , and even computer code . This chapter hypothesises that deep neural networks should be able to automatically extract features from source code. The experiments showed that even this was a conservative target: with deep neural networks one can bypass static feature extraction and learn optimisation heuristics directly on raw code.

















Figure  shows the proposed methodology. Instead of manually extracting features from input programs to generate training data, program code is used directly in the training data. Programs are fed through a series of neural networks which learn how code correlates with performance. Internally and without prior knowledge, the networks construct complex abstractions of the input program characteristics and correlations between those abstractions and performance. This chapter proposes replacing the need for compile-time or static code features, merging feature and heuristic construction into a single process of joint learning. The system admits auxiliary features to describe information unavailable at compile time, such as the sizes of runtime input parameters. Beyond these optional inclusions, the system is able to learn optimisation heuristics without human guidance.

By employing transfer learning , the proposed approach is able to produce high quality heuristics even when learning on a small number of programs. The properties of the raw code that are abstracted by the beginning layers of our neural networks are mostly independent of the optimisation problem. Parts of the network may be reused across heuristics, and, in the process, can speed up learning considerably.

The approach is evaluated on two problems: heterogeneous device mapping and GPU thread coarsening. Good heuristics for these two problems are important for extracting performance from heterogeneous systems, and the fact that machine learning has been used before for heuristic construction for these problems allows direct comparison. Prior machine learning approaches resulted in good heuristics which extracted 73 and 79 of the available performance respectively but required extensive human effort to select the appropriate features. Nevertheless, the approach presented in this chapter was able to outperform them by 14 and 12, which indicates a better identification of important program characteristics, without any expert help.

This chapter is organised as follows: Section  introduces DeepTune, a novel system for building optimisation heuristics. Section  describes the experimental setup of two case studies: heterogeneous device mapping and thread coarsening. Section  contains the results of the case studies. Finally Section  contains concluding remarks.
DeepTune: Learning On Raw Program Code


DeepTune is an end-to-end machine learning pipeline for optimisation heuristics. Its primary input is the source code of a program to be optimised, and through a series of neural networks, it directly predicts the optimisation which should be applied. By learning on source code, the approach is not tied to a specific compiler, platform, or optimisation problem. The same design can be reused to build multiple heuristics. The most important innovation of DeepTune is that it forgoes the need for human experts to select and tune appropriate features.


System Overview

Figure  provides an overview of the system. A source re-writer removes semantically irrelevant information (such as comments) from the source code of the target program and passes it to a language model. The language model converts the arbitrary length stream of code into a fixed length vector of real values which fully capture the properties and structure of the source, replacing the role of hand designed features. This vector can then optionally be concatenated with auxiliary inputs, which allow passing additional data about runtime or architectural parameters to the model for heuristics which need more than just compile-time information. Finally, a standard feed-forward network is used to predict the best heuristic parameters to optimise the program.













DeepTune is open source . The model is using Keras, with TensorFlow  and Theano  back-ends.


Language Model

Learning effective representations of source code is a difficult task. A successful model must be able to:


  derive semantic and syntactic patterns of a programming language entirely from sample codes;
  identify the patterns and representation in source codes which are relevant to the task at hand; and
  discriminate performance characteristics arising from potentially subtle differences in similar codes.


To achieve this task, state-of-the-art language modelling techniques are employed, coupled with a series of generic, language agnostic code transformations.

Source Re-writer

To begin with, a series of source normalising transformations are applied, extending the system described in Section . These transformations, implemented as an LLVM pass, parse the AST, removing conditional compilation, then rebuild the input source code using a consistent code style and identifier naming scheme. The role of source normalisation is to simplify the task of modelling source code by ensuring that trivial semantic differences in programs such as the choice of variable names or the insertion of comments do not affect the learned model. Figures  and  show the source rewriting process applied to a simple program.

Sequence Encoder

A source code is encoded as a sequence of integers for interpretation by neural networks, where each integer is an index into a predetermined vocabulary. In , a character based vocabulary is used. This minimises the size of the vocabulary, but leads to long sequences which are harder to extract structure from. In , a token based vocabulary is used. This leads to shorter sequences, but causes an explosion in the vocabulary size, as every identifier and literal must be represented uniquely.

A hybrid, partially tokenised vocabulary approach is used. This allows common multi-character sequences such as float and if to be represented as unique vocabulary items, while literals and other infrequently used words are encoded at the character level.

First, a candidate vocabulary  is assembled for the OpenCL programming language containing the 208 data types, keywords, and language builtins of the OpenCL programming language. From this, the subset of the candidate vocabulary  which is required to encode a corpus of 45k lines of GPGPU benchmark suite kernels is derived. Beginning with the first character in the corpus, the algorithm consumes the longest matching sequence from the candidate vocabulary.[inline]insert maximal munch algorithm, Algorithm  This process continues until every character in the corpus has been consumed. The resulting derived vocabulary consists of 128 symbols which we use to encode new program sources. Figure  shows the vocabulary derived for a single input source code Figure .


  [1]
Candidate vocabulary , string .
Vocabulary .



  
  
  
    
    
    
    
  



  [Deriving a vocabulary from a string]
    Deriving a vocabulary from a string.
  
  


language=[OpenCL]C








































Embedding

During encoding, tokens in the vocabulary are mapped to unique integer values, e.g. float , int  1. The integer values chosen are arbitrary, and offer a sparse data representation, meaning that a language model cannot infer the relationships between tokens based on their mappings. This is in contrast to the dense representations of other domains, such as pixels in images, which can be interpolated between to derive the differences in colours.

To mitigate this, an embedding is used, which translates tokens in a sparse, integer encoded vocabulary into a lower dimensional vector space, allowing semantically related tokens like float and int to be mapped to nearby points . An embedding layer maps each token in the integer encoded vocabulary to a vector of real values. Given a vocabulary size  and embedding dimensionality , an embedding matrix  is learned during training, so that an integer encoded sequences of tokens  is mapped to the matrix . We use an embedding dimensionality .

Sequence Characterisation

Once source codes have been encoded into sequences of embedding vectors, neural networks are used to extract a fixed size vector which characterises the entire sequence. This is comparable to the hand engineered feature extractors used in prior works, but is a learned process that occurs entirely --- and automatically --- within the hidden layers of the network.

The Long Short-Term Memory (LSTM) architecture is used for sequence characterisation . LSTMs implements a Recurrent Neural Network in which the activations of neurons are learned with respect not just to their current inputs, but to previous inputs in a sequence. Unlike regular recurrent networks in which the strength of learning decreases over time (a symptom of the vanishing gradients problem ), LSTMs employ a forget gate with a linear activation function, allowing them to retain activations for arbitrary durations. This makes them effective at learning complex relationships over long sequences , an especially important capability for modelling program code, as dependencies in sequences frequently occur over long ranges (for example, a variable may be declared as an argument to a function and used throughout).

The LSTM network has two layers of cells. The network receives a sequence of embedding vectors, and returns a single output vector, characterising the entire sequence.


Auxiliary Inputs

An arbitrary number of additional real valued auxiliary inputs may be optionally used to augment the source code input. These inputs are provided as a means of increasing the flexibility of the system, for example, to support applications in which the optimisation heuristic depends on dynamic values which cannot be statically determined from the program code . When present, the values of auxiliary inputs are concatenated with the output of the language model, and fed into a heuristic model.


Heuristic Model

The heuristic model takes the learned representations of the source code and auxiliary inputs (if present), and uses these values to make the final optimisation prediction.

First the values are normalised. Normalisation is necessary because the auxiliary inputs can have any values, whereas the language model activations are in the range [0,1]. If we did not normalise, then scaling the auxiliary inputs could affect the training of the heuristic model. Normalisation occurs in batches. The batch normalisation method of  is used, in which each scalar of the heuristic model's inputs  is normalised to a mean 0 and standard deviation of 1:





where  and  are scale and shift parameters, learned during training.

The final component of DeepTune is comprised of two fully connected neural network layers. The first layer consists of 32 neurons. The second layer consists of a single neuron for each possible heuristic decision. Each neuron applies an activation function  over its inputs. Rectifier activation functions  are used in the first layer due to their improved performance during training of deep networks . For the output layer, sigmoid activation functions  are used which provide activations in the range .

The activation of each neuron in the output layer represents the model's confidence that the corresponding decision is the correct one. Taking the  of the output layer produces the decision with the largest activation. For example, for a binary optimisation heuristic the final layer will consist of two neurons, and the predicted optimisation is the neuron with the largest activation.


Training the network

DeepTune is trained in the same manner as prior works, the key difference being that instead of having to manually create and extract features from programs, the raw program codes themselves are used.

The model is trained with Stochastic Gradient Descent (SGD), using the Adam optimiser . For training data , SGD attempts to find the model parameters  that minimise the output of a loss function:





where loss function  computes the logarithmic difference between the predicted and expected values.

To reduce training time, multiple inputs are batched together and fed into the neural network simultaneously, reducing the frequency of costly weight updates during back-propagation. This requires that the inputs to the language model be the same length. Sequences are padded up to a fixed length of 1024 tokens using a special padding token, allowing matrices of batchsize  maxseqlen tokens to be processed simultaneously. Batching and padding sequences to a maximum length is only to improve training time. In production use, sequences do not need to be padded, allowing classification of arbitrary length codes.

Experimental Methodology


DeepTune is applied to two heterogeneous compiler-based machine learning tasks and its performance compared to state-of-the-art approaches that use expert selected features.


Case Study A: OpenCL Heterogeneous Mapping

OpenCL provides a platform-agnostic framework for heterogeneous parallelism. This allows a program written in OpenCL to execute transparently across a range of different devices, from CPUs to GPUs and FPGAs. Given a program and a choice of execution devices, the question then is on which device should we execute the program to maximise performance?

State-of-the-art

In , Grewe et al. develop a predictive model for mapping OpenCL kernels to the optimal device in CPU/GPU heterogeneous systems. They use supervised learning to construct decision trees, using a combination of static and dynamic kernel features. The static program features are extracted using a custom LLVM pass; the dynamic features are taken from the OpenCL runtime.

Expert Chosen Features

Table  shows the features used by their work. Each feature is an expression built upon the code and runtime metrics given in Table .











































Experimental Setup

The predictive model of Grewe et al.   is replicated. The same experimental setup is used as in Section  in which the experiments are extended to a larger set of 71 programs, summarised in Table . The programs were evaluated on two CPU-GPU platforms, detailed in Table .













































































DeepTune Configuration 

Figure a shows the neural network configuration of DeepTune for the task of predicting optimal device mapping. The OpenCL kernel source code is used as input, along with the two dynamic values work-group size and data size available to the OpenCL runtime.

Model Evaluation 

Stratified 10-fold cross-validation is used to evaluate the quality of the predictive models . Each program is randomly allocated into one of 10 equally-sized sets; the sets are balanced to maintain a distribution of instances from each class consistent with the full set. A model is trained on the programs from all but one of the sets, then tested on the programs of the unseen set. This process is repeated for each of the 10 sets, to construct a complete prediction over the whole data set.












Case Study B: OpenCL Thread Coarsening Factor

Thread coarsening is an optimisation for parallel programs in which the operations of two or more threads are fused together. This optimisation can prove beneficial on certain combinations of programs and architectures, for example programs with a large potential for Instruction Level Parallelism on Very Long Instruction Word architectures.

State-of-the-art Magni et al. present a predictive model for OpenCL thread coarsening in . They implement an iterative heuristic which determines whether a given program would benefit from coarsening. If yes, then the program is coarsened, and the process repeats, allowing further coarsening. In this manner, the problem is reduced from a multi-label classification problem into a series of binary decisions, shown in Figure . They select from one of six possible coarsening factors: , divided into 5 binary choices.



























































Expert Chosen Features

Magni et al. followed a very comprehensive feature engineering process. 17 candidate features were assembled from previous studies of performance counters and computed theoretical values . For each candidate feature they compute its coarsening delta, reflecting the change in each feature value caused by coarsening: , adding it to the feature set. Then they use Principle Component Analysis (PCA) on the 34 candidates and selected the first 7 principle components, accounting for 95 of variance in the space.

Experimental Setup

The experimental setup of Magni et al.  is replicated. The thread coarsening optimisation is evaluated on 17 programs, listed in Table . Four different GPU architectures are used, listed in Table .

DeepTune Configuration

Figure b shows the neural network configuration. The OpenCL kernel is the sole input the coarsening factor is the predicted output.

Model Evaluation

Compared to Case Study A, the size of the evaluation is small. We use leave-one-out cross-validation to evaluate the models. For each program, a model is trained on data from all other programs and used to predict the coarsening factor of the excluded program.

The parameters of the neural network is not described in , so an additional, nested cross-validation process is used to find the optimal model parameters. For every program in the training set, 48 combinations of network parameters are evaluated. The best performing configuration is selected from these 768 results to train a model for prediction on the excluded program. This nested cross-validation is repeated for each of the training sets. No such tuning of hyper-parameters is performed for DeepTune.


Comparison of Case Studies

For the two different optimisation heuristics, the authors arrived at very different predictive model designs, with very different features. By contrast, the DeepSmith approach is exactly the same for both problems. None of DeepTune's parameters were tuned for the case studies presented above. Their settings represent conservative choices expected to work reasonably well for most scenarios.

Table  shows the similarity of the models. The only difference between the network designs is the auxiliary inputs for Case Study A and the different number of optimisation decisions. The differences between DeepTune configurations is only two lines of code: the first, adding the two auxiliary inputs; the second, increasing the size of the output layer for Case Study B from two neurons to six. The description of these differences is larger than the differences themselves.































Experimental Results


This section evaluates the effectiveness of DeepTune for two distinct optimisation tasks: predicting the optimal device to run a given program, and predicting thread coarsening factors.

First DeepTune is compared against two expert-tuned predictive models, showing that DeepTune outperforms the state-of-the-art in both cases. Section  describes leveraging knowledge learned from training DeepTune for one heuristic to boost training for the other heuristic, further improving performance. Finally, Section  analyses the working mechanism of DeepTune.


Case Study A: OpenCL Heterogeneous Mapping

Selecting the optimal execution device for OpenCL kernels is essential for maximising performance. For a CPU/GPU heterogeneous system, this presents a binary choice. In this experiment, the approach is compared against a static single-device approach and the Grewe et al. predictive model. The static mapping selects the device which gave the best average case performance over all the programs. On the AMD platform, the best-performing device is the CPU; on the NVIDIA platform, it is the GPU.

Figure  shows the accuracy of both predictive models and the static mapping approach for each of the benchmark suites. The static approach is accurate for only 58.8 of cases on AMD and 56.9 on NVIDIA. This suggests the need for choosing the execution device on a per program basis. The Grewe et al. model achieves an average accuracy of 73, a significant improvement over the static mapping. By automatically extracting useful feature representations from the source code, DeepTune gives an average accuracy of 82, an improvement over both schemes.










Using the static mapping as a baseline, the relative performance of each program is computed using the device selected by the Grewe et al. and DeepTune models. Figure  shows these speedups. Both predictive models significantly outperform the static mapping; the Grewe et al. model achieves an average speedup of  on AMD and  on NVIDIA (geometric mean ). In 90 of cases, DeepTune matches or outperforms the predictions of the Grewe et al. model, achieving an average speedup of  on AMD and  on NVIDIA (geometric mean ). This 14 improvement in performance comes at a greatly reduced cost, requiring no intervention by humans.











Case Study B: OpenCL Thread Coarsening Factor

Exploiting thread coarsening for OpenCL kernels is a difficult task. On average, coarsening slows programs down. The speedup attainable by a perfect heuristic is only .

Figure  shows speedups achieved by the Magni et al. and DeepTune models for all programs and platforms. The performance of programs without coarsening is used as baseline. On the four experimental platforms (AMD HD 5900, Tahiti 7970, NVIDIA GTX 480, and Tesla K20c), the Magni et al. model achieves average speedups of , , , and , respectively. DeepTune outperforms this, achieving speedups of , , , and .

Some programs --- especially those with large divergent regions or indirect memory accesses --- respond very poorly to coarsening. No performance improvement is possible on the mvCoal and spmv programs. Both models fail to achieve positive average speedups on the NVIDIA Tesla K20c, because thread coarsening does not give performance gains for the majority of the programs on this platform.

The disappointing results for both predictive models may be attributed to the small training program set used by Magni et al. (only 17 programs in total). As a result, the models suffer from sparse training data. Chapter  presents a methodology for overcoming data sparsity using additional programs; the following subsection describes and tests a novel strategy for training optimisation heuristics on a small number of programs by exploiting knowledge learned from other optimisation domains.


Transfer Learning Across Problem Domains


There are inherent differences between the tasks of building heuristics for heterogeneous mapping and thread coarsening, evidenced by the contrasting choices of features and models in Grewe et al. and Magni et al. However, in both cases, the first role of DeepTune is to extract meaningful abstractions and representations of OpenCL code. Prior research in deep learning has shown that models trained on similar inputs for different tasks often share useful commonalities. The idea is that in neural network classification, information learned at the early layers of neural networks (i.e. closer to the input layer) will be useful for multiple tasks. The later the network layers are (i.e. closer to the output layer), the more specialised the layers become .

Hypothesising that this would be the case for DeepTune would enable the novel transfer of information across different optimisation domains. To test this, the language model --- the Embedding, and LSTM1,2 layers --- trained for the heterogeneous mapping task was extracted and transferred over to the new task of thread coarsening. Since DeepTune keeps the same design for both optimisation problems, this is as simple as copying the learned weights of the three layers. The model is then trained as normal.

As shown in Figure , the newly trained model, DeepTune-TL has improved performance for 3 of the 4 platforms: , , , , providing an average 12 performance improvement over Magni et al.  In 81 of cases, the use of transfer learning matched or improved the optimisation decisions of DeepTune, providing up to a 16 improvement in per platform performance.










On the NVIDIA Tesla K20c, the platform for which no predictive model achieves positive average speedups, DeepTune-TL matches or improve performance in the majority of cases, but over-coarsening on three of the programs causes a modest reduction in average performance. For this platform, further performance results are suspected necessary due to its unusual optimisation profile.


DeepTune Internal Activation States


In previous sections DeepTune is shown to automatically outperform state-of-the-art predictive models for which experts have invested a great amount of time in engineering features. This subsection attempts to illuminate the inner workings, using a single example from Case Study B: predicting the thread coarsening factor for Parboil's mriQ benchmark on four different platforms.

Figure  shows the DeepTune configuration, with visual overlays showing the internal state. From top to bottom, the input to the model is the 267 lines of OpenCL code for the mriQ kernel. This source code is preprocessed, formatted, and rewritten using variable and function renaming, shown in Figure b. The rewritten source code is tokenised and encoded in a -of- vocabulary. Figure c shows the first 80 elements of this encoded sequence as a heat map in which each cell's colour reflects its encoded value. The input, rewriting, and encoding is the same for each of the four platforms.














The encoded sequences are then passed into the Embedding layer. This maps each token of the vocabulary to a point in a 64 dimension vector space. Embeddings are learned during training so as to cluster semantically related tokens together. As such, they may differ between the four platforms. Figure d shows a PCA projection of the embedding space for one of the platforms, showing multiple clusters of tokens. By honing in on one of the clusters and annotating each point with its corresponding token, it can be observed that the cluster contains the semantically related OpenCL address space modifiers private, global, and readonly.

Two layers of 64 LSTM neurons model the sequence of embeddings, with the neuron activations of the second layer being used to characterise the entire sequence. Figure e shows the neurons in this layer for each of the four platforms, using a red-blue heat map to visualise the intensity of each activation. Comparing the activations between the four platforms, we note a number of neurons in the layer with different responses across platforms. This indicates that the language model is partly specialised to the target platform. Subsequent experiments in  support this reasoning, where a platform-agnostic language model achieves slightly poorer performance.

As information flows through the network, the layers become progressively more specialised to the specific platform. This can be seen in Figure f, which shows the two layers of the heuristic model. The activations within these increasingly diverge. The mean variance of activations across platforms increases threefold compared to the language model, from 0.039 to 0.107. Even the activations of the AMD HD 5900 and AMD Tahiti 7970 platforms are dissimilar, despite the final predicted coarsening factor for both platforms being the same. The largest activation of the output layer is taken in Figure g as the final predicted coarsening factor. For this particular program, a state-of-the-art model achieves 54 of the maximum performance. DeepTune achieves 99.

Summary


Applying machine learning to compiler and runtime optimisations requires generating features first. This is a time consuming process, it needs supervision by an expert, and even then one cannot be sure that the selected features are optimal. This chapter presents a novel tool for building optimisation heuristics, DeepTune, which forgoes feature extraction entirely, relying on powerful language modelling techniques to automatically build effective representations of programs directly from raw source code. The result translates into a huge reduction in development effort, improved heuristic performance, and more simple model designs.

The approach is fully automated. Using DeepTune, developers no longer need to spend months using statistical methods and profile counters to select program features via trial and error. It is worth mentioning that the model design or parameters are not tailored for the optimisation task at hand, yet DeepTune achieves performance on par with and in most cases exceeding state-of-the-art predictive models.

In this chapter, DeepTune is used to automatically construct heuristics for two challenging compiler and runtime optimisation problems. In both cases, DeepTune is found to outperform state-of-the-art predictive models by 14 and 12. The DeepTune architecture is shown also to allow the exploitation of information learned from another optimisation problem to give the learning a boost. Doing so provides up to a 16 performance improvement when training using a handful of programs. This approach may prove useful in other domains for which training data are a scarce resource.



  Conclusions


This thesis new techniques for the generation and optimisation of programs using machine learning, thereby lowering the cost to construct compilers. In particular, Chapter  presented a new methodology for generating executable benchmarks to address the problem of benchmark scarcity. Then Chapter  extended this technique to compiler fuzzing, thereby addressing the compiler validation problem. Finally, Chapter 

Section  summarizes the main contributions of this thesis, Section  presents a critical analysis of this work, and future work is described in Section .


Contributions


The problems addressed in this thesis are well established. This section summarises the main contributions of this thesis with respect to these problems.


Benchmark Scarcity

There is a shortage of benchmarks, forcing compiler developers to work with a sparse sampling of the program space. Chapter  develops a novel generator for compiler benchmarks, capable of generating an unbounded number of benchmarks. The usefulness of the generated benchmarks is evaluated on a state-of-the-art learned optimisation heuristic, finding that the additional exploration of the program space provided by the generated benchmarks improves performance by .

Compared to previous works , the generation of benchmarks is entirely automatic, requiring no expert tuning or direction. Only a corpus of example programs is needed to guide the distributions of generated programs. Despite no a priori knowledge of the language, the generator is capable of producing executable benchmarks of such quality that professional software developers cannot distinguish code generated by it from handwritten code.

The approach, in generating an unbounded number of runnable programs, enables a finer grained exploration of the compiler optimisation feature space than was previously possible, without the development costs previously associated with benchmark generation. This simplifies the construction of compilers by enabling performance models to be learned from automatically generated data.




Compiler Test Case Generation

Chapter  extends the technique of Chapter  to the domain of compiler test case generation. Like in the domain of benchmarking, the state-of-the-art in compiler test case generation is an enormous engineering undertaking . The technique presented in the chapter presents an enormous reduction in developer effort compared to the state-of-the-art grammar-based approach. The technique presented in this thesis can be implemented in as few as 500 lines of code. The state-of-the-art approach is over 50,000 lines of code . This  reduction in code size is complemented by improved portability of the implementation, with only parts of the stack being specific to the input language of the compiler being tested. The remainder being language agnostic.

The portability of the approach is demonstrated by extending the generator from OpenCL to Solidity in only 12 lines of code. By contrast, extending a state-of-the-art generator from C to OpenCL required over 8000 lines of code .

Despite its simplicity, the proposed technique is effective. To date, 67 new bugs have been identified and reported in OpenCL compilers. Many of the bugs identified could not be exposed by state-of-the-art approaches due to the limitations in the expressiveness of grammar-based approaches. The expressiveness of the generated test cases is limited only by the code that has been uploaded to GitHub; this led to unintentional outcomes such as exploiting compiler-specific features to expose a bug in compilers' error handling for intrinsics.

[inline]TODO: how does it relate to the message of the thesis.


Constructing Compiler Optimizations

Constructing features for machine learning is time consuming and error prone. Additionally, in machine learning for compilers, the choice of features typically couples the learning system tightly with the implementation of the compiler. Chapter  proposes a technique to address both issues. Instead of a numerical feature representation of programs, the source code of the entire program is fed directly into the learning system, greatly simplifying the approach.

The technique is evaluated for two distinct optimisation problems, finding that in both cases, the approach is able to match or outperform the state-of-the-art approach using hand-crafted features, achieving speedups of  and . This is spite of using the same approach for both problems, without any specialising the structure of the learning system to the task being learned. In abstracting the structure of the solution from the problem, the approach enables the novel transfer of information learned for one task to the other. By enabling transfer learning, the performance of a predictive model improves by a further , despite only being provided with information learned for a different optimisation task.

In bypassing the need to engineer features, the proposed technique simplifies the construction of optimisation heuristics through machine learning, while leading to higher performance in the heuristics themselves. Since compilers typically contain hundreds or even thousands of distinct optimisation heuristics, techniques like the one proposed that enable the sharing of information between tasks are prudent to the widespread adoption of machine learning in optimising compilers.


Critical Analysis


This section contains a critical analysis of the work.


Generative Models for Source Code

Chapters  and  present generative models that enable the synthesis of more human-like programs than current state-of-the-art program generators, and without the expert guidance required by template based generators, but they have limitations. The technique of seeding the language models with the start of a function means that it cannot support user defined types, or calls to user-defined functions. In turn, this restricts the inputs that can be fed to generated programs. Currently only  scalars and arrays may be used as inputs, whereas 6 (2.3) of the OpenCL benchmark kernels identified in Table  use irregular data types as inputs. This may be addressed through recursive program synthesis, whereby a call to a user-defined function or unrecognised type will trigger candidate functions and type definitions to be synthesised.

This work evaluates the use of recurrent neural networks for generating programs in the OpenCL and Solidity programming languages. Although quite dissimilar (once extends the C programming language, the other is derived from JavaScript), it is unclear whether the generative modelling approach will prove effective for all possible language syntaxes. Unlike a random grammar enumerator, the ability to generate programs of arbitrary syntaxes cannot be guaranteed.

By learning from a corpus of programs assembled from GitHub, the model induces the biases of programs on GitHub. The contents of the GitHub corpus used in this work were only lightly vetted to ensure that it did not contain programs that would later be used to evaluate the model. This did not preclude the model training on program that may not be considered representative of true handwritten code. For example, inspecting the corpus revealed a small number of large, automatically generated programs. Additionally, test cases for an OpenCL static analysis tool were found that deliberately contain runtime defects. While the corpus was filtered to ensure that training programs were syntactically valid, no checks were made to ensure that programs had correct semantics.


Rejection Sampling for Program Generation

The techniques presented in this work sample recurrent neural networks on a per-token basis to generate programs. Once an entire sample has been generated, the sample can be checked to see if it is a valid program. If not, the entire sample is discarded. Although automatic, this rejection sampling approach is wasteful. Grammar-based sampling approaches have been proposed that could increase the likelihood of generating a valid program through masked sampling . Of course, this would make the generator more complicated. Ultimately there is a trade off between implementation complexity and sampling efficiency. This work emphasises simplicity.

Moreover, rejection sampling results in a bias towards shorter programs. This is because the probability that a sample is a valid program is inversely proportional to its length. This may skew the distribution of generated programs away from the training programs. This issue, arising from rejection sampling, can coincidentally be alleviated through further rejection sampling. To correct the bias towards shorter programs, an additional filter could be placed on the output of the generative model that discards samples with a random probability inversely proportional to their length. By removing more short samples than long, the bias in the distribution can be corrected for.


Characterisation of OpenCL Compiler Bugs

In Chapter , DeepSmith is proposed for generating compiler test cases, and compared against the state-of-the-art CLSmith. For each approach, the number of bug-exposing test cases are reported. However, it is not possible to determine which generator identified more unique bugs. To determine this, one would need to de-duplicate the counts by locating the exact bug-exposing property of each test case and correlating it with a compiler defect. There are two challenges preventing this: the first is the amount of compute required to perform automated test case reduction in many thousands of CLSmith programs; the second is the that in many cases it is not possible to identify the root cause of a compiler bug without access to its source code.

While it is not possible to compare the rate at which DeepSmith and CLSmith identify unique bugs, the properties of each approach can be used to partially characterise the bugs that can be found. DeepSmith is capable of exposing bugs that CLSmith cannot; for example, by generating plausible but malformed inputs to expose bugs in compiler error handling, or by generating programs with thread-dependent control-flow which CLSmith's static analyses prevent. However, CLSmith is also capable of exposing bugs that DeepSmith cannot. For example, CLSmith programs makes heavy use of structs, whereas DeepSmith does not support structs. As such I believe the approach presented in this work to complement the existing state-of-the-art; it is not intended to replace it.


Driving arbitrary OpenCL kernels

This thesis presents a technique for driving any OpenCL kernel that has scalar and array inputs. This host driver accepts as input an OpenCL kernel, which it then compiles, produces input data sets, and runs the compiled kernel using the data sets. The host driver generates data sets from uniform random distributions, as do many OpenCL benchmark suites. For cases where non-uniform inputs are required (e.g. profile-directed feedback), an alternate methodology for generating inputs must be adopted.









Sequential Models for Program Code

Chapter  feeds a sequence of program tokens into a recurrent neural network to predict an optimisation decision that should be made on it. By using the text serialised representation of a program (its source code), this makes the approach vulnerable to code order. The text inputs used to evaluate the approach are single kernels. It is not clear how the approach will respond to multi-procedure inputs, where the order that procedures are declared may have a large impact on the pattern of activations that it produces in the recurrent neural network.





The higher-dimensional representation of source code text over numerical features means that larger data sets are required to combat sparsity in training data. For example, the evaluation in Section  uses sequences padded to 1024 tokens, providing a 1024-dimension input representation. This provides a much more sparse space representation than the 4-dimension feature space used by the state-of-the-art approach.

A common criticism of machine learning systems is that they are black boxes. When the system fails to produce the desired result, there is no obvious method to correct the system so as to prevent similar errors. Still, in traditional machine learning it may be possible to correct problems by adjusting the features. Deep learning, with its typically low-level input representation, does not always have this option. For the approach presented in this thesis, there is no meaningful way to improve the model based on analysis of failure cases.


Future Work


This section briefly outlines some of the avenues for future research that this thesis creates.

Guided Program Synthesis to Minimise Benchmarking Costs

This thesis presents a technique for the unguided synthesis of compiler benchmarks. Using the technique may provide a fine grained exploration of the space of representative programs. For some use cases, a more efficient use of data can be achieved through directed program generation.

One approach could be to employ a rejection filter that tests for the presence of a property of interest. Another approach would be to train the generative model simultaneously for both the structure of programs (as is done in this work), along with a representation of the properties of interest (such as a feature vector). At sample time, the target feature vector could be used as input to steer the program generation. A third approach would use the learned language model not to generate programs, but to guide an existing program generator, such as CSmith.

If successful, such a technique enable the exploration of larger feature spaces than is currently possible by efficiently navigating the generation of benchmarks to map out the decision surface.


Neural Model Selection through Adversarial Games

Section  uses Turing tests to evaluate the quality of synthetic code. The task presented to human participants was to identify whether a series of code snippets were written by hand or machine. This was used to evaluate whether or not the model produced human-like output. In future work, this approach could be extended to aid in the challenging task of model selection by instead presenting the participants with pairs of samples side by side, and asking the participant to select the sample which is more human-like. If the two samples were both generated by different configurations of generative model, this would provide a means to compare generative models on the otherwise hard-to-assess quality of ``humanness''. The selection of the best model from a pool of candidates can thus be turned into a series of games. Each game pits a single sample from a pair of generative models head-to-head with a human selecting the winner, and an ELO rating can be used to assign scores and matches. The limitation of the work would likely be the availability of human participants.


Learning Representations for Dynamic Program Inputs

Chapter  presents an approach for learning optimisation heuristics from the raw representation of a program, but in the presence of dynamic properties, traditional feature extraction must be used. For example, feeding in the size of input data sets. In future work, this approach could be extended to also account for dynamic properties. Unlike with program source code, it is not clear what the raw representation of program inputs may be, perhaps the sequence of bytes that the program reads and writes, but the value of being able to model runtime properties at a low level would enable optimisations to be learned that cannot currentlyWTF do you mean here Chris.


Towards General-Purpose Program Comprehension

The techniques presented in this thesis apply recurrent neural networks to the task of modelling the syntax and semantics of programming languages. For each task, such as program generation or optimisation, an artificial neural networks is trained from scratch. Chapter  explores the use of transfer learning to seed an artificial neural network with information from another task. Future work could explore this idea further by iteratively training and retraining a single network across a wide range of tasks, with the goal of finding a common set of model parameters which work can be used as a base for each task.

An ambitious goal would be the development and distribution of a model architecture for general-purpose program comprehension. Such a system would enable, with little to no effort, the model to be re-purposed for a variety of compiler tasks. This is analogous to the widespread distribution of pre-trained state-of-the-art models in the field of image recognition. If developing an image classifier, a user can start by retraining an existing model such as ResNet , rather than constructing a model from scratch. This drastically simplifies the adoption of machine learning for image classification as the model architecture has been pre-selected and tuned, and reduces the amount of training data required.

A prerequisite for developing this system will be applying techniques such as those proposed in this thesis to a wide range of different compiler tasks. My hope in publicly releasing of all software developed in this work is to expedite discover in other domains.







  

tocchapterBibliography


