"""Library that handles custom stuff for working with this project."""
import datetime
import git
import typing
import pathlib
import shutil
import subprocess
import re
import os
import glob

from labm8 import app
from labm8 import bazelutil
from labm8 import fs
from labm8 import humanize
from tools.source_tree import source_tree

FLAGS = app.FLAGS

# A list of relative paths to include in every export of this project. Glob
# patterns are expanded.
_ALWAYS_EXPORTED_FILES = [
    '.bazelrc',  # Not strictly required, but provides consistency.
    'configure',  # Needed to generate config proto.
    'BUILD',  # Top-level BUILD file is always needed.
    'WORKSPACE',  # Implicit dependency of everything.
    'README.md',
    'tools/Brewfile.travis',  # Needed by Travis CI.
    'tools/bzl/*',  # Implicit dependency of WORKSPACE file.
    'tools/BUILD',  # Needed by //tools/bzl:maven_jar.bzl.
    'tools/download_file.py',  # Needed by //tools/bzl:maven_jar.bzl.
    'tools/util.py',  # Needed by //tools:download_file.py.
    'third_party/*.BUILD',  # Implicit dependencies of WORKSPACE file.
    'third_party/py/tensorflow/BUILD.in',  # Needed by ./configure
    'tools/workspace_status.sh',  # Needed by .bazelrc
    # tools/requirements.txt is always needed, but is handled separately.
]

# A list of relative paths to files which are excluded from export. Glob
# patterns are NOT supported.
_NEVER_EXPORTED_FILES = [
    'config.pbtxt',  # Generated by ./configure
    'third_party/py/tensorflow/BUILD',  # Generated by ./configure
]


class PhdWorkspace(bazelutil.Workspace):

  def __init__(self, *args, **kwargs):
    super(PhdWorkspace, self).__init__(*args, **kwargs)
    self._repo = git.Repo(self.workspace_root)
    if not (self.workspace_root / 'tools' / 'requirements.txt').is_file():
      raise OSError("Expected file toos/requirements.txt not found")

  @property
  def git_repo(self) -> git.Repo:
    return self._repo

  def GetAlwaysExportedFiles(self) -> typing.Iterable[str]:
    """Get hardcoded additional files to export."""
    relpaths = []
    for p in _ALWAYS_EXPORTED_FILES:
      abspaths = glob.glob(f'{self.workspace_root}/{p}')
      relpaths += [
          os.path.relpath(path, self.workspace_root) for path in abspaths
      ]
    return relpaths

  def GetAuxiliaryExportFiles(self, paths: typing.Set[str]) -> typing.List[str]:
    """Get a list of auxiliary files to export."""

    def GlobToPaths(glob_pattern: str) -> typing.List[str]:
      abspaths = glob.glob(glob_pattern)
      return [os.path.relpath(path, self.workspace_root) for path in abspaths]

    auxiliary_exports = []
    for path in paths:
      dirname = (self.workspace_root / path).parent
      auxiliary_exports += GlobToPaths(f'{dirname}/DEPS.txt')
      auxiliary_exports += GlobToPaths(f'{dirname}/README*')
      auxiliary_exports += GlobToPaths(f'{dirname}/LICENSE*')

    return auxiliary_exports

  def FilterExcludedPaths(
      self, paths: typing.List[pathlib.Path]) -> typing.Iterable[pathlib.Path]:
    return [path for path in paths if path not in _NEVER_EXPORTED_FILES]

  def GetAllSourceTreeFiles(
      self, targets: typing.List[str], excluded_targets: typing.Iterable[str],
      extra_files: typing.List[str],
      file_move_mapping: typing.Dict[str, str]) -> typing.List[pathlib.Path]:
    """Get the full list of source files to export for targets."""
    excluded_targets = set(excluded_targets)

    file_set = set(extra_files).union(set(file_move_mapping.values()))
    for target in targets:
      file_set = file_set.union(
          set(self.GetDependentFiles(target, excluded_targets)))
      file_set = file_set.union(set(self.GetBuildFiles(target)))
      file_set = file_set.union(
          set(self.GetDependentFiles(target, excluded_targets)))
      file_set = file_set.union(
          set(self.GetDependentFiles(target, excluded_targets)))

    file_set = file_set.union(set(self.GetAlwaysExportedFiles()))
    file_set = file_set.union(set(self.GetAuxiliaryExportFiles(file_set)))
    filtered_files = self.FilterExcludedPaths(file_set)

    return list(sorted(filtered_files))

  def GetPythonRequirementsForTarget(
      self, targets: typing.List[str]) -> typing.List[str]:
    """Get the subset of requirements.txt which is needed for a target."""
    # The set of bazel dependencies for all targets.
    dependencies = set()
    for target in targets:
      bazel = self.BazelQuery([f'deps({target})'], stdout=subprocess.PIPE)
      grep = subprocess.Popen(['grep', '^@pypi__'],
                              stdout=subprocess.PIPE,
                              stdin=bazel.stdout,
                              universal_newlines=True)
      stdout, _ = grep.communicate()
      if bazel.returncode:
        raise OSError("bazel query failed")
      output = stdout.rstrip()
      if output:
        dependencies = dependencies.union(set(output.split('\n')))

    with open(self.workspace_root / 'tools/requirements.txt') as f:
      all_requirements = set(f.readlines())

    needed = []
    all_dependencies = set()
    # This is a pretty hacky approach that tries to match the package component
    # of the generated @pypi__<package>_<vesion> package to the name as it
    # appears in tools/requirements.txt.
    for dependency in dependencies:
      if not dependency.startswith('@pypi__'):
        continue
      dependency = dependency[len('@pypi__'):].lower().split('//:')[0]
      all_dependencies.add(dependency)

    def _GetMatchingRequirements(dependency: str, all_requirements):
      requirements = []
      for requirement in all_requirements:
        requirement_to_match = requirement.replace('-', '_').lower().rstrip()
        requirement_to_match = requirement_to_match.split('==')[0]
        requirement_to_match = re.sub(r'#.*', '', requirement_to_match)
        if not requirement_to_match:
          continue
        if dependency.startswith(requirement_to_match):
          requirements.append(requirement.rstrip())
      return requirements

    for dependency in all_dependencies:
      requirements = _GetMatchingRequirements(dependency, all_requirements)
      if not requirements:
        continue
      needed += requirements
      # Total hack workaround for the fact that
      # //third_party/py/scipy:BUILD pulls in multiple packages.
      for requirement in requirements:
        if requirement.startswith('scipy'):
          needed += _GetMatchingRequirements('scikit', all_requirements)

    return list(sorted(set(needed)))

  def CreatePythonRequirementsFileForTargets(
      self, workspace: bazelutil.Workspace, targets: typing.List[str]) -> None:
    # Export the subset of python requirements that are needed.
    print('tools/requirements.txt')
    requirements = self.GetPythonRequirementsForTarget(targets)
    requirements_path = (workspace.workspace_root / 'tools' /
                         'requirements.txt')
    with open(requirements_path, 'w') as f:
      print('\n'.join(requirements), file=f)

  def CopyFilesToDestination(self, workspace: bazelutil.Workspace,
                             files: typing.List[str]) -> None:
    for relpath in files:
      print(relpath)

      src_path = self.workspace_root / relpath
      dst_path = workspace.workspace_root / relpath

      if not src_path.is_file():
        raise OSError(f"File `{relpath}` not found")

      dst_path.parent.mkdir(exist_ok=True, parents=True)
      shutil.copy(src_path, dst_path)

  def MoveFilesToDestination(self, workspace: bazelutil.Workspace,
                             file_move_mapping: typing.Dict[str, str]) -> None:
    with fs.chdir(workspace.workspace_root):
      for src_relpath, dst_relpath in file_move_mapping.items():
        print(dst_relpath)

        src_path = self.workspace_root / src_relpath
        dst_path = workspace.workspace_root / dst_relpath
        if not src_path.is_file():
          raise OSError(f"File `{src_relpath}` not found")

        dst_path.parent.mkdir(exist_ok=True, parents=True)
        # We can't simply `git mv` because in incremental exports, this move
        # may have already been applied. Instead, we manually copy the file
        # from the source workspace, and delete the corresponding file in the
        # destination workspace.
        shutil.copy(src_path, dst_path)
        dst_src_path = workspace.workspace_root / src_relpath
        if dst_src_path.is_file():
          subprocess.check_call(['git', 'rm', src_relpath])

  def ExportToRepo(self, repo: git.Repo, targets: typing.List[str],
                   src_files: typing.List[str], extra_files: typing.List[str],
                   file_move_mapping: typing.Dict[str, str]) -> None:
    """Export the requested targets to the destination directory."""
    # The timestamp for the export.
    timestamp = datetime.datetime.utcnow()

    # Export the git history.
    app.Log(1, 'Exporting git history for %s files',
            humanize.Commas(len(src_files)))
    for file in src_files:
      print(file)

    source_tree.ExportGitHistoryForFiles(self.git_repo, repo, src_files)

    # Make manual adjustments.
    exported_workspace = bazelutil.Workspace(pathlib.Path(
        repo.working_tree_dir))
    self.CreatePythonRequirementsFileForTargets(exported_workspace, targets)
    self.CopyFilesToDestination(exported_workspace, extra_files)
    self.MoveFilesToDestination(exported_workspace, file_move_mapping)

    if not repo.is_dirty(untracked_files=True):
      return

    app.Log(1, 'Creating automated subtree export commit')
    repo.git.add('.')
    author = git.Actor(name='[Git export bot]', email='/dev/null')
    repo.index.commit(f'Automated subtree export at {timestamp.isoformat()}',
                      author=author,
                      committer=author,
                      skip_hooks=True)
