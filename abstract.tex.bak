% 6 word hypothesis: Deep learning solves hard compiler problems.

Compilers are a fundamental technology, yet constructing them is difficult and time consuming. A modern optimizing compiler comprises thousands of components and millions of lines of code, with the demands of data intensive workloads requiring ever-more aggressive optimisation. In addition, the rapid transition to heterogeneous parallelism has created demand for entirely new compilers to support this diverse range of hardware, this has left compiler developers struggling to keep up. The cost of this is poor performance and bugs in software. What is needed are techniques that to radically reduce the cost of compiler construction.

This thesis presents deep learning methodologies to simplify compiler construction. First, a generative model for source code is described, capable of producing executable programs derived from language models trained on open source corpora. Secondly, this thesis explores the use of recurrent neural networks for code comprehension through learning optimisation heuristics directly on raw source code.

Unlike prior approaches, the generative model presented in this thesis is inferred entirely from example code. It requires no grammar or prior knowledge of the language being modelled, yet is capable of producing code of such quality that professional software developers cannot distinguish generated from handwritten.

The effectiveness of programs generated using this approach is investigated in two orthogonal domains. In the first, generated programs are used as benchmarks to supplement the training data of predictive models for compiler optimisations. The additional fine-grained exploration of the feature space that training on an additional 1000 generated programs provides yields a $1.27\times$ speedup of a state-of-the-art predictive model. In addition, the extra information automatically exposes weaknesses in the feature design which, when corrected, yields a further $4.30\times$ improvement in performance.

The second domain for which automatic program generation is applied is compiler validation. The generative model is extended and used to enable compiler fuzzing. Compared to a state-of-the-art fuzzer, the proposed approach presents an enormous reduction in developer effort, requiring $100\times$ fewer lines of code to implement, and is capable of generating an expressive range of tests that expose bugs that the state-of-the-art cannot. In a testing campaign of 10 OpenCL compilers, 67 new bugs are identified and reported, many of which are now fixed.

Finally, this thesis presents a new methodology for constructing compiler heuristics, which offers to radically reduce the cost of applying machine learning to compiler heuristics. Unlike state-of-the-art approaches in which program features have to be expertly engineered and selected, the proposed approach uses recurrent neural networks which learn directly over the textual representation of program code. Doing so yields $1.14\times$
and $1.12\times$ performance improvements in state-of-the-art predictive models. Additionally, by using the same neural network structure for different optimisation problems, this enables the novel transfer of information between optimisation problems.