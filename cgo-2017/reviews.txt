===========================================================================
                           CGO 2017 Review #38A
---------------------------------------------------------------------------
        Paper #38: Synthesizing Benchmarks for Predictive Modeling
---------------------------------------------------------------------------

                      Overall merit: 4. Accept
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

The paper notes that benchmarks do not do a good job of covering the
many-dimensional parameter space of compiler optimizations.  Basically there
are too few benchmarks to do an adequate job.  To overcome this limitation,
the paper proposes synthesizing an arbitrarily large number of benchmarks.
To do so, it proposes a deep learning technique that leverages (a) recent
advances in deep learning and (b) the large number of programs available in
open source repositories like Github.  Deep learning is then used to
understand the characteristics of the sample benchmark programs from Github
and then synthesize programs with much broader combinations of
characteristics.  Not only do the synthesized programs represent a broad
variety of characteristics, they are also indistinguishable from
human-generated code.  A panel of 15 volunteers could not distinguish actual
human code from synthesized code -- a marked difference with previous code
generators like CLSmith.  Armed with this large set of synthetic benchmarks,
the authors use them to train for a set of standard benchmarks and obtain
performance that is 50-60% better on average than the previous state of the
art -- where training on synthetic benchmarks was not available.

                      ===== Comments for author =====

The paper proposes a very interesting idea and obtains compelling and
surprising results -- I would not have expected that a synthetic code
generator would be able to create code that is essentially indistinguishable
from human code.

However, I do have a number of questions and concerns:

The way in which input data is generated was not completely clear.
In particular, the approach seems to assume that input data is essentially
arrays, potentially of multiple dimensions.  Can the approach handle more
irregular data inputs, e.g. linked lists, graphs, A-B trees, heaps, etc?

Similarly, the use of random values for payload input data suggests that none
of the optimizations included profile-directed feedback, since
profile-directed feedback generally looks at predominant branch directions,
and random data may yield differences in branch direction vs non-random data.
Whatever the case, it would be helpful for the paper to clarify this issue.

In the human judging, a natural giveaway about CLGen code is the naming of
identifers and functions:  a, b, c, aa, bb, cc, AA, BB, CC, etc.  Were
real benchmarks modified to use this same naming convention?  If so, the
text should make that clear.

The last paragraph of Section 4.3 about synthesizing OpenCL is not completely
clear.  It would help the exposition if a figure could be included showing
the seed text, some intermediate state, and the final generated kernel.

Similarly the discussion in Section 6.1 of humans inability to distinguish
CLGen code from human-generated code is slightly unclear.  It would help to
say that in 96% of cases, the judges correctly labeled CLSmith code as
machine generated, while CLgen code was correctly labeled machine generated
only 52% of the time, essentially indistinguishable from chance.  As written,
the precise meaning of 96% and 52% is unclear.  

Section 8.1 notes that the basic predictive model achieves a speedup of 1.57x
on AMD and 3.26x on NVIDIA.  Section 8.2 notes that the extended predictive
model almost reverses these improvements:  AMD has a 3.68x speedup and NVIDIA
a 1.64x speedup.  It would be helpful to comment about why AMD does so much
better with the extended model vs the base model and NVIDIA does so much
worse.

===========================================================================
                           CGO 2017 Review #38B
---------------------------------------------------------------------------
        Paper #38: Synthesizing Benchmarks for Predictive Modeling
---------------------------------------------------------------------------

                      Overall merit: 4. Accept
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

This paper proposes a technique to automatically generate synthetic benchmarks that "look like" have been written by a human.
The ultimate goal of this technique is to enhance the quality of predictive modeling, which today are built using only a few programs.
So this paper helps the accuracy of predictive modeling techniques.
Moreover, the proposed work can be used to better design (or test) features used for prediction purposes.

                      ===== Comments for author =====

I like this work.
It is a great example of what we can do today by exploiting github-ilke services and the newest improvements in machine learning.

Authors mentioned that the inputs of an OpenCL kernel are randomly chosen.
However, inputs of a real program tend to not follow a uniform distribution; actually, they tend to cluster in a few hot spots.
How do you envision to overcome this limitation in your work?

Authors claim that their work is able to cover the feature space at any desired granularity.
Hence, their work can potentially cover the whole space.
I didn't understand how this can be guaranteed. Please add a few more words about it.
Also, what's the distributions over this space of the synthetic benchmarks automatically generated to perform the evaluation described in this paper?

Section 2, Figure 3a. I couldn't properly interpreted the data shown in this plot until I read the whole paper. This reduces the readability of this paper.
I suggest to extend this section just enough to avoid this problem.

Section 8.2: typo: "This a result" -> "This is a result"

===========================================================================
                           CGO 2017 Review #38C
---------------------------------------------------------------------------
        Paper #38: Synthesizing Benchmarks for Predictive Modeling
---------------------------------------------------------------------------

                      Overall merit: 4. Accept
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

This paper targets the problem of creating predictive models that can recommend, from static source code, which kernels of an OpenCL program should run on the CPU and which on the GPU. To create a good model, the model is trained both on a set of benchmark programs and a much larger set of synthetic programs. The synthetic programs are created by CLgen, an OpenCL program generator that generates human-like OpenCL code. CLgen is itself a product of using deep learning over a GitHub corpus of OpenCL programs, and the generated programs are post-processed to remove compiler and runtime errors to produce a final set of synthetic programs. These synthetic programs represent runnable, human-like OpenCL code, and provide predictive models with lots of training data. A predictive model trained in this way provides significantly better results (the resulting CPU-GPU mapping is 25-150% faster) than the same model trained only on the limited set of available benchmark programs.

                      ===== Comments for author =====

This paper was an interesting read, and the writing is clear and did a good job of guiding the reader through the complex pipeline from generating synthetic programs to using them to train a predictive model. I wish there had been a single overview figure showing the entire system, but it is not so hard to align the relevant parts of Figure 4 with Figure 1 to see the big picture.

CLgen is a neat system. I was frankly surprised that it works as well as it does, given the strict requirements for code that parses, compiles and runs. The user study presented in Section 6.1 is a nice result, and is a convincing way to demonstrate that CLgen generates “human” code. It would be hard to convincingly demonstrate this otherwise, and I applaud the authors for going through with a user study.

CLgen provides a strong boost in the performance of predictive modeling. Taking these experiments one step further to refine the features used (Section 8.2) is a nice result as well, and further highlights the utility of the synthetic benchmarks. Even if previous work had identified these aliasing cases where the same features lead to very different behavior, it would be hard to do anything about it.

It seems clear that CLgen will be useful in other lines of research around heterogeneous systems. Other research limited by a paucity of benchmarks might be accelerated, too. I think making CLgen available, or making it into a web service, could be a valuable contribution to the community.

Overall, my enthusiasm for this work is tempered a bit by the narrow focus of the present paper, which focuses only on a particular kind of predictive modeling. Still, the work is nicely done and the results are compelling.

It would have been nice to see quantitative results on the rejection filter, to see, e.g., how many generated programs fail to compile, fail the nondeterminism check, the timeout check, etc.

Figure 9 appears to have very small error bars on each data point (the horizontal black lines in the plot). What do these error bars represent?

===========================================================================
                           CGO 2017 Review #38D
---------------------------------------------------------------------------
        Paper #38: Synthesizing Benchmarks for Predictive Modeling
---------------------------------------------------------------------------

                      Overall merit: 4. Accept
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

Summary:

This paper presents a deep learning approach to program synthesis,
applied to the generation of OpenCL kernels. The system learned a
model of OpenCL programs from examples/codes mined on Github, and is
used to generate 1000 new OpenCL programs. For evaluation, these 1000
programs are used during training of a machine learning model to
predict whether to use CPU or GPU as the executing platform, in
addition to the original set of benchmarks used in a reference
paper. Results clearly show improved quality of results when using the
larger training database, most likely due to the existence of training
points very close (in terms of features and expected mapping decision)
to the test benchmarks.

The paper looks excellent, and is as far as I know the first
application of deep learning for program synthesis. But my concern
relates to reproducibility of the work, as too little information is
given in the paper to be able to duplicate the results. Additional
information needs to be provided, or alternatively the model (or the
system to train it) should be made publicly available.


Strengths:

+ First real, compelling use of deep learning applied to program
synthesis. If the system delivers as suggested in the paper and there
is no hidden flaw, this is a breakthrough result.

+ Clear gains expected by using the synthesized benchmarks.

+ The paper is well written and easy to follow.


Weaknesses:

- Missing information about the model's behavior (e.g., % of programs
  generated by the model which are rejected), how it was implemented,
  what seeds were used, etc.

- It appears to be able to generate programs which are very close in
  terms of features to the unseen benchmarks, ie, the resulting ML
  heuristic for CPU/GPU mapping likely performs better because there
  are now points in the training set very similar to the unseen
  benchmarks. This is not necessarily a problem, by far, but it makes
  it difficult to distinguish between over-specialization vs. better
  generality in the final model.

                      ===== Comments for author =====

Questions for the rebuttal:

1) Are any of the benchmarks in the 7 test suites in the github
repositories you mine? That is, does the DNN training ever involves
the benchmarks you later evaluate on?

2) What is the proportion of (a) valid (compilation test) and (b)
useful programs you obtain from the DNN vs. the total numbers of
programs generated by the DNN?

3) Describe how the "seed text" is built.

4) Do you plan a public release of the system? (Note: your answer to
this question will NOT affect my grade).

5) Which system (e.g., Torch) did you use to build the model(s)?


Detailed comments:

I really loved this paper. My biggest concern is essentially that the
system looks "too good to be true" to some extent, and that I would
prefer (a) more information is given about the DNN system; and (b)
ideally a path to reproducibility (via software release, model
release, or simply more details about the implementation).

I have various questions/comments, in addition to the 3 questions you
shall answer in the rebuttal.

- Intro (or Sec. 2): some discussion on the impact of having an
  extensive feature coverage in a training set, i.e., on
  specialization vs. generality of a model is desired. In particular,
  having a large-enough training set where nearly all cases of feature
  combinations may lead to a model with poor generality (but great
  efficiency on the tested benchmarks). Please discuss the pros/cons,
  do not limit to raising the advantages of a large & diverse training
  set.

- Typo, Fig. 2: polybench and "ploybench" are probably the same test
  suite, up to a typo.

- You write "we are able to find multiple programs with nearly
  identical features but different best heuristic values". I think it
  is a huge strength of your work. Please elaborate with numbers
  (e.g., we found xxx cases like that, a total of yyy% of the
  generated cases), and possibly also discuss why in the results you
  "succeed" with SHOC.gemm and "fail" with polybench.gemm: do they
  have the same features? If so, discuss these cases in more details.

- Sec. 4.2 and 4.3 should be significantly expanded. For example:
   - can you give a brief walk-through example of how a program is
     generated? Show the seed text, and how letters are generated
     one-by-one. Termination criteria are important to illustrate.

   - illustrate also the 'seed text' that is used, and describe how
     those seeds have been generated / selected.

   - Please provide statistics about using your model:
      - what is the % of programs generated that (a) fail the
        compilation test? (b) fail the usefulness test?
      - what is the number of different seeds you have used/needed to
        generate 1000 benchs?

   - some statistics on the training set generated (1000 kernels)
     would be interesting, like avg/stddev of number of lines, of
     number of loops, of number of flops, etc.

   - it is not clear how you can generate kernels which does not
     "segfault", in particular regarding array accesses (e.g., out of
     bound). Please develop on this.

   - be specific about whether the 7 benchmark suites are part of the
     github repositories which are mined or not.

- In Sec. 7.1 you mention 142 kernels used, but table 3 sums at
  256. Typo?

- You must discuss the seeds used in your approach in great details
  (see above, but it is also critical in Sec. 7)

- My reading of the paper is that you end up having programs in the
  training set that are now very similar (in terms of features and
  mapping decision) to the unseen benchmarks. It poses a real problem
  then to use simply leave-one-out cross-validation. Although you must
  use this for "apple-to-apple" comparison with [9], you should
  complement this with a k-fold cross-validation, to observe how both
  [9] and [9]+CLGen QoR evolves with smaller values of k. In fact, I
  want to see [9]+CLGen being trained on only the synthesized
  benchmarks, and the quality of this model compared to [9].

- You compare the perf. improvement of [9]+CLGen over [9], it would be
  good to report what is the maximal speedup (ie, oracle) achievable,
  to see how far from "optimal" is [9]+CLGen.

- Sec. 8.2: Define F3.

- Typo: gemmv -> gemm.

- Please discuss similarities/differences between polybench.gemm and
  SHOC.gemm, and why you "fail" for only one of the two.

- There are quite a few cases of slowdown yours vs. [9] for the GTX,
  small slowdows but still. This is not well explained in your paper
  (possible causes? solutions?)

