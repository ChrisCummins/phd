We thank the reviewers for their time in writing their insightful reviews and comments.

# Review #38A

A.1. We only handle scalars or arrays. We are investigating how to extend that. Only 6 (2.3%) of the benchmark kernels we evaluate on use irregular data types. We will make this more clear.

A.2. We do not use profile-directed feedback because the work we are comparing against did not. We will clarify. Possibly, if targeting different optimisations in the future, it could be added by using multiple profiling runs with random data to get average branch frequencies.

A.3. All programs used in the human judging test had identifier renaming applied. We will clarify this in the paper, and are releasing the test online.

A.4. We will add a figure in Section 4.3 to visualise the kernel synthesis process.

A.5. We agree that the scoring of human judges is unclear. We will clarify this.

A.6. Speedups between 8.1 and 8.2 are not comparable: Section 8.1 speedups are relative to a static mapping, Section 8.2 is relative to the predictive model in [9]. In both cases we outperform both the static mapping and other predictive model. We will make this clear.

# Review #38B

B.1. We only consider random input values. You raise a great point regarding non-uniform distribution, currently we do nothing with it. We do have some ideas about how to tackle this problem. Conceivably we could apply a similar approach of sequence modelling that we use for generating source code to generate datasets.

B.2. You are right that we cannot guarantee to cover the entire feature space. We expect an unbounded number of unique programs, but that is not the same guarantee. We will amend the text to make this clear.

B.3. The distribution of programs is a great question. We currently only consider coverage of the feature space, not distribution within that coverage. We will look into this.

B.4. We will develop our explanation of Figure 3a, thanks for pointing this out.

# Review #38C

C.1. We will look into combining Figures 1 and 4, thanks for the comment.

C.2. Our software stack is ready to be open sourced and to submitted to the AEC, which we hope will expedite discovery in other domains.

C.3. We envision our paper as a proof of concept for an exciting new avenue of program generation. We will be exploring other uses for CLgen in the future.

C.4. We will add a table with numbers of kernels discarded at each stage of the rejection pipeline in the paper.

C.5. The error bars in Figure 9 show standard deviation over 10 random samplings. This will be clarified.

# Review #38D

D.1. None of the benchmark kernels used in the evaluation are present in the training corpus, and none of the generated programs match those in the corpus. We will make this clear in the paper.

D.2. Unfortunately, we did not record those data. We will rerun for the final version and include a table.

D.3. The seed text is built by serialising a list of argument types into a kernel prototype. We will clarify.

D.4. See C.2.

D.5. The DNN is implemented in Torch. We will add to paper.

D.6. To some extent we leave the issue of how to generalize over the training data to the 'backend' ML tool (which we don't alter). Many tools attempt avoid overfitting even with extensive training examples, those which do not might be in trouble. We will add a discussion.

D.7. We will quantify the identical feature values discovered in the paper. For SHOC and polybench see D.12.

D.8. We will improve Sections 4.2. and 4.3 per your suggestions.

D.9. We will discuss seed generation in detail.

D.10. Embarrassed not to have thought to compare against synthetic-only, sorry. Will add to paper. Will look at k-fold also.

D.11. We will compare to the Oracle performance in the final paper.

D.12. Regarding SHOC.gemm and polybench.gemm. The implementations are different and the features are different. We will discuss.

D.13. We do not yet know of the cause for occasional slowdowns on GTX. Possibly this is a remaining limitations of the predictive model tool or remaining weakness in the features. We will look into these possibilities.
