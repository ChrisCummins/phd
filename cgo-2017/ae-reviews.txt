----------------------- REVIEW 1 ---------------------
PAPER: 13
TITLE: Synthesizing Benchmarks for Predictive Modeling
AUTHORS: Chris Cummins, Pavlos Petoumenos, Zheng Wang and Hugh Leather

OVERALL EVALUATION: 4
REVIEWER'S CONFIDENCE: 4
Documentation: 4
Packaging: 4
Installation: 4
Use case: 4
Expected behavior: 4
Relevance to paper: 4
Customization and reusability: 0

----------- OVERALL EVALUATION -----------
The document provided two ways to check the artifact: 1) A pre-configured live server interacted with a web browser and 2) a virtual machine image. I verified the artifact through the first way, which is very convenient to use. All the environment is just a browser. The whole checking process is guided by some pre-written code blocks, which drive to generate the benchmarks, train the model and performance comparison. The output is clear and consistent with the results presented in the paper.

----------- Documentation -----------
The author provided an online step-by-step tutorial to reproduce the experimental results. It is very clear and easy to read.

----------- Packaging -----------
Nothing missed.

----------- Installation -----------
It has been configured in advance.

----------- Use case -----------
Yes, it is easy to use via the given APIs.

----------- Expected behavior -----------
It works well.

----------- Relevance to paper -----------
The results are consistent with that presented in the paper.

----------------------- REVIEW 2 ---------------------
PAPER: 13
TITLE: Synthesizing Benchmarks for Predictive Modeling
AUTHORS: Chris Cummins, Pavlos Petoumenos, Zheng Wang and Hugh Leather

OVERALL EVALUATION: 4
REVIEWER'S CONFIDENCE: 3
Documentation: 4
Packaging: 3
Installation: 4
Use case: 3
Expected behavior: 4
Relevance to paper: 3
Customization and reusability: 0

----------- OVERALL EVALUATION -----------
The artifact is very easy to use with a demonstration hosted on the website and its code is publicly available and hosted on Github. I think this is a high quality artifact for people who want to reproduce their results and continue to do research on top of their tool.

----------- Documentation -----------
Yes, well documented.

----------- Packaging -----------
The authors provide user-friendly interface on their webpage to reproduce the results in the paper.

----------- Installation -----------
The artifact can be fully evaluated via web browser, so there is no need to install it. Thanks for the authors' efforts.

----------- Use case -----------
It seems to me that most of data in the paper can be reproduced. Very nice and clean figures and charts! However, I didn't find the results from your website for Figure 9 in the paper.

----------- Expected behavior -----------
The artifact works well for me. There is no crash or unexpected output.

----------- Relevance to paper -----------
The artifact is very impressive. It seems to me that the results are consistent with paper.

----------------------- REVIEW 3 ---------------------
PAPER: 13
TITLE: Synthesizing Benchmarks for Predictive Modeling
AUTHORS: Chris Cummins, Pavlos Petoumenos, Zheng Wang and Hugh Leather

OVERALL EVALUATION: 3
REVIEWER'S CONFIDENCE: 4
Documentation: 4
Packaging: 4
Installation: 4
Use case: 3
Expected behavior: 3
Relevance to paper: 4
Customization and reusability: 4

----------- OVERALL EVALUATION -----------
I think overall the artifact is well documented and well scripted but it can be parameterized in a better way to generate more experimental data for a closer comparison to the paper.

----------- Documentation -----------
The documentation was clear and several ways to evaluate the artifact are clearly enumerated.

----------- Packaging -----------
The materials are well packaged and the artifacts provide 3 modes of evaluation. The scripts and results are packaged in multiple forms including python based Jupyter notebook which is very convenient for partially running experiments for review.

----------- Installation -----------
The evaluation requires specific hardware but the remote access provided is sufficient to evaluate the artifacts. The pre-installed system is useful to evaluate the artifact.

----------- Use case -----------
In the 'AE notebook' mode, the figure generated corresponding to the Figure 7 does not evaluate all the benchmarks in the paper. The subset of benchmarks shows a positive speedup on average with CLgen's synthetic benchmarks. This result is a very small subset of that of the paper. The subset size should have been more even though it's a snapshot of the entire evaluation in the paper. On the AMD hardware, in the 'AE notebook' mode, the plots corresponding to Figure 8, for the extended model, do not show similar speedups: example benchmarks like AMD.PrefixSum, AMD.ScanLargeArrays. This could be an effect of small number of synthetic benchmarks that refine the model in the 'AE notebook' mode. Even in the 'Paper' mode, it was not clear how to repeat the experiments on a larger set of benchmarks and more synthetic benchmarks.

----------- Expected behavior -----------
The scripts are thorough and well documented. However, it is difficult even in 'Paper' mode, to find out how to generate kernels in hundreds and evaluate benchmarks in hundreds like in the original paper. The small subset of experimental data presented for evaluation (example Figure 8), in 'Paper' mode, can be made larger. Could the authors provide a larger subset of experimental data?

----------- Relevance to paper -----------
On the small subset of experimental data, the artifact covers all the experiments in the paper and also presents additional experiments in graphical form: For example, synthesized programs with identical features as benchmarks but with different behavior.

----------- Customization and reusability -----------
The artifact is reusable: scripts and documentation have good coverage. There is more documentation required as to how the the experiments can be evaluated on a larger set of benchmarks and with more generated synthetic benchmarks.
