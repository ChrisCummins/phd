Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Mathis2019,
author = {Mathis, B. and Kampmann, A. and Gopinath, R. and H{\"{o}}schele, M. and Mera, M. and Zeller, A.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2019 - Mathis et al. - Parser-Directed Fuzzing.pdf:pdf},
isbn = {9781450367127},
keywords = {2019,acm reference format,alexander kamp-,and andreas zeller,bj{\"{o}}rn mathis,fuzzing,mann,matthias h{\"{o}}schele,micha{\"{e}}l mera,parser-directed,parsers,rahul gopinath,test generation},
title = {{Parser-Directed Fuzzing}},
year = {2019}
}
@inproceedings{Leyton2010a,
abstract = {Algorithmic Skeletons offer high-level abstractions for paral- lel programming based on recurrent parallelism patterns. Patterns can be combined and nested into more complex parallelism behaviors. Program- mers fill the skeleton patterns with the functional (business) code, which transforms the generic skeleton into a specific application. However, when the functional code generate exceptions, this exposes the programmer to details of the skeleton library, breaking the high-level abstraction prin- ciple. Furthermore, related parallel activities must be stopped as the exception is raised. This paper describes how to handle exceptions in Al- gorithmic Skeletons without breaking the high-level abstractions of the programming model.We describe both the behavior of the framework in a formal way, and its implementation in Java: the Skandium Library.},
annote = {NULL},
author = {Leyton, Mario and Henrio, Ludovic},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Leyton, Henrio - Exceptions for Algorithmic Skeletons.pdf:pdf},
keywords = {algorithmic skeletons,exceptions,semantics},
title = {{Exceptions for Algorithmic Skeletons}},
year = {2010}
}
@article{ISO2005,
author = {ISO},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - ISO - ISO C 99 Working Std.pdf:pdf},
pages = {550},
title = {{ISO C 99 Working Std.}},
year = {2005}
}
@article{Rossum2010,
abstract = {Python is an extensible, interpreted, object-oriented programming language. It supports a wide range of applications, from simple text processing scripts to interactive Web browsers. While the Python Reference Manual describes the exact syntax and semantics of the language, it does not describe the standard library that is distributed with the language, and which greatly enhances its immediate usability. This library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs. This library reference manual documents Python's standard library, as well as many optional library modules (which may or may not be available, depending on whether the underlying platform supports them and on the configuration choices made at compile time). It also documents the standard types of the language and its built-in functions and exceptions, many of which are not or incompletely documented in the Reference Manual. This manual assumes basic knowledge about the Python language. For an informal introduction to Python, see the Python Tutorial; the Python Reference Manual remains the highest authority on syntactic and semantic questions. Finally, the manual entitled Extending and Embedding the Python Interpreter describes how to add new extensions to Python and how to embed it in other applications.},
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - The Python Library Reference.pdf:pdf},
issn = {0169-118X},
title = {{The Python Library Reference}},
url = {http://scholar.google.com/scholar?q=intitle:Python+Library+Reference{\#}0},
year = {2016}
}
@article{KhronosOpenCLGroupInc2015,
annote = {NULL},
author = {{Khronos OpenCL Group Inc}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Khronos OpenCL Group Inc - The OpenCL Specification.pdf:pdf},
title = {{The OpenCL Specification}},
year = {2015}
}
@inproceedings{Fursin2010,
abstract = {Iterative compilation is an efficient approach to optimize programs on rapidly evolving hardware, but it is still only scarcely used in practice due to a necessity to gather a large number of runs often with the same data set and on the same environment in order to test many different optimizations and to select the most appropriate ones. Natu- rally, in many cases, users cannot afford a training phase, will run each data set once, develop new programs which are not yet known, and may regularly change the environment the programs are run on. In this article, we propose to overcome that practical obstacle using Col- lective Optimization, where the task of optimizing a program leverages the experience of many other users, rather than being performed in iso- lation, and often redundantly, by each user. Collective optimization is an unobtrusive approach, where performance information obtained af- ter each run is sent back to a central database, which is then queried for optimizations suggestions, and the program is then recompiled ac- cordingly. We show that it is possible to learn across data sets, pro- grams and architectures in non-dynamic environments using static func- tion cloning and run-time adaptation without even a reference run to compute speedups over the baseline optimization. We also show that it is possible to simultaneously learn and improve performance, since there are no longer two separate training and test phases, as in most studies. We demonstrate that extensively relying on competition among pairs of optimizations (program reaction to optimizations) provides a robust and efficient method for capturing the impact of optimizations, and for reusing this knowledge across data sets, programs and environments.We implemented our approach in GCC and will publicly disseminate it in the near future.},
annote = {This paper describes distributing the task of autotuning information gathering by having a central database which autotuners communicate with. It uses the md5sum of a program's sources to identify programs.},
author = {Fursin, G. and Temam, O.},
booktitle = {HiPEAC},
doi = {10.1007/978-3-540-92990-1_5},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Fursin, Temam - Collective Optimization.pdf:pdf},
issn = {15443566},
month = {dec},
publisher = {Springer},
title = {{Collective Optimization}},
url = {http://dx.doi.org/10.1007/978-3-540-92990-1{\_}5},
volume = {5409},
year = {2009}
}
@article{Ansel2010,
abstract = {Building adaptable and more efficient programs for the multi-core era is now within reach.},
annote = {This paper describes PetaBricks, an implicitly parallel programming langague which allows programmers to specify algorithmic choice at the language level.
















This paper presents no original research or novel ideas. Rather, it serves as a supplement to their previous PetaBricks paper [1]. Their kmeans example program is unclear and poorly explained.
















[1] Ansel, Jason, et al. PetaBricks: a language and compiler for algorithmic choice. Vol. 44. No. 6. ACM, 2009.},
author = {Ansel, J. and Chan, C.},
doi = {10.1145/1836543.1836554},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Ansel, Chan - PetaBricks.pdf:pdf},
issn = {15284972},
journal = {XRDS: Crossroads, The ACM Magazine for Students},
month = {sep},
number = {1},
title = {{PetaBricks}},
url = {http://dl.acm.org/citation.cfm?doid=1836543.1836554},
volume = {17},
year = {2010}
}
@article{Fleming1986,
abstract = {Using the arithmetic mean to summarize normalized benchmark results leads to mistaken conclusions that can be avoided by using the preferred method: the geometric mean.},
annote = {Report geometric mean when comparing normalised benchmark results (i.e. speedups). Cited by 201.},
author = {Fleming, Philip J. and Wallace, John J.},
doi = {10.1145/5666.5673},
file = {:Users/cec/Google Drive/Mendeley Library/1986 - Fleming, Wallace - How not to lie with statistics the correct way to summarize benchmark results.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
title = {{How not to lie with statistics: the correct way to summarize benchmark results}},
volume = {29},
year = {1986}
}
@inproceedings{He2014,
abstract = {Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central tomost on- line advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3{\%}, an improvement with sig- nificant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surpris- ingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic re- gression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with. 1.},
annote = {NULL},
author = {He, X. and Bowers, S. and Candela, J. Q. and Pan, J. and Jin, O. and Xu, T. and Liu, B. and Xu, T. and Shi, Y. and Atallah, A. and Herbrich, R.},
booktitle = {SigKDD},
doi = {10.1145/2648584.2648589},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - He et al. - Practical Lessons from Predicting Clicks on Ads at Facebook.pdf:pdf},
isbn = {9781450329996},
title = {{Practical Lessons from Predicting Clicks on Ads at Facebook}},
url = {http://dl.acm.org/citation.cfm?doid=2648584.2648589},
year = {2014}
}
@inproceedings{Truong2016,
annote = {NULL},
author = {Truong, L. and Barik, R. and Totoni, E. and Liu, H. and Markley, C. and Fox, A. and Shpeisman, T.},
booktitle = {PLDI},
doi = {10.1145/2908080.2908105},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Truong et al. - Latte a language, compiler, and runtime for elegant and efficient deep neural networks.pdf:pdf},
isbn = {9781450342612},
keywords = {Compiler,Deep Learning,Domain Specific Language,Neural Networks,Optimization},
title = {{Latte: a language, compiler, and runtime for elegant and efficient deep neural networks}},
year = {2016}
}
@inproceedings{Zaharia2008,
abstract = {MapReduce is emerging as an important programming model for large-scale data-parallel applications such as web indexing, data mining, and scientific simulation. Hadoop is an open-source implementation of MapRe- duce enjoying wide adoption and is often used for short jobs where low response time is critical. Hadoop's per- formance is closely tied to its task scheduler, which im- plicitly assumes that cluster nodes are homogeneous and tasks make progress linearly, and uses these assumptions to decide when to speculatively re-execute tasks that ap- pear to be stragglers. In practice, the homogeneity as- sumptions do not always hold. An especially compelling setting where this occurs is a virtualized data center, such as Amazon's Elastic Compute Cloud (EC2). We show that Hadoop's scheduler can cause severe performance degradation in heterogeneous environments. We design a new scheduling algorithm, Longest Approximate Time to End (LATE), that is highly robust to heterogeneity. LATE can improve Hadoop response times by a factor of 2 in clusters of 200 virtual machines on EC2.},
annote = {NULL},
author = {Zaharia, Matei and Konwinski, Andy and Joseph, Ad and Katz, Rh and Stoica, Ion},
booktitle = {OSDI},
doi = {10.1109/IPDPSW.2010.5470880},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Zaharia et al. - Improving MapReduce Performance in Heterogeneous Environments.pdf:pdf},
isbn = {9781424465330},
issn = {00267902},
title = {{Improving MapReduce Performance in Heterogeneous Environments.}},
url = {http://www.usenix.org/event/osdi08/tech/full{\_}papers/zaharia/zaharia{\_}html/},
year = {2008}
}
@phdthesis{Potter,
author = {Potter, R.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Potter - Thesis Background Section.pdf:pdf},
title = {{Thesis Background Section}}
}
@inproceedings{Moss1998,
abstract = {Program execution speed on moden computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, obtaining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empiricial results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervided learning methods perform nearly optimally with respect to the features used.},
annote = {NULL},
author = {Moss, Eliot and Utgoff, Paul and Cavazos, John and Precup, Doina and Stefanovic, D and Brodley, Carla and Scheeff, David},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Moss et al. - Learning to schedule straight-line code.pdf:pdf},
isbn = {0262100762},
issn = {10495258},
title = {{Learning to schedule straight-line code}},
url = {http://books.nips.cc/papers/files/nips10/0929.pdf},
volume = {10},
year = {1998}
}
@inproceedings{Luk2005,
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evalu- ation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to pro- vide easy-to-use, portable, transparent, and efficient instrumenta- tion. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level with- out the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different archi- tectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, unin- strumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses sev- eral techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instru- mentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly avail- able for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium R ?, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 down- loads from its website.},
annote = {NULL},
author = {Luk, C. and Cohn, R. and Muth, R. and Patil, H. and Klauser, A. and Wallace, S. and Janapa, V. and Lowney, G.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Luk et al. - Pin Building Customized Program Analysis Tools with Dynamic Instrumentation.pdf:pdf},
isbn = {1595930566},
keywords = {Instrumentation,dynamic compilation,program analysis tools},
number = {6},
title = {{Pin: Building Customized Program Analysis Tools with Dynamic Instrumentation}},
volume = {40},
year = {2005}
}
@article{Arjovsky2017a,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
annote = {Reathrough notes here: http://www.alexirpan.com/2017/02/22/wasserstein-gan.html},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, M. and Chintala, S. and Bottou, L.},
eprint = {1701.07875},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Arjovsky, Chintala, Bottou - Wasserstein GAN.pdf:pdf},
journal = {arXiv:1701.07875},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@inproceedings{Yu2015,
annote = {NULL},
author = {Yu, Leiming and Zhang, Yan and Gong, Xiang and Roy, Nilay and Makowski, Lee and Kaeli, David},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Yu et al. - High Performance Computing of Fiber Scattering Simulation Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334075},
keywords = {Cluster,Fiber Scattering Simulation,GPU,Permission},
title = {{High Performance Computing of Fiber Scattering Simulation Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Irsoy2014,
abstract = {Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture - a deep recursive neural network (deep RNN) - constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.},
annote = {NULL},
author = {Irsoy, O. and Cardie, C.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Irsoy, Cardie - Deep recursive neural networks for compositionality in language.pdf:pdf},
issn = {10495258},
number = {January},
title = {{Deep recursive neural networks for compositionality in language}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84937828128{\&}partnerID=tZOtx3y1},
volume = {3},
year = {2014}
}
@article{Yu2016,
annote = {NULL},
author = {Yu, Jintao and Nane, Razvan and Haron, Adib and Hamdioui, Said and Corporaal, Henk and Bertels, Koen},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Yu et al. - Skeleton-Based Design and Simulation Flow for Computation-In-Memory Architectures.pdf:pdf},
isbn = {9781450343305},
title = {{Skeleton-Based Design and Simulation Flow for Computation-In-Memory Architectures}},
year = {2016}
}
@article{Veldhuizen2003,
abstract = {We sketch a proof of a well-known folk theorem that C++ templates are Turing complete. The absence of a formal semantics for C++ template instantiation makes a rigorous proof unlikely.},
annote = {NULL},
author = {Veldhuizen, Todd L},
doi = {10.1.1.14.3670},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Veldhuizen - C Templates are Turing Complete.pdf:pdf},
title = {{C++ Templates are Turing Complete}},
year = {2003}
}
@inproceedings{Krishnamurthy1995,
annote = {NULL},
author = {Krishnamurthy, Arvind and Yelick, Katherine A},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Krishnamurthy, Yelick - Optimizing Parallel Programs with Explicit Synchronization.pdf:pdf},
title = {{Optimizing Parallel Programs with Explicit Synchronization}},
year = {1995}
}
@inproceedings{Gupta2003,
annote = {NULL},
author = {Gupta, S and Dutt, N},
booktitle = {VLSI},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Gupta, Dutt - SPARK A high-level synthesis framework for applying parallelizing compiler transformations.pdf:pdf},
publisher = {IEEE},
title = {{SPARK: A high-level synthesis framework for applying parallelizing compiler transformations}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1183177},
year = {2003}
}
@article{Mou2015,
annote = {NULL},
author = {Mou, L. and Men, R. and Li, G. and Zhang, L. and Jin, Z.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Mou et al. - On End-to-End Program Generation from User Intention by Deep Neural Networks.pdf:pdf},
journal = {arXiv:1510.07211},
keywords = {deep learning,program generation,recurrent network},
title = {{On End-to-End Program Generation from User Intention by Deep Neural Networks}},
year = {2015}
}
@inproceedings{Margiolas2016,
abstract = {Accelerators, such as Graphic Processing Units (GPUs), are popular components of modern parallel systems. Their energy-efficient performance make them attractive components for modern data center nodes. However, they lack control for fair resource sharing amongst multiple users. This paper presents a runtime and Just In Time compiler that enable resource sharing control and software managed scheduling on accelerators. It is portable and transparent, requiring no modification or recompilation of existing systems or user applications. We provide an extensive evaluation of our scheme with over 40,000 different workloads on 2 platforms and we deliver fairness improvements ranging from 6.8x to 13.66x. In addition, we also deliver system throughput speedups ranging from 1.13x to 1.31x.},
annote = {NULL},
author = {Margiolas, C. and O'Boyle, M.},
booktitle = {CGO},
doi = {10.1145/2854038.2854040},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Margiolas, O'Boyle - Portable and Transparent Software Managed Scheduling on Accelerators for Fair Resource Sharing.pdf:pdf},
isbn = {9781450337786},
keywords = {GPUs,OpenCL,accelerator sharing,accelerators,fair resource sharing,heterogeneous computing,multi-tasking,resource management},
publisher = {IEEE},
title = {{Portable and Transparent Software Managed Scheduling on Accelerators for Fair Resource Sharing}},
url = {http://dl.acm.org/citation.cfm?id=2854038.2854040},
year = {2016}
}
@article{Hwu1993,
annote = {NULL},
author = {Hwu, W. W. and Mahlke, S. A. and Chen, W. Y. and Chang, P. P. and Warter, N. J. and Bringmann, R. A. and Ouellette, R. G. and Hank, R. E. and Kiyohara, T. and Haab, G. E. and Holm, J. G. and Lavery, D. M.},
doi = {10.1007/BF01205185},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Hwu et al. - The superblock An effective technique for VLIW and superscalar compilation.pdf:pdf},
issn = {0920-8542},
journal = {The Journal of Supercomputing},
month = {may},
number = {1-2},
title = {{The superblock: An effective technique for VLIW and superscalar compilation}},
url = {http://link.springer.com/10.1007/BF01205185},
volume = {7},
year = {1993}
}
@article{Lee2009,
abstract = {GPGPUs have recently emerged as powerful vehicles for general- purpose high-performance computing. Although a new Compute Unified Device Architecture (CUDA) programming model from NVIDIA offers improved programmability for general computing, programming GPGPUs is still complex and error-prone. This pa- per presents a compiler framework for automatic source-to-source translation of standard OpenMP applications into CUDA-based GPGPU applications. The goal of this translation is to further im- prove programmability and make existing OpenMP applications amenable to execution on GPGPUs. In this paper, we have iden- tified several key transformation techniques, which enable efficient GPU global memory access, to achieve high performance. Experi- mental results from two important kernels (JACOBI and SPMUL) and two NAS OpenMP Parallel Benchmarks (EP and CG) show that the described translator and compile-time optimizations work well on both regular and irregular applications, leading to perfor- mance improvements of up to 50Xover the unoptimized translation (up to 328X over serial on a CPU). Categories},
annote = {From Duplicate 2 (OpenMP to GPGPU: A Compiler Framework for Automatic Translation and Optimization - Lee, Seyong; Min, S J; Eigenmann, Rudolf)

They've developed a source-to-source translator from OpenCL to CUDA. Cited by 384.},
author = {Lee, Seyong and Min, S J and Eigenmann, Rudolf},
doi = {10.1145/1504176.1504194},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Lee, Min, Eigenmann - OpenMP to GPGPU A Compiler Framework for Automatic Translation and Optimization.pdf:pdf},
isbn = {9781605583976},
issn = {03621340},
journal = {PPoPP},
keywords = {P2P,automatic translation,com-,compiler,cuda,gpu,openmp,optimization,translation},
title = {{OpenMP to GPGPU: A Compiler Framework for Automatic Translation and Optimization}},
url = {http://dl.acm.org/citation.cfm?id=1504194},
year = {2009}
}
@article{Liao2018a,
abstract = {We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.},
archivePrefix = {arXiv},
arxivId = {1803.06272},
author = {Liao, Renjie and Brockschmidt, Marc and Tarlow, Daniel and Gaunt, Alexander L. and Urtasun, Raquel and Zemel, Richard},
eprint = {1803.06272},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Liao et al. - Graph Partition Neural Networks for Semi-Supervised Classification.pdf:pdf},
title = {{Graph Partition Neural Networks for Semi-Supervised Classification}},
url = {http://arxiv.org/abs/1803.06272},
year = {2018}
}
@inproceedings{Kamil2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Kamil, Shoaib and Cheung, Alvin and Itzhaky, Shachar and Solar-Lezama, Armando},
booktitle = {PLDI},
doi = {10.1145/2908080.2908117},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Kamil et al. - Verified lifting of stencil computations.pdf:pdf},
isbn = {9781450342612},
keywords = {Domain-specific Languages,Program Synthesis,Verified Lifting},
title = {{Verified lifting of stencil computations}},
year = {2016}
}
@inproceedings{Regehr2012a,
abstract = {To report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated test-case reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domain-specific test-case reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by compiler developers. We conclude that effective program reduction requires more than straightforward delta debugging.},
annote = {NULL},
author = {Regehr, J. and Chen, Y. and Cuoq, P. and Eide, E. and Ellison, C. and Yang, X.},
booktitle = {PLDI},
doi = {10.1145/2254064.2254104},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Regehr et al. - Test-Case Reduction for C Compiler Bugs.pdf:pdf},
isbn = {978-1-4503-1205-9},
issn = {0362-1340},
keywords = {automated testing,bug reporting,compiler defect,compiler testing,random testing,test-case minimization},
title = {{Test-Case Reduction for C Compiler Bugs}},
year = {2012}
}
@article{Clifton-Everest2014,
abstract = {Special purpose embedded languages facilitate generating high-performance code from purely functional high-level code; for ex- ample, we want to program highly parallel GPUs without the usual high barrier to entry and the time-consuming development process. We pre- viously demonstrated the feasibility of a skeleton-based, generative ap- proach to compiling such embedded languages. In this paper, we (a) describe our solution to some of the practical prob- lems with skeleton-based code generation and (b) introduce our approach to enabling interoperability with native code. In particular, we show, in the context of a functional embedded language for GPU programming, how template meta programming simplifies code generation and optimi- sation. Furthermore, we present our design for a foreign function interface for an embedded language.},
annote = {NULL},
author = {Clifton-Everest, Robert and McDonell, Trevor L and Chakravarty, Manuel MT and Keller, Gabriele},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Clifton-Everest et al. - Embedding foreign code.pdf:pdf},
journal = {PADL},
title = {{Embedding foreign code}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-04132-2{\_}10},
year = {2014}
}
@inproceedings{Blumofe1995,
abstract = {Cilk (pronounced ``silk'') is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the ``work'' and ``critical path'' of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of ``fully strict'' (well-structured) programs, the Cilk scheduler achieves space, time and communication bounds all within a constant factor of {\{}optimal.The{\}} Cilk runtime system currently runs on the Connection Machine {\{}CM5{\}} {\{}MPP,{\}} the Intel Paragon {\{}MPP,{\}} the Silicon Graphics Power Challenge {\{}SMP,{\}} and the {\{}MIT{\}} Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the {\{}*Socrates{\}} chess program, which won third prize in the 1994 {\{}ACM{\}} International Computer Chess Championship.},
annote = {Cited by 1618.},
author = {Blumofe, Robert D and Joerg, Christopher F and Kuszmaul, Bradley C and Leiserson, Charles E and Randall, Keith H and Zhou, Yuli},
booktitle = {PPoPP},
doi = {10.1006/jpdc.1996.0107},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Blumofe et al. - Cilk an efficient multithreaded runtime system.pdf:pdf},
isbn = {0897917016},
issn = {0743-7315},
keywords = {parallel},
publisher = {ACM Press},
title = {{Cilk: an efficient multithreaded runtime system}},
year = {1995}
}
@inproceedings{Zakai2013,
annote = {NULL},
author = {Zakai, A.},
booktitle = {OOPSLA},
doi = {10.1145/2048147.2048224},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Zakai - Emscripten An LLVM-to-JavaScript Compiler.pdf:pdf},
isbn = {9781450309424},
publisher = {ACM},
title = {{Emscripten: An LLVM-to-JavaScript Compiler}},
year = {2011}
}
@article{Eickenberg2016,
annote = {NULL},
author = {Eickenberg, M. and Gramfort, A. and Varoquaux, G. and Thirion, B.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Eickenberg et al. - Seeing it all Convolutional network layers map the function of the human visual system.pdf:pdf},
journal = {NeuroImage},
publisher = {Elsevier},
title = {{Seeing it all: Convolutional network layers map the function of the human visual system}},
year = {2016}
}
@article{Sanchez-Gonzalez2018,
abstract = {Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.},
archivePrefix = {arXiv},
arxivId = {1806.01242},
author = {Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
doi = {10.2174/157340310791658785},
eprint = {1806.01242},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Sanchez-Gonzalez et al. - Graph networks as learnable physics engines for inference and control.pdf:pdf},
isbn = {1806.01242v1},
issn = {00407453},
pmid = {21804773},
title = {{Graph networks as learnable physics engines for inference and control}},
url = {http://arxiv.org/abs/1806.01242},
year = {2018}
}
@article{Yi2012,
abstract = {We present POET, a scripting language designed for applying advanced program transformations to code in arbitrary programming languages as well as building ad- hoc translators between these languages. We have used POET to support a large number of compiler optimizations, including loop interchange, parallelization, block- ing, fusion/fission, strength reduction, scalar replacement, SSE vectorization, among others, and to fully support the code generation of several domain-specific languages, including automatic tester/timer generation, and automatically translating a finite- state-machine-based behavior modeling language to C++/Java code. This paper presents key design and implementation decisions of the POET language and show how to use various language features to significantly reduce the difficulty of supporting programmable compiler optimization for high performance computing and supporting ad-hoc code generation for various domain-specific languages.},
annote = {POET is a scripting language for applying program transformations to code in arbitrary languages and trasnforming between languages.




The first POET paper was 2007. What's changed since then??},
author = {Yi, Q. and Antonio, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Yi, Antonio - POET A Scripting Language For Applying Parameterized Source-to-source Program.pdf:pdf},
journal = {Software: Practice and Experience},
number = {6},
publisher = {Wiley Online Library},
title = {{POET: A Scripting Language For Applying Parameterized Source-to-source Program}},
volume = {42},
year = {2012}
}
@inproceedings{Juega2014,
abstract = {Graphics Processing Units (GPUs) are today's most power- ful coprocessors for accelerating massive data-parallel algo- rithms. However, programmers are forced to adopt new pro- gramming paradigms to take full advantage of their comput- ing capabilities; this requires signidicant programming and maintenance effort. As a result, there is an increasing in- Terest in the development of tools for automatic mapping of sequential code to GPUs. Current automatic tools require both a deep knowledge on the GPU architecture and the al- gorithm being mapped, which makes the mapping process a labor-intensive task. This paper proposes a technique that improves the code mapping of one of these tools, PPCG, removing the need for any user interaction. It relies on data reuse estimations to explore the mapping space and compute appropriate values for the number of threads per threadblock and tile sizes. Our results show speedups of 3x on average compared to the default code generated by PPCG. Copyright {\textcopyright} 2014 by the Association for Computing Machinery, Inc. (ACM).},
annote = {NULL},
author = {Juega, J.C.a and Gomez, J.I.a and Tenllado, C.a and Catthoor, F.b},
booktitle = {CGO},
doi = {10.1145/2544137.2544145},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Juega et al. - Adaptive mapping and parameter selection scheme to improve automatic code generation for GPUs.pdf:pdf},
isbn = {978-1-4503-2670-4},
keywords = {compilers,cuda,gpu,loop trans-,polyhedral model,tiling},
publisher = {IEEE},
title = {{Adaptive mapping and parameter selection scheme to improve automatic code generation for GPUs}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84900652090{\&}partnerID=40{\&}md5=43a2b741ca108e91c634cdbc60844205},
year = {2014}
}
@article{Czarnecki2004,
abstract = {A wide range of domain-specific languages (DSLs) has been implemented successfully by embedding them in general purpose languages. This paper reviews embedding, and summarizes how two alternative techniques staged interpreters and templates can be used to overcome the limitations of embedding. Both techniques involve a form of generative programming. The paper reviews and compares three programming languages that have special support for generative programming. Two of these languages (MetaOCaml and Template Haskell) are research languages, while the third (C++) is already in wide industrial use. The paper identifies several dimensions that can serve as a basis for comparing generative languages.},
annote = {Cited by 116.},
author = {Czarnecki, K. and O'Donnell, J. and Striegnitz, J. and Taha, W.},
doi = {10.1007/b98156},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Czarnecki et al. - DSL Implementation in MetaOCaml, Template Haskell, and C.pdf:pdf},
isbn = {978-3-540-22119-7},
issn = {03029743},
journal = {Domain-Specific Program Generation},
keywords = {macro},
title = {{DSL Implementation in MetaOCaml, Template Haskell, and C++}},
url = {http://link.springer.com/10.1007/b98156},
year = {2004}
}
@article{Version2016,
annote = {NULL},
author = {Keir, Paul},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Keir - DAGR A DSL for Legacy OpenCL Codes Porting SLAMBench KFusion to SYCL.pdf:pdf},
keywords = {C++,Computer Vision,GPGPU,OpenCL,Parallelism},
title = {{DAGR: A DSL for Legacy OpenCL Codes Porting SLAMBench KFusion to SYCL}},
year = {2016}
}
@article{Duncan1990,
abstract = {An attempt is made to place recent architectural innovations in
the broader context of parallel architecture development by surveying
the fundamentals of both newer and more established parallel computer
architectures and by placing these architectural alternatives in a
coherent framework. The primary emphasis is on architectural constructs
rather than specific parallel machines. Three categories of architecture
are defined and discussed: synchronous architectures, comprising vector,
SIMD (single-instruction-stream, multiple-data-stream) and systolic
machines; MIMD (multiple-instruction-stream, multiple-data-stream) with
either distributed or shared memory; and MIMD-based paradigms,
comprising MIMD/SIMD hybrid, dataflow, reduction, and wavefront types
},
annote = {NULL},
author = {Duncan, Ralph},
doi = {10.1109/2.44900},
file = {:Users/cec/Google Drive/Mendeley Library/1990 - Duncan - Survey of parallel computer architectures.pdf:pdf},
isbn = {0018-9162 VO - 23},
issn = {00189162},
journal = {Computer},
title = {{Survey of parallel computer architectures}},
volume = {23},
year = {1990}
}
@article{Jiang2010,
abstract = {This paper presents a finding and a technique on program behavior prediction. The finding is that surprisingly strong statistical correlations exist among the behaviors of different program components (e.g., loops) and among different types of program level behaviors (e.g., loop trip-counts versus data values). Furthermore, the correlations can be beneficially exploited: They help resolve the proactivity-adaptivity dilemma faced by existing program behavior predictions, making it possible to gain the strengths of both approaches - the large scope and earliness of offline-profiling - based predictions, and the cross-input adaptivity of runtime sampling-based predictions. The main technique contributed by this paper centers on a new concept, seminal behaviors. Enlightened by the existence of strong correlations among program behaviors, we propose a regression based framework to automatically identify a small set of behaviors that can lead to accurate prediction of other behaviors in a program. We call these seminal behaviors. By applying statistical learning techniques, the framework constructs predictive models that map from seminal behaviors to other behaviors, enabling proactive and cross-input adaptive prediction of program behaviors. The prediction helps a commercial compiler, the IBM XL C compiler, generate code that runs up to 45{\%} faster (5{\%}-13{\%} on average), demonstrating the large potential of correlation-based techniques for program optimizations. {\textcopyright} 2010 ACM.},
annote = {NULL},
author = {Jiang, Y. and Zhang, Z. Z. and Tian, K. and Mao, F. and Gethers, M. and Shen, X. and Gao, Y.},
doi = {10.1145/1772954.1772989},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Jiang et al. - Exploiting Statistical Correlations for Proactive Prediction of Program Behaviors.pdf:pdf},
isbn = {9781605586359},
journal = {CGO},
keywords = {dynamic optimizations,feedback directed optimizations,program be-,program behavior prediction,seminal behaviors},
title = {{Exploiting Statistical Correlations for Proactive Prediction of Program Behaviors}},
year = {2010}
}
@article{Yelick1998,
abstract = {Titaniumis a language and system for high-performance parallel scientific computing. Titanium uses Java as its base, thereby leveraging the advantages of that language and allowing us to focus attention on parallel computing issues. The main additions to Java are immutable classes, multi- dimensional arrays, an explicitly parallel SPMD model of computation with a global address space, and zone-based memory management. We discuss these features and our design approach, and report progress on the development of Titanium, including our current driving application: a three-dimensional adaptive mesh refinement parallel Poisson solver.},
annote = {NULL},
author = {Yelick, K. and Semenzato, L. and Pike, G. and Miyamoto, C. and Liblit, B. and Krishnamurthy, A. and Hilfinger, P. and Graham, S. and Gay, D. and Colella, P. and Aiken, A.},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Yelick et al. - Titanium A high-performance Java dialect.pdf:pdf},
issn = {1040-3108},
journal = {Concurrency and Computation: Practice and Experience},
number = {11},
title = {{Titanium: A high-performance Java dialect}},
volume = {10},
year = {1998}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
annote = {NULL},
author = {Mnih, V. and Kavukcuoglu, K. and Silver, D. and Rusu, A. and Veness, J. and Bellemare, M. G. and Graves, A. and Riedmiller, M. and Fidjeland, A. K. and Ostrovski, G. and Petersen, S. and Beattie, C. and Sadik, A. and Antonoglou, I. and King, H. and Kumaran, D. and Wierstra, D. and Legg, S. and Hassabis, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Mnih et al. - Human-level control through deep reinforcement learning.pdf:pdf},
journal = {Nature},
number = {7540},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{White,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.04742v1},
author = {White, M. and Tufano, M. and Mart{\'{i}}nez, M. and Monperrus, M. and Poshyvanyk, D.},
eprint = {arXiv:1707.04742v1},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - White et al. - Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities.pdf:pdf},
journal = {arXiv:1707.04742},
title = {{Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities}}
}
@article{Dahl2012,
abstract = {We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition.We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We il- lustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a chal- lenging business search dataset demonstrate thatCD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sen- tence accuracy improvement of 5.8{\%} and 9.2{\%} (or relative error reduction of 16.0{\%} and 23.2{\%}) over theCD-GMM-HMMstrained using the minimum phone error rate (MPE) and maximum-likeli- hood (ML) criteria, respectively.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Dahl, G. E. and Yu, D. and Deng, L. and Acero, A.},
doi = {10.1109/TASL.2011.2134090},
eprint = {1201.0490},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Dahl et al. - Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {TASLP},
number = {1},
pmid = {1000044560},
publisher = {IEEE/ACM},
title = {{Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition}},
volume = {20},
year = {2012}
}
@article{Zhang2018,
abstract = {The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks and domain specific languages (DSLs) lack flexibility, supporting only a limited set of optimizations. This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a separate scheduling language. The algorithm language simplifies the expression of the algorithms, while exposing opportunities for optimizations. We formulate graph optimizations as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together a large set of optimizations. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the optimizations. We evaluate GraphIt's performance with six algorithms on graphs with different structures and sizes. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 22 out of 27 experiments by up to 4.8{\$}\backslashtimes{\$}, and is never more than 43{\%} slower than the fastest framework. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.},
author = {Zhang, Y. and Yang, M. and Baghdadi, R. and Kamil, S. and Shun, J. and Amarasinghe, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Zhang et al. - GraphIt - A High-Performance DSL for Graph Analytics.pdf:pdf},
journal = {arXiv:1805.00923},
title = {{GraphIt - A High-Performance DSL for Graph Analytics}},
year = {2018}
}
@book{Balasundaram1991,
abstract = {The choice of the data domain partitioning scheme is an important factor in determining the available par- allelism and hence the performance of an application on a distributed memory multiprocessor. In this pa- per, we present a performance estimator for statically evaluating the relative efficiency of different data par- titioning schemes for any given program on any given distributed memory multiprocessor. Our methlod is not based on a theoretical machine model, but ixnstead uses a set of kernel routinea to “train” the estimator for each target machine. We also describe a prototype implementation of this technique and discuss an experimental evaluation of its accuracy.},
annote = {NULL},
author = {Balasundaram, V. and Fox, G. and Kennedy, K. and Kremer, U.},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Balasundaram et al. - A static performance estimator to guide data partitioning decisions.pdf:pdf},
publisher = {ACM},
title = {{A static performance estimator to guide data partitioning decisions}},
year = {1991}
}
@article{Harris2007,
abstract = {Common and important data parallel primitive Easy to implement in CUDA Harder to get it right Serves as a great optimization example Well walk step by step through 7 different versions Demonstrates},
annote = {NULL},
author = {Harris, M. and Blelloch, G. E. and Maggs, B. M. and Govindaraju, N. K. and Lloyd, B. and Wang, W. and Lin, M. and Manocha, D. and Smolarkiewicz, P. K. and Margolin, L. G.},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Harris et al. - Optimizing parallel reduction in CUDA.pdf:pdf},
journal = {NVIDIA Developer Technology},
number = {4},
title = {{Optimizing parallel reduction in CUDA}},
volume = {2},
year = {2007}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
annote = {NULL},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Goodfellow et al. - Generative Adversarial Networks.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv:1406.2661},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Generative Adversarial Networks}},
year = {2014}
}
@inproceedings{Wang2017a,
abstract = {Mutation analysis has many applications, such as asserting the quality of test suites and localizing faults. One important bottleneck of mutation analysis is scalability. The latest work explores the possibility of reducing the redundant execution via split-stream execution. However, split-stream execution is only able to remove redundant execution before the first mutated statement. In this paper we try to also reduce some of the redundant execution after the execution of the first mutated statement. We observe that, although many mutated statements are not equivalent, the execution result of those mutated statements may still be equivalent to the result of the original statement. In other words, the statements are equivalent modulo the current state. In this paper we propose a fast mutation analysis approach, AccMut. AccMut automatically detects the equivalence modulo states among a statement and its mutations, then groups the statements into equivalence classes modulo states, and uses only one process to represent each class. In this way, we can significantly reduce the number of split processes. Our experiments show that our approach can further accelerate mutation analysis on top of split-stream execution with a speedup of 2.56x on average.},
archivePrefix = {arXiv},
arxivId = {1702.06689},
author = {Wang, B. and Xiong, Y. and Shi, Y. and Zhang, L. and Hao, D.},
booktitle = {ISSTA},
doi = {10.1145/3092703.3092714},
eprint = {1702.06689},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Wang et al. - Faster Mutation Analysis via Equivalence Modulo States.pdf:pdf},
isbn = {9781450350761},
title = {{Faster Mutation Analysis via Equivalence Modulo States}},
url = {http://arxiv.org/abs/1702.06689},
year = {2017}
}
@article{Ryabko2016,
abstract = {The problem of forecasting conditional probabilities of the next event given the past is considered in a general probabilistic setting. Given an arbitrary (large, uncountable) set C of predictors, we would like to construct a single predictor that performs asymptotically as well as the best predictor in C, on any data. Here we show that there are sets C for which such predictors exist, but none of them is a Bayesian predictor with a prior concentrated on C. In other words, there is a predictor with sublinear regret, but every Bayesian predictor must have a linear regret. This negative finding is in sharp contrast with previous results that establish the opposite for the case when one of the predictors in C achieves asymptotically vanishing error. In such a case, if there is a predictor that achieves asymptotically vanishing error for any measure in C, then there is a Bayesian predictor that also has this property, and whose prior is concentrated on (a countable subset of) C.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1610.08239},
author = {Ryabko, D},
doi = {10.1007/978-3-319-46379-7_17},
eprint = {1610.08239},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ryabko - Things Bayes can't do.pdf:pdf},
journal = {arXiv:1610.08239},
title = {{Things Bayes can't do}},
year = {2016}
}
@incollection{Yearning-drafte,
abstract = {Suppose you are building a speech recognition system. Your system works by inputting an audio clip ​ A , and computing some Score​ A​ (​ S) for each possible output sentence ​ S . For example, you might try to estimate Score​ A​ (​ S) = P(​ S |​ A), the probability that the correct output transcription is the sentence ​ S , given that the input audio was ​ A. Given a way to compute Score​ A​ (​ S), you still have to find the English sentence ​ S that maximizes it: How do you compute the " arg max " above? If the English language has 50,000 words, then there are (50,000)​ N ​ possible sentences of length ​ N —far too many to exhaustively enumerate. So, you need to apply an approximate search algorithm, to try to find the value of ​ S that optimizes (maximizes) Score​ A​ (​ S). One example search algorithm is " beam search, " which keeps only ​ K top candidates during the search process. (For the purposes of this chapter, you don't need to understand the details of beam search.) Algorithms like this are not guaranteed to find the value of ​ S that maximizes Score​ A​ (​ S). Suppose that an audio clip ​ A records someone saying " I love machine learning. " But instead of outputting the correct transcription, your system outputs the incorrect " I love robots. " There are now two possibilities for what went wrong:},
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 44-46).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 44-46)}},
year = {2018}
}
@article{Chen2003,
abstract = {There are two fundamental limitations in software testing, known as the reliable test set problem and the oracle problem. Fault-based testing is an attempt by Morell to alleviate the reliable test set problem. In this paper, we propose to enhance fault-based testing to alleviate the oracle problem as well. We present an integrated method that combines metamorphic testing with fault-based testing using real and symbolic inputs. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Chen, T. Y. and Tse, T. H. and Zhou, Z.},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Chen, Tse, Zhou - Fault-based testing without the need of oracles.pdf:pdf},
journal = {Information and Software Technology},
keywords = {Fault-based testing,Metamorphic testing,Oracle problem,Symbolic execution},
number = {1},
title = {{Fault-based testing without the need of oracles}},
volume = {45},
year = {2003}
}
@article{Explanation2016,
annote = {NULL},
author = {Stanford},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Stanford - Stanford Encyclopedia of Philosophy Computational Complexity Theory.pdf:pdf},
title = {{Stanford Encyclopedia of Philosophy: Computational Complexity Theory}},
year = {2016}
}
@inproceedings{Noonan2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Noonan, Matt and Loginov, Alexey and Cok, David},
booktitle = {PLDI},
doi = {10.1145/2908080.2908119},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Noonan, Loginov, Cok - Polymorphic type inference for machine code.pdf:pdf},
isbn = {9781450342612},
keywords = {Binary Analysis,Polymorphism,Pushdown Automata,Reverse Engineering,Static Analyiss,Type Systems},
title = {{Polymorphic type inference for machine code}},
year = {2016}
}
@article{Zhai2014a,
abstract = {The world continues to generate quintillion bytes of data daily, leading to the pressing needs for new efforts in dealing with the grand challenges brought by Big Data. Today, there is a growing consensus among the computational intelligence communities that data volume presents an immediate challenge pertaining to the scalability issue. However, when addressing volume in Big Data analytics, researchers in the data analytics community have largely taken a one-sided study of volume, which is the "Big Instance Size" factor of the data. The flip side of volume which is the dimensionality factor of Big Data, on the other hand, has received much lesser attention. This article thus represents an attempt to fill in this gap and places special focus on this relatively under-explored topic of "Big Dimensionality", wherein the explosion of features (variables) brings about new challenges to computational intelligence. We begin with an analysis on the origins of Big Dimensionality. The evolution of feature dimensionality in the last two decades is then studied using popular data repositories considered in the data analytics and computational intelligence research communities. Subsequently, the state-of-the-art feature selection schemes reported in the field of computational intelligence are reviewed to reveal the inadequacies of existing approaches in keeping pace with the emerging phenomenon of Big Dimensionality. Last but not least, the "curse and blessing of Big Dimensionality" are delineated and deliberated.},
annote = {NULL},
author = {Zhai, Y. and Ong, Y. S. and Tsang, I. W.},
doi = {10.1109/MCI.2014.2326099},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Zhai, Ong, Tsang - The emerging Big dimensionality.pdf:pdf},
isbn = {1556-603X VO - 9},
issn = {1556603X},
journal = {IEEE CIM},
number = {3},
title = {{The emerging "Big dimensionality"}},
volume = {9},
year = {2014}
}
@article{Loogen2005,
abstract = {Eden extends the non-strict functional language Haskell with constructs to control parallel evaluation of processes. Although processes are defined explicitly, communication and syn- chronisation issues are handled in a way transparent to the programmer. In order to offer effective support for parallel evaluation, Eden's coordination constructs override the in- herently sequential demand-driven (lazy) evaluation strategy of its computation language Haskell. Eden is a general-purpose parallel functional language suitable for developing sophisticated skeletons—which simplify parallel programming immensely—as well as for exploiting more irregular parallelism that cannot easily be captured by a predefined skele- ton. The paper gives a comprehensive description of Eden, its semantics, its skeleton-based programming methodology—which is applied in three case studies—, its implementation and performance. Furthermore it points at many additional results that have been achieved in the context of the Eden project.},
annote = {NULL},
author = {Loogen, Rita and Ortega-Mallen, Yolanda and Pena-Mari, Ricardo},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Loogen, Ortega-Mallen, Pena-Mari - Parallel Functional Programming in Eden.pdf:pdf},
journal = {Journal of Functional Programming},
number = {3},
title = {{Parallel Functional Programming in Eden}},
volume = {15},
year = {2005}
}
@misc{Buss2011,
abstract = {The Standard Template Adaptive Parallel Library (STAPL) is a C++ parallel program- ming library that provides a collection of distributed data structures (pContainers) and parallel algorithms (pAlgorithms) and a generic methodology for extending them to provide customized functionality. STAPL algorithms are written in terms of views, which provide a generic access interface to pContainer data by abstracting common data structure concepts. Briefly, views allow the same pContainer to present multiple interfaces, e.g., enabling the same pMatrix to be ‘viewed' (or used) as a row-major or column-major matrix, or even as a vector. In this paper, we describe the stapl View concept and its properties. stapl Views generalize the iterator concept—a View corresponds to a collection of elements and provides an ADT for the data it represents. stapl Views enable parallelism by providing random ac- cess to the elements, and support for managing the tradeoff between the expressivity of the views and the performance of the parallel execution. Views trade additional parallelism en- abling information for reduced genericity. We illustrate the expressivity enabled by Views for several examples and examine the performance overhead incurred when using Views.},
annote = {STAPL is a Standard Template Adaptive Parallel Library. This paper describes the decoupling of algorithms from data structures. They provide a set of abstract data type iterators called Views, which are used to enable parallelism.},
author = {Buss, A. and Fidel, A. and Harshvardhan and Smith, T. and Tanase, G. and Thomas, N. and Xu, X. and Bianco, M. and Amato, N. M. and Rauchwerger, L.},
booktitle = {LCPC},
doi = {10.1007/978-3-642-19595-2_18},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Buss et al. - The STAPL pView.pdf:pdf},
isbn = {9783642195945},
issn = {03029743},
publisher = {Springer},
title = {{The STAPL pView}},
year = {2011}
}
@article{Li2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.05493v4},
author = {Li, Y. and Zemel, R. and Brockscmidt, M. and Tarlow, D.},
eprint = {arXiv:1511.05493v4},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Li et al. - Gated Graph Sequence Neural Networks.pdf:pdf},
journal = {arXiv:1511.05493},
title = {{Gated Graph Sequence Neural Networks}},
year = {2015}
}
@article{Beej2012,
annote = {NULL},
author = {Beej, Brian and Hall, Jorgensen},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Beej, Hall - Beej's Guide to Network Programming Using Internet Sockets.pdf:pdf},
title = {{Beej's Guide to Network Programming Using Internet Sockets}},
year = {2012}
}
@misc{Lee2001,
annote = {NULL},
author = {Lee, Kevin P},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Lee - A Guide to Writing Mathematics.pdf:pdf},
title = {{A Guide to Writing Mathematics}},
url = {papers2://publication/uuid/D0E53C5B-9965-43DF-9A66-95CA737CD555},
year = {2001}
}
@inproceedings{Belli2015b,
abstract = {Measuring and reporting performance of parallel computers con-stitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance im-provements of new techniques and are thus obliged to ensure repro-ducibility or at least interpretability. Our investigation of a strati-fied sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is of-ten unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting tech-niques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initi-ate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.},
annote = {NULL},
author = {Hoefler, Torsten and Belli, Roberto},
booktitle = {SC},
doi = {10.1145/2807591.2807644},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hoefler, Belli - Scientific Benchmarking of Parallel Computing Systems Twelve ways to tell the masses when reporting performance.pdf:pdf},
isbn = {9781450337236},
title = {{Scientific benchmarking of parallel computing systems}},
year = {2015}
}
@article{ISO2005a,
abstract = {Programming languages - C},
author = {ISO},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - ISO - ISO C 99 Working Std.pdf:pdf},
pages = {550},
title = {{ISO C 99 Working Std.}},
year = {2005}
}
@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
doi = {10.1109/WACV.2017.108},
eprint = {1706.01427},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Santoro et al. - A simple neural network module for relational reasoning.pdf:pdf},
isbn = {9781509048229},
issn = {21607516},
pages = {1--16},
pmid = {189384},
title = {{A simple neural network module for relational reasoning}},
url = {http://arxiv.org/abs/1706.01427},
year = {2017}
}
@unpublished{UniversityofEdinburgh2015d,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 7. Register Allocation.pdf:pdf},
title = {{7. Register Allocation}},
year = {2015}
}
@inproceedings{Barrett2015,
annote = {NULL},
author = {Barrett, Richard F and Stark, Dylan T and Vaughan, Courtenay T and Olivier, Stephen L and Pedretti, Kevin T and Grant, Ryan E},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Barrett et al. - Toward an Evolutionary Task Parallel Integrated MPI X Programming Model.pdf:pdf},
isbn = {9781450334044},
keywords = {high performance,scientific and engineering applications},
title = {{Toward an Evolutionary Task Parallel Integrated MPI + X Programming Model}},
year = {2015}
}
@article{Rossum2012d,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Porting Python 2 Code to Python 3.pdf:pdf},
title = {{Porting Python 2 Code to Python 3}},
year = {2016}
}
@inproceedings{Jin,
annote = {NULL},
author = {Jin, Y. and Liu, M. and Ma, X. and Liu, Q. and Logan, J. and Podhorszki, N. and Choi, J. Y. and Klasky, S.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Jin et al. - Combining Phase Identification and Statistic Modeling for Automated Parallel Benchmark Generation.pdf:pdf},
isbn = {9781450332057},
keywords = {HPC applications,automatic benchmark generation,phase identification,statistical profiling,trace},
title = {{Combining Phase Identification and Statistic Modeling for Automated Parallel Benchmark Generation}},
year = {2015}
}
@misc{UniversityofEdinburgh2009c,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - University of Edinburgh - IAML 2012 Resit Exam.pdf:pdf},
number = {August},
title = {{IAML 2012 Resit Exam}},
year = {2009}
}
@article{Pei2015,
abstract = {Fuzzers, or random testing tools, are powerful tools for finding bugs. A major problem with using fuzzersis that they often trigger many bugs that are already known. The fuzzer taming problem addresses this issue by ordering bug-triggering random test cases generated by a fuzzer such that test cases exposing diverse bugs are found early in the ranking. Previous work on fuzzer taming first reduces each test case into a minimal failure-inducing test case using delta debugging, then finds the ordering by applying the Furthest Point First algorithm over the reduced test cases. During the delta debugging process, a sequence of failing test cases is generated (the 'delta debugging trail'). We hypothesize that these additional failing test cases also contain relevant information about the bug and could be useful for fuzzertaming. In this paper, we propose to use these additional failing test cases generated during delta debugging to help tame fuzzers. Our experiments show that this allows for more diverse bugs to be found early in the furthest point first ranking.},
author = {Pei, Yuanli and Christi, Arpit and Fern, Xiaoli and Groce, Alex and Wong, Weng Keen},
doi = {10.1109/ICDMW.2014.58},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Pei et al. - Taming a fuzzer using delta debugging trails.pdf:pdf},
isbn = {978-1-4799-4274-9},
issn = {23759259},
journal = {ICDMW},
keywords = {Automated Testing,Fuzzer Taming,Fuzzing,Software Testing,Test-Case Reduction},
number = {January},
pages = {840--843},
title = {{Taming a fuzzer using delta debugging trails}},
volume = {2015-Janua},
year = {2015}
}
@article{Kearns1999,
abstract = {We present a provably efficient and near-optimal al- gorithmfor reinforcement learning inMarkov deci- sion processes (MDPs) whose transitionmodel can be factored as a dynamic Bayesian network (DBN). Our algorithm generalizes the recent E ? algorithm ofKearns and Singh, and assumes thatwe are given both an algorithm for approximate planning, and the graphical structure (but not the parameters) of theDBN.Unlike the originalE ? algorithm, our new algorithm exploits the DBN structure to achieve a running time that scales polynomially in the num- ber of parameters of theDBN,whichmay be expo- nentially smaller than the number of global states.},
annote = {Structure probabilistic models.},
author = {Kearns, M. and Koller, D.},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Kearns, Koller - Efficient reinforcement learning in factored MDPs.pdf:pdf},
issn = {10450823},
journal = {IJCAI},
title = {{Efficient reinforcement learning in factored MDPs}},
volume = {16},
year = {1999}
}
@article{Koukoutos2017a,
abstract = {Program synthesis and repair have emerged as an exciting area of research, driven by the potential for revolutionary advances in programmer productivity. Among most promising ideas emerging for synthesis are syntax-driven search, probabilistic models of code, and the use of input-output examples. Our paper shows how to combine these techniques and use them for program repair, which is among the most relevant applications of synthesis to general-purpose code. Our approach combines semantic specifications, in the form of pre- and post-conditions and input-output examples with syntactic specifications in the form of term grammars and AST-level statistics extracted from code corpora. We show that synthesis in this framework can be viewed as an instance of graph search, permitting the use of well-understood families of techniques such as A*. We implement our algorithm in a framework for verification, synthesis and repair of functional programs, demonstrating that our approach can repair programs that are beyond the reach of previous tools.},
author = {Koukoutos, M. and Raghothaman, M. and Kneuss, E. and Kuncak, V.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Koukoutos et al. - On Repair with Probabilistic Attribute Grammars.pdf:pdf},
journal = {arXiv:1707.04148},
title = {{On Repair with Probabilistic Attribute Grammars}},
year = {2017}
}
@article{Slee2007,
abstract = {Thrift is a software library and set of code-generation tools devel- oped at Facebook to expedite development and implementation of efficient and scalable backend services. Its primary goal is to en- able efficient and reliable communication across programming lan- guages by abstracting the portions of each language that tend to require the most customization into a common library that is imple- mented in each language. Specifically, Thrift allows developers to define datatypes and service interfaces in a single language-neutral file and generate all the necessary code to build RPC clients and servers. This paper details the motivations and design choices we made in Thrift, as well as some of the more interesting implementation details. It is not intended to be taken as research, but rather it is an exposition on what we did and why.},
annote = {Facebook's equivalane to Protocol Buffers.},
author = {Slee, M. and Agarwal, A. and Kwiatkowski, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Slee, Agarwal, Kwiatkowski - Thrift Scalable cross-language services implementation.pdf:pdf},
journal = {Facebook White Paper},
number = {8},
title = {{Thrift: Scalable cross-language services implementation}},
volume = {5},
year = {2007}
}
@inproceedings{Su2013,
annote = {NULL},
author = {Su, Yu and Ye, Ding and Xue, Jingling},
booktitle = {HiPC},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Su, Ye, Xue - Accelerating Inclusion-based Pointer Analysis on Heterogeneous CPU-GPU Systems.pdf:pdf},
isbn = {9781479907304},
title = {{Accelerating Inclusion-based Pointer Analysis on Heterogeneous CPU-GPU Systems}},
year = {2013}
}
@misc{UniversityofEdinburgh2014bc,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 14.pdf:pdf},
title = {{IAML 14}},
year = {2014}
}
@inproceedings{Zhu2015a,
abstract = {The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we pro-pose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hier-archies, e.g., language or image parse structures. We leverage the models for semantic composi-tion to understand the meaning of text, a funda-mental problem in natural language understand-ing, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that with-out considering the structures.},
annote = {NULL},
author = {Zhu, X. and Sobhani, P. and Guo, H.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Zhu, Sobhani, Guo - Long Short-Term Memory Over Recursive Structures.pdf:pdf},
title = {{Long Short-Term Memory Over Recursive Structures}},
volume = {37},
year = {2015}
}
@inproceedings{Namolaru2010a,
annote = {NULL},
author = {Namolaru, M. and Cohen, A. and Fursin, G. and Zaks, A. and Freund, A.},
booktitle = {CASES},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Namolaru et al. - Practical Aggregation of Semantical Program Properties for Machine Learning Based Optimization.pdf:pdf},
title = {{Practical Aggregation of Semantical Program Properties for Machine Learning Based Optimization}},
year = {2010}
}
@article{Cole2004,
abstract = {Skeleton and pattern based parallel programming promise significant benefits but remain absent from mainstream practice. We consider why this situation has arisen and propose a number of design principles which may help to redress it. We sketch the eSkel library, which represents a concrete attempt to apply these principles. eSkel is based on C and MPI, thereby embedding its skeletons in a conceptually familiar framework. We present an application of eSkel and analyse it as a response to our manifesto.},
annote = {NULL},
author = {Cole, M.},
doi = {10.1016/j.parco.2003.12.002},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Cole - Bringing skeletons out of the closet a pragmatic manifesto for skeletal parallel programming.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
month = {mar},
number = {3},
publisher = {Elsevier},
title = {{Bringing skeletons out of the closet: a pragmatic manifesto for skeletal parallel programming}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167819104000080},
volume = {30},
year = {2004}
}
@inproceedings{Ravishankar,
annote = {NULL},
author = {Ravishankar, Mahesh and Holewinski, Justin and Carolina, North and Grover, Vinod},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ravishankar et al. - Forma A DSL for Image Processing Applications to Target GPUs and Multi-core CPUs.pdf:pdf},
isbn = {9781450334075},
keywords = {dsl,gpgpu,image processing,multi-core,stencils},
title = {{Forma : A DSL for Image Processing Applications to Target GPUs and Multi-core CPUs}},
year = {2015}
}
@article{Liu2016,
abstract = {Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.},
author = {Liu, P. and Qiu, X. and Huang, X.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Liu, Qiu, Huang - Recurrent Neural Network for Text Classification with Multi-Task Learning.pdf:pdf},
journal = {IJCAI},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Recurrent Neural Network for Text Classification with Multi-Task Learning}},
year = {2016}
}
@article{GuidovanRossumandFredL.Drake2003a,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - The Python Language Reference Manual.pdf:pdf},
isbn = {0954161785},
title = {{The Python Language Reference Manual}},
year = {2016}
}
@article{Oord2016,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
author = {Oord, A. and Kalchbrenner, N. and Vinyals, O. and Espeholt, L. and Graves, A. and Kavukcuoglu, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Oord et al. - Conditional Image Generation with PixelCNN Decoders.pdf:pdf},
journal = {arXiv:1606.05328},
title = {{Conditional Image Generation with PixelCNN Decoders}},
year = {2016}
}
@inproceedings{EdlervonKoch2013,
abstract = {Efficiently executing sequential legacy binaries on chip multi- processors (CMPs) composed of many, small cores is one of today's most pressing problems. Single-threaded execution is a suboptimal option due to CMPs' lower single-core performance, while multi- threaded execution relies on prior parallelization, which is severely hampered by the low-level binary representation of applications compiled and optimized for a single-core target. A recent tech- nology to address this problem is Dynamic Binary Parallelization (DBP), which creates a Virtual Execution Environment (VEE) tak- ing advantage of the underlying multicore host to transparently par- allelize the sequential binary executable. While still in its infancy, DBP has received broad interest within the research community. The combined use of DBP and thread-level speculation (TLS) has been proposed as a technique to accelerate legacy uniprocessor code on modern CMPs. In this paper, we investigate the limits of DBP and seek to gain an understanding of the factors contributing to these limits and the costs and overheads of its implementation. We have performed an extensive evaluation using a parameteriz- able DBP system targeting a CMP with light-weight architectural TLS support. We demonstrate that there is room for a significant reduction of up to 54{\%} in the number of instructions on the crit- ical paths of legacy SPEC CPU2006 benchmarks. However, we show that it is much harder to translate these savings into actual performance improvements, with a realistic hardware-supported implementation achieving a speedup of 1.09 on average.},
annote = {Dynamic Binary Parallelisation uses a virtual execution environment to transparallently parallelise a sequential binary executable. This is a hard problem, and this paper investigates the limts of DBP and shows that a realistic speedup of {\~{}}1.09 is all that is achievable.},
author = {{Edler von Koch}, Tobias JK and Franke, Bj{\"{o}}rn},
booktitle = {VEE},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Edler von Koch, Franke - Limits of Region-Based Dynamic Binary Parallelization.pdf:pdf},
isbn = {9781450312660},
keywords = {Automatic Parallelization,Dynamic Binary Parallelization,Runtime Systems,Thread-level speculation,Transactional Memory},
title = {{Limits of Region-Based Dynamic Binary Parallelization}},
year = {2013}
}
@misc{ImpetusTechnologies,
annote = {NULL},
author = {{Impetus Technologies}, Inc},
title = {{Jumbune: Optimize Hadoop Solutions}},
url = {http://www.jumbune.org/},
urldate = {2016-02-16}
}
@article{Liao2007,
abstract = {OpenMP has gained wide popularity as an API for parallel programming on shared memory and distributed shared memory platforms. It is also a promising candi- date to exploit the emerging multicore, multithreaded pro- cessors. In addition, there is an increasing trend to combine OpenMP with MPI to take full advantage of mainstream su- percomputers consisting of clustered SMPs. All of these re- quire that attention be paid to the quality of the compiler's translation of OpenMP and the flexibility of runtime sup- port. Many compilers and runtime libraries have an in- ternal cost model that helps evaluate compiler transforma- tions, guides adaptive runtime systems, and helps achieve load balancing. But existing models are not sufficient to support OpenMP, especially on new platforms. In this pa- per, we present our experience adapting the cost models in OpenUH, a branch of Open64, to estimate the execution cy- cles of parallel OpenMP regions using knowledge of both software and hardware. Our OpenMP cost model reuses major components from Open64, along with extensions to consider more OpenMP details. Preliminary evaluations of the model are presented using kernel benchmarks. The challenges and possible extensions for modeling OpenMP on multicore platforms are also discussed.},
annote = {NULL},
author = {Liao, Chunhua and Chapman, Barbara},
doi = {10.1109/IPDPS.2007.370398},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Liao, Chapman - A compile-time cost model for OpenMP.pdf:pdf},
isbn = {1-4244-0909-8},
journal = {IPDPS},
publisher = {Citeseer},
title = {{A compile-time cost model for OpenMP}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4228126},
year = {2007}
}
@article{Wigley2015,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.04964v2},
author = {Wigley, P. B. and Everitt, P. J. and Hengel, A. V. D. and Bastian, J. W. and Sooriyabandara, M. A. and Mcdonald, G. D. and Hardman, K. S. and Quinlivan, C. D. and Manju, P. and Kuhn, C. C. N. and Petersen, I. R. and Luiten, A. and Hope, J. J. and Robins, N. P. and Hush, M. R.},
doi = {10.1038/srep25890},
eprint = {arXiv:1507.04964v2},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Wigley et al. - Fast machine-learning online optimization of ultra-cold-atom experiments.pdf:pdf},
issn = {2045-2322},
journal = {Nature},
number = {April},
title = {{Fast machine-learning online optimization of ultra-cold-atom experiments}},
url = {http://dx.doi.org/10.1038/srep25890},
year = {2015}
}
@inproceedings{Chena,
annote = {NULL},
author = {Chen, Y. and Su, T. and Sun, C. and Su, Z. and Zhao, J.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Chen et al. - Coverage-Directed Differential Testing of JVM Implementations.pdf:pdf},
isbn = {9781450342612},
keywords = {differential testing,fuzz testing,java virtual},
title = {{Coverage-Directed Differential Testing of JVM Implementations}},
year = {2016}
}
@inproceedings{Grauer-Gray2012,
abstract = {GPU cluster is an important architecture being used for large scientific and engineering applications. However, manually developed GPU cluster application is still a very difficult task. To alleviate this problem, we adopt the OpenACC standard for directive-based approach and proposed some extension to support GPU cluster programming. The extensions are constructs and clauses used to define the memory distribution and dependency of tasks on cluster nodes. We propose framework and technique used to implement a source-to-source compiler to support the proposed constructs and clauses. The experiment conducted on the source code translation tool developed in this work shows that the speedup close to hand code can be achieved on commonly used scientific application with much less programming effort.},
annote = {NULL},
author = {Grauer-Gray, S. and Xu, L. and Searles, R. and Ayalasomayajula, S. and Cavazos, J.},
booktitle = {InPar},
doi = {10.1109/InPar.2012.6339595},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Grauer-Gray et al. - Auto-tuning a High-Level Language Targeted to GPU Codes.pdf:pdf},
isbn = {9781467326322},
keywords = {Auto-tuning,Belief Propagation,CUDA,GPU,OpenCL,Optimization},
title = {{Auto-tuning a High-Level Language Targeted to GPU Codes}},
year = {2012}
}
@inproceedings{Beard2015,
annote = {NULL},
author = {Beard, J. and Buhler, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Beard, Buhler - Deadlock-free Buffer Configuration for Stream Computing.pdf:pdf},
isbn = {9781450334044},
keywords = {buffer configuration,deadlock avoidance,stream computing},
title = {{Deadlock-free Buffer Configuration for Stream Computing}},
year = {2015}
}
@book{Katajainen1997,
abstract = {The efficiency of mergesort programs is analysed under a simple unit-cost model. In our analysis the time performance of the sorting programs includes the costs of key comparisons, element moves and address calcula- tions. The goal is to establish the best possible time-bound relative to the model when sorting n integers. By the well-known information-theoretic ar- gument n log 2 n - O(n) is a lower bound for the integer-sorting problem in our framework. New implementations for two-way and four-way bottom-up mergesort are given, the worst-case complexities of which are shown to be bounded by 5.5nlog 2 n + O(n) and 3.25nlog 2 n + O(n), respectively. The theoretical findings are backed up with a series of experiments which show the practical relevance of our analysis when implementing library routines for internal-memory computations.},
annote = {NULL},
author = {Katajainen, J. and Trgff, J. L.},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Katajainen, Trgff - A Meticulous Analysis of Mergesort Programs.pdf:pdf},
number = {9400952},
title = {{A Meticulous Analysis of Mergesort Programs}},
volume = {2},
year = {1997}
}
@inproceedings{Marques2013,
abstract = {The Graphics Processing Unit (GPU) is gaining popular- ity as a co-processor to the Central Processing Unit (CPU). However, harnessing its capabilities is a non-trivial exercise that requires good knowledge of parallel programming, more so when the complexity of these applications is increasingly rising. Languages such as StreamIt [1] and Lime [2] have addressed the offloading of composed computations to GPUs. However, to the best of our knowledge, no support exists at library level. To this extent, we propose Marrow, an algorithmic skeleton frame- work for the orchestration of OpenCL computations. Marrow expands the set of skeletons currently available for GPU computing, and enables their combination, through nesting, into complex structures. Moreover, it introduces optimizations that overlap communication and computa- tion, thus conjoining programming simplicity with performance gains in many application scenarios. We evaluated the framework from a perfor- mance perspective, comparing it against hand-tuned OpenCL programs. The results are favourable, indicating that Marrow's skeletons are both flexible and efficient in the context of GPU computing.},
annote = {NULL},
author = {Marques, R. and Paulino, H. and Alexandre, F. and Medeiros, P. D.},
booktitle = {Euro-Par},
doi = {10.1007/978-3-642-40047-6_86},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Marques et al. - Algorithmic skeleton framework for the orchestration of GPU computations.pdf:pdf},
isbn = {9783642400469},
issn = {03029743},
publisher = {Springer},
title = {{Algorithmic skeleton framework for the orchestration of GPU computations}},
volume = {8097 LNCS},
year = {2013}
}
@article{Coates2001,
annote = {NULL},
author = {Coates, Allison L and Division, Computer Science and Baird, Henry S and Fatema, Richard J and Division, Computer Science},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Coates et al. - Pessimal Print A Reverse Turing Test.pdf:pdf},
keywords = {document image analysis,document image degradation,evaluation methods,hu-,legibility,machine discrimination,man,ocr,turing test},
title = {{Pessimal Print: A Reverse Turing Test}},
year = {2001}
}
@inproceedings{Donaldson2017,
author = {Donaldson, A. and Evrard, H. and Lascu, A. and Thomson, P.},
booktitle = {OOPSLA},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Donaldson et al. - Automated Testing of Graphics Shader Compilers.pdf:pdf},
publisher = {ACM},
title = {{Automated Testing of Graphics Shader Compilers}},
year = {2017}
}
@misc{Fifield,
annote = {NULL},
author = {Fifield, Jeff and Keryell, Ronan and Ratigner, Herv{\'{e}} and Styles, Henry and Wu, Jim},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Fifield et al. - Optimizing OpenCL applications on Xilinx FPGA.pdf:pdf},
title = {{Optimizing OpenCL applications on Xilinx FPGA}},
year = {2016}
}
@inproceedings{Trifunovic2011a,
annote = {Cited by 39.},
author = {Trifunovic, K. and Cohen, A. and Edelsohn, D. and Li, F. and Grosser, T. and Jagasia, H. and Ladelsky, R. and Pop, S. and Sj, J.},
booktitle = {GROW},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Trifunovic et al. - GRAPHITE Two Years After First Lessons Learned From Real-World Polyhedral Compilation.pdf:pdf},
title = {{GRAPHITE Two Years After: First Lessons Learned From Real-World Polyhedral Compilation}},
year = {2010}
}
@unpublished{Maier,
abstract = {We describe the first ever parallelisation of an algebraic computation at modern HPC scale. Our case study poses challenges typical of the domain: it is a multi-phase application with dynamic task creation and irregular parallelism over complex control and data structures. Our starting point is a sequential algorithm for finding invariant bilinear forms in the representation theory of Hecke algebras, implemented in the GAP compu- tational group theory system. After optimising the sequential code we develop a parallel algorithm that exploits the new skeleton-based SGP2 framework to par- allelise the three most computationally-intensive phases. To this end we develop a new domain-specific skeleton, parBufferTryReduce.We report good par- allel performance both on a commodity cluster and on a national HPC, delivering speedups up to 548 over the optimised sequential implementation on 1024 cores.},
annote = {NULL},
author = {Maier, P. and Livesey, D. and Loidl, H. W. and Trinder, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Maier et al. - High-Performance Computer Algebra A Hecke Algebra Case Study.pdf:pdf},
institution = {School of Computing Science, University of Glasgow, Glasgow, UK},
title = {{High-Performance Computer Algebra: A Hecke Algebra Case Study}},
url = {http://www.macs.hw.ac.uk/{~}hwloidl/publications/HPCA.pdf},
year = {2014}
}
@inproceedings{Jablin2012,
abstract = {The performance benefits of GPU parallelism can be enormous, but unlocking this performance potential is challenging. The ap- plicability and performance of GPU parallelizations is limited by the complexities of CPU-GPU communication. To address these communications problems, this paper presents the first fully auto- matic system for managing and optimizing CPU-GPU communca- tion. This system, called the CPU-GPU Communication Man- ager (CGCM), consists of a run-time library and a set of com- piler transformations that work together to manage and optimize CPU-GPU communication without depending on the strength of static compile-time analyses or on programmer-supplied annota- tions. CGCM eases manual GPU parallelizations and improves the applicability and performance of automatic GPU parallelizations. For 24 programs, CGCM-enabled automatic GPU parallelization yields a whole program geomean speedup of 5.36x over the best sequential CPU-only execution.},
annote = {This paper describes a system for managing and optimising CPU-GPU communication using a runtime library and optimising compiler, respectively. The paper uses the argument that manually managing communication is time consuming and error prone.},
author = {Jablin, T. B. and Prabhu, P. and Jablin, J. A. and Johnson, N. P. and Beard, S. R. and August, D. I.},
booktitle = {PLDI},
doi = {10.1145/2345156.1993516},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Jablin et al. - Automatic CPU-GPU communication management and optimization.pdf:pdf},
isbn = {9781450306638},
issn = {03621340},
keywords = {communication,gpu,management,optimization},
month = {aug},
publisher = {ACM},
title = {{Automatic CPU-GPU communication management and optimization}},
url = {http://dl.acm.org/citation.cfm?doid=2345156.1993516},
year = {2011}
}
@inproceedings{Zhang2018a,
abstract = {Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren's internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.},
archivePrefix = {arXiv},
arxivId = {1809.02840},
author = {Zhang, L. and Rosenblatt, G. and Fetaya, E. and Liao, R. and Byrd, W. E. and Might, M. and Urtasun, R. and Zemel, R.},
booktitle = {NeurIPS},
doi = {10.1143/JJAP.32.3987},
eprint = {1809.02840},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Zhang et al. - Neural Guided Constraint Logic Programming for Program Synthesis.pdf:pdf},
isbn = {9781680832921},
issn = {0021-4922},
title = {{Neural Guided Constraint Logic Programming for Program Synthesis}},
url = {http://arxiv.org/abs/1809.02840},
year = {2018}
}
@article{Dastgeer2015a,
annote = {Cited by 0.},
author = {Dastgeer, U. and Kessler, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Dastgeer, Kessler - Smart Containers and Skeleton Programming for GPU-Based Systems.pdf:pdf},
journal = {IJPP},
keywords = {gpu-based systems,memory management,runtime optimizations,skeleton programming,skepu,smart containers},
publisher = {Springer},
title = {{Smart Containers and Skeleton Programming for GPU-Based Systems}},
year = {2015}
}
@inproceedings{Dotsenko2011,
abstract = {We present an auto-tuning framework for FFTs on graphics pro- cessors (GPUs). Due to complex design of the memory and com- pute subsystems on GPUs, the performance of FFT kernels over the range of possible input parameters can vary widely. We gen- erate several variants for each component of the FFT kernel that, for different cases, are likely to perform well. Our auto-tuner com- poses variants to generate kernels and selects the best ones. We present heuristics to prune the search space and profile only a small fraction of all possible kernels. We compose optimized kernels to improve the performance of larger FFT computations. We imple- ment the system using the NVIDIA CUDA API and compare its performance to the state-of-the-art FFT libraries. On a range of NVIDIA GPUs and input sizes, our auto-tuned FFTs outperform the NVIDIA CUFFT 3.0 library by up to 38×and deliver up to 3× higher performance compared to a manually-tuned FFT.},
address = {New York, New York, USA},
annote = {This paper describes an offline autotuner for FFT transforms on GPUs using CUDA. The autotuner sweeps the parameter space and builds a performance profile of optimal kernel variants for every step of a global FFT. The optimisation space includes FFT, stride, and kernel parameters.},
author = {Dotsenko, Y. and Baghsorkhi, S. S. and Lloyd, B. and Govindaraju, N. K.},
booktitle = {PPoPP},
doi = {10.1145/1941553.1941589},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Dotsenko et al. - Auto-tuning of fast fourier transform on graphics processors.pdf:pdf},
isbn = {9781450301190},
keywords = {FFT,Fast Fourier Transform,GPU,auto-tuning,high performance,performance analysis,performance tuning},
publisher = {ACM Press},
title = {{Auto-tuning of fast fourier transform on graphics processors}},
url = {http://portal.acm.org/citation.cfm?doid=1941553.1941589},
year = {2011}
}
@article{Ogilvie2017,
abstract = {Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times – a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.},
annote = {NULL},
author = {Ogilvie, W. F. and Petoumenos, P. and Wang, Z. and Leather, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Ogilvie et al. - Minimizing the cost of iterative compilation with active learning.pdf:pdf},
isbn = {978-1-5090-4931-8},
journal = {CGO},
keywords = {active learning,compilers,iterative compila-,machine learning,sequential analysis,tion},
publisher = {IEEE},
title = {{Minimizing the cost of iterative compilation with active learning}},
year = {2017}
}
@inproceedings{Haidara,
annote = {NULL},
author = {Haidar, A. and Luszczek, P. and Dongarra, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Haidar, Luszczek, Dongarra - Optimization for Performance and Energy for Batched Matrix Computations on GPUs Categories and Subje.pdf:pdf},
isbn = {9781450334075},
keywords = {Batched factorization,hardware accelerators,numerical linear algebra,numerical software libraries,one-sided factorization algorithms},
title = {{Optimization for Performance and Energy for Batched Matrix Computations on GPUs Categories and Subject Descriptors}},
year = {2015}
}
@article{Recht2018,
author = {Recht, B. and Roelofs, R. and Schmidt, L. and Shankar, V.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Recht et al. - Do CIFAR-10 Classifiers Generalize to CIFAR-10.pdf:pdf},
journal = {arXiv:1806.00451},
title = {{Do CIFAR-10 Classifiers Generalize to CIFAR-10?}},
year = {2018}
}
@article{Bunel,
abstract = {This paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.},
annote = {The authors propose an "adaptive neural compiler", which compiles a program to a neural network, and then finds sets of weights for the network such that it performs the correct IO mapping as efficiently a possible for a given distribution of inputs.

The paper takes a theoretical approach and shows that for small programs, it is possible to learn effective optimisations for different input distributions. It does not scale to large (i.e. real world) applications.},
archivePrefix = {arXiv},
arxivId = {1605.07969},
author = {Bunel, R. and Desmaison, A. and Kohli, P. and Torr, P. H. S. and {Pawan Kumar}, M.},
eprint = {1605.07969},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Bunel et al. - Adaptive Neural Compilation.pdf:pdf},
journal = {arXiv:1605.07969},
title = {{Adaptive Neural Compilation}},
year = {2016}
}
@misc{UniversityofEdinburgh2014u,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 10.pdf:pdf},
title = {{IAML 10}},
year = {2014}
}
@phdthesis{Cummins2015,
abstract = {The physical limitations of microprocessor design have forced the industry to- wards increasingly heterogeneous architectures to extract performance. This trend has not been matched with software tools to cope with such parallelism, leading to a growing disparity between the levels of available performance and the ability for application developers to exploit it. Algorithmic skeletons simplify parallel programming by providing high-level, reusable patterns of computation. Achieving performant skeleton implementa- tions is a difficult task; developers must attempt to anticipate and tune for a wide range of architectures and use cases. This results in implementations that target the general case and cannot provide the performance advantages that are gained from tuning low level optimisation parameters. To address this, I present OmniTune — an extensible and distributed frame- work for runtime autotuning of optimisation parameters. Targeting the work- group size of OpenCL kernels, I demonstrate an implementation of OmniTune for stencil codes on CPUs and multi-GPU systems. I show in a comprehensive evaluation of 2.7×105 test cases that simple heuristics cannot provide portable performance across the range of architectures, kernels, and datasets which algo- rithmic skeletons must target. OmniTune uses procedurally generated synthetic benchmarks and machine learning to predict workgroup sizes for unseen programs. In an evaluation of 429 combinations of programs, architectures, and datasets, with up to 7.3×103 parameter values for each, OmniTune is able to achieve a median 94{\%} of the avail- able performance, providing a 1.33× speedup over the values selected by human experts, without requiring any user intervention. This adaptive tuning provides a median speedup of 3.79× (max 74.0×) over the best possible performance which can be achieved without autotuning.},
annote = {NULL},
author = {Cummins, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Cummins - Autotuning Stencils Codes with Algorithmic Skeletons.pdf:pdf},
school = {University of Edinburgh},
title = {{Autotuning Stencils Codes with Algorithmic Skeletons}},
year = {2015}
}
@inproceedings{Rawat2016,
annote = {NULL},
author = {Rawat, P. S. R and Hong, C. and Ravishankar, M. and Grover, V.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rawat et al. - Resource Conscious Reuse-Driven Tiling for GPUs.pdf:pdf},
isbn = {9781450341219},
keywords = {code generation,fusion,gpu,stencil computations},
publisher = {ACM},
title = {{Resource Conscious Reuse-Driven Tiling for GPUs}},
year = {2016}
}
@inproceedings{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
annote = {NULL},
author = {Papineni, K. and Roukos, S. and Ward, T. and Zhu, W.},
booktitle = {ACL},
doi = {10.3115/1073083.1073135},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Papineni et al. - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@article{Rossum2009,
abstract = {This document describes how to write modules in C or C++ to extend the Python interpreter with new modules. Those modules can define new functions but also new object types and their methods. The document also describes how to embed the Python interpreter in another application, for use as an extension language. Finally, it shows howto compile and link extension modules so that they can be loaded dynamically (at run time) into the interpreter, if the underlying operating system supports this feature.},
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Extending and Embedding Python.pdf:pdf},
title = {{Extending and Embedding Python}},
year = {2016}
}
@inproceedings{Sim2012,
annote = {NULL},
author = {Sim, J. and Dasgupta, A. and Kim, H. and Vuduc, R.},
booktitle = {PPoPP},
doi = {10.1145/2370036.2145819},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Sim et al. - A Performance Analysis Framework for Identifying Potential Benefits in GPGPU Applications.pdf:pdf},
isbn = {9781450311601},
issn = {03621340},
keywords = {analytical model,cuda,formance benefit prediction,gpgpu architecture,per-,performance prediction},
publisher = {ACM},
title = {{A Performance Analysis Framework for Identifying Potential Benefits in GPGPU Applications}},
year = {2012}
}
@inproceedings{Golan-gueta,
annote = {NULL},
author = {Golan-gueta, Guy and Yahav, Eran},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Golan-gueta, Yahav - Automatic Scalable Atomicity via Semantic Locking.pdf:pdf},
isbn = {9781450332057},
keywords = {automatic synchronization,semantics,transactions},
number = {i},
title = {{Automatic Scalable Atomicity via Semantic Locking}},
year = {2015}
}
@article{Zhou2016a,
abstract = {We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFatNet opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 4-bit gradients to get 47$\backslash${\%} top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.},
author = {Zhou, S. and Ni, Z. and Zhou, X. and Wen, H. and Wu, Y. and Zou, Y.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zhou et al. - DoReFa-Net Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients.pdf:pdf},
journal = {arXiv:1606.06160},
title = {{DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients}},
year = {2016}
}
@article{Study,
annote = {NULL},
author = {{Embecosm Limited}},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Embecosm Limited - Superoptimization Feasibility Study.pdf:pdf},
title = {{Superoptimization: Feasibility Study}}
}
@phdthesis{Javed2011a,
annote = {NULL},
author = {Javed, Noman},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Javed - Metaprogrammed Algorithmic Skeletons Implementations, Performances and Semantics.pdf:pdf},
isbn = {0000000000000},
title = {{Metaprogrammed Algorithmic Skeletons: Implementations, Performances and Semantics}},
year = {2011}
}
@misc{Haller2009a,
abstract = {There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM. VMs usually map their threads to heavyweight OS processes. Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer. In this paper we show how thread-based and event-based programming can be unified under a single actor abstraction. Using advanced abstraction mechanisms of the Scala programming language, we implement our approach on unmodified JVMs. Our programming model integrates well with the threading model of the underlying VM. ?? 2008 Elsevier B.V. All rights reserved.},
annote = {NULL},
author = {Haller, Philipp and Odersky, Martin},
booktitle = {Theoretical Computer Science},
doi = {10.1016/j.tcs.2008.09.019},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Haller, Odersky - Scala Actors Unifying thread-based and event-based programming.pdf:pdf},
isbn = {0981531601},
issn = {03043975},
keywords = {Actors,Concurrent programming,Events,Threads},
number = {2-3},
pmid = {20959587},
publisher = {Elsevier B.V.},
title = {{Scala Actors: Unifying thread-based and event-based programming}},
url = {http://dx.doi.org/10.1016/j.tcs.2008.09.019},
volume = {410},
year = {2009}
}
@inproceedings{Karimi2010,
abstract = {CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.},
annote = {Cited by 144.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
booktitle = {ICPP},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Fang, Varbanescu, Sips - A Comprehensive Performance Comparison of CUDA and OpenCL.pdf:pdf},
isbn = {978-1-4577-1336-1},
publisher = {IEEE},
title = {{A Comprehensive Performance Comparison of CUDA and OpenCL}},
url = {http://arxiv.org/abs/1005.2581},
year = {2011}
}
@inproceedings{Leather2009,
abstract = {Many problems in embedded compilation require one set of op- timizations to be selected over another based on run time per- formance. Self-tuned libraries, iterative compilation and machine learning techniques all compare multiple compiled program ver- sions. In each, program versions are timed to determine which has the best performance. The program needs to be run multiple times for each version because there is noise inherent in most performance measurements. The number of runs must be enough to compare different versions, despite the noise, but executing more than this will waste time and energy. The compiler writer must either risk taking too few runs, potentially getting incorrect results, or taking too many runs increasing the time for their experiments or reducing the number of program versions evaluated. Prior works choose constant size sampling plans where each compiled version is executed a fixed number of times without regard to the level of noise. In this paper we develop a sequential sampling plan which can automatically adapt to the experiment so that the compiler writer can have both confidence in the results and also be sure that no more runs were taken than were needed.We show that our system is able to correctly determine the best optimization settings with between 76{\%} and 87{\%} fewer runs than needed by a brute force, constant sampling size approach.We also compare our approach to JavaSTATS(10); we needed 77{\%} to 89{\%} fewer runs than it needed.},
annote = {NULL},
author = {Leather, H. and O'Boyle, M. and Worton, B.},
booktitle = {LCTES},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Leather, O'Boyle, Worton - Raced Profiles Efficient Selection of Competing Compiler Optimizations.pdf:pdf},
publisher = {ACM},
title = {{Raced Profiles: Efficient Selection of Competing Compiler Optimizations}},
year = {2009}
}
@article{Catanzaro2010,
abstract = {Exploiting parallelism may require developers to think differently about how their programs are written.},
annote = {The shift towards parallel computing is a retreat from sequential processor design problems. They present SEJITS (Selective Just-In-Time Specializeation).


Cited by 7.},
author = {Catanzaro, Bryan and Keutzer, Kurt},
doi = {10.1145/1836543.1836552},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Catanzaro, Keutzer - Parallel computing with patterns and frameworks.pdf:pdf},
isbn = {1528-4972},
issn = {15284972},
journal = {XRDS: Crossroads, The ACM Magazine for Students},
number = {5},
title = {{Parallel computing with patterns and frameworks}},
volume = {17},
year = {2010}
}
@inproceedings{Weber2015,
abstract = {Auto-tuning for Graphics Processing Units (GPUs) has be- come very popular in recent years. It removes the necessity to hand-tune GPU code especially when a new hardware architecture is released. Our auto-tuner optimizes memory access patterns. This is a key aspect to exploit the full per- formance of modern GPUs. As the memory hierarchy has historically changed in nearly every GPU generation, it was necessary to reoptimize the code for all of these new ar- chitectures. Unfortunately, the solution space for memory optimizations in large applications can easily reach millions of configurations for a single kernel. This vast number of im- plementations cannot be fully evaluated in a feasible time. In this paper we present an adaptive profiling algorithmthat aims at finding a near optimal configuration within a frac- tion of the global optimum, while reducing the profiling time by several orders of magnitude compared to an exhaustive search. Our algorithm is aimed at and evaluated on large real-world applications.},
annote = {NULL},
author = {Weber, N. and Amend, S. C. and Goesele, M.},
booktitle = {PMBS},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Weber, Amend, Goesele - Guided Profiling for Auto-Tuning Array Layouts on GPUs.pdf:pdf},
isbn = {9781450340090},
publisher = {ACM},
title = {{Guided Profiling for Auto-Tuning Array Layouts on GPUs}},
year = {2015}
}
@techreport{Hoffmann2011,
abstract = {This paper presents SEEC, a self-aware programming model, designed to reduce programming effort in modern multicore systems. In the SEEC model, application pro- grammers specify application goals and progress, while systems programmers separately specify actions system software and hardware can take to affect an application (e.g. resource allocation). The SEEC runtime moni- tors applications and dynamically selects actions to meet application goals optimally (e.g. meeting performance while minimizing power consumption). The SEEC run- time optimizes system behavior for the application rather than requiring the application programmer to optimize for the system. This paper presents a detailed discussion of the SEEC model and runtime as well as several case studies demonstrating their benefits. SEEC is shown to optimize performance perWatt for a video encoder, find optimal resource allocation for an application with com- plex resource usage, and maintain the goals of multiple applications in the face of environmental fluctuations.},
annote = {SEEC is a multicore programming model that allows application programmers to specify goals and progress, and system programmers to implement actions which can affect the rate of application progress.},
author = {Hoffmann, H. and Maggio, M. and Marco, D. and Leva, A. and Agarwal, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Hoffmann et al. - SEEC A Framework for Self-aware Management of Multicore Resources.pdf:pdf},
institution = {MIT},
title = {{SEEC: A Framework for Self-aware Management of Multicore Resources}},
year = {2011}
}
@inproceedings{Margiolas2014a,
annote = {NULL},
author = {Margiolas, C. and O'Boyle, M.},
booktitle = {CGO},
doi = {10.1145/2581122.2544156},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Margiolas, O'Boyle - Portable and Transparent Host-Device Communication Optimization for GPGPU Environments.pdf:pdf},
isbn = {9781450326704},
keywords = {communication optimization,gpu,heterogeneous computing,opencl,profiling,runtime,tracing},
publisher = {IEEE},
title = {{Portable and Transparent Host-Device Communication Optimization for GPGPU Environments}},
year = {2014}
}
@article{Wold1984,
abstract = {In some signal processing applications, it is desirable to build very high performance fast Fourier transform (FFT) processors. To meet the performance requirements, these processors are typically highly pipelined. Until the advent of VLSI, it was not possible to build a single chip which could be used to construct pipeline FFT processors of a reasonable size. However, VLSI implementations have constraints which differ from those of discrete implementations, requiring another look at some of the typical FFT'algorithms in the light of these constraints.},
annote = {NULL},
author = {Wold, E. H. and Despain, A. M.},
doi = {10.1109/TC.1984.1676458},
file = {:Users/cec/Google Drive/Mendeley Library/1984 - Wold, Despain - Pipeline and Parallel-Pipeline FFT Processors for VLSI Implementations.pdf:pdf},
issn = {00189340},
journal = {TC},
keywords = {CORDIC,fast Fourier transform,integrated circuits,parallel processors,pipeline processors,signal processing},
number = {5},
title = {{Pipeline and Parallel-Pipeline FFT Processors for VLSI Implementations}},
volume = {C-33},
year = {1984}
}
@article{Kirkpatrick2016,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.00796},
author = {Kirkpatrick, J. and Pascanu, R. and Rabinowitz, N. and Veness, J. and Desjardins, G. and Rusu, A. A. and Milan, K. and Quan, J. and Ramalho, T. and Grabska-Barwinska, A. and Hassabis, D. and Clopath, C. and Kumaran, D. and Hadsell, R.},
doi = {10.1073/PNAS.1611835114},
eprint = {1612.00796},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Kirkpatrick et al. - Overcoming Catastrophic Forgetting in Neural Networks.pdf:pdf},
issn = {0027-8424},
journal = {PNAS},
title = {{Overcoming Catastrophic Forgetting in Neural Networks}},
year = {2017}
}
@article{Thomson2009,
abstract = {Institute for Computing Systems Architecture},
annote = {NULL},
author = {Thomson, John D},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Thomson - Using Machine Learning to Automate Compiler Optimisation.pdf:pdf},
keywords = {Informatics,Institute of Computing Systems Architecture},
title = {{Using Machine Learning to Automate Compiler Optimisation}},
url = {http://hdl.handle.net/1842/3194},
year = {2009}
}
@article{Bojanowski2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor- mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog- nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
annote = {NULL},
author = {Bojanowski, P. and Grave, E. and Joulin, A. and Mikolov, T. and Grave, E. and Bojanowski, P. and Mikolov, T. and Lake, B. M. and Ullman, T. D. and Tenenbaum, J. B. and Gershman, S. J. and Schmidhuber, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Bojanowski et al. - Bag of Tricks for Efficient Text Classification.pdf:pdf},
journal = {arXiv:1607.01759},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Bag of Tricks for Efficient Text Classification}},
year = {2016}
}
@inproceedings{Steuwer2011,
abstract = {While CUDA and OpenCL made general-purpose programming for Graphics Processing Units (GPU) popular, using these programming approaches remains complex and error-prone because they lack high-level abstractions. The especially challenging systems with multiple GPU are not addressed at all by these low-level programming models. We propose SkelCL – a library providing so-called algorithmic skeletons that capture recurring patterns of parallel compu- tation and communication, together with an abstract vector data type and constructs for specifying data distribution. We demonstrate that SkelCL greatly simplifies programming GPU systems. We report the competitive performance results of SkelCL using both a simple Mandelbrot set computation and an industrial-strength medical imaging application. Because the library is implemented using OpenCL, it is portable across GPU hardware of different vendors.},
annote = {SkelCL is a skeleton library for programming GPUs with OpenCL. The library supports programming of multiple GPUs and implements four data-parallel skeletons: Map, Zip, Reduce, and Scan. The skeleton abstractions create a {\~{}}5{\%} overhead in performance, with a reduction in host program code size of {\~{}}4-20x.








The design of the skeleton library seems solid. Particular nice features include the ability to supply muscle functions which accept arbitrary extra arguments, for example, a map function which accepts two arguments instead of one. Muscle functions are supplied as strings, which seems incredibly unsafe and prone to error, and significantly reduces the ability of editors/IDEs to detect errors.},
author = {Steuwer, Michel and Kegel, Philipp and Gorlatch, Sergei},
booktitle = {IPDPSW},
doi = {10.1109/IPDPS.2011.269},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Steuwer, Kegel, Gorlatch - SkelCL - A Portable Skeleton Library for High-Level GPU Programming.pdf:pdf},
isbn = {978-1-61284-425-1},
keywords = {Algorithmic Skeletons,CUDA,GPU Computing,GPU Programming,Multi-GPU Systems,OpenCL,SkelCL},
month = {may},
publisher = {IEEE},
title = {{SkelCL - A Portable Skeleton Library for High-Level GPU Programming}},
year = {2011}
}
@inproceedings{Achour2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Achour, S. and Sarpeshkar, R. and Rinard, M. C.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Achour, Sarpeshkar, Rinard - Configuration Synthesis for Programmable Analog Devices with Arco.pdf:pdf},
isbn = {9781450342612},
keywords = {analog computing,compilers,languages},
title = {{Configuration Synthesis for Programmable Analog Devices with Arco}},
year = {2016}
}
@misc{Fritts2009,
abstract = {The first step towards the design of video processors and systems is to achieve an understanding of themajor applications, including not only the theory, but also the workload characteristics of the many image and video compression standards. Introduced in 1997, theMediaBench benchmark suite provided the first set of full application-level benchmarks for multimedia, and has consequently enabled significant research in computer architecture and compiler research for media systems. To expedite the next generation of multimedia systems research, we are developing theMediaBench II benchmark suite, incorporating benchmarks from the latest multimedia technologies, and providing both a single composite benchmark suite (MB2comp) as well as separate sub-suites for each area of multimedia. For video, MediaBench II Video (MB2video) includes both the popular mainstream video compression standards, such as JPEG, H.263, andMPEG-2, and the more recent and emerging standards, includingMPEG-4, JPEG-2000, and H.264. This paper first discusses the goals for MediaBench II and the design of the MB2video sub-suite. The paper then presents the results of a comprehensive workload evaluation of MB2video. In particular, while the workload evaluation demonstrates the high processing regularity of video workloads, as compared with general-purpose workloads, it also illustrates how the growing complexity of the emerging video standards is beginning to negatively impact video workload characteristics.},
annote = {MediaBench II Video is a benchmark suite for video compression standards: JPEG, H.263, MPEG-2, MPEG-4, JPEG-2000, H.264.},
author = {Fritts, Jason E. and Steiling, Frederick W. and Tucek, Joseph a. and Wolf, Wayne},
booktitle = {Electronic Imaging},
doi = {10.1016/j.micpro.2009.02.010},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Fritts et al. - MediaBench II video Expediting the next generation of video systems research.pdf:pdf},
issn = {01419331},
keywords = {media processors,mediabench,multimedia benchmarks,processor design,workload evaluation},
month = {jun},
number = {4},
publisher = {International Society for Optics and Photonics},
title = {{MediaBench II video: Expediting the next generation of video systems research}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S014193310900026X},
volume = {33},
year = {2005}
}
@inproceedings{Potter2015,
abstract = {Parallel primitives libraries reduce the burden of knowledge required for developers to begin developing parallel applications and accelerating them with OpenCL. Unfortunately some current libraries implement primitives as individual kernels and so incur a high performance cost in off-chip memory operations for intermediate variables. We describe a methodology for creating efficient domain specific embedded languages on top of the SYCL for OpenCL standard for parallel programming. Using this approach, a small example language was developed which provides an environment for composing image processing pipelines from a library of more primitive operations, while retaining the capability to generate a single kernel from a complex expression, and so eliminate unnecessary intermediate loads and stores to global memory. This elimination of global memory accesses leads to a 2.75x speedup over implementing an unsharp mask in OpenCLIPP. We give details of our domain specific embedded language, and provide experimental performance measurements of both primitive performance and an unsharp mask operation composed of multiple primitives.},
annote = {Critique: Lacks motivation. By Section 5 (implementation), I'm not convinced of what the driving goal is, or why the problem they're trying to overcome is important. How important is kernel fusion? What's wrong with the string manipulation approach?},
author = {Potter, R. and Keir, P. and Bradford, R.J. and Murray, A.},
booktitle = {IWOCL},
doi = {10.1145/2791321.2791332},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Potter et al. - Kernel composition in SYCL.pdf:pdf},
isbn = {9781450334846},
keywords = {GPUs,Kernel fusion,OpenCL,SYCL},
title = {{Kernel composition in SYCL}},
year = {2015}
}
@inproceedings{Owens2006,
abstract = {The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programma-bility, have made graphics hardware a compelling platform for computationally demanding tasks in a wide varietyof application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware.We begin with the technical motivations that underlie general-purpose computation on graphics processors(GPGPU) and describe the hardware and software developments that have led to the recent interest in this field.We then aim the main body of this report at two separate audiences. First, we describe the techniques used inmapping general-purpose computation to graphics hardware. We believe these techniques will be generally usefulfor researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we surveyand categorize the latest developments in general-purpose application development on graphics hardware.},
annote = {A pre-CUDA/OpenCL survey of GPGPU programming. Cited by 2165.},
author = {Owens, John D. and Luebke, David and Govindraju, Naga and Harris, Mark and Kruger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
booktitle = {Computer Graphics Forum},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Owens et al. - A Survey of General Purpose Computation on Graphics Hardware.pdf:pdf},
title = {{A Survey of General Purpose Computation on Graphics Hardware}},
url = {http://www.cs.virginia.edu/papers/ASurveyofGeneralPurposeComputationonGraphicsHardware.pdf},
year = {2006}
}
@article{Blanchette2008,
abstract = {This manual gathers together the key insights into API designt that were discovered through many years of software development on the Qt application development framework at Trolltech (now part of Nokia). When designing and implementing a library, you should also keep other factors in mind, such as efficiency and ease of implementation, in addition to pure API considerations. And although the focus is on public APIs, there is no harm in applying the principles described here when writing application code or internal library code.},
annote = {NULL},
author = {Blanchette, Jasmin},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Blanchette - The Little Manual of API Design.pdf:pdf},
journal = {Trolltech, Nokia},
title = {{The Little Manual of API Design}},
url = {http://mailmongodb.googlecode.com/svn-history/r6/trunk/MMDB/doc/api-design.pdf},
year = {2008}
}
@article{Futamura1999,
abstract = {This paper reports the relationship between formal description of semantics (i.e., intepreter) of a programming language and an actual compiler. The paper also describes a method to automatically generate an actual compiler froma a formal description which is, in some sense, the partial evaluation of a computation process. The compiler-compiler inspired by this method differs from conventional ones in that the compiler-compiler based on our method can describe an evaluation procedure (interpreter) in defining the semantics of a programming language, while the conventional one describes a translation process.},
annote = {The 3 Futamura projections say that 1) Speciialising an interpreter for a given source code yields an executable. 2) Specializing the specializer for the interpreter yields a compiler. 3) Specializing the specializer for itself yields a tool that can convert any interpreter to an equivalent compiler. See: http://en.wikipedia.org/wiki/Partial{\_}evaluation{\#}Futamura{\_}projections Cited by 565.},
author = {Futamura, Yoshihiko},
doi = {10.1023/A:1010095604496},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Futamura - Partial Evaluation of Computation Process — An Approach to a Compiler-Compiler.pdf:pdf},
issn = {13883690},
journal = {Higher-Order and Symbolic Computation},
keywords = {compiler,futamura projections,interpreter,partial evaluation,program transformation},
number = {5},
title = {{Partial Evaluation of Computation Process — An Approach to a Compiler-Compiler}},
url = {http://link.springer.com/article/10.1023/A:1010095604496},
volume = {12},
year = {1999}
}
@inproceedings{Haidar,
annote = {NULL},
author = {Haidar, A. and Dong, T. and Luszczek, P. and Tomov, S. and Dongarra, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Haidar et al. - Towards Batched Linear Solvers on Accelerated Hardware Platforms.pdf:pdf},
isbn = {9781450332057},
keywords = {batched factorization,hard-,numerical linear algebra,numerical software libraries,one-sided factoriza-,ware accelerators},
title = {{Towards Batched Linear Solvers on Accelerated Hardware Platforms}},
year = {2015}
}
@inproceedings{Belli2015a,
annote = {NULL},
author = {Hoefler, Torsten and Belli, Roberto},
booktitle = {SC},
doi = {10.1145/2807591.2807644},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hoefler, Belli - Scientific Benchmarking of Parallel Computing Systems Twelve ways to tell the masses when reporting performance.pdf:pdf},
isbn = {9781450337236},
title = {{Scientific Benchmarking of Parallel Computing Systems: Twelve ways to tell the masses when reporting performance results}},
year = {2015}
}
@inproceedings{Pabon2014,
abstract = {This paper presents a novel way to introduce self-configura- tion and self-optimization autonomic characteristics to al- gorithmic skeletons using event driven programming tech- niques. Based on an algorithmic skeleton language, we show that the use of events greatly improves the estimation of the remaining computation time for skeleton execution. Events allow us to precisely monitor the status of the execution of algorithmic skeletons. Using such events, we provide a framework for the execution of skeletons with a very high level of adaptability. We focus mainly on guaranteeing a given execution time for a skeleton, by optimizing autonom- ically the number of threads allocated. The proposed solu- tion is independent from the platform chosen for executing the skeleton for example we illustrate our approach in a mul- ticore setting, but it could also be adapted to a distributed execution environment.},
annote = {NULL},
author = {Pab{\'{o}}n, G and Henrio, L},
booktitle = {PMAM},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Pab{\'{o}}n, Henrio - Self-Configuration and Self-Optimization Autonomic Skeletons using Events.pdf:pdf},
title = {{Self-Configuration and Self-Optimization Autonomic Skeletons using Events}},
url = {http://dl.acm.org/citation.cfm?id=2560699},
year = {2014}
}
@article{Motzkin1973,
abstract = {Research on linear inequalities systems prior to 1947 consisted of isolated efforts by a few investigators. A case in point is the elimination technique for reducing the number of variables in the system. A description of the method can be found in Fourier [l], Dines [2], and Motzkin [3]. It differs from its analog for systems of equations in that (unfortunately) each step in the elimination can greatly increase the number of inequalities in the remaining variables. For years the method was referred to as the Motzkin Elimination Method. However, because of the odd grave-digging custom of looking for artifacts in long forgotten papers, it is now known as the Fourier-Motzkin Elimination Method and perhaps will eventually be known as the Fourier-Dines-Motzkin Elimination Method. Given},
annote = {NULL},
author = {Motzkin, S.},
file = {:Users/cec/Google Drive/Mendeley Library/1973 - Motzkin - Fourier-Motzkin elimination and its dual.pdf:pdf},
journal = {Journal of Combinatorial Theory, Series A},
number = {3},
title = {{Fourier-Motzkin elimination and its dual}},
volume = {14},
year = {1973}
}
@inproceedings{Allamanis2013a,
abstract = {The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new "lens" for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program's core logic based solely on general information theoretic criteria.},
annote = {From Duplicate 1 (Mining source code repositories at massive scale using language modeling - Allamanis, Miltiadis; Sutton, Charles)

From Duplicate 1 (Mining source code repositories at massive scale using language modeling - Allamanis, Miltiadis; Sutton, Charles)

n-gram language model of {\textgreater}1 billion tokens (352 million lines of Java).

From Duplicate 2 (Mining source code repositories at massive scale using language modeling - Allamanis, Miltiadis; Sutton, Charles)

Cited by 48.

From Duplicate 2 (Mining source code repositories at massive scale using language modeling - Allamanis, Miltiadis; Sutton, Charles)

Cited by 48.},
author = {Allamanis, M. and Sutton, C.},
booktitle = {MSR},
doi = {10.1109/MSR.2013.6624029},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Allamanis, Sutton - Mining Source Code Repositories at Massive Scale using Language Modeling.pdf:pdf},
isbn = {9781467329361},
issn = {21601852},
title = {{Mining Source Code Repositories at Massive Scale using Language Modeling}},
year = {2013}
}
@inproceedings{Fedorova2007,
abstract = {We make a case that a thread scheduler for heterogeneous multicore systems should target three objectives: optimal performance, core assignment balance and response time fairness. Performance optimization via optimal thread-to-core assignment has been explored in the past; in this paper we demonstrate the need for balanced core assignment. We show that unbalanced core assignment results in completion time jitter and inconsistent priority enforcement; we then present a simple fix to the Linux scheduler that eliminates these problems. The second part of the paper addresses the problem of building the HMC scheduler that balances all three objectives. This is a difficult optimization problem. We introduce a definition of this scheduling problem in terms of these three objectives and present a blueprint for a self-tuning algorithm based on reinforcement learning that maximizes a performance function that is an arbitrary weighted sum of these three objectives. Implementing and evaluating this algorithm is the subject of our future work.},
annote = {NULL},
author = {Fedorova, A. and Vengerov, D. and Doucette, D.},
booktitle = {OSHMA},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Fedorova, Vengerov, Doucette - Operating System Scheduling On Heterogeneous Core Systems.pdf:pdf},
publisher = {Citeseer},
title = {{Operating System Scheduling On Heterogeneous Core Systems}},
year = {2007}
}
@article{Seff2017,
author = {Seff, A. and Beatson, A. and Suo, D. and Liu, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Seff et al. - Continual Learning in Generative Adversarial Nets.pdf:pdf},
journal = {arXiv:1705.08395},
title = {{Continual Learning in Generative Adversarial Nets}},
year = {2017}
}
@misc{Ghaoui2011,
annote = {NULL},
author = {Ghaoui, L. E. and Li, G.},
booktitle = {CIDU},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Ghaoui, Li - Sparse machine learning methods for understanding large text corpora.pdf:pdf},
title = {{Sparse machine learning methods for understanding large text corpora}},
url = {http://www.cs.umbc.edu/{~}kanishk1/papers/CIDU{\_}12{\_}text.pdf},
year = {2011}
}
@inproceedings{Nugteren2014a,
abstract = {The shift toward parallel processor architectures has made programming and code generation increasingly challenging. To address this programmability challenge, this article presents a technique to fully automat- ically generate efficient and readable code for parallel processors (with a focus on GPUs). This is made possible by combining algorithmic skeletons, traditional compilation, and “algorithmic species,” a classifi- cation of program code. Compilation starts by automatically annotating C code with class information (the algorithmic species). This code is then fed into the skeleton-based source-to-source compiler BONES to generate CUDA code. To generate efficient code, BONES also performs optimizations including host-accelerator transfer optimization and kernel fusion. This results in a unique approach, integrating a skeleton-based compiler for the first time into an automated flow. The benefits are demonstrated experimentally for PolyBench GPU kernels, showing geometric mean speed-ups of 1.4× and 2.4× compared to PPCG and PAR4ALL,and forfive Rodinia GPU benchmarks, showing a gap of only 1.2× compared to hand-optimized code.},
annote = {Sourfce-to-source Ruby script for transforming C to CUDA using Skeletons and source annotations.

Nice quote: "several "optimizations" within skeletons are not permutations of the original code."

Cited by 0.},
author = {Nugteren, C. and Corporaal, H.},
booktitle = {TACO},
doi = {10.1145/2665079},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Nugteren, Corporaal - Bones An Automatic Skeleton-Based C-to-CUDA Compiler for GPUs.pdf:pdf},
title = {{Bones: An Automatic Skeleton-Based C-to-CUDA Compiler for GPUs}},
year = {2014}
}
@misc{Karampatsis2016,
annote = {NULL},
author = {Karampatsis, R. M.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Karampatsis - Comparing Large Scale Language Models for Source Code.pdf:pdf},
title = {{Comparing Large Scale Language Models for Source Code}},
year = {2016}
}
@misc{Meyers2014,
annote = {NULL},
author = {Meyers, Scott},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Meyers - C Type Deduction and Why You Care.pdf:pdf},
title = {{C++ Type Deduction and Why You Care}},
year = {2014}
}
@inproceedings{Madsen,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Madsen, Magnus and Yee, Ming-ho and Lhotak, Ondrej},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Madsen, Yee, Lhotak - From Datalog to FLIX A Declarative Language for Fixed Points on Lattices.pdf:pdf},
isbn = {9781450342612},
keywords = {datalog,logic programming,static analysis},
title = {{From Datalog to FLIX: A Declarative Language for Fixed Points on Lattices}},
year = {2016}
}
@article{Eager2007,
abstract = {It would be wonderful if we could write programs that were guaranteed to work correctly and never needed to be debugged. Until that halcyon day, the normal pro-gramming cycle is going to involve writing a program, compiling it, executing it, and then the (somewhat) dreaded scourge of debugging it. And then repeat until the pro-gram works as expected. $\backslash$n$\backslash$nIt is possible to debug programs by in- serting code that prints values of various in-teresting variables. Indeed, in some situa-tions, such as debugging kernel drivers, this may be the preferred method. There are low-level debuggers that allow you to step through the executable program, instruc-tion by instruction, displaying registers and memory contents in binary. $\backslash$n$\backslash$nBut it is much easier to use a source-lev-el debugger which allows you to step through a program's source, set break- points, print variable values, and perhaps a few other functions such as allowing you to call a function in your program while in the debugger. The problem is how to coordi-nate two completely different programs, the compiler and the debugger, so that the program can be debugged},
annote = {NULL},
author = {Eager, Michael J},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Eager - Introduction to the DWARF debugging format.pdf:pdf},
journal = {Group},
title = {{Introduction to the DWARF debugging format}},
year = {2007}
}
@inproceedings{Melnik2010,
abstract = {Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.},
author = {Melnik, S. and Gubarev, A. and Long, J. J. and Romer, G. and Shivakumar, S. and Tolton, M. and Vassilakis, T.},
booktitle = {VLDB},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Melnik et al. - Dremel Interactive Analysis of Web-Scale Datasets.pdf:pdf},
title = {{Dremel: Interactive Analysis of Web-Scale Datasets}},
year = {2010}
}
@article{Lake,
annote = {NULL},
author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
doi = {10.1126/science.aab3050},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Lake, Salakhutdinov, Tenenbaum - Human-level concept learning through probabilistic program induction.pdf:pdf},
title = {{Human-level concept learning through probabilistic program induction}}
}
@inproceedings{Kruger2003,
abstract = {Nowadays, direct volume rendering via 3D textures has positioned itself as an efficient tool for the display and visual analysis of volumetric scalar fields. It is commonly accepted, that for reasonably sized data sets appropriate quality at interactive rates can be achieved by means of this technique. However, despite these benefits one important issue has received little attention throughout the ongoing discussion of texture based volume rendering: the integration of acceleration techniques to reduce per-fragment operations. In this paper, we address the integration of early ray termination and empty-space skipping into texture based volume rendering on graphical processing units (GPU). Therefore, we describe volume ray-casting on programmable graphics hardware as an alternative to object-order approaches. We exploit the early z-test to terminate fragment processing once sufficient opacity has been accumulated, and to skip empty space along the rays of sight. We demonstrate performance gains up to a factor of 3 for typical renditions of volumetric data sets on the ATI 9700 graphics card.},
annote = {Cited by 801.},
author = {Kruger, J. and Westermann, R.},
booktitle = {VIS},
doi = {10.1109/VISUAL.2003.1250384},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Kruger, Westermann - Acceleration techniques for GPU-based volume rendering.pdf:pdf},
isbn = {0-7803-8120-3},
keywords = {programmable graphics hard-,volume rendering},
title = {{Acceleration techniques for GPU-based volume rendering}},
year = {2003}
}
@inproceedings{Bunel2017a,
abstract = {Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language pro-cessing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transfor-mation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifica-tions to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consider-ation. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (" Hacker's Delight ") programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.},
annote = {From Duplicate 1 (Learning to Superoptimize Programs - Bunel, R.; Desmaison, A.; Kumar, M. P.; Torr, P. H. S.)

Review: https://openreview.net/forum?id=BJnqmRAA{\&}noteId=BJnqmRAA},
archivePrefix = {arXiv},
arxivId = {1611.01787},
author = {Bunel, R. and Desmaison, A. and Kumar, M. P. and Torr, P. H. S.},
booktitle = {ICLR},
eprint = {1611.01787},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Bunel et al. - Learning to Superoptimize Programs.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Learning to Superoptimize Programs}},
year = {2017}
}
@techreport{Padua1993,
annote = {NULL},
author = {Padua, David a and Eigenmann, Rudolf and Hoeflinger, Jay and Petersen, Paul and Tu, Peng and Weatherford, Stephen and Faigin, Keith},
booktitle = {CSRD Rept. No. 1306.},
doi = {10.1.1.31.6716},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Padua et al. - Polaris A new-generation parallelizing compiler for MPPs.pdf:pdf},
institution = {Univ. of Illinois at Urbana-Champaign},
number = {1306},
publisher = {Citeseer},
title = {{Polaris: A new-generation parallelizing compiler for MPPs}},
year = {1993}
}
@article{Jozefowicz2016a,
abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
annote = {CNNs and LSTMs for large scale language modeling. They release the best models to the community.},
author = {Jozefowicz, R. and Vinyals, O. and Schuster, M. and Shazeer, N. and Wu, Y.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Jozefowicz et al. - Exploring the Limits of Language Modeling.pdf:pdf},
journal = {arXiv:1602.02410},
title = {{Exploring the Limits of Language Modeling}},
year = {2016}
}
@article{Schadd2008,
author = {Schadd, M. P. D. and Winands, M. H. M. and van den Herik, H. J. and Chaslot, G. M. J. B. and Uiterwijk, J. W. H. M.},
doi = {10.1007/978-3-540-87608-3_1},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Schadd et al. - Single-Player Monte-Carlo Tree Search.pdf:pdf},
title = {{Single-Player Monte-Carlo Tree Search}},
url = {http://link.springer.com/10.1007/978-3-540-87608-3{\_}1},
year = {2008}
}
@inproceedings{Ipek2005,
abstract = {Accurately modeling and predicting performance for large- scale applications becomes increasingly difficult as system complexity scales dramatically. Analytic predictive models are useful, but are dif- ficult to construct, usually limited in scope, and often fail to capture subtle interactions between architecture and software. In contrast, we employ multilayer neural networks trained on input data from execu- tions on the target platform. This approach is useful for predicting many aspects of performance, and it captures full system complexity. Our mod- els are developed automatically from the training input set, avoiding the difficult and potentially error-prone process required to develop analytic models. This study focuses on the high-performance, parallel applica- tion SMG2000, a much studied code whose variations in execution times are still not well understood. Our model predicts performance on two large-scale parallel platforms within 5{\%}-7{\%} error across a large, multi- dimensional parameter space.},
annote = {NULL},
author = {Ipek, E and Supinski, B R De and Schulz, M},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Ipek, Supinski, Schulz - An Approach to Performance Prediction for Parallel Applications.pdf:pdf},
publisher = {Springer},
title = {{An Approach to Performance Prediction for Parallel Applications}},
year = {2005}
}
@phdthesis{Dastgeer2011,
abstract = {In this thesis, we address issues associated with programming modern heterogeneous sys- tems while focusing on a special kind of heterogeneous systems that include multicore CPUs and one or more GPUs, called GPU-based systems. We leverage the skeleton pro- gramming approach to achieve high level abstraction for efficient and portable program- ming of these GPU-based systems and present our work on SkePU which is a skeleton library for these systems. We first extend the existing SkePU library with a two-dimensional (2D) data type and accordingly generalized skeleton operations, and implement several new applications that utilize these new features. Furthermore, we consider the algorithmic choice present in SkePU and implement support to specify and automatically optimize the algorithmic choice for a skeleton call, on a given platform. To show how to achieve high performance, we provide a case-study on an optimized GPU-based skeleton implementation for 2D convolution computations and introduce two metrics to maximize resource utilization on a GPU. By devising a mechanism to au- tomatically calculate these two metrics, performance can be retained while porting an application from one GPU architecture to another. Another contribution of this thesis is the implementation of runtime support for task par- allelism in SkePU. This is achieved by integration with the StarPU runtime system. By this integration, support for dynamic scheduling and load balancing for SkePU skeleton programs is achieved. Furthermore, a capability to do hybrid execution by parallel exe- cution on all available CPUs and GPUs in a system, even for a single skeleton invocation, is developed. SkePU initially supported only data-parallel skeletons. The first task-parallel skeleton (farm) in SkePU is implemented with support for performance-aware scheduling and hierarchical parallel execution by enabling all data parallel skeletons to be usable as tasks inside the farm construct. Experimental evaluations are carried out and presented for algorithmic selection, perfor- mance portability, dynamic scheduling and hybrid execution aspects of our work.},
annote = {NULL},
author = {Dastgeer, U.},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Dastgeer - Skeleton Programming for Heterogeneous GPU-based Systems.pdf:pdf},
isbn = {9789173930666},
title = {{Skeleton Programming for Heterogeneous GPU-based Systems}},
year = {2011}
}
@incollection{Aarts2006,
annote = {Cited by 142.},
author = {Aarts, E. and Encarna{\c{c}}{\~{a}}o, J.},
booktitle = {True Visions: The Emergence of Ambient Intelligence},
doi = {10.1007/978-3-540-28974-6_1},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Aarts, Encarna{\c{c}}{\~{a}}o - Into ambient intelligence.pdf:pdf},
isbn = {3540289720},
publisher = {Springer},
title = {{Into ambient intelligence}},
year = {2006}
}
@inproceedings{White2015a,
abstract = {Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.},
annote = {NULL},
author = {White, M. and Vendome, C. and Linares-Vasquez, M. and Poshyvanyk, D.},
booktitle = {MSR},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - White et al. - Toward Deep Learning Software Repositories.pdf:pdf},
keywords = {Software repositories,deep learning,machine learning,n-grams,neural networks,software language models},
title = {{Toward Deep Learning Software Repositories}},
year = {2015}
}
@inproceedings{Yaneva2017a,
abstract = {Embedded software is found everywhere from our highly visible mobile devices to the confines of our car in the form of smart sensors. Embedded software companies are under huge pressure to produce safe applications that limit risks, and testing is absolutely critical to alleviate concerns regarding safety and user privacy. This requires using large test suites throughout the development process, increasing time-to-market and ultimately hindering competitivity. Speeding up test execution is, therefore, of paramount impor-tance for embedded software developers. This is traditionally achieved by running, in parallel, multiple tests on large-scale clusters of com-puters. However, this approach is costly in terms of infrastructure maintenance and energy consumed, and is at times inconvenient as developers have to wait for their tests to be scheduled on a shared resource. We propose to look at exploiting GPUs (Graphics Processing Units) for running embedded software testing. GPUs are readily available in most computers and offer tremendous amounts of paral-lelism, making them an ideal target for embedded software testing. In this paper, we demonstrate, for the first time, how test execu-tions of embedded C programs can be automatically performed on a GPU, without involving the end user. We take a compiler-assisted approach which automatically compiles the C program into GPU kernels for parallel execution of the input tests. Using this tech-nique, we achieve an average speedup of 16× when compared to CPU execution of input tests across nine programs from an industry standard embedded benchmark suite.},
author = {Yaneva, V. and Rajan, A. and Dubach, C.},
booktitle = {ISSTA},
doi = {10.1145/3092703.3092720},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Yaneva, Rajan, Dubach - Compiler-assisted test acceleration on GPUs for embedded software(2).pdf:pdf},
isbn = {9781450350761},
keywords = {auto-,compilers,embedded software,functional testing,gpus},
title = {{Compiler-assisted test acceleration on GPUs for embedded software}},
year = {2017}
}
@article{Misra2010,
abstract = {This article presents a comprehensive overview of the hardware realizations of artificial neural network (ANN) models, known as hardware neural networks (HNN), appearing in academic studies as prototypes as well as in commercial use. HNN research has witnessed a steady progress for more than last two decades, though commercial adoption of the technology has been relatively slower. We study the overall progress in the field across all major ANN models, hardware design approaches, and applications. We outline underlying design approaches for mapping an ANN model onto a compact, reliable, and energy efficient hardware entailing computation and communication and survey a wide range of illustrative examples. Chip design approaches (digital, analog, hybrid, and FPGA based) at neuronal level and as neurochips realizing complete ANN models are studied. We specifically discuss, in detail, neuromorphic designs including spiking neural network hardware, cellular neural network implementations, reconfigurable FPGA based implementations, in particular, for stochastic ANN models, and optical implementations. Parallel digital implementations employing bit-slice, systolic, and SIMD architectures, implementations for associative neural memories, and RAM based implementations are also outlined. We trace the recent trends and explore potential future research directions. {\textcopyright} 2010 Elsevier B.V.},
annote = {NULL},
author = {Misra, J. and Saha, I.},
doi = {10.1016/j.neucom.2010.03.021},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Misra, Saha - Artificial neural networks in hardware A survey of two decades of progress.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Analog neural design,CNN implementation,Digital neural design,FPGA based ANN implementation,Hardware neural network,Hybrid neural design,Neurochip,Neuromorphic system,Optical neural network,Parallel neural architecture,RAM based implementation},
number = {1-3},
pmid = {18263386},
title = {{Artificial neural networks in hardware: A survey of two decades of progress}},
volume = {74},
year = {2010}
}
@phdthesis{Fenech,
abstract = {With commodity multicore architectures already prevalent, the microprocessor industry is poised to leap into the manycore era within the next few years. To avail of such machines' multiprocessing capabilities, software developers are increasingly incentivized to parallelize their programs. This trend poses new challenges for the system scheduler, which will now need to consider the parallel characteristics of the respective programs when seeking to maximize the system-level performance of its multiprogram workloads. In this project, we reconcile two orthogonal approaches: work-stealing task-scheduling for efficiently unravelling structured parallelism from each program, and scalability-based processor-partitioning for dynamically optimizing the programs' core allocations within the context of the current multiprogram workload on the manycore machine. We experimentally demonstrate that, for low- to moderate-scalability programs, our multiprogram scheduler can succeed in achieving modest improvements over mainstream thread and task schedulers.},
annote = {NULL},
author = {Fenech, Karl},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Fenech - Scheduling Task-Parallel Programs in a Multiprogram Workload.pdf:pdf},
keywords = {algorithmic skeletons,divide-and-conquer,dynamic optimization,manycore multiprocessing,multiprogram workloads,parallel programming,processor partitioning,scalability,scheduling,structured parallelism},
school = {School of Informatics, University of Edinburgh},
title = {{Scheduling Task-Parallel Programs in a Multiprogram Workload}},
url = {http://dogmamix.com/TPMP/TPMP-065.pdf},
year = {2013}
}
@article{Russ2016,
annote = {NULL},
author = {Russ, B and Mike, M and Steve, H},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Russ, Mike, Steve - GCHQ BOILING FROGS.pdf:pdf},
title = {{GCHQ: BOILING FROGS ?}},
year = {2016}
}
@phdthesis{Shalev-shwartz2007,
annote = {NULL},
author = {Shalev-Shwartz, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Shalev-Shwartz - Online Learning Theory, Algorithms, and Applications.pdf:pdf},
number = {July},
title = {{Online Learning: Theory, Algorithms, and Applications}},
year = {2007}
}
@article{Ben-Nun,
abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human-and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data-and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that with a single RNN architecture and pre-trained fixed embeddings, inst2vec outperforms specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.},
author = {Ben-Nun, T. and Jakobovits, A. S. and Hoefler, T.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ben-Nun, Jakobovits, Hoefler - Neural Code Comprehension A Learnable Representation of Code Semantics.pdf:pdf},
journal = {arXiv:1806.07336},
keywords = {TO{\_}COMPARE{\_}AGAINST},
mendeley-tags = {TO{\_}COMPARE{\_}AGAINST},
title = {{Neural Code Comprehension: A Learnable Representation of Code Semantics}},
year = {2018}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
annote = {NULL},
author = {Hochreiter, S. and Schmidhuber, J.},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Hochreiter, Schmidhuber - Long Short-Term Memory.pdf:pdf},
journal = {Neural Computation},
number = {8},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@inproceedings{Diehlb,
author = {Diehl, L.},
booktitle = {AAIP},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Diehl - Verified Stack-Based Genetic Programming via Dependent Types.pdf:pdf},
title = {{Verified Stack-Based Genetic Programming via Dependent Types}},
year = {2011}
}
@article{Graefe2016,
annote = {NULL},
author = {Graefe, A. and Haim, M. and Haarmann, B. and Brosius, H.-B.},
doi = {10.1177/1464884916641269},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Graefe et al. - Readers perception of computer-generated news Credibility, expertise, and readability.pdf:pdf},
isbn = {4989218094},
issn = {1464-8849},
journal = {Journalism},
number = {February},
title = {{Readers perception of computer-generated news: Credibility, expertise, and readability}},
url = {http://jou.sagepub.com/cgi/doi/10.1177/1464884916641269},
year = {2016}
}
@techreport{Kaashoek2010,
abstract = {MapReduce is a programming model for data-parallel programs originally intended for data centers. MapRe- duce simplifies parallel programming, hiding synchro- nization and task management. These properties make it a promising programming model for future},
annote = {NULL},
author = {Kaashoek, F and Morris, R},
doi = {10.1.1.186.5309},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Kaashoek, Morris - Optimizing MapReduce for Multicore Architectures.pdf:pdf},
title = {{Optimizing MapReduce for Multicore Architectures}},
url = {http://dspace.mit.edu/handle/1721.1/54692{\%}5Cnpapers2://publication/uuid/2CFFAFE2-607F-4F70-A260-0A2D761EC027},
year = {2010}
}
@inproceedings{Andrychowicz2016a,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, M. and Denil, M. and Gomez, S. and Hoffman, M. W. and Pfau, D. and Schaul, T. and de Freitas, N.},
booktitle = {NIPS},
doi = {10.1007/s10115-008-0151-5},
eprint = {1606.04474},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Andrychowicz et al. - Learning to Learn by Gradient Descent by Gradient Descent(2).pdf:pdf},
isbn = {1011500801515},
issn = {0219-1377},
pmid = {207591},
title = {{Learning to Learn by Gradient Descent by Gradient Descent}},
year = {2016}
}
@inproceedings{Martino2014,
annote = {NULL},
author = {Martino, B Di and Esposito, A and Barbato, A},
booktitle = {IDCS},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Martino, Esposito, Barbato - High Performance Cloud A MapReduce and GPGPU Based Hybrid Approach.pdf:pdf},
publisher = {Springer},
title = {{High Performance Cloud: A MapReduce and GPGPU Based Hybrid Approach}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-11692-1{\_}6},
year = {2014}
}
@misc{Johnson2016,
annote = {CGO'17 Best paper nominees},
author = {Johnson, T. and Amini, M. and Li, D.},
booktitle = {LLVM Project Blog},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Johnson, Amini, Li - ThinLTO Scalable and Incremental LTO.pdf:pdf},
title = {{ThinLTO: Scalable and Incremental LTO}},
year = {2016}
}
@inproceedings{Lam2004,
abstract = {This paper proposes an algorithm that improves the local- ity of a loop nest by transforming the code via interchange, reversal, skewing and tiling. The loop transformation rrl- gorithm is based on two concepts: a mathematical for- mulation of reuse and locality, and a loop transformation theory that unifies the various transforms as unimodular matrix tmnsfonnations. The algorithm haa been implemented in the SUIF (Stan- ford University Intermediate Format) compiler, and is suc- cessful in optimizing codes such as matrix multiplica- tion, successive over-relaxation (SOR), LU decomposition without pivoting, and Givens QR factorization. Perfor- mance evaluation indicates that locatity optimization is es- pecially crucial for scaling up the performance of parallel code.},
annote = {Cited by 1462.},
author = {Wolf, M. E. and Lam, M. S.},
booktitle = {PLDI},
doi = {10.1145/989393.989437},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Wolf, Lam - A data locality optimizing algorithm.pdf:pdf},
isbn = {0897914287},
issn = {03621340},
number = {6},
publisher = {ACM},
title = {{A data locality optimizing algorithm}},
volume = {26},
year = {1991}
}
@inproceedings{Reble2015,
annote = {NULL},
author = {Reble, Pablo and Lankes, Stefan and Fischer, Fabian and M{\"{u}}ller, Matthias S},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Reble et al. - Effective Communication for a System of Cluster-on-a-Chip Processors.pdf:pdf},
isbn = {9781450334044},
keywords = {cluster-on-a-chip,emulation of,low-level communication,message passing,on-chip interconnect},
title = {{Effective Communication for a System of Cluster-on-a-Chip Processors}},
year = {2015}
}
@inproceedings{Kisuki,
annote = {Cited by 68.},
author = {Kisuki, T. and Bohrweg, N. and Beaulieu, C. D.},
booktitle = {High Performance Computing},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Kisuki, Bohrweg, Beaulieu - A Feasibility Study in Iterative Compilation.pdf:pdf},
publisher = {Springer},
title = {{A Feasibility Study in Iterative Compilation}},
year = {1999}
}
@inproceedings{Walsh1996,
abstract = {The Paragen system is a new technique for the automatic conversion of sequential programs into functionally equivalent parallel programs. This technique utilizes GP to generate a highly paral- lel program from an original sequential program, while preserving functionality. We show that a genetic search avoids the complexities introduced by standard data dependency techniques and can also introduce further efficiencies by automati- cally reordering program statements. This paper gives a brief introduction to the problem of au- toparallelisation followed by a discussion of the design and implementation issues of the system. Results demonstrate the effectiveness of the Para- gen system by the automatic conversion of a num- ber of complex program segments.},
annote = {NULL},
author = {Walsh, Paul and Ryan, Conor},
booktitle = {GECCO},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Walsh, Ryan - Paragen A Novel Technique for the Autoparallelisation of Sequential Programs using Genetic Programming.pdf:pdf},
keywords = {genetic algorithms,genetic programming},
publisher = {MIT Press},
title = {{Paragen: A Novel Technique for the Autoparallelisation of Sequential Programs using Genetic Programming}},
url = {http://cognet.mit.edu/library/books/view?isbn=0262611279},
year = {1996}
}
@inproceedings{Bocchino2009,
abstract = {Today's shared-memory parallel programming models are complex and error-prone. While many parallel programs are intended to be deterministic, unanticipated thread interleavings can lead to subtle bugs and nondeterministic semantics. In this paper, we demonstrate that a practical type and effect system can simplify parallel programming by guaranteeing deterministic semantics with modular, compile-time type checking even in a rich, concurrent object-oriented language such as Java. We describe an object-oriented type and effect system that provides several new capabilities over previous systems for expressing deterministic parallel algorithms. We also describe a language called Deterministic Parallel Java (DPJ) that incorporates the new type system features, and we show that a core subset of DPJ is sound. We describe an experimental validation showing that DPJ can express a wide range of realistic parallel programs; that the new type system features are useful for such programs; and that the parallel programs exhibit good performance gains (coming close to or beating equivalent, nondeterministic multithreaded programs where those are available).},
annote = {NULL},
author = {Bocchino, R. L. and Vakilian, M. and Adve, V. S. and Dig, D. and Adve, S. V. and Heumann, S. and Komuravelli, R. and Overbey, J. and Simmons, P. and Sung, H.},
booktitle = {OOPSLA},
doi = {10.1145/1639949.1640097},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Bocchino et al. - A type and effect system for deterministic parallel Java.pdf:pdf},
isbn = {9781605587660},
issn = {03621340},
keywords = {commutativity,determinism,deterministic parallelism,effect systems,effects},
publisher = {ACM},
title = {{A type and effect system for deterministic parallel Java}},
year = {2009}
}
@inproceedings{Yan2017,
author = {Yan, G. and Lu, J. and Shu, Z. and Kucuk, Y.},
booktitle = {PAC},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Yan et al. - ExploitMeter Combining Fuzzing with Machine Learning for Automated Evaluation of Software Exploitability.pdf:pdf},
publisher = {IEEE},
title = {{ExploitMeter: Combining Fuzzing with Machine Learning for Automated Evaluation of Software Exploitability}},
year = {2017}
}
@inproceedings{Wong2010,
author = {Wong, H. and Papadopoulou, M. and Sadooghi-Alvandi, M. and Moshovos, A.},
booktitle = {ISPASS},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Wong et al. - Demystifying GPU Microarchitecture through Microbenchmarking.pdf:pdf},
title = {{Demystifying GPU Microarchitecture through Microbenchmarking}},
year = {2010}
}
@article{Bunkute2014,
abstract = {A protein's isoelectric point or pI corresponds to the solu- tion pH at which its net surface charge is zero. Since the earliest days of solution biochemistry, the pI has been recorded and report- ed, and thus literature reports of pI abound. The protein isoelectric point database (PIP-DB) has collected and collated this legacy data to provide an increasingly comprehensive database for comparison and benchmarking purposes. A web application has been developed to warehouse this database and provide public access to this unique resource. PIPD is a web-enabled SQL database with an html GUI front-end. PIPD is fully searchable across a range of characteristics.},
annote = {NULL},
author = {Bunkute, E. and Cummins, C. and Crofts, F. and Bunce, G. and Nabney, I. T. and Flower, D. R.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bunkute et al. - PIP-DB The Protein Isoelectric Point Database.pdf:pdf},
journal = {Bioinformatics},
publisher = {Oxford University Press},
title = {{PIP-DB: The Protein Isoelectric Point Database}},
year = {2014}
}
@inproceedings{Neelakantan2016,
abstract = {Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.04834},
author = {Neelakantan, A. and Le, Q. V. and Sutskever, I.},
booktitle = {ICLR},
doi = {10.1016/j.physa.2015.05.013},
eprint = {1511.04834},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Neelakantan, Le, Sutskever - Neural Programmer Inducing Latent Programs with Gradient Descent.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Neural Programmer: Inducing Latent Programs with Gradient Descent}},
url = {http://arxiv.org/abs/1511.04834},
year = {2016}
}
@article{Ioannidis2018,
abstract = {The era of data deluge has sparked the interest in graph-based learning methods in a number of disciplines such as sociology, biology, neuroscience, or engineering. In this paper, we introduce a graph recurrent neural network (GRNN) for scalable semi-supervised learning from multi-relational data. Key aspects of the novel GRNN architecture are the use of multi-relational graphs, the dynamic adaptation to the different relations via learnable weights, and the consideration of graph-based regularizers to promote smoothness and alleviate over-parametrization. Our ultimate goal is to design a powerful learning architecture able to: discover complex and highly non-linear data associations, combine (and select) multiple types of relations, and scale gracefully with respect to the size of the graph. Numerical tests with real data sets corroborate the design goals and illustrate the performance gains relative to competing alternatives.},
archivePrefix = {arXiv},
arxivId = {1811.02061},
author = {Ioannidis, Vassilis N. and Marques, Antonio G. and Giannakis, Georgios B.},
doi = {10.1109/CIT/IUCC/DASC/PICOM.2015.235},
eprint = {1811.02061},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ioannidis, Marques, Giannakis - A Recurrent Graph Neural Network for Multi-Relational Data.pdf:pdf},
isbn = {0092861512},
issn = {2168-4790},
title = {{A Recurrent Graph Neural Network for Multi-Relational Data}},
url = {http://arxiv.org/abs/1811.02061},
year = {2018}
}
@article{Vinyals,
annote = {Cited by 312.},
author = {Vinyals, O. and Toshev, A. and Bengio, S. and Erhan, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Vinyals et al. - Show and Tell A Neural Image Caption Generator.pdf:pdf},
journal = {CVPR},
publisher = {IEEE},
title = {{Show and Tell: A Neural Image Caption Generator}},
year = {2015}
}
@article{Wu2016b,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Y. and Schuster, M. and Chen, Z. and Le, Q. V. and Norouzi, M. and Macherey, W. and Krikun, M. and Cao, Y. and Gao, Q. and Macherey, K. and Klingner, J. and Shah, A. and Johnson, M. and Liu, X. and Kaiser, {\L}. and Gouws, S. and Kato, Y. and Kudo, T. and Kazawa, H. and Stevens, K. and Kurian, G. and Patil, N. and Wang, W. and Young, C. and Smith, J. and Riesa, J. and Rudnick, A. and Vinyals, O. and Corrado, G. and Hughes, M. and Dean, J.},
eprint = {1609.08144v2},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Wu et al. - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Anderson2017,
abstract = {Deep Neural Networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. Many different algorithms have been proposed to implement the most computationally expen-sive layers of DNNs. Further, each of these algorithms has a large number of variants, which offer different trade-offs of parallelism, data locality, memory footprint, and execution time. In addition, specific algorithms operate much more efficiently on specialized data layouts and formats. We state the problem of optimal primitive selection in the presence of data format transformations, and show that it is NP-hard by demonstrating an embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP). We propose an analytic solution via a PBQP solver, and evaluate our approach experimentally by optimizing sev-eral popular DNNs using a library of more than 70 DNN primitives, on an embedded platform and a general purpose platform. We show experimentally that significant gains are possible versus the state of the art vendor libraries by us-ing a principled analytic solution to the problem of layout selection in the presence of data format transformations.},
author = {Anderson, A. and Gregg, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Anderson, Gregg - Optimal DNN Primitive Selection with Partitioned Boolean Quadratic Programming.pdf:pdf},
journal = {arXiv:1710.01079},
title = {{Optimal DNN Primitive Selection with Partitioned Boolean Quadratic Programming}},
year = {2017}
}
@inproceedings{Katta2014,
abstract = {Software-DefinedNetworking (SDN) enables fine-grained poli- cies for firewalls, load balancers, routers, traffic monitoring, and other functionality. While Ternary Content Address- able Memory (TCAM) enables OpenFlow switches to pro- cess packets at high speed based on multiple header fields, today's commodity switches support just thousands to tens of thousands of rules. To realize the potential of SDN on this hardware, we need efficient ways to support the abstraction of a switch with arbitrarily large rule tables. To do so, we de- fine a hardware-software hybrid switch design that relies on rule caching to provide large rule tables at low cost. Unlike traditional caching solutions, we neither cache individual rules (to respect rule dependencies) nor compress rules (to preserve the per-rule traffic counts). Instead we“splice”long dependency chains to cache smaller groups of rules while preserving the semantics of the network policy. Our design satisfies four core criteria: (1) elasticity (combining the best of hardware and software switches), (2) transparency (faith- fully supporting native OpenFlow semantics, including traf- fic counters), (3) fine-grained rule caching (placing popular rules in the TCAM, despite dependencies on less-popular rules), and (4) adaptability (to enable incremental changes to the rule caching as the policy changes).},
address = {New York, New York, USA},
annote = {NULL},
author = {Katta, N. and Alipourfard, O. and Rexford, J. and Walker, D.},
booktitle = {HotSDN},
doi = {10.1145/2620728.2620734},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Katta et al. - Infinite CacheFlow in software-defined networks.pdf:pdf},
isbn = {9781450329897},
publisher = {ACM Press},
title = {{Infinite CacheFlow in software-defined networks}},
url = {http://dl.acm.org/citation.cfm?doid=2620728.2620734},
year = {2014}
}
@phdthesis{Hanlon2014,
abstract = {Parallelism has become the principal means of sustaining growth in computational performance but there has been relatively little development in general-purpose computer architectures or programming models that can deal effectively with large amounts of it. A new general-purpose model of parallel computingwould enable standardisation between architectures, high-volume production and software that is portable between different machines, now and as they develop with future technology. There is substantial opportunity to support this in emerging areas of embedded computing, where the problems of sensing, interaction and decision making can exploit large amounts of parallelism. This thesis demonstrates the essential aspects of a scalable general-purpose model of parallel computation by proposing a Universal Parallel Architecture (UPA), based on a highly-connected communication network, and a high-level parallel programming language for it called sire that can be compiled using simple techniques. The design of sire combines the essential capabilities of shared- memory programming with the benefits of message passing to support a range of programming paradigms and to provide powerful capabilities for abstraction to build and compose subroutines and data structures in a distributed context. The design also enables program code to be distributed at run time to reuse memory and for processor allocation to be dealt with during compilation so that the overheads of using distributed parallelism are minimal. To evaluate whether the UPA is practical to build, a high-level implementation model using current technologies is described. It demonstrates that the cost of generality is relatively small; for a system with 4,096 processors, an overall investment of around 25{\%} of the system is required for the communication network. Executing on specific UPA implementations, sire's primitives for parallelism, communication and abstraction incur minimal overheads, demonstrating its close correspondence to the UPA and its scalability. Furthermore, as well as executing highly-parallel programs, the UPA can support sequential programming techniques by emulating large memories, allowing general sequential programs to be executed with a factor of 2 to 3 overhead when compared to contemporary sequential machines.},
annote = {NULL},
author = {Hanlon, JW},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Hanlon - Scalable abstractions for general-purpose parallel computation.pdf:pdf},
title = {{Scalable abstractions for general-purpose parallel computation}},
url = {http://jwhanlon.com/docs/thesis-print.pdf},
year = {2014}
}
@inproceedings{Rubin2014,
annote = {NULL},
author = {Rubin, Norm},
booktitle = {CGO},
doi = {10.1145/2555243.2558891},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Rubin - Heterogeneous computing - What Does It Mean for Compiler Research.pdf:pdf},
isbn = {9781450326568},
publisher = {IEEE},
title = {{Heterogeneous computing - What Does It Mean for Compiler Research}},
url = {http://dl.acm.org/citation.cfm?doid=2555243.2558891},
year = {2014}
}
@misc{Stirling,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 27. Random variables, Expectation, and Variance.pdf:pdf},
number = {Chapter 7},
title = {{27. Random variables, Expectation, and Variance}},
volume = {7},
year = {2015}
}
@incollection{Yearning-draftd,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapter 40-43).pdf:pdf},
title = {{Machine Learning Yearning (Chapter 40-43)}},
year = {2018}
}
@incollection{Nga,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 31-33).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 31-33)}},
year = {2018}
}
@article{Cawley2010,
abstract = {Model selection strategies for machine learning algorithms typically involve$\backslash$nthe numerical optimisation of an appropriate model selection criterion, often$\backslash$nbased on an estimator of generalisation performance, such as k-fold $\backslash$ncross-validation. The error of such an estimator can be broken down into bias $\backslash$nand variance components. While unbiasedness is often cited as a beneficial $\backslash$nquality of a model selection criterion, we demonstrate that a low variance is $\backslash$nat least as important, as a non-negligible variance introduces the potential $\backslash$nfor over-fitting in model selection as well as in training the model. While $\backslash$nthis observation is in hindsight perhaps rather obvious, the degradation in $\backslash$nperformance due to over-fitting the model selection criterion can be $\backslash$nsurprisingly large, an observation that appears to have received little $\backslash$nattention in the machine learning literature to date. In this paper, we show $\backslash$nthat the effects of this form of over-fitting are often of comparable $\backslash$nmagnitude to differences in performance between learning algorithms, and thus $\backslash$ncannot be ignored in empirical evaluation. Furthermore, we show that some $\backslash$ncommon performance evaluation practices are susceptible to a form of selection $\backslash$nbias as a result of this form of over-fitting and hence are unreliable. We $\backslash$ndiscuss methods to avoid over-fitting in model selection and subsequent $\backslash$nselection bias in performance evaluation, which we hope will be incorporated $\backslash$ninto best practice. While this study concentrates on cross-validation based $\backslash$nmodel selection, the findings are quite general and apply to any model $\backslash$nselection practice involving the optimisation of a model selection criterion $\backslash$nevaluated over a finite sample of data, including maximisation of the Bayesian $\backslash$nevidence and optimisation of performance bounds.},
annote = {NULL},
author = {Cawley, G. C. and Talbot, N L. C.},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Cawley, Talbot - On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {JMLR},
keywords = {bias-variance trade-off,model selection,over-,performance evaluation,selection bias},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
url = {http://jmlr.csail.mit.edu/papers/v11/cawley10a.html{\%}5Cnhttp://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf},
volume = {11},
year = {2010}
}
@misc{AlanBundy2014,
abstract = {Here are some guidelines on being supervised by me for either a PhD, MPhil, MSc or UG project. They include some obligations for me and some for you; I will do my bit if you do yours.},
annote = {A short list of general good advice for research students, which lends itself to periodic review in order to keep points fresh in the mind.},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bundy - How to be My Student.pdf:pdf},
title = {{How to be My Student}},
year = {2014}
}
@inproceedings{Workshop,
annote = {NULL},
author = {Workshop, International and Sciences, Environmental and York, New and Workshop, Qsar and Workshop, The International},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Workshop et al. - A Message from the Workshop Organizers.pdf:pdf},
isbn = {9781595938787},
title = {{A Message from the Workshop Organizers}},
year = {2015}
}
@article{Bowman2015,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
author = {Bowman, S. R. and Vilnis, L. and Vinyals, O. and Dai, A. M. and Jozefowicz, R. and Bengio, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Bowman et al. - Generating Sentences from a Continuous Space.pdf:pdf},
journal = {arXiv:1511.06349},
title = {{Generating Sentences from a Continuous Space}},
year = {2015}
}
@inproceedings{Koc2017,
author = {Koc, U. and Saadatpanah, P. and Foster, J. S. and Porter, A. A.},
booktitle = {MAPL},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Koc et al. - Learning a Classifier for False Positive Error Reports Emitted by Static Code Analysis Tools.pdf:pdf},
keywords = {classifier,long short-term memories,naive bayes,program slicing,static code analysis},
title = {{Learning a Classifier for False Positive Error Reports Emitted by Static Code Analysis Tools}},
year = {2017}
}
@inproceedings{Sedaghati2015,
annote = {NULL},
author = {Sedaghati, Naser and Parthasarathy, Srinivasan},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Sedaghati, Parthasarathy - Characterizing Dataset Dependence for Sparse Matrix-Vector Multiplication on GPUs.pdf:pdf},
isbn = {9781450334051},
title = {{Characterizing Dataset Dependence for Sparse Matrix-Vector Multiplication on GPUs}},
year = {2015}
}
@inproceedings{Girshick2014,
annote = {NULL},
author = {Girshick, R. and Donahue, J. and Darrell, T. and Malik, J.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.81},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Girshick et al. - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
pmid = {26656583},
publisher = {IEEE},
title = {{Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}},
year = {2014}
}
@inproceedings{Voss2000,
annote = {ADAPT is a dynamic optimisation framework, which decouples the process of the dynamic compilation of variants from the process of selecting these variants at runtime. This decoupling is achieved by executing a separate optimizer thread that requests optimisation services on a remote machine, which generates code variants.




An undeniably novel approach and a solid paper, but it is dated and somewhat impractical.},
author = {Voss, Michael J and Eigenmann, Rudolf},
booktitle = {Parallel Processing},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Voss, Eigenmann - ADAPT Automated De-Coupled Adaptive Program Transformation.pdf:pdf},
publisher = {IEEE},
title = {{ADAPT: Automated De-Coupled Adaptive Program Transformation}},
year = {2000}
}
@inproceedings{Verplaetse2000,
abstract = {In the process of designing complex chips and systems, the use of benchmark designs is often necessary. However, the existing benchmark suites are not sufficient for the evaluation of new architectures and EDA tools; synthetic benchmark circuits are a viable alternative. In this paper, a systematic approach for the generation and evaluation of synthetic benchmark circuits is presented. A number of existing benchmark generation methods are examined using direct validation of size and topological parameters. This exposes certain features and drawbacks of the different methods},
annote = {NULL},
author = {Verplaetse, P. and Campenhout, J. Van and Stroobandt, D.},
booktitle = {ISCAS},
doi = {10.1109/ISCAS.2000.858726},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Verplaetse, Campenhout, Stroobandt - On synthetic benchmark generation methods.pdf:pdf},
isbn = {0-7803-5482-6},
issn = {02714310},
number = {FEBRUARY},
title = {{On synthetic benchmark generation methods}},
volume = {4},
year = {2000}
}
@misc{UniversityofEdinburgh2014,
annote = {NULL},
author = {{University of Edinburgh}},
doi = {10.1145/365719.366426},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 2. Compiling techniques.pdf:pdf},
issn = {00010782},
title = {{2. Compiling techniques}},
volume = {9},
year = {2015}
}
@inproceedings{Voss2001,
abstract = {Compile-time optimization is often limited by a lack of tar- get machine and input data set knowledge. Without this information, compilers may be forced to make conservative assumptions to preserve correctness and to avoid perfor- mance degradation. In order to cope with this lack of in- formation at compile-time, adaptive and dynamic systems can be used to perform optimization at runtime when com- plete knowledge of input and machine parameters is avail- able. This paper presents a compiler-supported high-level adaptive optimization system. Users describe, in a domain specific language, optimizations performed by stand-alone optimization tools and backend compiler flags, as well as heuristics for applying these optimizations dynamically at runtime. The ADAPT compiler reads these descriptions and generates application-specific runtime systems to ap- ply the heuristics. To facilitate the usage of existing tools and compilers, overheads are minimized by decoupling op- timization from execution. Our system, ADAPT, supports a range of paradigms proposed recently, including dynamic compilation, parameterization and runtime sampling. We demonstrate our system by applying several optimization techniques to a suite of benchmarks on two target machines. ADAPT is shown to consistently outperform statically gen- erated executables, improving performance by as much as 70{\%}.},
annote = {ADAPT allows programmers to specify optimisations and heuristics which are executed dynamically.},
author = {Voss, Michael J and Eigenmann, Rudolf},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Voss, Eigenmann - High-Level Adaptive Program Optimization with ADAPT.pdf:pdf},
isbn = {1581133464},
publisher = {ACM},
title = {{High-Level Adaptive Program Optimization with ADAPT}},
year = {2001}
}
@inproceedings{Bilmes1997,
address = {New York, NY, USA},
annote = {The paper describes a coding style and guidelines for writing high performance C code, with a useful checklist of optimisations, i.e. Use local variables to explicitly remove false dependencies. They then write a code generator and parameter search engine for automatically producing this high-performance code.

It's a nice paper, and has plenty of useful optimisation advice.

Cited by 540.},
author = {Bilmes, J. and Asanovic, K. and Chin, C. and Demmel, J.},
booktitle = {SC},
doi = {10.1145/263580.263662},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Bilmes et al. - Optimizing Matrix Multiply Using PHiPAC A Portable, High-performance, ANSI C Coding Methodology.pdf:pdf},
publisher = {ACM},
title = {{Optimizing Matrix Multiply Using PHiPAC: A Portable, High-performance, ANSI C Coding Methodology}},
url = {http://doi.acm.org/10.1145/263580.263662},
year = {1997}
}
@inproceedings{Liu2014a,
abstract = {General sparse matrix-matrix multiplication (SpGEMM) is a fundamental building block for numerous applications such as algebraic multigrid method, breadth first search and shortest path problem. Compared to other sparse BLAS routines, an efficient parallel SpGEMM algorithm has to handle extra irregularity from three aspects: (1) the number of the nonzero entries in the result sparse matrix is unknown in advance, (2) very expensive parallel insert operations at random positions in the result sparse matrix dominate the execution time, and (3) load balancing must account for sparse data in both input matrices. Recent work on GPU SpGEMM has demonstrated rather good both time and space complexity, but works best for fairly regular matrices. In this work we present a GPU SpGEMM algorithm that particularly focuses on the above three problems. Memory pre-allocation for the result matrix is organized by a hybrid method that saves a large amount of global memory space and efficiently utilizes the very limited on-chip scratchpad memory. Parallel insert operations of the nonzero entries are implemented through the GPU merge path algorithm that is experimentally found to be the fastest GPU merge approach. Load balancing builds on the number of the necessary arithmetic operations on the nonzero entries and is guaranteed in all stages. Compared with the state-of-the-art GPU SpGEMM methods in the CUSPARSE library and the CUSP library and the latest CPU SpGEMM method in the Intel Math Kernel Library, our approach delivers excellent absolute performance and relative speedups on a benchmark suite composed of 23 matrices with diverse sparsity structures.},
annote = {NULL},
author = {Liu, W. and Vinter, B.},
booktitle = {IPDPS},
doi = {10.1109/IPDPS.2014.47},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Liu, Vinter - An efficient GPU general sparse matrix-matrix multiplication for irregular data.pdf:pdf},
isbn = {9780769552071},
issn = {23321237},
keywords = {GPU,linear algebra,matrix multiplication,merging,parallel algorithms,sparse matrices},
title = {{An efficient GPU general sparse matrix-matrix multiplication for irregular data}},
year = {2014}
}
@article{Jaitly2015,
abstract = {Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1511.04868},
author = {Jaitly, N. and Sussillo, D. and Le, Q. V. and Vinyals, O. and Sutskever, I. and Bengio, S.},
eprint = {1511.04868},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Jaitly et al. - A Neural Transducer.pdf:pdf},
journal = {arXiv:1511.04868},
month = {nov},
title = {{A Neural Transducer}},
url = {http://arxiv.org/abs/1511.04868},
year = {2016}
}
@article{Steuwer2013,
abstract = {Application development for modern high-performance systems with Graphics Processing Units (GPUs) relies on low-level programming approaches like CUDA and OpenCL, which leads to complex, lengthy and error-prone programs. In this paper, we present SkelCL – a high-level programming model for systems with multiple GPUs and its implementa- tion as a library on top of OpenCL. SkelCL provides three main enhancements to the OpenCL standard: 1) computations are conveniently expressed using parallel patterns (skeletons); 2) memory management is simplified using parallel container data types; 3) an automatic data (re)distribution mechanism allows for scalability when using multi-GPU systems. We use a real-world example from the field of medical imaging to motivate the design of our programming model and we show how application development using SkelCL is simplified without sacrificing performance: we were able to reduce the code size in our imaging example application by 50{\%} while introducing only a moderate runtime overhead of less than 5{\%}.},
annote = {This paper presents SkelCL for the purposes of medical imaging, in order to reduce code size of a Medical Imaging example program by 50{\%} while decreasing performance by 5{\%}.


The argument is convincing and well made, although the author makes the mistake of associating LOC with "programmer effort" linearly. In actuality, a code base which is twice the size requires *more* than twice the programming effort, due to the increased chance of bugs, and the increasing complexity of the extra code.},
author = {Steuwer, Michel and Gorlatch, Sergei},
doi = {10.1016/j.procs.2013.05.239},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Steuwer, Gorlatch - High-level Programming for Medical Imaging on Multi-GPU Systems Using the SkelCL Library.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Algorithmic Skeletons,Image Reconstruction,LM OSEM Algorithm,Multi-GPU Computing,SkelCL},
month = {jan},
publisher = {Elsevier B.V.},
title = {{High-level Programming for Medical Imaging on Multi-GPU Systems Using the SkelCL Library}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050913003827},
volume = {18},
year = {2013}
}
@incollection{Squyres2014,
abstract = {Open MPI [GFB+04] is an open source software implementation of The Message Passing Interface (MPI) standard. Before the architecture and innards of Open MPI will make any sense, a little background on the MPI standard must be discussed.},
annote = {A relatively low-level few of OpenMPI's architecture and abstractions. The Lessons Learned section (15.3) makes three interesting points about software:


* It's ok to have ugly code in the name of performance, as long as the design and abstraction is sound;
* Don't re-invent needlessly;
* Optimise for the common case.},
author = {Squyres, Jeffrey M},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Squyres - Open MPI.pdf:pdf},
number = {Volume 2},
title = {{Open MPI}},
volume = {2},
year = {2014}
}
@inproceedings{Godefroid2017,
abstract = {Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss (and measure) the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn{\&}fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs.},
author = {Godefroid, P. and Peleg, H. and Singh, R.},
booktitle = {ASE},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Godefroid, Peleg, Singh - Learn{\&}Fuzz Machine Learning for Input Fuzzing.pdf:pdf},
keywords = {Cite{\_}ISSTA18},
mendeley-tags = {Cite{\_}ISSTA18},
title = {{Learn{\&}Fuzz: Machine Learning for Input Fuzzing}},
year = {2017}
}
@book{Han2011,
annote = {NULL},
author = {Han, J. and Kamber, M. and Pei, J.},
publisher = {Elsevier},
title = {{Data mining: concepts and techniques}},
year = {2011}
}
@article{Zandifar,
abstract = {This paper describes the stapl Skeleton Framework, a high- level skeletal approach for parallel programming. This framework ab- stracts the underlying details of data distribution and parallelism from programmers and allows them express parallel programs as a composi- tion of either existing elementary skeletons such as map, map-reduce, scan, zip, butterfly, allreduce, alltoall or user-defined custom skeletons. Skeletons in this framework are defined as parametric data flow graphs, and their compositions are defined in terms of data flow graph compo- sitions. Defining the composition in this manner, allows dependencies between skeletons to be defined in terms of point-to-point dependencies, avoiding unnecessary global synchronizations. In addition, we show a transformation which enables applications written in terms of skeletons to run on more than 100,000 cores. To show the ease of composability and expressivity, we implemented the NAS Integer Sort (IS) and (EP) benchmarks using skeletons and demonstrate comparable performance to the hand-optimized reference implementations.},
annote = {STAPL is a distributed skeleton framework built on top of an adaptive parallelism library of the same name. It targets 100,000+ core computers.},
author = {Zandifar, M. and Thomas, N. and Amato, N. and Rauchwerger, L.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Zandifar et al. - The STAPL Skeleton Framework.pdf:pdf},
title = {{The STAPL Skeleton Framework}}
}
@article{Valiant1990,
abstract = {The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be efficiently implemented in hardware. The author argues that an analogous bridge between software and hardware is required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.},
annote = {NULL},
author = {Valiant, Leslie G},
file = {:Users/cec/Google Drive/Mendeley Library/1990 - Valiant - A bridging model for parallel computation.pdf:pdf},
journal = {Communications of the ACM},
number = {8},
title = {{A bridging model for parallel computation}},
volume = {33},
year = {1990}
}
@article{Cvitkovic2018a,
abstract = {A major challenge when using techniques from Natural Language Processing for supervised learning on computer program source code is that many words in code are neologisms. Reasoning over such an unbounded vocabulary is not something NLP methods are typically suited for. We introduce a deep model that contends with an unbounded vocabulary (at training or test time) by embedding new words as nodes in a graph as they are encountered and processing the graph with a Graph Neural Network.},
author = {Cvitkovic, Milan and Singh, Badal and Anandkumar, Anima},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Cvitkovic, Singh, Anandkumar - Deep Learning On Code with an Unbounded Vocabulary.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Deep Learning On Code with an Unbounded Vocabulary}},
url = {https://github.com/mwcvitkovic/Deep},
year = {2018}
}
@inproceedings{Mytkowicz2009,
abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences.},
annote = {NULL},
author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
booktitle = {ASPLOS},
doi = {10.1145/1508284.1508275},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Mytkowicz et al. - Producing wrong data without doing anything obviously wrong!.pdf:pdf},
isbn = {9781605584065},
issn = {03621340},
keywords = {bias,keywords,measurement,performance},
publisher = {ACM},
title = {{Producing wrong data without doing anything obviously wrong!}},
volume = {44},
year = {2009}
}
@inproceedings{Haidl2017,
annote = {NULL},
author = {Haidl, M. and Steuwer, M. and Humernbrum, T. and Gorlatch, S.},
booktitle = {PMAM},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Haidl et al. - Towards Composable GPU Programming Programming GPUs with Eager Actions and Lazy Views.pdf:pdf},
isbn = {9781450348836},
title = {{Towards Composable GPU Programming: Programming GPUs with Eager Actions and Lazy Views}},
year = {2017}
}
@inproceedings{Eizenberg,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Eizenberg, Ariel and Hu, Shiliang and Pokam, Gilles and Devietti, Joseph},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Eizenberg et al. - REMIX Online Detection and Repair of Cache Contention for the JVM.pdf:pdf},
isbn = {9781450342612},
keywords = {advantage and that copies,bear this notice and,cache coherence,classroom use is granted,copies are not made,false sharing,for profit or commercial,java,or,or distributed,or hard copies of,part or all of,permission to make digital,the full citation,this work for personal,without fee provided that},
title = {{REMIX: Online Detection and Repair of Cache Contention for the JVM}},
year = {2016}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distin-guished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using " local search " to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were em-ployed, aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.},
annote = {NULL},
author = {Schmidhuber, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Schmidhuber - Deep Learning in Neural Networks An Overview.pdf:pdf},
journal = {Neural networks},
keywords = {TOPRINT,TOSTUDY},
mendeley-tags = {TOPRINT,TOSTUDY},
title = {{Deep Learning in Neural Networks: An Overview}},
year = {2014}
}
@inproceedings{Lutz,
annote = {NULL},
author = {Lutz, T. and Fensch, C. and Cole, M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lutz, Fensch, Cole - Helium A Transparent Inter-kernel Optimizer for OpenCL Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334075},
keywords = {gpgpu,inter-kernel optimization,jit compi-,opencl,profiling},
title = {{Helium: A Transparent Inter-kernel Optimizer for OpenCL Categories and Subject Descriptors}},
year = {2015}
}
@article{Kotzmann2008,
annote = {Cited by 139.},
author = {Kotzmann, Thomas and Wimmer, Christian and M{\"{o}}ssenb{\"{o}}ck, Hanspeter and Rodriguez, Thomas and Russell, Kenneth and Cox, David},
doi = {10.1145/1369396.1370017},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Kotzmann et al. - Design of the Java HotSpot client compiler for Java 6.pdf:pdf},
issn = {15443566},
journal = {TACO},
month = {may},
number = {1},
title = {{Design of the Java HotSpot client compiler for Java 6}},
url = {http://portal.acm.org/citation.cfm?doid=1369396.1370017},
volume = {5},
year = {2008}
}
@inproceedings{Kitzelmann2009,
abstract = {Inductive programming (IP)—the use of inductive reasoning methods for programming, algorithm design, and software development— is a currently emerging research field. A major subfield is inductive pro-gram synthesis, the (semi-)automatic construction of programs from ex-emplary behavior. Inductive program synthesis is not a unified research field until today but scattered over several different established research fields such as machine learning, inductive logic programming, genetic programming, and functional programming. This impedes an exchange of theory and techniques and, as a consequence, a progress of inductive programming. In this paper we survey theoretical results and methods of inductive program synthesis that have been developed in different re-search fields until today.},
author = {Kitzelmann, E.},
booktitle = {AAIP},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Kitzelmann - Inductive Programming A Survey of Program Synthesis Techniques.pdf:pdf},
title = {{Inductive Programming: A Survey of Program Synthesis Techniques}},
year = {2009}
}
@article{Peverati2014,
archivePrefix = {arXiv},
arxivId = {1212.0944},
author = {Peverati, R. and Truhlar, D. G.},
eprint = {1212.0944},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Peverati, Truhlar - The Quest for a Universal Density Functional The Accuracy of Density Functionals Across a Broad Spectrum of D.pdf:pdf},
journal = {Phil. Trans. R. Soc A.},
title = {{The Quest for a Universal Density Functional: The Accuracy of Density Functionals Across a Broad Spectrum of Databases in Chemistry and Physics}},
volume = {372},
year = {2014}
}
@article{Grant2000,
abstract = {We present the design of DyC, a dynamic-compilation system for C based on run-time specialization. Directed by a fewdeclarative user annotations that specify the variables and code on which dynamic compilation should take place, a binding-time analysis computes the set of run-time constants at each program point in the annotated procedure's control-flow graph; the analysis supports program- point-specific polyvariant division and specialization. The results of the analysis guide the construction of a run-time specializer for each dynamically compiled region; the specializer supports various caching strategies for managing dynamically generated code and mixes of speculative and demand-driven specialization of dynamic branch successors. Most of the key cost/benefit trade-offs in the binding-time analysis and the run-time specializer are open to user control through declarative policy annotations. DyC has been implemented in the context of an optimizing compiler, and initial results have been promising. The speedups we have obtained are good, and the dynamic-compilation overhead is among the lowest of any dynamic-compilation system, typically 20-200 cycles per instruction generated on a Digital Alpha 21164. The majority of DyC's functionality has been used to dynamically compile an instruction-set simulator. Only three annotations were required, but a fewother changes to the program had to be made due to DyC's lack of support for static global variables. This deficiency and DyC's rudimentary support for partially static data structures are the primary obstacles to making DyC easy to use.},
annote = {NULL},
author = {Grant, B. and Mock, M. and Philipose, M. and Chambers, C. and Eggers, S. J.},
doi = {10.1016/S0304-3975(00)00051-7},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Grant et al. - DyC an expressive annotation-directed dynamic compiler for C.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {analysis,c language,constant,dataflow,dynamic compilation,folding,partial evaluation,program optimization,run-time code generation,specialization},
number = {1},
title = {{DyC: an expressive annotation-directed dynamic compiler for C}},
volume = {248},
year = {2000}
}
@article{Vasilescu,
abstract = {—StackOverflow is a popular on-line programming question and answer community providing its participants with rapid access to knowledge and expertise of their peers, especially benefitting coders. Despite the popularity of StackOverflow, its role in the work cycle of open-source developers is yet to be understood: on the one hand, participation in it has the potential to increase the knowledge of individual developers thus improving and speeding up the development process. On the other hand, participation in StackOverflow may interrupt the regular working rhythm of the developer, hence also possibly slow down the development process. In this paper we investigate the interplay between Stack-Overflow activities and the development process, reflected by code changes committed to the largest social coding repository, GitHub. Our study shows that active GitHub committers ask fewer questions and provide more answers than others. Moreover, we observe that active StackOverflow askers distribute their work in a less uniform way than developers that do not ask questions. Finally, we show that despite the interruptions incurred, the StackOverflow activity rate correlates with the code changing activity in GitHub.},
annote = {NULL},
author = {Vasilescu, Bogdan and Filkov, Vladimir and Serebrenik, Alexander},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Vasilescu, Filkov, Serebrenik - StackOverflow and GitHub Associations Between Software Development and Crowdsourced Knowledge.pdf:pdf},
title = {{StackOverflow and GitHub: Associations Between Software Development and Crowdsourced Knowledge}}
}
@inproceedings{Patros2015,
annote = {NULL},
author = {Patros, P. and Aubanel, E. and Bremner, D. and Dawson, M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Patros et al. - A Java Util Concurrent Park Contention Tool Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334044},
keywords = {java,juc,juc park con-,lock profiling,thread park},
title = {{A Java Util Concurrent Park Contention Tool Categories and Subject Descriptors}},
year = {2015}
}
@article{Zhang2016,
abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a {\{}$\backslash$em plausible{\}} color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and explore using class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward operation in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test", asking human subjects to choose between a generated and ground truth color image. Our method successfully fools humans 20$\backslash${\%} of the time, significantly higher than previous methods.},
annote = {NULL},
author = {Zhang, R. and Isola, P. and Efros, A. A.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zhang, Isola, Efros - Colorful Image Colorization.pdf:pdf},
journal = {arXiv:1603.08511},
keywords = {cnns,colorization,vision for graphics},
title = {{Colorful Image Colorization}},
year = {2016}
}
@inproceedings{Buchwald2018,
author = {Buchwald, S. and Fried, A. and Hack, S.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Buchwald, Fried, Hack - Synthesizing an Instruction Selection Rule Library from Semantic Specifications.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Synthesizing an Instruction Selection Rule Library from Semantic Specifications}},
year = {2018}
}
@techreport{Hayhurst2001,
abstract = {Ndition/decision coverage (MC/DC) for aviation software products that must comply with software. This tutorial provides a practical approach to assessing modified regulatory guidance for DO-178B level A allows a certification authority or verification analyst to evaluate MC/DC claims without the aid of a coverage tool. The tutorials approach to MC/DC is a 5-step process that In addition to the MC/DC approach, the tutorial addresses factors to consider in selecting and qualifying a structural coverage analysis tool, tips for reviewing life cycle data related to MC/DC, and pitfalls common to structural coverage analysis.},
annote = {NULL},
author = {Hayhurst, Kelly J and Veerhusen, Dan S and Chilenski, John J and Rierson, Leanna K},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Hayhurst et al. - A practical tutorial on modified conditiondecision coverage.pdf:pdf},
isbn = {2001210876},
title = {{A practical tutorial on modified condition/decision coverage}},
url = {http://portal.acm.org/citation.cfm?id=886632},
year = {2001}
}
@article{Johnson2016a,
abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-the-art results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
annote = {Zero-shot translation(!?)},
archivePrefix = {arXiv},
arxivId = {1611.04558},
author = {Johnson, M. and Schuster, M. and Le, Q. V. and Krikun, M. and Wu, Y. and Chen, Z. and Thorat, N. and Vi{\'{e}}gas, F. and Wattenberg, M. and Corrado, G. and Hughes, M. and Dean, J.},
eprint = {1611.04558},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Johnson et al. - Google's Multilingual Neural Machine Translation System Enabling Zero-Shot Translation.pdf:pdf},
journal = {arXiv:1611.04558},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}},
year = {2016}
}
@misc{UniversityofEdinburgh2014x,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 3.pdf:pdf},
title = {{IAML 3}},
year = {2014}
}
@article{Karsai2003,
abstract = {Self-adaptive software systems use observations of their own behav- ior, and that of their environment, to select and enact adaptations in accordance with some objective(s). This adaptation is a higher-level system function that performs optimizations, manages faults, or otherwise supports achieving an ob- jective via changes in the running system. In this paper, we show how this ca- pability can be realized using techniques found in hierarchical control systems, and we discuss interrelated issues of stability, assurance, and implementation.},
annote = {This paper describes the realization of self-adaptive software systems using techniques from hiearchical control systems.},
author = {Karsai, G. and Ledeczi, A. and Sztipanovits, J. and Peceli, G. and Simon, G. and Kovacshazy, T.},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Karsai et al. - An Approach to Self-adaptive Software Based on Supervisory Control.pdf:pdf},
journal = {Self-adaptive Software: Applications},
publisher = {Springer},
title = {{An Approach to Self-adaptive Software Based on Supervisory Control}},
year = {2003}
}
@inproceedings{Saeed,
abstract = {Traditionally, datawarehousingworkloads have been processed us- ing CPU-focused clusters, such as those that make up the bulk of available machines in Amazon's EC2, and the focus on improv- ing analytics performance has been to utilize a homogenous, multi- threaded CPU environment with optimized algorithms for this in- frastructure. The increasing availability of highly parallel accelera- tors, like theGPUand Xeon Phi discrete accelerators, in these types of clusters has provided an opportunity to further accelerate analyt- ics operations but at a high programming cost due to optimizations required to fully utilize each of these new pieces of hardware. This work describes and analyzes highly parallel relational al- gebra primitives that are developed to focus on data warehousing queries through the use of a common OpenCL framework that can be executed both on standard multi-threaded processors and on emerging accelerator architectures. As part of this work, we pro- pose a set of data-intensive benchmarks to help compare and dif- ferentiate the performance of accelerator hardware and to deter- mine the key characteristics for efficiently running data warehous- ing queries on accelerators.},
annote = {NULL},
author = {Saeed, I.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Saeed - A Portable Benchmark Suite for Highly Parallel Data Intensive Query Processing.pdf:pdf},
isbn = {9781450334051},
keywords = {Benchmarking,Heterogeneous Computing,OpenCL,Relational Databases,SHOC,TPC- H},
title = {{A Portable Benchmark Suite for Highly Parallel Data Intensive Query Processing}},
year = {2015}
}
@article{Hutter2009,
abstract = {The identification of performance-optimizing parameter settings is an important part of the de- velopment and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categori- cal parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluat- ing individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT.We also present what is, to our knowledge, the first published work on automatically config- uring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.},
annote = {Automatic parameter setting for algorithms.

Cited by 325.},
author = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin and Stutzle, Thomas},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Hutter et al. - ParamILS An Automatic Algorithm Configuration Framework.pdf:pdf},
journal = {JAIR},
number = {1},
title = {{ParamILS: An Automatic Algorithm Configuration Framework}},
volume = {36},
year = {2009}
}
@article{Jaderberg2016b,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the- art on Atari, averaging 880{\%} expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87{\%} expert human performance on Labyrinth.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03044v2},
author = {Jaderberg, M. and Mnih, V. and Czarnecki, W. M. and Schaul, T. and Leibo, J. Z. and Silver, D. and Kavukcuoglu, K.},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1509.03044v2},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Jaderberg et al. - Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv:1611.05397},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
year = {2016}
}
@inproceedings{Allamanis2016,
abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
annote = {NULL},
author = {Allamanis, M. and Peng, H. and Sutton, C.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Allamanis, Peng, Sutton - A Convolutional Attention Network for Extreme Summarization of Source Code.pdf:pdf},
keywords = {neural attention models,source code naturalness},
title = {{A Convolutional Attention Network for Extreme Summarization of Source Code}},
year = {2016}
}
@article{Lucia,
annote = {NULL},
author = {Machado, Nuno and Lucia, Brandon and Rodrigues, Luis},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Machado, Lucia, Rodrigues - Production-guided Concurrency Debugging.pdf:pdf},
isbn = {9781450340922},
title = {{Production-guided Concurrency Debugging}}
}
@inproceedings{Ansel2012,
abstract = {Modern high performance libraries, such as ATLAS and FFTW, and programming languages, such as PetaBricks, have shown that autotuning computer programs can lead to significant speedups. However, autotuning can be burdensome to the deployment of a program, since the tuning process can take a long time and should be re- run whenever the program, microarchitecture, execution environment, or tool chain changes. Failure to re-autotune programs often leads to widespread use of sub-optimal algorithms. With the growth of cloud computing, where computations can run in environments with unknown load and migrate between different (possibly unknown) microarchitectures, the need for online autotuning has become increasingly important. We present SiblingRivalry, a new model for always- on online autotuning that allows parallel programs to continuously adapt and optimize themselves to their environment. In our system, requests are processed by dividing the available cores in half, and processing two identical requests in parallel on each half. Half of the cores are devoted to a known safe program configuration, while the other half are used for an experimental program configuration chosen by our self-adapting evolutionary algorithm. When the faster configuration completes, its results are returned, and the slower configuration is terminated. Over time, this constant experimentation allows programs to adapt to changing dynamic environments and often outperform the original algorithm that uses the entire system.},
annote = {The paper describes SiblingRivalry, a model for the online autotuning of PetaBricks using evolutionary tuning algorithms. When a request is made, the available cores are split in half, and processed twice, once using a safe configuration, and the other using an experimental configuration. The best performing is the survivor.
















The 50/50 split},
author = {Ansel, J. and Reilly, U. O.},
booktitle = {CASES},
doi = {10.1145/2380403.2380425},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Ansel, Reilly - SiblingRivalry Online Autotuning Through Local Competitions.pdf:pdf},
isbn = {9781450314244},
keywords = {autotuning,evolutionary algorithm,genetic algorithm},
publisher = {ACM},
title = {{SiblingRivalry: Online Autotuning Through Local Competitions}},
url = {http://doi.acm.org/10.1145/2380403.2380425},
year = {2012}
}
@inproceedings{Putnam2015,
abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurable fabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6×8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables. In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95{\%} for a fixed latency distribution—or, while maintaining equivalent throughput, reduces the tail latency by 29{\%}.},
annote = {NULL},
author = {Putnam, A. and Caulfield, A. M. and Chung, E. S. and Chiou, D. and Constantinides, K. and Demme, J. and Esmaeilzadeh, H. and Fowers, J. and Gopal, G. P. and Gray, J. and Haselman, M. and Hauck, S. and Heil, S. and Hormati, A. and Kim, J. Y. and Lanka, S. and Larus, J. and Peterson, E. and Pope, S. and Smith, A. and Thong, J. and Xiao, P. Y. and Burger, D.},
booktitle = {ISCA},
doi = {10.1109/MM.2015.42},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Putnam et al. - A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services.pdf:pdf},
isbn = {9781479943968},
issn = {02721732},
keywords = {FPGA,datacenter,reconfigurable computing,web search},
title = {{A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services}},
year = {2014}
}
@article{Squires1984,
annote = {NULL},
author = {Kelly, W. and Pugh, W.},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Kelly, Pugh - A framework for unifying reordering transformations.pdf:pdf},
pmid = {20314319},
title = {{A framework for unifying reordering transformations}},
year = {1998}
}
@article{Rauchwerger,
abstract = {Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is reexecuted serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching: it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks, which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods},
annote = {From Duplicate 2 (The LRPD test: Speculative run-time parallelization of loops with privatization and reduction parallelization - Rauchwerger, Lawrence; Padua, David A)

Cited by 417.},
author = {Rauchwerger, L. and Padua, D. A.},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Rauchwerger, Padua - The LRPD test Speculative run-time parallelization of loops with privatization and reduction parallelization.pdf:pdf},
journal = {TPDS},
keywords = {Compilers,DOALL,Parallel processing,Privatization,Reduction,Run-time,Speculative},
number = {2},
publisher = {IEEE},
title = {{The LRPD test: Speculative run-time parallelization of loops with privatization and reduction parallelization}},
volume = {10},
year = {1999}
}
@article{Mcmillan2003,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Socket Programming HOWTO.pdf:pdf},
keywords = {C++ programming},
title = {{Socket Programming HOWTO}},
year = {2016}
}
@inproceedings{Saillard2015,
annote = {NULL},
author = {Saillard, E. and Carribault, P. and Barthou, D.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Saillard, Carribault, Barthou - Static Dynamic Validation of MPI Collective Communications in Multi-threaded Context.pdf:pdf},
isbn = {9781450332057},
keywords = {Control Flow,MPI+OpenMP,Static,Verification},
title = {{Static / Dynamic Validation of MPI Collective Communications in Multi-threaded Context}},
year = {2015}
}
@inproceedings{Wang2014,
abstract = {The performance of R, a popular data analysis language, was never properly understood. Some claimed their R codes ran as efficiently as any native code, others quoted orders of magnitude slowdown of R codes with respect to equivalent C implementations.We found both claims to be true depend- ing on how an R code is written. This paper introduces a first classification of R programming styles into Type I (looping over data), Type II (vector programming), and Type III (glue codes). The most serious overhead of R are mostly mani- fested on Type I R codes, whereas many Type III R codes can be quite fast. This paper focuses on improving the performance of Type I R codes.We propose the ORBIT VM, an extension of the GNU R VM, to perform aggressive removal of allocated ob- jects and reduction of instruction path lengths in the GNU R VM via profile-driven specialization techniques. The OR- BIT VM is fully compatible with the R language and is purely based on interpreted execution. It is a specialization JIT and runtime focusing on data representation specializa- tion and operation specialization. For our benchmarks of Type I R codes, ORBIT is able to achieve an average of 3.5X speedups over the current release of GNU RVMand outper- forms most other R optimization projects that are currently available.},
annote = {This paper presents an optimised subset of the R VM called "ORBIT". It is a specialisation JIT.},
author = {Wang, H. and Padua, D.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wang, Padua - Optimizing R VM Allocation Removal and Path Length Reduction via Interpreter-level Specialization.pdf:pdf},
isbn = {9781450326704},
keywords = {dynamic scripting language,r,specialization},
publisher = {IEEE},
title = {{Optimizing R VM: Allocation Removal and Path Length Reduction via Interpreter-level Specialization}},
year = {2014}
}
@misc{Hileman,
annote = {Rebase flowchart. See: https://presentate.com/bobthecow/talks/changing-history},
author = {Hileman, Justin},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Hileman - Git pretty.pdf:pdf},
title = {{Git pretty}},
year = {2012}
}
@inproceedings{Mikolov2015,
annote = {NULL},
author = {Mikolov, T.},
booktitle = {Interspeech},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Mikolov - Recurrent Neural Network based Language Model.pdf:pdf},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Liaw2002,
abstract = {Recently there has been a lot of interest in “ensemble learning” — methods that generate many classifiers and aggregate their results. Two well-known methods are boosting (see, e.g., Shapire et al., 1998) and bagging Breiman (1996) of classification trees. In boosting, successive trees give extra weight to points incorrectly predicted by earlier predictors. In the end, a weighted vote is taken for prediction. In bagging, successive trees do not depend on earlier trees — each is independently constructed using a bootstrap sample of the data set. In the end, a simple majority vote is taken for prediction.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1609-3631},
author = {Liaw, A. and Wiener, M.},
doi = {10.1177/154405910408300516},
eprint = {1609-3631},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Liaw, Wiener - Classification and Regression by randomForest.pdf:pdf},
isbn = {1609-3631},
issn = {16093631},
journal = {R news},
number = {3},
pmid = {21196786},
title = {{Classification and Regression by randomForest}},
volume = {2},
year = {2002}
}
@inproceedings{Cadar2016a,
abstract = {The reliability of program analysis tools is clearly important if such tools are to play a serious role in improving the quality and integrity of software systems, and the confidence which users place in such systems. Yet our experience is that, currently, little attention is paid to analysing the correctness of program analysers themselves, beyond regression testing. In this position paper we present our vision that, by 2025, the use of more rigorous analyses to check the reliability of program analysers will be commonplace. Inspired by recent advances in compiler testing, we set out initial steps towards this vision, building upon techniques such as cross-checking, program transformation and program generation.},
author = {Cadar, C. and Donaldson, A.},
booktitle = {ICSE},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Cadar, Donaldson - Analysing the Program Analyser.pdf:pdf},
keywords = {cross-checking,program analysis,program generators,program transformations,testing},
title = {{Analysing the Program Analyser}},
year = {2016}
}
@misc{Might,
annote = {Keep pushing.},
author = {Might, Matt},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Might - The illustrated guide to a Ph.D.pdf:pdf},
title = {{The illustrated guide to a Ph.D.}}
}
@article{Demme2012,
abstract = {An important aspect of system optimization research is the discovery of program traits or behaviors. In this paper, we present an automated method of program characterization which is able to examine and cluster program graphs, i.e., dynamic data graphs or control flow graphs. Our novel approximate graph clustering technology allows users to find groups of program fragments which contain similar code idioms or patterns in data reuse, control flow, and context. Patterns of this nature have several potential applications including development of new static or dynamic optimizations to be implemented in software or in hardware. For the SPEC CPU 2006 suite of benchmarks, our results show that approximate graph clustering is effective at grouping behaviorally similar functions. Graph based clustering also produces clusters that are more homogeneous than previously proposed non-graph based clustering methods. Further qualitative analysis of the clustered functions shows that our approach is also able to identify some frequent unexploited program behaviors. These results suggest that our approximate graph clustering methods could be very useful for program characterization. {\textcopyright} 2012 ACM.},
annote = {NULL},
author = {Demme, J. and Sethumadhavan, S.},
doi = {10.1145/2086696.2086700},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Demme, Sethumadhavan - Approximate Graph Clustering for Program Characterization.pdf:pdf},
issn = {15443566},
journal = {TACO},
number = {4},
publisher = {ACM},
title = {{Approximate Graph Clustering for Program Characterization}},
volume = {8},
year = {2012}
}
@inproceedings{Prabhu2010,
abstract = {Execution order constraints imposed by dependences can serialize computation, preventing parallelization of code and algorithms. Speculating on the value(s) carried by dependences is one way to break such critical dependences. Value speculation has been used effectively at a low level, by compilers and hardware. In this paper, we focus on the use of speculation by programmers as an algorithmic paradigm to parallelize seemingly sequential code. We propose two new language constructs, speculative compo- sition and speculative iteration. These constructs enable program- mers to declaratively express speculative parallelism in programs: to indicate when and how to speculate, increasing the parallelism in the program, without concerning themselves with mundane im- plementation details. We present a core language with speculation constructs and mutable state and present a formal operational semantics for the language. We use the semantics to define the notion of a correct speculative execution as one that is equivalent to a non-speculative execution. In general, speculation requires a runtime mechanism to undo the effects of speculative computation in the case of mis- predictions. We describe a set of conditions under which such rollback can be avoided. We present a static analysis that checks if a given program satisfies these conditions. This allows us to implement speculation efficiently, without the overhead required for rollbacks. We have implemented the speculation constructs as a C{\#} library, along with the static checker for safety. We present an empirical evaluation of the efficacy of this approach to parallelization.},
annote = {Speculative execution is a method for achieving parallelism in "dependent" environments, e.g. parsers, and lexical analysers, where parallelism would usually not be possible, due to the execution at timet relying on results created at time t-n. Speculative execution splits the input into chunks, where each chunk n, n{\textgreater}1 is processed "speculatively", i.e. by making assumptions about the state of the algorithm at the end of processing chunk i-1. When the state assumption is incorrect, the processing must be "rolled back", and re-executed with correct state information instead. As the state is assumed, instead of calculated, each chunk can be processed independently of every other chunk, allowing parallelism in otherwise strictly serial environments. Theoretically, such parallelism is guaranteed to either give a speedup (when assumptions are correct), or result in only a minor, constant, slowdown when assumptions are incorrect. A C{\#} library supporting "Speculative parallelism", along with a formal operational semantics for speculative algorithms expressed using the framework, and non-speculative algorithms. The authors also use the operational semantics to prove the equivalence of the two classes of algorithms, as well as define a method for statically checking that impure speculative algorithms execute the same transitions, and produce the same output as their non speculative equivalents. Possibly overly theoretical for the purposes of the survey paper, and focussed on a library, not a piece of compiler technology. However, the techniques used to verify that parallelism is possible/legal may be important to discuss in the paper, as well as the operational equivalence of the parallel and serial code. The results/methodology section is shit. Cited by 49.},
author = {Prabhu, Prakash},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Prabhu - Safe Programmable Speculative Parallelism.pdf:pdf},
keywords = {purity,rollback freedom,safety,speculative parallelism,value speculation},
publisher = {ACM},
title = {{Safe Programmable Speculative Parallelism}},
year = {2010}
}
@misc{KhronosOpenCLGroupInc2015a,
annote = {NULL},
author = {{Khronos OpenCL Group Inc}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Khronos OpenCL Group Inc - OpenCL 2.0 Reference Card.pdf:pdf},
title = {{OpenCL 2.0 Reference Card}},
year = {2015}
}
@inproceedings{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y. and Lamblin, P.},
booktitle = {NIPS},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Bengio, Lamblin - Greedy layer-wise training of deep networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
number = {1},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy layer-wise training of deep networks}},
url = {https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
year = {2007}
}
@inproceedings{Allamanis2014,
abstract = {We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic role. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present HAGGIS, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply HAGGIS to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q{\&}A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.},
annote = {NULL},
author = {Allamanis, M. and Sutton, C.},
booktitle = {FSE},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Allamanis, Sutton - Mining Idioms from Source Code.pdf:pdf},
keywords = {code idioms,naturalness of source code,syntactic code patterns},
publisher = {ACM},
title = {{Mining Idioms from Source Code}},
year = {2014}
}
@incollection{Bosch2005,
annote = {NULL},
author = {Bosch, Robert and Trick, Michael},
booktitle = {Search Methodologies},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bosch, Trick - Integer Programming.pdf:pdf},
publisher = {Springer},
title = {{Integer Programming}},
year = {2014}
}
@misc{Schroeder1989,
abstract = {Number theory, an abstract branch of mathematics that deals with relationships between whole numbers, has provided highly useful answers to numerous real-world problems. The author briefly reviews earlier uses of number theory and then examines recent applications to music, cryptography, and error-correction codes},
annote = {NULL},
author = {{University of Edinburgh}},
doi = {10.1109/45.41531},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 12. Number theory.pdf:pdf},
issn = {0278-6648},
title = {{12. Number theory}},
volume = {8},
year = {2015}
}
@article{Tate2009,
abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The op- timizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuris- tic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
annote = {Cited by 81.},
archivePrefix = {arXiv},
arxivId = {1012.1255},
author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
doi = {10.2168/LMCS-},
eprint = {1012.1255},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Tate et al. - Equality Saturation A New Approach to Optimization.pdf:pdf},
journal = {LMCS},
publisher = {ACM},
title = {{Equality Saturation: A New Approach to Optimization}},
url = {http://arxiv.org/abs/1012.1255},
year = {2009}
}
@inproceedings{Jantz2010,
abstract = {Compiler optimization phase ordering is a longstanding problem, and is of particular relevance to the performance-oriented and cost- constrained domain of embedded systems applications. Optimiza- tion phases are known to interact with each other, enabling and disabling opportunities for successive phases. Therefore, varying the order of applying these phases often generates distinct out- put codes, with different speed, code-size and power consumption characteristics. Most current approaches to address this issue fo- cus on developing innovative methods to selectively evaluate the vast phase order search space to produce a good (but, potentially suboptimal) representation for each program. In contrast, the goal of this work is to study and identify com- mon causes of optimization phase interactions across all phases, and then devise techniques to eliminate them, if and when possi- ble. We observe that several phase interactions are caused by false register dependence during many optimization phases. We further find that depending on the implementation of optimization phases, even an increased availability of registers may not be able to sig- nificantly reduce such false register dependences. We explore the potential of cleanup phases, such as register remapping and copy propagation, at reducing false dependences. We show that innova- tive implementation and application of these phases to reduce false register dependences not only reduces the size of the phase order search space substantially, but can also improve the quality of code generated by optimizing compilers.},
annote = {NULL},
author = {Jantz, Michael R and Kulkarni, Prasad A},
booktitle = {CASES},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Jantz, Kulkarni - Eliminating false phase interactions to reduce optimization phase order search space.pdf:pdf},
isbn = {9781605589039},
keywords = {algorithms,false register dependence,measurements,performance,phase ordering},
publisher = {ACM},
title = {{Eliminating false phase interactions to reduce optimization phase order search space}},
year = {2010}
}
@inproceedings{Taylor,
author = {Taylor, B. and Marco, V. S. and Wang, Z.},
booktitle = {LCTES},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Taylor, Marco, Wang - Adaptive Optimization for OpenCL Programs on Embedded Heterogeneous Systems.pdf:pdf},
isbn = {9781450350303},
keywords = {energy efficiency,heterogeneous multi-cores,opencl,predictive modeling},
title = {{Adaptive Optimization for OpenCL Programs on Embedded Heterogeneous Systems}},
year = {2017}
}
@misc{CarnegieMellonUniversity2006,
annote = {NULL},
author = {{Carnegie Mellon University}},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Carnegie Mellon University - 10. Regression Trees.pdf:pdf},
title = {{10. Regression Trees}},
year = {2006}
}
@inproceedings{Fursin2007a,
abstract = {Iterative feedback-directed optimization is now a popu- lar technique to obtain better performance and code size improvements for statically compiled programs over the default settings in a compiler. The offline evaluation of multiple optimization strategies for a given program is a potentially costly operation. The number of iter- ations typically grows with the complexity of the pro- gram transformation search space, and with the number of input datasets used for performance assessment. In addition, as the behavior of a program can vary con- siderably across different datasets, it is often preferable to generate different optimization versions, covering the full spectrum of the program's representative datasets. Continuous and collective optimization are targeted at these issues. Continuous optimization searches for the best program transformation at run-time, taking advan- tages of the phase behavior of programs to evaluate multiple optimization versions within a single run, and dynamically adapting to changing execution contexts. Collective optimization interleaves optimization itera- tions with program executions along the lifetime of the program. In both cases, the user expects the optimiza- tion process to learn from the past execution contexts and program behavior. The user also assumes the sys- tem will be fully transparent, take negligible overhead for the incremental profiling, learning, decision and code generation steps, while bringing significant perfor- mance benefits over the lifetime of the program. In order to explore multiple optimization options, we propose a simple and practical solution based on cloning of all procedures, applying any complex optimizations to these clones and randomly selecting either original or transformed procedures at run-time. Obtaining execu- tion time distribution among original and cloned pro- cedures, we can statistically determine the influence of compiler optimizations on the code in a single run. The simplicity of the implementation makes this tech- nique reliable, secure and easy to debug. Yet it en- ables practical transparent low-overhead continuous op- timizations for programs statically compiled with GCC while avoiding complex dynamic recompilation frame- works. In addition, our framework can enable program self-adaptation at fine-grain level for different environ- ments such as parallel heterogeneous and reconfigurable systems with different ISA, and for different constraints such as performance, code size and power consumption.},
annote = {This paper presents an implemenation of Continuous Collective Compilation in GCC, wherein the most time-consuming procedures are cloned and different optimisations applied to each. At run-time, these clones are switched between in order to converge on the optimum configuration for a given task.




The paper is light on details and only presents results for two benchmarks, but I like the concept.},
author = {Fursin, G. and Miranda, C. and Pop, S. and Cohen, A. and Temam, O.},
booktitle = {GCC Developers' Summit},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Fursin et al. - Practical Run-time Adaptation with Procedure Cloning to Enable Continuous Collective Compilation.pdf:pdf},
title = {{Practical Run-time Adaptation with Procedure Cloning to Enable Continuous Collective Compilation}},
year = {2007}
}
@inproceedings{Joseph2017,
annote = {NULL},
author = {Joseph, K. and Wei, W. and Carley, K. C.},
booktitle = {CSCW},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Joseph, Wei, Carley - Girls rule, boys drool Extracting semantic and affective stereotypes on Twitter.pdf:pdf},
isbn = {9781450343350},
keywords = {computational social science,identity,social psychology,stereo-},
title = {{Girls rule, boys drool: Extracting semantic and affective stereotypes on Twitter}},
year = {2017}
}
@article{Liao2018,
abstract = {We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.},
archivePrefix = {arXiv},
arxivId = {1803.06272},
author = {Liao, R. and Brockschmidt, M. and Tarlow, D. and Gaunt, A. L. and Urtasun, R. and Zemel, R.},
eprint = {1803.06272},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Liao et al. - Graph Partition Neural Networks for Semi-Supervised Classification.pdf:pdf},
journal = {arXiv:1803.06272},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Graph Partition Neural Networks for Semi-Supervised Classification}},
url = {http://arxiv.org/abs/1803.06272},
year = {2018}
}
@inproceedings{Santambrogio2010,
abstract = {Self-aware computer systems will be capable of adapting their behavior and resources thousands of times a second to automatically find the best way to accomplish a given goal despite changing environmental conditions and demands. Such a capability benefits a broad spectrum of computer systems from embedded systems to supercomputers and is particularly useful for meeting power, performance, and resource-metering challenges in mobile computing, cloud computing, multicore computing, adaptive and dynamic compilation environments, and parallel operating systems.},
annote = {Cited by 20.},
author = {Santambrogio, M D and Hoffmann, H and Eastep, J and Agarwal, A},
booktitle = {AHS},
doi = {10.1109/AHS.2010.5546266},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Santambrogio et al. - Enabling technologies for self-aware adaptive systems.pdf:pdf},
isbn = {978-1-4244-5887-5},
month = {jun},
publisher = {Ieee},
title = {{Enabling technologies for self-aware adaptive systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5546266},
year = {2010}
}
@inproceedings{Groce2014,
abstract = {In random testing, it is often desirable to produce a 'quick test' - an extremely inexpensive test suite that can serve as a frequently applied regression and allow the benefits of random testing to be obtained even in very slow or over-subscribed test environments. Delta debugging is an algorithm that, given a failing test case, produces a smaller test case that also fails, and typically executes much more quickly. Delta debugging of random tests can produce effective regression suites for previously detected faults, but such suites often have little power for detecting new faults, and in some cases provide poor code coverage. This paper proposes extending delta debugging by simplifying tests with respect to code coverage, an instance of a generalization of delta debugging we call cause reduction. We show that test suites reduced in this fashion can provide very effective quick tests for real-world programs. For Mozilla's Spider Monkey JavaScript engine, the reduced suite is more effective for finding software faults, even if its reduced runtime is not considered. The effectiveness of a reduction-based quick test persists through major changes to the software under test. {\textcopyright} 2014 IEEE.},
author = {Groce, A. and Alipour, M. A. and Zhang, C. and Chen, Y. and Regehr, J.},
booktitle = {ICST},
doi = {10.1109/ICST.2014.37},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Groce et al. - Cause reduction for quick testing.pdf:pdf},
isbn = {9780769551852},
issn = {2159-4848},
keywords = {random testing,regression testing,test case minimization},
title = {{Cause reduction for quick testing}},
year = {2014}
}
@inproceedings{Che2010,
abstract = {The recently released Rodinia benchmark suite enables users to evaluate heterogeneous systems including both accelerators, such as GPUs, and multicore CPUs. As Rodinia sees higher levels of acceptance, it becomes important that researchers understand this new set of benchmarks, especially in how they differ from previous work. In this paper, we present recent extensions to Rodinia and conduct a detailed characterization of the Rodinia benchmarks (including performance results on an NVIDIA GeForce GTX480, the first product released based on the Fermi architecture). We also compare and contrast Rodinia with Parsec to gain insights into the similarities and differences of the two benchmark collections; we apply principal component analysis to analyze the application space coverage of the two suites. Our analysis shows that many of the workloads in Rodinia and Parsec are complementary, capturing different aspects of certain performance metrics.},
annote = {NULL},
author = {Che, Shuai and Sheaffer, Jeremy W. and Boyer, Michael and Szafaryn, Lukasz G. and Wang, Liang and Skadron, Kevin},
booktitle = {IISWC},
doi = {10.1109/IISWC.2010.5650274},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Che et al. - A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads.pdf:pdf},
isbn = {9781424492978},
publisher = {IEEE},
title = {{A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads}},
year = {2010}
}
@article{Hudak1996,
annote = {NULL},
author = {Hudak, Paul and Haven, New},
doi = {10.1145/242224.242477},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Hudak, Haven - Building Domain-Specific Embedded Languages.pdf:pdf},
issn = {03600300},
journal = {CSUR},
number = {December},
title = {{Building Domain-Specific Embedded Languages}},
volume = {28},
year = {1996}
}
@article{Bacon1994,
abstract = {In the last three decades a large number of compiler transformations for optimizing programs have been implemented. Most optimizations for uniprocessors reduce the number of instructions executed by the program using transformations based on the analysis of scalar quantities and data-flow techniques. In contrast, optimizations for high-performance superscalar, vector, and parallel processors maximize parallelism and memory locality with transformations that rely on tracking the properties of arrays using loop dependence analysis.This survey is a comprehensive overview of the important high-level program restructuring techniques for imperative languages, such as C and Fortran. Transformations for both sequential and various types of parallel architectures are covered in depth. We describe the purpose of each transformation, explain how to determine if it is legal, and give an example of its application.Programmers wishing to enhance the performance of their code can use this survey to improve their understanding of the optimizations that compilers can perform, or as a reference for techniques to be applied manually. Students can obtain an overview of optimizing compiler technology. Compiler writers can use this survey as a reference for most of the important optimizations developed to date, and as bibliographic reference for the details of each optimization. Readers are expected to be familiar with modern computer architecture and basic program compilation techniques.},
annote = {NULL},
author = {Bacon, D. F. and Graham, S. L. and Sharp, O. J.},
doi = {10.1145/197405.197406},
file = {:Users/cec/Google Drive/Mendeley Library/1994 - Bacon, Graham, Sharp - Compiler transformations for high-performance computing.pdf:pdf},
issn = {03600300},
journal = {CSUR},
number = {4},
title = {{Compiler transformations for high-performance computing}},
volume = {26},
year = {1994}
}
@article{Shotts2013,
annote = {NULL},
author = {Shotts, William E. Jr.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Shotts - The Linux Command Line, 2nd Internet Edition.pdf:pdf},
title = {{The Linux Command Line, 2nd Internet Edition}},
year = {2013}
}
@article{Groce2007,
abstract = {Most flight software testing at the Jet Propulsion Laboratory relies on the use of hand-produced test scenarios and is executed on systems as similar as possible to actual mission hardware. We report on a flight software development effort incorporating large-scale (biased) randomized testing on commodity desktop hardware. The results show that use of a reference implementation, hardware simulation with fault injection, a testable design, and test minimization enabled a high degree of automation in fault detection and correction. Our experience will be of particular interest to developers working in domains where on-time delivery of software is critical (a strong argument for randomized automated testing) but not at the expense of correctness and reliability (a strong argument for model checking, theorem proving, and other heavyweight techniques). The effort spent in randomized testing can prepare the way for generating more complete confidence using heavyweight techniques.},
annote = {NULL},
author = {Groce, Alex and Holzmann, Gerard and Joshi, Rajeev},
doi = {10.1109/ICSE.2007.68},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Groce, Holzmann, Joshi - Randomized differential testing as a prelude to formal verification.pdf:pdf},
isbn = {0769528287},
issn = {02705257},
journal = {ICSE},
title = {{Randomized differential testing as a prelude to formal verification}},
year = {2007}
}
@article{Saltz1991,
abstract = {In this paper, we extend the class of problenis that can be effcrtively mn- piled by parallelizing compilers. This is accornplislictl with the doconsider coil- struct which would allow these compilers to paralldizc{\~{}} many prohlcnis in $\backslash$vliit 11 siil)staiitial loop-lcvel parallelisin is available hit cattnot bc tlctcctctl hy staiitl;irtl corn pile-t iriie analysis. $\backslash$Ire describe and espcriincii till IJ analyze mcdia nisni.; II srd to parallclize the work required for these types of loops. In each of t hcse met Iiotls. a new loop structure is produced hy modifying the loop to be parallclizcd. $\backslash$$\backslash$'P also prcscnt the rules by wliicli these loop transformations may be autoiiiatcd in oi t1c.r. that they be iriclridccl in langiiage conipilers. Tlic itiaiii application a1c.n of our w- scarcli involves problems in scientific computations aittl engineering. The wot kIo;itl iiscd in our experirnents iitcludcs a mixture of real {\~{}})i{\~{}})l)lcins as wc4l as synt 1101 i- lit1 rally gcncratetl inputs. I2rorn our estcwsivc tests oii t I:iicorc hliil tiitt;is/:l{\~{}}({\~{}}, IVV have reaclicd the conclusion that for the types of $\backslash$voi liloatls $\backslash$vv Iiavc. itivfsligat (VI, self-execution almost always perforins better tltitii {\~{}}{\~{}}tc{\~{}}-sc{\~{}}liccli{\~{}}li{\~{}}ig. b'iirtlier. 1 IIV irii- provcment in performance that accriics as a resiil t of global topological sori irig of indices as opposed to the less experisivc local sortiiig. is not very sigiiifirant iri t IIP case of self-execution.},
annote = {NULL},
author = {Saltz, Joel H and Mirchandaney, Ravi and Crowley, Kay},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Saltz, Mirchandaney, Crowley - Run-time parallelization and scheduling of loops.pdf:pdf},
journal = {TC},
number = {5},
publisher = {IEEE},
title = {{Run-time parallelization and scheduling of loops}},
volume = {40},
year = {1991}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1},
annote = {NULL},
author = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Mikolov et al. - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
journal = {arXiv:1301.3781},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@article{Reed2016,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.05396},
author = {Reed, S. and Akata, Z. and Yan, X. and Logeswaran, L.},
eprint = {1605.05396},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Reed et al. - Generative Adversarial Text to Image Synthesis.pdf:pdf},
journal = {arXiv:1605.05396},
title = {{Generative Adversarial Text to Image Synthesis}},
year = {2016}
}
@misc{UniversityofEdinburgh1993,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 7. Computational Complexity.pdf:pdf},
title = {{7. Computational Complexity}},
volume = {52},
year = {2015}
}
@article{Pierro2012,
annote = {NULL},
author = {Pierro, Massimo Di and Skinner, David},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Pierro, Skinner - Concurrency in Modern Programming Languages.pdf:pdf},
journal = {CS{\&}E},
title = {{Concurrency in Modern Programming Languages}},
year = {2012}
}
@misc{UniversityofEdinburghc,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 2. Computational Complexity.pdf:pdf},
title = {{2. Computational Complexity}},
year = {2015}
}
@misc{UniversityofEdinburgh2009a,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - University of Edinburgh - IAML 2014 Exam.pdf:pdf},
number = {April},
title = {{IAML 2014 Exam}},
year = {2009}
}
@article{Giannini2017a,
abstract = {This report provides an introduction to some Machine Learning tools within the most common development environments. It mainly focuses on practical problems, skipping any theoretical introduction. It is oriented to both students trying to approach Machine Learning and experts looking for new frameworks.},
archivePrefix = {arXiv},
arxivId = {1703.05298},
author = {Giannini, F. and Laveglia, V. and Rossi, A. and Zanca, D. and Zugarini, A.},
eprint = {1703.05298},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Giannini et al. - Neural Networks for Beginners. A fast implemention in Matlab, Torch, TensorFlow.pdf:pdf},
journal = {arXiv:1703.05298},
keywords = {TOPRINT,TOSTUDY},
mendeley-tags = {TOPRINT,TOSTUDY},
title = {{Neural Networks for Beginners. A fast implemention in Matlab, Torch, TensorFlow}},
year = {2017}
}
@article{Cao2003,
abstract = {Recently, support vector machine (SVM) has become a popular tool in time series forecast- ing. In developing a successful SVM forecastor, the ?rst step is feature extraction. This paper proposes the applications of principal component analysis (PCA), kernel principal component analysis (KPCA) and independent component analysis (ICA) to SVM for feature extraction. PCA linearly transforms the original inputs into new uncorrelated features. KPCA is a nonlinear PCA developed by using the kernel method. In ICA, the original inputs are linearly transformed into features which are mutually statistically independent. By examining the sunspot data, Santa Fe data set A and ?ve real futures contracts, the experiment shows that SVM by feature extraction using PCA, KPCA or ICA can perform better than that without feature extraction. Furthermore, among the three methods, there is the best performance in KPCA feature extraction, followed by ICA feature extraction.},
annote = {Ranking effectiveness of SVM (from best to worst): KPCA, ICA, PCA, and no feature extraction.},
author = {Cao, L. J. and Chua, K. S. and Chong, W. K. and Lee, H. P. and Gu, Q. M.},
doi = {10.1016/S0925-2312(03)00433-8},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Cao et al. - A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {independent component analysis,kernel principal component analysis,principal component analysis,support vector machines},
month = {sep},
number = {1-2},
title = {{A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231203004338},
volume = {55},
year = {2003}
}
@article{Browne2012,
abstract = {Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
author = {Browne, C. and Powley, E. and Whitehouse, D. and Lucas, S. and Cowling, P. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
doi = {10.1109/TCIAIG.2012.2186810},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Browne et al. - A Survey of Monte Carlo Tree Search Methods.pdf:pdf},
isbn = {1943-068X VO - 4},
issn = {1943-068X},
journal = {T-CIAIG},
keywords = {Artificial Intelligence (AI),Bandit-based methods,Computer Go,Game search,Monte Carlo Tree Search (MCTS),Upper Confidence Bounds (UCB),Upper Confidence Bounds for Trees (UCT)},
number = {1},
title = {{A Survey of Monte Carlo Tree Search Methods}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6145622},
volume = {4},
year = {2012}
}
@inproceedings{Larus1992,
abstract = {This paper presents algorithms for inserting monitoring code to profile and trace programs. greatly reduce the cost of measuring These algorithms programs. Profiling counts the number of times each basic block in a program executes and has a variety of applications. Instruction traces are the basis for trace-driven simulation and analysis, and are also used in trace-driven debugging. The profiling algorithm chooses a placement of counters that is optimized-and frequently optimal—with the expected or measured execution frequency respect to of each basic block and branch in the program. The tracing rithm instruments a program to obtain a subsequence algo- of the basic block trace-whose length to the program's execution—from is optimized with respect which the entire trace can be efficiently regenerated, Both algorithms have been implemented and produce a substantial profiling improvement over previous approaches. algorithm reduces the number of counters The by a factor of two and the number of counter increments by up to a factor of four. The tracing size and overhead of an already system by 20-40{\%}.},
annote = {A generally very strong paper. The results are somewhat suspect (see annotations), and the paper skips over the fact that mixing edge and vertices profiling produces better results than either one or the other. There is also no description of their experimental method.},
author = {Larus, R},
booktitle = {POPL},
file = {:Users/cec/Google Drive/Mendeley Library/1992 - Larus - Optimally Profiling and Tracing Programs.pdf:pdf},
title = {{Optimally Profiling and Tracing Programs}},
year = {1992}
}
@inproceedings{Sun2016a,
abstract = {Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions. This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable. We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes's effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed.},
author = {Sun, C. and Le, V. and Su, Z.},
booktitle = {OOPSLA},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Sun, Le, Su - Finding Compiler Bugs via Live Code Mutation.pdf:pdf},
keywords = {automated testing,compiler testing,equivalent,miscompilation,program variants},
title = {{Finding Compiler Bugs via Live Code Mutation}},
year = {2016}
}
@inproceedings{Nobre2016a,
archivePrefix = {arXiv},
arxivId = {1807.00638},
author = {Nobre, R. and Luıs, R. and Cardoso, J. M. P.},
booktitle = {CPC},
eprint = {1807.00638},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Nobre, Luıs, Cardoso - Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy Consumption.pdf:pdf},
title = {{Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy Consumption}},
year = {2016}
}
@inproceedings{Fang2011,
abstract = {Due to its applicability to numerous types of data, including telephone records, web documents, and click streams, the data stream model has recently attracted attention. For analysis of such data, it is crucial to process the data in a single pass, or a small number of passes, using little memory. This paper provides an OpenCL implementation for data streams clustering, and then presents several optimizations for it, which make it more efficient in terms of memory usage. In order to maximize performance for different prob- lem sizes and architectures, we also propose an auto-tuning solution. Experimental results show that our fully optimized implementation can perform 2.1x and 1.4x faster than the native OpenCL implementation on NVIDIA GTX480 and AMD HD5870, respectively; it can also achieve 1.4x to 3.3x speedup relative to the original CUDA implementation solution on GTX480.},
annote = {The paper describes an implementation of data streams clustering, and an autotuner for it. Clustering involves finding a partition of a dataset so that similar items are grouped together into partitions. The solution proposed uses a hybrid of model-based and empirical autotuning, using a rake-based optimisation.},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
booktitle = {CSE},
doi = {10.1109/CSE.2011.104},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Fang, Varbanescu, Sips - An Auto-tuning Solution to Data Streams Clustering in OpenCL.pdf:pdf},
isbn = {978-1-4577-0974-6},
keywords = {Auto-tuning,Clustering,Data Streams,OpenCL,Optimizations,Performance},
month = {aug},
publisher = {Ieee},
title = {{An Auto-tuning Solution to Data Streams Clustering in OpenCL}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6062935},
year = {2011}
}
@inproceedings{Daka2017a,
abstract = {The name of a unit test helps developers to understand the purpose and scenario of the test, and test names support developers when navigating amongst sets of unit tests. When unit tests are gener-ated automatically, however, they tend to be given non-descriptive names such as " test0 " , which provide none of the bene{\"{i}}¿¿ts a de-scriptive name can give a test. The underlying challenge is that automatically generated tests typically do not represent real sce-narios and have no clear purpose other than covering code, which makes naming them di{\"{i}}¿¿cult. In this paper, we present an auto-mated approach which generates descriptive names for automati-cally generated unit tests by summarizing API-level coverage goals. The tests are optimized to be short, descriptive of the test, have a clear relation to the covered code under test, and allow developers to uniquely distinguish tests in a test suite. An empirical evaluation with 47 participants shows that developers agree with the synthe-sized names, and the synthesized names are equally descriptive as manually written names. Study participants were even more accurate and faster at matching code and tests with synthesized names compared to manually derived names.},
author = {Daka, E. and Rojas, J. M. and Fraser, G.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Daka, Rojas, Fraser - Generating unit tests with descriptive names or would you name your children thing1 and thing2.pdf:pdf},
keywords = {2017,acm reference format,automated unit test generation,ermira daka,generating unit,gordon fraser,jos{\'{e}} miguel rojas and,name synthesis,test naming},
title = {{Generating unit tests with descriptive names or: would you name your children thing1 and thing2?}},
year = {2017}
}
@article{Caliskan-islam2016,
annote = {NULL},
author = {Caliskan-islam, A. and Bryson, J. J. and Narayanan, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Caliskan-islam, Bryson, Narayanan - Semantics derived automatically from language corpora necessarily contain human biases.pdf:pdf},
journal = {arXiv:1608.07187},
title = {{Semantics derived automatically from language corpora necessarily contain human biases}},
year = {2016}
}
@article{Mccall1996,
abstract = {General purpose parallel computing systems come in a variety of forms. We have various kinds of distributed memory architectures, shared memory multiprocessors, and clusters of workstations. New technologies may increase this range still further. Can one hope to design portable and scalable parallel software{\&} the face of such architectural diversity? In this paper we show that it is indeed possible to produce fully portable parallel software which will run with highly efficient, scalable and predictable performance on any general purpose parallel architecture. The approach we describe is based on the bulk synchronous parallel (BSP) model of computation. The BSP model provides a simple,unified framework for the design and programming of all kinds of general purpose parallel systems. Over the last few years, a number of important research activities in algorithms and architectures have been pursued as part of this new approach to scalable parallel computing. In this paper we give some simple BSP algorithms and show how they can be expressed as programs. We also briefly describe some of the BSP programming language developments which are now being pursued.},
annote = {NULL},
author = {Mccall, W F},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Mccall - Scalability, portability and predictability The BSP approach to parallel programming.pdf:pdf},
journal = {FGCS},
keywords = {bulk synchronous,parallel algorithms,parallel computing,parallel model,programming languages},
number = {4},
title = {{Scalability, portability and predictability: The BSP approach to parallel programming}},
volume = {12},
year = {1996}
}
@article{Gonzalez2010,
abstract = {Structured parallel programs ought to be conceived as two separate and complementary entities: compu- tation, which expresses the calculations in a procedural manner, and coordination, which abstracts the interaction and communication. By abstracting commonly used patterns of parallel computation, commu- nication, and interaction, algorithmic skeletons enable programmers to code algorithms without specifying platform-dependent primitives. This article presents a literature review on algorithmic skeleton frame- works (ASKF), parallel software development environments furnishing a collection of parameterizable algorithmic skeletons, where the control flow, nesting, resource monitoring, and portability of the resulting parallel program is dictated by the ASKF as opposed to the programmer. Consequently, the ASKF can be positioned as high-level structured parallel programming enablers, as their systematic utilization permits the abstract description of programs and fosters portability by focusing on the description of the algorithmic structure rather than on its detailed implementation.},
annote = {A literature review on algorithmic skeleton frameworks.},
author = {Gonzalez-Velez, Horacio},
doi = {10.1002/spe},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Gonzalez-Velez - A survey of algorithmic skeleton frameworks high-level structured parallel programming enablers.pdf:pdf},
journal = {Software: Practice and Experience},
keywords = {algorithmic skeletons,concurrent programming,parallel computing,parallel constructs,software patterns,structured parallelism,structures},
number = {12},
title = {{A survey of algorithmic skeleton frameworks: high-level structured parallel programming enablers}},
volume = {40},
year = {2010}
}
@article{Ihaka2009a,
abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new lan- guage provides advantages in the areas of portability, computational efficiency, memory management, and scoping.},
annote = {NULL},
author = {Ihaka, Ross and Gentleman, Robert},
doi = {10.2307/1390807},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Ihaka, Gentleman - R A language for data analysis and graphics.pdf:pdf},
isbn = {10618600},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {computer language,statistical computing},
number = {3},
pmid = {3279808},
title = {{R: A language for data analysis and graphics}},
volume = {5},
year = {2009}
}
@inproceedings{Magni2013,
abstract = {The difficulties posed by GPGPU programming and the need to increase productivity have guided research towards directive-based high-level programs for accelerators. This effort has led to the definition of the OpenACC industry standard. It significantly simplifies writing code for graph- ics engines leaving the programmer the opportunity to tune the application for the target hardware and input. In this paper we address the problem of choosing the best mapping of sequential OpenACC loops to the parallel thread- space for a given program and input size. We show that auto-tuning onmapping parameters can improve performance by up to 4.8x over the default chosen by a state-of-the- art compiler. To reduce the overhead of auto-tuning we introduce a search technique that exploits similarities in be- haviour across inputs using a nearest neighbour approach. This dramatically reduces the search for a good mapping (by 97{\%} compared to random search). Finally we propose a heuristic for stopping the focused search which, averaged across 12 benchmarks and 30 input sizes each, achieves a speedup over the default of 1.26x with only 8 sampling runs.},
annote = {NULL},
author = {Magni, A. and Grewe, D. and Johnson, N.},
booktitle = {GPGPU},
doi = {10.1145/2458523.2458530},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Magni, Grewe, Johnson - Input-aware auto-tuning for directive-based GPU programming.pdf:pdf},
isbn = {9781450320177},
keywords = {Autotuning,Experimentation,GPGPU,Languages,OpenACC,Performance},
title = {{Input-aware auto-tuning for directive-based GPU programming}},
url = {http://dl.acm.org/citation.cfm?doid=2458523.2458530},
year = {2013}
}
@inproceedings{Herfert,
abstract = {Reducing the test input given to a program while preserving some property of interest is important, e.g., to localize faults or to reduce test suites. The well-known delta debugging algorithm and its derivatives automate this task by repeatedly reducing a given input. Unfortunately, these approaches are limited to blindly removing parts of the input and cannot reduce the input by restructuring it. This paper presents the Generalized Tree Reduction (GTR) algorithm, an effective and efficient technique to reduce arbitrary test inputs that can be represented as a tree, such as program code, PDF files, and XML documents. The algorithm combines tree transformations with delta debugging and a greedy backtracking algorithm. To reduce the size of the considered search space, the approach automatically specializes the tree transformations applied by the algorithm based on examples of input trees. We evaluate GTR by reducing Python files that cause interpreter crashes, JavaScript files that cause browser inconsistencies, PDF documents with malicious content, and XML files used to tests an XML val-idator. The GTR algorithm reduces the trees of these files to 45.3{\%}, 3.6{\%}, 44.2{\%}, and 1.3{\%} of the original size, respectively, outperforming both delta debugging and another state-of-the-art algorithm.},
author = {Herfert, S. and Patra, J. and Pradel, M.},
booktitle = {ASE},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Herfert, Patra, Pradel - Automatically Reducing Tree-Structured Test Inputs.pdf:pdf},
title = {{Automatically Reducing Tree-Structured Test Inputs}},
year = {2017}
}
@inproceedings{Sui2016a,
abstract = {Approximate computing trades off accuracy of results for re-sources such as energy or computing time. There is a large and rapidly growing literature on approximate computing that has focused mostly on showing the benefits of approx-imate computing. However, we know relatively little about how to control approximation in a disciplined way. In this paper, we address the problem of controlling ap-proximation for non-streaming programs that have a set of " knobs " that can be dialed up or down to control the level of approximation of different components in the program. We formulate this control problem as a constrained opti-mization problem, and describe a system called Capri that uses machine learning to learn cost and error models for the program, and uses these models to determine, for a desired level of approximation, knob settings that optimize metrics such as running time or energy usage. Experimental results with complex benchmarks from different problem domains demonstrate the effectiveness of this approach.},
annote = {NULL},
author = {Sui, X. and Lenharth, A. and Fussell, D. S. and Pingali, K.},
booktitle = {ASPLOS},
doi = {10.1145/2872362.2872402},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Sui et al. - Proactive Control of Approximate Programs.pdf:pdf},
isbn = {9781450340915},
issn = {01635964},
publisher = {ACM},
title = {{Proactive Control of Approximate Programs}},
year = {2016}
}
@inproceedings{Jordan2012,
abstract = {In this paper we introduce a multi-objective auto- tuning framework comprising compiler and runtime components. Focusing on individual code regions, our compiler uses a novel search technique to compute a set of optimal solutions, which are encoded into a multi-versioned executable. This enables the runtime system to choose specifically tuned code versions when dynamically adjusting to changing circumstances. We demonstrate our method by tuning loop tiling in cache- sensitive parallel programs, optimizing for both runtime and efficiency. Our static optimizer finds solutions matching or surpassing those determined by exhaustively sampling the search space on a regular grid, while using less than 4{\%} of the computational effort on average. Additionally, we show that parallelism-aware multi-versioning approaches like our own gain a performance improvement of up to 70{\%} over solutions tuned for only one specific number of threads. I.},
annote = {This paper describes a compiler infrastructure for autotuning programs using multi-versioned executables.},
author = {Jordan, H. and Thoman, P. and Durillo, J. J. and Pellegrini, S. and Gschwandtner, P. and Fahringer, T. and Moritsch, H.},
booktitle = {SC},
doi = {10.1109/SC.2012.7},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Jordan et al. - A multi-objective auto-tuning framework for parallel codes.pdf:pdf},
isbn = {978-1-4673-0806-9},
month = {nov},
publisher = {Ieee},
title = {{A multi-objective auto-tuning framework for parallel codes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6468452},
year = {2012}
}
@phdthesis{Vanderbruggen2018,
author = {Vanderbruggen, T.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Vanderbruggen - Application of Deep-Learning to Compiler-Based Graphs.pdf:pdf},
title = {{Application of Deep-Learning to Compiler-Based Graphs}},
year = {2018}
}
@inproceedings{Ganapathi2009,
annote = {This paper argues for applying statistical machine learning (SML) techniques to develop autotuners for multicore software. They present an autotuner for Stencil codes which can achieve performance within 1{\%} or up to 18{\%} better than that of a human expert after 2 hours of running. They evaluate the performance of a randomly selected 1500 configurations (from a posible 10 million configs), and use Kernel Canonical Correlation Analysis (KCCA) to build correlations between tunable parameter values and measured performance values. Performance is measured using hardware counters (L1 cache misses, TLB misses, cycles per thread) and power consumtion in Watts/sec. KCCA seems like a strange choice: it scales exponentially with the feature vector sizes, and it takes 2 hours (!!!) to build the ML model for 400 sec worth of benchmark data. They present an interesting argument that enegy efficiency should be used as an autotuning target as well as just run time, since it was the power wall that lead to the multicore revolution in the first place. They explain the motivation and results well. I like that they compare their results with human expert and hardware upper bound. It is a solid paper which makes a compelling argument, but their choice of only 2 benchmarks and 2 platforms makes the evaluation of their autotuner a little limited. Cited by 58.},
author = {Ganapathi, A. and Datta, K. and Fox, A. and Patterson, D.},
booktitle = {HotPar},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Ganapathi et al. - A Case for Machine Learning to Optimize Multicore Performance.pdf:pdf},
title = {{A Case for Machine Learning to Optimize Multicore Performance}},
year = {2009}
}
@inproceedings{Cummins2017a,
annote = {NULL},
author = {Cummins, C. and Petoumenos, P. and Zang, W. and Leather, H.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Cummins et al. - Synthesizing Benchmarks for Predictive Modeling.pdf:pdf},
publisher = {IEEE},
title = {{Synthesizing Benchmarks for Predictive Modeling}},
year = {2017}
}
@article{Yin2017,
abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.},
archivePrefix = {arXiv},
arxivId = {1704.01696},
author = {Yin, P. and Neubig, G.},
eprint = {1704.01696},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Yin, Neubig - A Syntactic Neural Model for General-Purpose Code Generation.pdf:pdf},
journal = {arXiv:1704.01696},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{A Syntactic Neural Model for General-Purpose Code Generation}},
url = {http://arxiv.org/abs/1704.01696},
year = {2017}
}
@article{Ashouri2018,
abstract = {Since the mid-1990s, researchers have been trying to use machine-learning based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches and finally, the influential papers of the field.},
archivePrefix = {arXiv},
arxivId = {1801.04405},
author = {Ashouri, A. H. and Killian, W. and Cavazos, J. and Palermo, G. and Silvano, C.},
doi = {10.1145/3197978},
eprint = {1801.04405},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ashouri et al. - A Survey on Compiler Autotuning using Machine Learning(2).pdf:pdf},
issn = {0360-0300},
journal = {CSUR},
number = {5},
title = {{A Survey on Compiler Autotuning using Machine Learning}},
volume = {51},
year = {2018}
}
@article{Tian2015a,
abstract = {Modern graphics processing units (GPUs) include hardwarecontrolled caches to reduce bandwidth requirements and energy consumption. However, current GPU cache hierarchies are inefficient for general purpose GPU (GPGPU) computing. GPGPU workloads tend to include data structures that would not fit in any reasonably sized caches, leading to very low cache hit rates. This problem is exacerbated by the design of current GPUs, which share small caches between many threads. Caching these streaming data structures needlessly burns power while evicting data that may otherwise fit into the cache. We propose a GPU cache management technique to improve the efficiency of small GPU caches while further reducing their power consumption. It adaptively bypasses the GPU cache for blocks that are unlikely to be referenced again before being evicted. This technique saves energy by avoiding needless insertions and evictions while avoiding cache pollution, resulting in better performance. We show that, with a 16KB L1 data cache, dynamic bypassing achieves similar performance to a double-sized L1 cache while reducing energy consumption by 25{\%} and power by 18{\%}. The technique is especially interesting for programs that do not use programmer-managed scratchpad memories. We give a case study to demonstrate the inefficiency of current GPU caches compared to programmer-managed scratchpad memories and show the extent to which cache bypassing can make up for the potential performance loss where the effort to program scratchpad memories is impractical.},
annote = {NULL},
author = {Tian, Y. and Puthoor, S. and Greathouse, J. L. and Beckmann, B. M. and Jim{\'{e}}nez, D. A.},
doi = {10.1145/2716282.2716283},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Tian et al. - Adaptive GPU cache bypassing.pdf:pdf},
isbn = {9781450334075},
journal = {GPGPU},
keywords = {bypassing,graphics processing unit cache,prediction},
title = {{Adaptive GPU cache bypassing}},
url = {http://dl.acm.org/citation.cfm?doid=2716282.2716283},
year = {2015}
}
@inproceedings{Chaimov2015,
annote = {NULL},
author = {Chaimov, N. and Ibrahim, K. Z. and Williams, S.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chaimov, Ibrahim, Williams - Exploiting Communication Concurrency on High Performance Computing Systems.pdf:pdf},
isbn = {9781450334044},
title = {{Exploiting Communication Concurrency on High Performance Computing Systems}},
year = {2015}
}
@inproceedings{Maruyama2011,
abstract = {This paper proposes a compiler-based programming frame- work that automatically translates user-written structured grid code into scalable parallel implementation code for GPU- equipped clusters. To enable such automatic translations, we design a small set of declarative constructs that allow the user to express stencil computations in a portable and implicitly parallel manner. Our framework translates the user-written code into actual implementation code in CUDA for GPU acceleration and MPI for node-level parallelization with automatic optimizations such as computation and com- munication overlapping. We demonstrate the feasibility of such automatic translations by implementing several struc- tured grid applications in our framework. Experimental re- sults on the TSUBAME2.0 GPU-based supercomputer show that the performance is comparable as hand-written code and good strong and weak scalability up to 256 GPUs.},
annote = {Cited by 70.},
author = {Maruyama, Naoya},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Maruyama - Physis An Implicitly Parallel Programming Model for Stencil Computations on Large-Scale GPU-Accelerated Supercomputers.pdf:pdf},
isbn = {9781450307710},
keywords = {application framework,domain specific languages,high,perforamnce computing},
title = {{Physis: An Implicitly Parallel Programming Model for Stencil Computations on Large-Scale GPU-Accelerated Supercomputers}},
year = {2011}
}
@misc{CanergieMellonUniversity2005,
annote = {NULL},
author = {{Canergie Mellon University}},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Canergie Mellon University - 3. Principal Components Analysis.pdf:pdf},
title = {{3. Principal Components Analysis}},
year = {2005}
}
@inproceedings{Han2017a,
author = {Han, W. and Mawhirter, D. and Wu, B. and Buland, M.},
booktitle = {PACT},
doi = {10.1109/PACT.2017.41},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Han et al. - Graphie Large-Scale Asynchronous Graph Traversals on Just a GPU.pdf:pdf},
isbn = {9781509067640},
title = {{Graphie: Large-Scale Asynchronous Graph Traversals on Just a GPU}},
year = {2017}
}
@article{Mcsherry,
annote = {NULL},
author = {Mcsherry, Frank and Isard, Michael and Murray, Derek G},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Mcsherry, Isard, Murray - Scalability! But at what COST.pdf:pdf},
title = {{Scalability! But at what COST?}}
}
@inproceedings{Alsaber2013,
annote = {NULL},
author = {Alsaber, N. and Kulkarni, M.},
booktitle = {PPoPP},
doi = {10.1145/2464996.2465021},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Alsaber, Kulkarni - SemCache Semantics-aware Caching for Efficient GPU Offloading.pdf:pdf},
isbn = {978-1-4503-2130-3},
keywords = {communication optimization,gpgpu,gpu offloading},
title = {{SemCache: Semantics-aware Caching for Efficient GPU Offloading}},
url = {http://doi.acm.org/10.1145/2464996.2465021},
year = {2015}
}
@inproceedings{Singh1998,
abstract = {We are frequently called upon to perform multiple tasks that com- pete for our attention and resource. Often we know the optimal solution to each task in isolation; in this paper, we describe how this knowledge can be exploited to eﬃciently ﬁnd good solutions for doing the tasks in parallel. We formulate this problem as that of dynamically merging multiple Markov decision processes (MDPs) into a composite MDP, and present a new theoretically-sound dy- namic programming algorithm for ﬁnding an optimal policy for the composite MDP. We analyze various aspects of our algorithm and illustrate its use on a simple merging problem.},
annote = {NULL},
author = {Singh, S. and Cohn, D.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Singh, Cohn - How to Dynamically Merge Markov Decision Processes.pdf:pdf},
isbn = {0262100762},
issn = {1049-5258},
keywords = {dynamic programming,dynamical merging,markov decision processes,multiple task scheduling},
publisher = {Citeseer},
title = {{How to Dynamically Merge Markov Decision Processes}},
volume = {10},
year = {1998}
}
@inproceedings{Salimans2016a,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classifica-tion on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a hu-man error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable fea-tures of ImageNet classes.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, T. and Goodfellow, I. and Zaremba, W. and Cheung, V. and Radford, A. and Chen, Xi},
booktitle = {NIPS},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Salimans et al. - Improved Techniques for Training GANs.pdf:pdf},
isbn = {0924-6495},
issn = {09246495},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{Brockschmidt2018,
abstract = {Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.},
archivePrefix = {arXiv},
arxivId = {1805.08490},
author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L. and Polozov, Oleksandr},
eprint = {1805.08490},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Brockschmidt et al. - Generative Code Modeling with Graphs.pdf:pdf},
isbn = {0265-0215 (Print)},
pmid = {15109186},
title = {{Generative Code Modeling with Graphs}},
url = {http://arxiv.org/abs/1805.08490},
year = {2018}
}
@article{Buhrmester2011,
abstract = {Which statement conveys greater risk: 100 people die from cancer every day or 36,500 people die from cancer every year? In statistics where both frequencies and temporal information are used to convey risk, two theories predict opposite answers to this question. Construal level theory predicts that 100 people die from cancer every day will be judged as more risky, while the ratio bias predicts that the equivalent 36,500 people die from cancer every year will result in higher risk judgments. An experiment investigated which format produces higher risk ratings, and whether ratings are influenced by increasing the salience of the numerical or temporal part of the statistic. Forty-eight participants were randomly assigned to a numerical, temporal or control salience condition, and rated risk framed as number of deaths per day or per year. The year format was found to result in higher perceived risk, indicating that the ratio bias effect is dominant, but there was no effect of salience.},
annote = {NULL},
author = {Buhrmester, M. and Kwang, T. and Gosling, S. D.},
doi = {10.1177/1745691610393980},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Buhrmester, Kwang, Gosling - Amazon's Mechanical Turk A new source of inexpensive, yet high-quality, data.pdf:pdf},
isbn = {1930-2975},
issn = {19302975},
journal = {PPS},
keywords = {construal level theory,framing,health statistics,ratio bias,risk perception},
number = {1},
pmid = {26162106},
title = {{Amazon's Mechanical Turk: A new source of inexpensive, yet high-quality, data?}},
url = {http://www.sas.upenn.edu/{~}baron/journal/8210/jdm8210.pdf},
volume = {6},
year = {2011}
}
@inproceedings{Toffola2018,
author = {Toffola, L. D. and Pradel, M. and Gross, T. R.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Toffola, Pradel, Gross - Synthesizing programs that expose performance bottlenecks.pdf:pdf},
keywords = {TOSKIM,dynamic analysis,fuzzing,performance bottlenecks,profiling,program synthesis},
mendeley-tags = {TOSKIM},
title = {{Synthesizing programs that expose performance bottlenecks}},
url = {https://dl.acm.org/citation.cfm?id=3168830},
year = {2018}
}
@article{Burke2013,
abstract = {Hyper-heuristics comprise a set of approaches that are motivated (at least in part) by the goal of automating the design of heuristic methods to solve hard computational search problems. An underlying strategic research challenge is to develop more generally applicable search methodologies. The term hyper-heuristic is relatively new; it was first used in 2000 to describe heuristics to choose heuristics in the context of combinatorial optimisation. However, the idea of automating the design of heuristics is not new; it can be traced back to the 1960s. The definition of hyper-heuristics has been recently extended to refer to a search method or learning mechanism for selecting or generating heuristics to solve computational search problems. Two main hyper-heuristic categories can be considered: heuristic selection and heuristic generation.The distinguishing feature of hyper-heuristics is that they operate on a search space of heuristics (or heuristic components) rather than directly on the search space of solutions to the underlying problem that is being addressed. This paper presents a critical discussion of the scientific literature on hyper-heuristics including their origin and intellectual roots, a detailed account of the main types of approaches, and an overview of some related areas. Current research trends and directions for future research are also discussed.},
annote = {Cited by 203.},
author = {Burke, E. K. and Gendreau, M. and Hyde, M. and Kendall, G. and Ochoa, G. and {\"{O}}zcan, E. and Qu, R.},
doi = {10.1057/jors.2013.71},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Burke et al. - Hyper-heuristics a survey of the state of the art.pdf:pdf},
isbn = {0160-5682},
issn = {0160-5682},
journal = {JORS},
keywords = {combinatorial,evolutionary computation,hyper-heuristics,machine learning,metaheuristics,optimisation,scheduling},
title = {{Hyper-heuristics: a survey of the state of the art}},
volume = {64},
year = {2013}
}
@inproceedings{Chugh2015,
abstract = {We present the Sketch-n-Sketch editor for Scalable Vector Graphics (SVG) that integrates programmatic and direct manipulation, two modes of interaction with complementary strengths. In Sketch-n-Sketch, the user writes a program to generate an output SVG canvas. Then the user may directly manipulate the canvas while the system infers real-time updates to the program in order to match the changes to the output. To achieve this, we propose (i) a technique called trace-based program synthesis that takes program execution history into account in order to constrain the search space and (ii) heuristics for dealing with ambiguities. Based on our experience writing more than 40 examples and from the results of a study with 25 participants, we conclude that Sketch-n-Sketch provides a novel and effective workflow between the boundaries of existing programmatic and direct manipulation systems.},
address = {Santa Barbara, CA},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1507.02988},
author = {Chugh, R. and Hempel, B. and Spradlin, M. and Albers, J.},
booktitle = {PLDI},
doi = {10.1145/2908080.2908103},
eprint = {1507.02988},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Chugh et al. - Programmatic and Direct Manipulation, Together at Last.pdf:pdf},
isbn = {9781450342612},
keywords = {prodirect manipulation,svg,user studies},
title = {{Programmatic and Direct Manipulation, Together at Last}},
year = {2016}
}
@article{Citron2006,
annote = {NULL},
author = {Citron, Daniel and Hurani, Adham and Gnadrey, Alaa},
doi = {10.1145/1186736.1186738},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Citron, Hurani, Gnadrey - The Harmonic or Geometric Mean Does It Really Matter.pdf:pdf},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
number = {4},
pmid = {1186738},
title = {{The Harmonic or Geometric Mean: Does It Really Matter?}},
url = {http://portal.acm.org/citation.cfm?doid=1186736.1186738},
volume = {34},
year = {2006}
}
@article{Radford,
author = {Radford, Alec and Salimans, Tim},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Radford, Salimans - Improving Language Understanding by Generative Pre-Training.pdf:pdf},
pages = {1--12},
title = {{Improving Language Understanding by Generative Pre-Training}}
}
@phdthesis{Rodrigues2014,
abstract = {Parallel programming is a demanding task for developers partly because achieving scal- able parallel speedup requires drawing upon a repertoire of complex, algorithm-specific, architecture-aware programming techniques. Ideally, developers of programming tools would be able to build algorithm-specific, high-level programming interfaces that hide the complex architecture-aware details. However, it is a monumental undertaking to develop such tools from scratch, and it is challenging to provide reusable functionality for developing such tools without sacrificing the hosted interface's performance or ease of use. In particular, to get high performance on a cluster of multicore computers without requiring developers to manually place data and computation onto processors, it is necessary to combine prior methods for shared memory parallelism with new methods for algorithm-aware distribution of computation and data across the cluster. This dissertation presents Triolet, a programming language and compiler for high-level programming of parallel loops for high-performance execution on clusters of multicore com- puters. Triolet adopts a simple, familiar programming interface based on traversing collec- tions of data. By incorporating semantic knowledge of how traversals behave, Triolet achieves efficient parallel execution and communication. Moreover, Triolet's performance on sequen- tial loops is comparable to that of low-level C code, ranging from seven percent slower to 2.8× slower on tested benchmarks. Triolet's design demonstrates that it is possible to decouple the design of a compiler from the implementation of parallelism without sacrificing perfor- mance or ease of use: parallel and sequential loops are implemented as library code and compiled to efficient code by an optimizing compiler that is unaware of parallelism beyond the scope of a single thread. All handling of parallel work partitioning, data partitioning, and scheduling is embodied in library code. During compilation, library code is inlined into a program and specialized to yield customized parallel loops. Experimental results from a 128-core cluster (with 8 nodes and 16 cores per node) show that loops in Triolet outperform loops in Eden, a similar high-level language. Triolet achieves significant parallel speedup over sequential C code, with performance ranging from slightly faster to 4.3× slower than manually parallelized C code on compute-intensive loops. Thus, Triolet demonstrates that a library of container traversal functions can deliver cluster-parallel performance comparable to manually parallelized C code without requiring programmers to manage parallelism. This programming approach opens the potential for future research into parallel programming frameworks.},
annote = {NULL},
author = {Rodrigues, C},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Rodrigues - Supporting high-level, high-performance parallel programming with library-driven optimization.pdf:pdf},
title = {{Supporting high-level, high-performance parallel programming with library-driven optimization}},
url = {https://www.ideals.illinois.edu/handle/2142/49427},
year = {2014}
}
@misc{Etessamib,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 24. Trees.pdf:pdf},
number = {Chapter 11},
title = {{24. Trees}},
year = {2015}
}
@article{Petrov2018,
author = {Petrov, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Petrov - Algorithms Behind Modern Storage Systems.pdf:pdf},
journal = {acmqueue},
number = {april},
pages = {31--51},
title = {{Algorithms Behind Modern Storage Systems}},
year = {2018}
}
@article{Pan2010,
annote = {NULL},
author = {Pan, S. J. and Yang, Q.},
doi = {10.1109/TKDE.2009.191},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Pan, Yang - A Survey on Transfer Learning.pdf:pdf},
journal = {TKDE},
number = {10},
publisher = {IEEE},
title = {{A Survey on Transfer Learning}},
volume = {22},
year = {2010}
}
@unpublished{Jones,
annote = {A great slide deck which gives sensible (although sometimes controversial) guidelines for writing a paper, including this high-level structure:


* Here is a problem
* It's an interesting problem
* It's an unsolved problem
* {\_}Here is my idea{\_}
* My idea works (details, data)
* Here's how my idea compares to other people's approaches},
author = {Jones, Simon Peyton},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Jones - How to write a great research paper.pdf:pdf},
title = {{How to write a great research paper}}
}
@inproceedings{Coplin2015b,
abstract = {This paper studies the effects of source-code optimizations on the$\backslash$r$\backslash$nperformance, power draw, and energy consumption of a modern$\backslash$r$\backslash$ncompute GPU. We evaluate 128 versions of two n-body codes: a$\backslash$r$\backslash$ncompute-bound regular implementation and a memory-bound irregular$\backslash$r$\backslash$nimplementation. Both programs include six optimizations$\backslash$r$\backslash$nthat can be individually enabled or disabled. We measured the active$\backslash$r$\backslash$nruntime and the power consumption of each code version on$\backslash$r$\backslash$nthree inputs, various GPU clock frequencies, two arithmetic precisions,$\backslash$r$\backslash$nand with and without ECC. This paper investigates which$\backslash$r$\backslash$noptimizations primarily improve energy efficiency, which ones$\backslash$r$\backslash$nmainly boost performance, and which ones help both aspects. Some$\backslash$r$\backslash$noptimizations also have the added benefit of reducing the power$\backslash$r$\backslash$ndraw. Our analysis shows that individual and combinations of optimizations$\backslash$r$\backslash$ncan alter the performance and energy consumption of a$\backslash$r$\backslash$nGPU kernel by up to a factor of five.},
annote = {NULL},
author = {Coplin, J. and Burtscher, M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Coplin, Burtscher - Effects of Source-Code Optimizations on GPU Performance and Energy Consumption.pdf:pdf},
isbn = {9781450334075},
keywords = {efficiency,gpu architectures,performance evaluation,power and energy,source-code optimization},
title = {{Effects of Source-Code Optimizations on GPU Performance and Energy Consumption}},
year = {2015}
}
@article{Ashouri2017,
author = {Ashouri, A. H. and Bignoli, A. and Palermo, G. and Silvano, C. and Kulkarni, S. and Cavazos, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Ashouri et al. - MiCOMP Mitigating the Compiler Phase-ordering Problem Using Optimization Sub-sequences and Machine Learning.pdf:pdf},
journal = {TACO},
title = {{MiCOMP: Mitigating the Compiler Phase-ordering Problem Using Optimization Sub-sequences and Machine Learning}},
year = {2017}
}
@article{Drepper2007,
abstract = {As CPU cores become both faster and more numerous, the limiting factor for most programs is now, and will be for some time, memory access. Hardware designers have come up with ever more sophisticated memory handling and acceleration techniquessuch as CPU cachesbut these cannot work optimally without some help from the programmer. Unfortunately, neither the structure nor the cost of using the memory subsystem of a computer or the caches on CPUs is well understood by most programmers. This paper explains the structure of memory subsys- tems in use on modern commodity hardware, illustrating why CPU caches were developed, how they work, and what programs should do to achieve optimal performance by utilizing them.},
annote = {NULL},
author = {Drepper, U.},
doi = {10.1.1.91.957},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Drepper - What every programmer should know about memory.pdf:pdf},
issn = {0361526X},
pmid = {11807537},
title = {{What every programmer should know about memory}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.957{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://diyhpl.us/{~}bryan/papers2/distributed/distributed-systems/what-every-programmer-should-know-about-memory.2007.pdf},
volume = {3},
year = {2007}
}
@inproceedings{Pacanu2013,
annote = {NULL},
author = {Pacanu, R. and Mikolov, T. and Bengio, Y.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Pacanu, Mikolov, Bengio - On the Difficulties of Training Recurrent Neural Networks.pdf:pdf},
title = {{On the Difficulties of Training Recurrent Neural Networks}},
year = {2013}
}
@misc{IntelCorporation2012,
annote = {NULL},
author = {{Intel Corporation}},
title = {{OpenCL* Optimization Guide}},
url = {https://software.intel.com/sites/landingpage/opencl/optimization-guide/index.htm},
year = {2012}
}
@inproceedings{Friedley2013,
annote = {NULL},
author = {Friedley, A. and Bronevetsky, G. and Lumsdaine, A. and Hoefler, T.},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Friedley et al. - Hybrid MPI Efficient Message Passing for Multi-core Systems.pdf:pdf},
publisher = {ACM},
title = {{Hybrid MPI : Efficient Message Passing for Multi-core Systems}},
year = {2013}
}
@article{Agrawal2004,
abstract = {We present an unconditional deterministic polynomial-time algorithm that determines whether an input number is prime or composite.},
annote = {NULL},
author = {Agrawal, M. and Kayal, N. and Saxena, N.},
doi = {10.4007/annals.2004.160.781},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Agrawal, Kayal, Saxena - PRIMES is in P.pdf:pdf},
issn = {0003486X},
journal = {Annals of Mathematics},
number = {2},
title = {{PRIMES is in P}},
volume = {160},
year = {2004}
}
@inproceedings{Wu2014,
abstract = {GitHub provides various social features for developers to collaborate with others. Those features are important for developers to coordinate their work (Dabbish et al., 2012; Marlow et al., 2013). We hypothesized that the social system of GitHub users was bound by system interactions such that contributing to similar code repositories would lead to users following one another on GitHub or vice versa. Using a quadratic assignment procedure (QAP) correlation, however, only a weak correlation among followship and production activities (code, issue, and wiki contributions) was found. Survey with GitHub users revealed an ecosystem on the Internet for software developers, which includes many platforms, such as Forrst, Twitter, and Hacker News, among others. Developers make social introductions and other interactions on these platforms and engage with one anther on GitHub. Due to these preliminary findings, we describe GitHub as a part of a larger ecosystem of developer interactions.},
annote = {Cited by 7.},
author = {Wu, Y. and Kropczynski, J. and Shih, P. C. and Carroll, J. M.},
booktitle = {CSCW},
doi = {10.1145/2556420.2556483},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wu et al. - Exploring the Ecosystem of Software Developers on GitHub and Other Platforms.pdf:pdf},
isbn = {9781450325417},
keywords = {Ecosystem,Follow,GitHub,Social connection},
title = {{Exploring the Ecosystem of Software Developers on GitHub and Other Platforms}},
year = {2014}
}
@inproceedings{Braione2017,
author = {Braione, P. and Denaro, G. and Mattavelli, A. and Pezz{\`{e}}, M.},
booktitle = {ISSTA},
doi = {10.1145/3092703.3092715},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Braione et al. - Combining symbolic execution and search-based testing for programs with complex heap inputs.pdf:pdf},
isbn = {9781450350761},
keywords = {-  Software and its engineering  -{\textgreater}  Software test,Formal software verification,acm reference format,automatic test case generation,search-based,software engineering,symbolic execution},
title = {{Combining symbolic execution and search-based testing for programs with complex heap inputs}},
url = {http://dl.acm.org/citation.cfm?doid=3092703.3092715},
year = {2017}
}
@inproceedings{Benoit2005a,
abstract = {We present an overview of eSkel, a library for skeletal parallel programming. eSkel aims to maximise the conceptual flexibility afforded by its component skeletons and to facilitate dynamic selection of skeleton compositions. We present simple examples which illustrate these proper- ties, and discuss the implementation challenges which the model poses.},
annote = {NULL},
author = {Benoit, A. and Cole, M. and Gilmore, S. and Hillston, J.},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Benoit et al. - Flexible Skeletal Programming with eSkel.pdf:pdf},
publisher = {Springer},
title = {{Flexible Skeletal Programming with eSkel}},
year = {2005}
}
@inproceedings{Kima,
annote = {NULL},
author = {Kim, Jinwook and Kim, Min-soo},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kim, Kim - GStream A Graph Streaming Processing Method for Large-Scale Graphs on GPUs.pdf:pdf},
isbn = {9781450332057},
keywords = {gpu,graph processing,large-scale,stream},
title = {{GStream : A Graph Streaming Processing Method for Large-Scale Graphs on GPUs}},
year = {2015}
}
@incollection{Aldinucci2011,
annote = {NULL},
author = {Aldinucci, M. and Danelutto, M. and Kilpatrick, P. and Torquati, M.},
booktitle = {Programming Multi-core and Many-core Computing Systems},
chapter = {13},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Aldinucci et al. - FastFlow high-level and efficient streaming on multi-core (A FastFlow short tutorial).pdf:pdf},
publisher = {Wiley},
title = {{FastFlow: high-level and efficient streaming on multi-core (A FastFlow short tutorial)}},
year = {2011}
}
@inproceedings{Spector2005,
abstract = {One of Push's attractive features in this context is$\backslash$nits transparent support for the expression and$\backslash$nevolution of modular architectures and complex control$\backslash$nstructures, achieved through explicit code$\backslash$nself-manipulation. The latest version of Push, Push3,$\backslash$nenhances this feature by permitting explicit$\backslash$nmanipulation of an execution stack that contains the$\backslash$nexpressions that are queued for execution in the$\backslash$ninterpreter. presents a series of examples in which$\backslash$nPush3 was used with a simple genetic programming system$\backslash$n(PushGP) to evolve programs with non-trivial control$\backslash$nstructures.},
author = {Spector, L. and Klein, J. and Keijzer, M.},
booktitle = {GECCO},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Spector, Klein, Keijzer - The Push3 execution stack and the evolution of control.pdf:pdf},
keywords = {Fibonacci sequence,combinators,experimentation,exponentiation,factorial,genetic algorithms,genetic programming,iteration,languages,parity,push,recursion,reversing a list,sorting,stack-based genetic programming},
title = {{The Push3 execution stack and the evolution of control}},
year = {2005}
}
@article{Rossum2012e,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Functional Programming HOWTO.pdf:pdf},
title = {{Functional Programming HOWTO}},
year = {2016}
}
@inproceedings{Gramoli,
annote = {NULL},
author = {Gramoli, Vincent},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gramoli - More Than You Ever Wanted to Know about Synchronization Synchrobench , Measuring the Impact of the Synchronization on C.pdf:pdf},
isbn = {9781450332057},
keywords = {benchmark,data structure,lock-freedom,reusability},
title = {{More Than You Ever Wanted to Know about Synchronization Synchrobench , Measuring the Impact of the Synchronization on Concurrent Algorithms}},
year = {2015}
}
@inproceedings{Dubach2012,
abstract = {Languages such as OpenCL and CUDA offer a standard interface for general-purpose programming of GPUs. However, with these languages, programmers must explicitly manage numerous low- level details involving communication and synchronization. This burden makes programming GPUs difficult and error-prone, ren- dering these powerful devices inaccessible to most programmers. We desire a higher-level programming model that makes GPUs more accessible while also effectively exploiting their computa- tional power. This paper presents features of Lime, a new Java- compatible language targeting heterogeneous systems, that allow an optimizing compiler to generate high quality GPU code. The key insight is that the language type system enforces isolation and immutability invariants that allow the compiler to optimize for a GPU without heroic compiler analysis. We evaluate the performance of the resulting code, comparing against an implementation on the Java Virtual Machine (with JIT) and against hand-tuned native OpenCL code. The compiler attains GPU speedups relative to JVM/JIT between 12x and 431x, while achieving between 75{\%} and 140{\%} of the performance of native OpenCL code.},
annote = {This paper presents Lime, a high-level JVM language targeting heterogenous parallelism. The type system enforces isolation and immutability invariants.




This paper is worth citing as an approach to high-level parallelism through the creation of new languages.},
author = {Dubach, C. and Cheng, P. and Bacon, D. F. and Fink, S. J.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Dubach et al. - Compiling a High-Level Language for GPUs (via Language Support for Architectures and Compilers).pdf:pdf},
isbn = {9781450312059},
keywords = {GPU,Heterogeneous,Java,Map Reduce,OpenCL,Streaming},
title = {{Compiling a High-Level Language for GPUs (via Language Support for Architectures and Compilers)}},
year = {2012}
}
@misc{Bundy2014a,
abstract = {Writing a good research grant proposal is not easy. This document is an attempt to collect together a number of suggestions about what makes a good proposal.},
annote = {A set of guidelines for writing grant proposals. There's a useful cross-over with the research proposals I will be writing.




I think the most profound criteria for assessing an {\_}idea{\_} for a research proposal is "Is it a research problem, or is it just a routine application of known techniques?"},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bundy - Writing a good grant proposal.pdf:pdf},
title = {{Writing a good grant proposal}},
year = {2014}
}
@inproceedings{Balaprakash2012,
abstract = {Automatic performance tuning of computationally intensive kernels in scientific applications is a promising approach to achieving good performance on different machines while preserving the kernel implementation's readability and portability. A major bottleneck in automatic performance tuning is the computation time required to test a large number of possible code variants, which grows exponentially with the number of tuning parameters. Consequently, the design, development, and analysis of effective search techniques capable of quickly finding high-performing parameter configurations have gained significant attention in recent years. An important element needed for this research is a collection of test problems that allow performance engineering and mathematical optimization researchers to conduct rigorous algorithmic development and experimental studies. In this paper, we describe a set of extensible and portable search problems in automatic performance tuning (SPAPT) whose goal is to aid in the development and improvement of search strategies. SPAPT is a test suite that contains representative serial code implementations from a number of lower-level performance-tuning tasks in scientific applications. We present an illustrative experimental study on several problems from the test suite. We discuss important issues such as modeling, search space characteristics, and performance objectives. {\textcopyright} 2012 Published by Elsevier Ltd.},
author = {Balaprakash, P. and Wild, S. M. and Norris, B.},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2012.04.214},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Balaprakash, Wild, Norris - SPAPT Search problems in automatic performance tuning.pdf:pdf},
issn = {18770509},
keywords = {Autotuning,Benchmark,Empirical tuning,Kernels,Optimization,Performance tuning,Test suite},
title = {{SPAPT: Search problems in automatic performance tuning}},
year = {2012}
}
@phdthesis{Guo2012a,
abstract = {Research programming is a type of programming activity where the goal is to write computer programs to obtain insights from data. Millions of professionals in fields ranging from science, engineering, business, finance, public policy, and journalism, as well as numerous students and computer hobbyists, all perform research programming on a daily basis. My thesis is that by understanding the unique challenges faced during research programming, it becomes possible to apply techniques from dynamic program analy- sis, mixed-initiative recommendation systems, and OS-level tracing to make research programmers more productive. This dissertation characterizes the research programming process, describes typi- cal challenges faced by research programmers, and presents five software tools that I have developed to address some key challenges. 1.) ProactiveWrangler is an interac- tive graphical tool that helps research programmers reformat and clean data prior to analysis. 2.) IncPy is a Python interpreter that speeds up the data analysis scripting cycle and helps programmers manage code and data dependencies. 3.) SlopPy is a Python interpreter that automatically makes existing scripts error-tolerant, thereby also speeding up the data analysis scripting cycle. 4.) Burrito is a Linux-based system that helps programmers organize, annotate, and recall past insights about their experiments. 5.) CDE is a software packaging tool that makes it easy to deploy, archive, and share research code. Taken together, these five tools enable re- search programmers to iterate and potentially discover insights faster by offloading the burdens of data management and provenance to the computer.},
annote = {NULL},
author = {Guo, Philip J},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Guo - Software tools to facilitate research programming.pdf:pdf},
number = {May},
school = {Stanford Univeristy},
title = {{Software tools to facilitate research programming}},
year = {2012}
}
@article{Lloyd1999,
abstract = {Computers are physical systems: what they can and cannot do is dictated by the laws of physics. In particular, the speed with which a physical device can process information is limited by its energy and the amount of information that it can process is limited by the number of degrees of freedom it possesses. This paper explores the physical limits of computation as determined by the speed of light {\$}c{\$}, the quantum scale {\$}\backslashhbar{\$} and the gravitational constant {\$}G{\$}. As an example, quantitative bounds are put to the computational power of an `ultimate laptop' with a mass of one kilogram confined to a volume of one liter.},
annote = {Cited by 493.},
archivePrefix = {arXiv},
arxivId = {quant-ph/9908043},
author = {Lloyd, Seth},
eprint = {9908043},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Lloyd - Ultimate physical limits to computation.pdf:pdf},
journal = {Nature},
month = {aug},
number = {6799},
primaryClass = {quant-ph},
title = {{Ultimate physical limits to computation}},
url = {http://arxiv.org/abs/quant-ph/9908043},
volume = {406},
year = {1999}
}
@article{Hellerstein,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.03651v1},
author = {Hellerstein, J. M. and Faleiro, J. and Gonzalez, J. E and Schleier-smith, J. and Sreekanti, V. and Tumanov, A. and Wu, C.},
eprint = {arXiv:1812.03651v1},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Hellerstein et al. - Serverless Computing One Step Forward , Two Steps Back.pdf:pdf},
journal = {arXiv:182.03651},
title = {{Serverless Computing: One Step Forward , Two Steps Back}}
}
@inproceedings{Sainath2015,
abstract = {Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) have shown improvements over Deep Neural Networks (DNNs) across a wide variety of speech recognition tasks. CNNs, LSTMs and DNNs are complementary in their modeling capabilities, as CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and DNNs are appropriate for mapping features to a more separable space. In this paper, we take advantage of the complementarity of CNNs, LSTMs and DNNs by combining them into one unified architecture. We explore the proposed architecture, which we call CLDNN, on a variety of large vocabulary tasks, varying from 200 to 2,000 hours. We find that the CLDNN provides a 4-6{\%} relative improvement in WER over an LSTM, the strongest of the three individual models.},
annote = {Cited by 28.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sainath, T. N. and Vinyals, O. and Senior, A. and Sak, H.},
booktitle = {ICASSP},
doi = {10.1109/ICASSP.2015.7178838},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Sainath et al. - Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks.pdf:pdf},
isbn = {9781467369978},
issn = {15206149},
pmid = {25246403},
title = {{Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks}},
volume = {2015-Augus},
year = {2015}
}
@inproceedings{Ramachandran2015,
annote = {NULL},
author = {Ramachandran, Arunmoezhi},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ramachandran - CASTLE Fast Concurrent Internal Binary Search Tree using Edge-Based Locking.pdf:pdf},
isbn = {9781450332057},
keywords = {binary search tree,concurrent data structure,edge-based locking,insert and delete opera-,inter-,lock-based algorithm,nal representation,nizing ordered data that,supports search},
title = {{CASTLE : Fast Concurrent Internal Binary Search Tree using Edge-Based Locking}},
year = {2015}
}
@misc{Dastgeer2013,
abstract = {In earlier work, we have developed the SkePU skeleton programming library for modern multicore systems equipped with one or more programmable GPUs. The library internally provides four types of implementations (implemen- tation variants) for each skeleton: serial C++, OpenMP, CUDA and OpenCL targeting either CPU or GPU execution respectively. Deciding which implemen- tation would run faster for a given skeleton call depends upon the computation, problem size(s), system architecture and data locality. In this paper, we present our work on automatic selection between these im- plementation variants by an offline machine learning method which generates a compact decision treewith lowtraining overhead. The proposed selectionmecha- nism is flexible yet high-level allowing a skeleton programmer to control different training choices at a higher abstraction level.We have evaluated our optimization strategy with 9 applications/kernels ported to our skeleton library and achieve on average more than 94{\%} (90{\%}) accuracy with just 0.53{\%} (0.58{\%}) training space exploration on two systems. Moreover, we discuss one application scenariowhere local optimization considering a single skeleton call can prove sub-optimal, and propose a heuristic for bulk implementation selection considering more than one skeleton call to address such application scenarios.},
annote = {This paper describes an offline machine learning based autotuner for SkePU which uses a decision tree to decide which implementation of a skeleton should be used (C++, OpenMP, CUDA, OpenCL).




There is some super relevant stuff in here for me, and it's worth a re-read.},
author = {Dastgeer, U. and Li, L. and Kessler, C.},
booktitle = {APPT},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Dastgeer, Li, Kessler - Adaptive Implementation Selection in the SkePU Skeleton Programming Library.pdf:pdf},
keywords = {GPU programming,Skeleton programming,adaptive offline learning,automated performance tuning,implementation selection},
publisher = {Springer},
title = {{Adaptive Implementation Selection in the SkePU Skeleton Programming Library}},
year = {2013}
}
@inproceedings{Ghuloum2006,
abstract = {A systematic approach for reducing the cost of regression testing and fault localization through the use of incremental data flow analysis is described. Incremental data flow analysis is used to identify that portion of a program affected by a change so that testing efforts can be focused accordingly. The analysis allows the partitioning of existing test cases into relevant, nonrelevant, and invalid classes. This greatly reduces the effort associated with validating a program following modification},
address = {Portland, OR},
annote = {NULL},
author = {Ghuloum, Abdulaziz},
booktitle = {Scheme and Functional Programming Workshop},
doi = {10.1109/CMPSAC.1989.65142},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Ghuloum - An Incremental Approach to Compiler Construction.pdf:pdf},
publisher = {Citeseer},
title = {{An Incremental Approach to Compiler Construction}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.170{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@article{Zhang2015,
annote = {NULL},
author = {Zhang, Weizhe and Cheng, Albert and Subhlok, Jaspal},
doi = {10.1109/TC.2015.2417526},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Zhang, Cheng, Subhlok - DwarfCode A Performance Prediction Tool for Parallel Applications.pdf:pdf},
isbn = {1274844582612},
issn = {0018-9340},
journal = {TC},
number = {c},
title = {{DwarfCode: A Performance Prediction Tool for Parallel Applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7098397},
volume = {9340},
year = {2015}
}
@phdthesis{Steuwer2015a,
annote = {NULL},
author = {Steuwer, Michel},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Steuwer - Improving Programmability and Performance Portability on many-core processors.pdf:pdf},
title = {{Improving Programmability and Performance Portability on many-core processors}},
year = {2015}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
annote = {NULL},
author = {Gatys, L. A. and Ecker, A. S. and Bethge, M.},
doi = {10.1561/2200000006},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gatys, Ecker, Bethge - A Neural Algorithm of Artistic Style.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv:1508.06576},
keywords = {eural algorithm of artistic,style},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Ward2006,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Distributing Python Modules.pdf:pdf},
title = {{Distributing Python Modules}},
year = {2016}
}
@inproceedings{Bodin1998,
abstract = {This paper investigates the applicability of iterative search techniques in program optimisation. Iterative com- pilation is usually considered too expensive for general pur- pose computing but is applicable to embedded applications where the cost is easily amortised over the number of em- bedded systems produced. This paper presents a case study, where an iterative search algorithm is used to investigate a non-linear transformation space and find the fastest execu- tion time within a fixed number of evaluations. By using profile feedback in the form of execution time, it searches a large but restricted transformation space and shows per- formance improvement over existing approaches. We show that in the case of large transformation spaces, we can achieve within 0.3{\%} of the best possible time by visiting less then 0.25{\%} of the space using a simple algorithm and find the minimum after visiting up to less than 1 {\%} of the space. 1.},
annote = {Cited by 117.},
author = {Bodin, F. and Kisuki, T. and Knijnenburg, P. M. W. and O'Boyle, M. and Rohou, E.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Bodin et al. - Iterative compilation in a non-linear optimisation space.pdf:pdf},
publisher = {ACM},
title = {{Iterative compilation in a non-linear optimisation space}},
year = {1998}
}
@phdthesis{Dubach2005,
annote = {NULL},
author = {Dubach, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Dubach - Java Byte Code Synthesis for Reconfigurable Computing Platforms.pdf:pdf},
title = {{Java Byte Code Synthesis for Reconfigurable Computing Platforms}},
year = {2005}
}
@article{Wang2016a,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
annote = {NULL},
author = {Wang, J. and Kurth-Nelson, Z. and Tirumala, D. and Soyer, H. and Leibo, J. Z. and Munos, R. and Blundell, C. and Kumaran, D. and Botvinick, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Wang et al. - Learning to reinforcement learn.pdf:pdf},
journal = {arXiv:1611.05763},
title = {{Learning to reinforcement learn}},
year = {2016}
}
@incollection{Flannery1986,
author = {Flannery, B. P. and Teukolsky, S. and Press, W. H. and Vetterling, W. T.},
booktitle = {Numerical Recipes in C: The Art of Scientific Computing},
file = {:Users/cec/Google Drive/Mendeley Library/1986 - Flannery et al. - Are Two Distributions Different.pdf:pdf},
title = {{Are Two Distributions Different?}},
year = {1986}
}
@inproceedings{Botorog1996,
abstract = {In this paper we present Skil, an imperative language enhancedwith higher-order functions and currying, as well aswith a polymorphic type system. The high level of Skil al- lows the integration of algorithmic skeletons, i.e. of higher- order functions representing parallel computation patterns. At the same time, the language can be efficiently imple- mented. After describing a series of skeletons which work with distributed arrays, we give two examples of parallel programs implemented on the basis of skeletons, namely shortest paths in graphs and Gaussian elimination. Run- time measurements show that we approach the efficiency of message-passing C up to a factor between 1 and 2.5.},
annote = {Skil is an imperative language for writing algorithmic skeletons, which extends C by adding functional features and polymorphic types. Data-parallel skeletons are benchmarked and shown to approach the efficiency of MPI up to a factor of 1-2.5x.},
author = {Botorog, G.H. and Kuchen, H.},
booktitle = {HPDC},
doi = {10.1109/HPDC.1996.546194},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Botorog, Kuchen - Skil an imperative language with algorithmic skeletons for efficient distributed programming.pdf:pdf},
isbn = {0-8186-7582-9},
publisher = {Ieee},
title = {{Skil: an imperative language with algorithmic skeletons for efficient distributed programming}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=546194},
year = {1996}
}
@misc{UniversityofEdinburgh2014ba,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 13.pdf:pdf},
title = {{IAML 13}},
year = {2014}
}
@article{Gal2015,
abstract = {A long strand of empirical research has claimed that dropout cannot be applied between the recurrent connections of a recurrent neural network (RNN). The reasoning has been that the noise hinders the network's ability to model sequences, and instead should be applied to the RNN's inputs and outputs alone. But dropout is a vital tool for regularisation, and without dropout in recurrent layers our models overfit quickly. In this paper we show that a recently developed theoretical framework, casting dropout as approximate Bayesian inference, can give us mathematically grounded tools to apply dropout within the recurrent layers. We apply our new dropout technique in long short-term memory (LSTM) networks and show that the new approach significantly outperforms existing techniques.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Gal, Y. and Ghahramani, Z.},
doi = {10.1201/9781420049176},
eprint = {1512.05287},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gal, Ghahramani - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
issn = {0302-9743},
journal = {arXiv:1512.05287},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
year = {2015}
}
@incollection{Yearning-drafta,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 28-30).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 28-30)}},
year = {2018}
}
@inproceedings{Kuck1981,
abstract = {Dependence graphs can be used as a vehicle for formulating and implementing compiler optimizations. This paper defines such graphs and discusses two kinds of transformations. The first are simple rewriting transformations that remove dependence arcs. The second are abstraction transformations that deal more globally with a dependence graph. These transformations have been implemented and applied to several different types of high-speed architectures.},
annote = {NULL},
author = {Kuck, D. J. and Kuhn, R. H. and Padua, D. and Leasure, B. and Wolfe, M.},
booktitle = {POPL},
doi = {10.1145/567532.567555},
file = {:Users/cec/Google Drive/Mendeley Library/1981 - Kuck et al. - Dependence graphs and compiler optimizations.pdf:pdf},
isbn = {089791029X},
publisher = {ACM},
title = {{Dependence graphs and compiler optimizations}},
url = {http://portal.acm.org/citation.cfm?doid=567532.567555},
year = {1981}
}
@incollection{Lattner2014,
abstract = {This chapter discusses some of the design decisions that shaped LLVM , an umbrella project that 1 hosts and develops a set of close-knit low-level toolchain components (e.g., assemblers, compilers, debuggers, etc.), which are designed to be compatible with existing tools typically used on Unix systems. The name "LLVM" was once an acronym, but is now just a brand for the umbrella project. While LLVM provides some unique capabilities, and is known for some of its great tools (e.g., the Clang compiler , a C/C++/Objective-C compiler which provides a number of benefits over 2 the GCC compiler), the main thing that sets LLVM apart from other compilers is its internal architecture.},
annote = {A very nice and brisk overview of the LLVM toolchain components, covering the structure of the LLVM project, and design of the optimisation chain and code generators. While giving a high level overview of these broad topics, the paper still finds time to give several detailed and code-driven examples of specific features, such as optimisation pass implementation. Lattner makes a strong argument for the technical advantages of LLVM over existing tools, without overstating claims of success.},
author = {Lattner, Chris},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Lattner - LLVM.pdf:pdf},
title = {{LLVM}},
year = {2014}
}
@inproceedings{Micolet2016,
annote = {NULL},
author = {Micolet, P. and Smith, A. and Dubach, C.},
booktitle = {LCTES},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Micolet, Smith, Dubach - A Machine Learning Approach to Mapping Streaming Workloads to Dynamic Multicore Processors.pdf:pdf},
isbn = {9781450343169},
keywords = {dynamic multicore processor,machine learning,streaming programming languages},
publisher = {ACM},
title = {{A Machine Learning Approach to Mapping Streaming Workloads to Dynamic Multicore Processors}},
year = {2016}
}
@article{Chen2012,
annote = {This study investigates dataset portability during iterative compilation. It presents 1000 datasets and 32 benchmark programs, and tests these. They find that optimising programs accross datasets is easy, and only a handful of optimisations yield most of the speedup. They conclude that "iterative optimisation is an irreplaceable and important compiler strategy".








The paper concedes that it cannot assert that the conclusions generalize beyond the benchmark suites, copmilers used (ICC {\&} GCC), and Intel architecture. This raises questions as to the value of the paper's contribution.},
author = {Chen, Y. and Fang, S. and Huang, Y. and Eeckhout, L. and Fursin, G. and Temam, O. and Wu, C.},
doi = {10.1145/2355585.2355594},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Chen et al. - Deconstructing Iterative Optimization.pdf:pdf},
issn = {15443566},
journal = {TACO},
month = {sep},
number = {3},
title = {{Deconstructing Iterative Optimization}},
url = {http://dl.acm.org/citation.cfm?doid=2355585.2355594},
volume = {9},
year = {2012}
}
@misc{UniversityofEdinburgh2009,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - University of Edinburgh - IAML 2012 Exam.pdf:pdf},
number = {May},
title = {{IAML 2012 Exam}},
year = {2009}
}
@misc{Goddardb,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 6.pdf:pdf},
title = {{IAML 6}},
year = {2014}
}
@inproceedings{Guzman2014,
abstract = {Emotions have a high impact in productivity, task quality, creativity, group rapport and job satisfaction. In this work we use lexical sentiment analysis to study emotions expressed in commit comments of different open source projects and analyze their relationship with different factors such as used programming language, time and day of the week in which the commit was made, team distribution and project approval. Our results show that projects developed in Java tend to have more negative commit comments, and that projects that have more distributed teams tend to have a higher positive polarity in their emotional content. Additionally, we found that commit comments written on Mondays tend to a more negative emotion. While our results need to be confirmed by a more representative sample they are an initial step into the study of emotions and related factors in open source projects.},
annote = {People are grumpier on Mondays, and programming in Java. Cited by 17.},
author = {Guzman, E. and Az{\'{o}}car, D. and Li, Y.},
booktitle = {MSR},
doi = {10.1145/2597073.2597118},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Guzman, Az{\'{o}}car, Li - Sentiment Analysis of Commit Comments in GitHub an Empirical Study.pdf:pdf},
isbn = {9781450328630},
keywords = {Human Factors in Software Engineering,Sentiment Analysis},
title = {{Sentiment Analysis of Commit Comments in GitHub: an Empirical Study}},
year = {2014}
}
@inproceedings{Dixon,
abstract = {The biggest gain in fast processing of big-data will most likely be a result of mapping computation onto clusters of machines while ex- ploiting per-processor parallelism by means of vector instructions and multi-threading. As a result, a new generation of parallel com- puting infrastructure is needed, driven by the need for application scalability through harnessing growing computing resources in the public cloud, in private data centers and custom clusters, and even on the desktop. This paper uses Our Pattern Language (OPL) to guide the de- sign of a pattern oriented software framework for analytics applica- tions which enables scalability, flexibility, modularity and portabil- ity. Using a compute intensive financial application as a motivating example, we demonstrate how following a pattern oriented design approach leads to parallel code in which the domains of concerns for modeling and mapping the computations to the architecture are cleanly delineated, the code is portable across architectures, and accessible to both an application developer and systems developer. In future work, we seek to demonstrate this software framework by architecting and developing a portable and scalable version of quantlib, a popular open-source quantitative finance library.},
annote = {NULL},
author = {Dixon, Matthew},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Dixon - A Pattern Oriented Approach for Designing Scalable Analytics Applications (Invited Talk).pdf:pdf},
isbn = {9781450334051},
keywords = {Pattern Oriented Software Frameworks,Python,Quantitative Finance},
title = {{A Pattern Oriented Approach for Designing Scalable Analytics Applications (Invited Talk)}},
year = {2015}
}
@inproceedings{Phillips2010,
abstract = {This paper describes the use of CUDA to accelerate the Himeno benchmark on clusters with GPUs. The implementation is designed to optimize memory bandwidth utilization. Our approach achieves over 83{\%} of the theoretical peak bandwidth on a NVIDIA Tesla C1060 GPU and performs at over 50 GFlops. A multi-GPU implementation that utilizes MPI alongside CUDA streams to overlap GPU execution with data transfers allows linear scaling and performs at over 800 GFlops on a cluster with 16 GPUs. The paper presents the optimizations required to achieve this level of performance.},
annote = {Cited by 57.},
author = {Phillips, E. H. and Fatica, M.},
booktitle = {IPDPS},
doi = {10.1109/IPDPS.2010.5470394},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Phillips, Fatica - Implementing the Himeno benchmark with CUDA on GPU clusters.pdf:pdf},
isbn = {9781424464432},
issn = {1530-2075},
title = {{Implementing the Himeno benchmark with CUDA on GPU clusters}},
year = {2010}
}
@article{Stroustrup2010,
annote = {NULL},
author = {Stroustrup, Bjarne},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Stroustrup - New Value Terminology.pdf:pdf},
title = {{"New" Value Terminology}},
url = {http://www2.research.att.com/{~}bs/terminology.pdf},
year = {2010}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the varia- tional autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of vari- ability observed in highly-structured sequential data (such as speech). We em- pirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02216v1},
author = {Chung, J. and Kastner, K. and Dinh, L. and Goel, K. and Courville, A. and Bengio, Y.},
eprint = {arXiv:1506.02216v1},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chung et al. - A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
issn = {10495258},
journal = {arXiv:1506.02216},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
year = {2015}
}
@misc{UniversityofEdinburgh2014l,
annote = {NFAs and DFAs.},
author = {{University of Edinburgh}},
doi = {10.1145/365719.366426},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 3. Compiling techniques.pdf:pdf},
issn = {00010782},
title = {{3. Compiling techniques}},
volume = {9},
year = {2015}
}
@article{Choi2016,
annote = {Strikingly similar to our PACT'17 submission. They do synthetic benchmark generation, sequence encoding, padding, embedding, variable renaming. Their synthetic benchmarks look very simple. See examples here: https://raw.githubusercontent.com/mjc92/buffer{\_}overrun{\_}memory{\_}networks/master/test{\_}1{\_}100.txt},
author = {Choi, M. and Jeong, S. and Oh, H. and Choo, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Choi et al. - End-to-End Prediction of Buffer Overruns from Raw Source Code via Neural Memory Networks.pdf:pdf},
journal = {arXiv:1703.02458},
title = {{End-to-End Prediction of Buffer Overruns from Raw Source Code via Neural Memory Networks}},
year = {2017}
}
@article{Hielscher2013,
abstract = {Productivity languages such as NumPy and Matlab make it much easier to implement data-intensive numerical algorithms. However, these languages can be intolerably slow for programs that don't map well to their built-in primitives. In this paper, we discuss locality optimizations for our system Parakeet, a just-in-time compiler and runtime system for an array-oriented subset of Python. Parakeet dynamically compiles whole user functions to high performance multi-threaded native code. Parakeet makes extensive use of the classic data parallel operators Map, Reduce, and Scan. We introduce a new set of data parallel operators,TiledMap, TiledReduce, and TiledScan, that break up their computations into local pieces of bounded size so as better to make use of small fast memories. We introduce a novel tiling transformation to generate tiled operators automatically. Applying this transformation once tiles the program for cache, and applying it again enables tiling for registers. The sizes for cache tiles are left unspecified until runtime, when an autotuning search is performed. Finally, we evaluate our optimizations on benchmarks and show significant speedups on programs that exhibit data locality.},
annote = {This paper introduces tiled Map, Reduce, and Scan operations for Parakeet, an implicitly paralle, JIT compiled subset of Python. Tile size is dynamically autotuned by evaluating mutliple configurations simultaneously in order to search the space, before settling on the best performance.},
archivePrefix = {arXiv},
arxivId = {1304.1835},
author = {Hielscher, Eric and Rubinsteyn, Alex and Shasha, Dennis},
eprint = {1304.1835},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Hielscher, Rubinsteyn, Shasha - Locality Optimization for Data Parallel Programs.pdf:pdf},
journal = {arXiv:1304.1835},
month = {apr},
title = {{Locality Optimization for Data Parallel Programs}},
url = {http://arxiv.org/abs/1304.1835},
volume = {1},
year = {2013}
}
@inproceedings{Frigo2007,
abstract = {We present a cache oblivious algorithm for stencil computations, which arise for example in finite-difference methods. Our algorithm applies to arbitrary stencils in n -dimensional spaces. On an "ideal cache" of size Z , our algorithm saves a factor of {\&}Theta;( Z 1/ n cache misses compared to a naive algorithm, and it exploits temporal locality optimally throughout the entire memory hierarchy.},
annote = {Cited by 109.},
author = {Frigo, Matteo and Strumpen, Volker},
booktitle = {SC},
doi = {10.1007/s11227-007-0111-y},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Frigo, Strumpen - Cache oblivious stencil computations.pdf:pdf},
isbn = {1595931678},
issn = {09208542},
keywords = {Analysis of algorithms,Cache oblivious algorithms,Performance analysis,Stencil computations,System simulation},
publisher = {ACM},
title = {{Cache oblivious stencil computations}},
volume = {39},
year = {2005}
}
@article{Hu2017,
abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
annote = {NULL},
author = {Hu, Z. and Yang, Z. and Liang, X. and Salakhutdinov, R. and Xing, E. P.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Hu et al. - Controllable Text Generation.pdf:pdf},
journal = {arXiv:1703.00955},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Controllable Text Generation}},
year = {2017}
}
@article{Mcaleer2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.07470v1},
author = {Mcaleer, S. and Agostinelli, F. and Shmakov, A. and Baldi, P.},
eprint = {arXiv:1805.07470v1},
file = {:Users/cec/Google Drive/Mendeley Library//2018 - Mcaleer et al. - Solving the Rubik's Cube Without Human Knowledge.pdf:pdf},
journal = {arXiv:1805.07470},
title = {{Solving the Rubik's Cube Without Human Knowledge}},
year = {2018}
}
@article{Wang2014b,
abstract = {General purpose GPU based systems are highly attractive as they give potentially massive performance at little cost. Re- alizing such potential is challenging due to the complexity of programming. This paper presents a compiler based ap- proach to automatically generate optimized OpenCL code from data-parallel OpenMP programs for GPUs. Such an approach brings together the benefits of a clear high lev- el language (OpenMP) and an emerging standard (OpenCL) for heterogeneous multi-cores. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architec- tures and uses predictive modeling to automatically deter- mine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on the multi-core host. We applied our approach to the entire NAS parallel benchmark suite and evaluated it on two distinct GPU based systems: Core i7/NVIDIA GeForce GTX 580 and Core i7/AMD Radeon 7970. We achieved average (up to) speedups of 4.51x and 4.20x (143x and 67x) respectively over a sequential base- line. This is, on average, a factor 1.63 and 1.56 times faster than a hand-coded, GPU-specific OpenCL implementation developed by independent expert programmers.},
annote = {NULL},
author = {Wang, Z. and Grewe, D. and O'Boyle, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wang, Grewe, O'Boyle - Automatic and Portable Mapping of Data Parallel Programs to OpenCL for GPU-based Heterogeneous Systems.pdf:pdf},
journal = {TACO},
number = {4},
title = {{Automatic and Portable Mapping of Data Parallel Programs to OpenCL for GPU-based Heterogeneous Systems}},
volume = {11},
year = {2014}
}
@misc{Arapinis2014e,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 06. Relations.pdf:pdf},
title = {{06. Relations}},
year = {2015}
}
@inproceedings{Diego,
annote = {NULL},
author = {Saygin, A. P. and Cicekli, I. and Akman, V.},
booktitle = {The Turing Test},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Saygin, Cicekli, Akman - Turing Test 50 Years Later.pdf:pdf},
keywords = {chatbots,chinese room,consciousness,imitation game,intelligence,loebner contest,philosophy of mind,turing test},
publisher = {Springer},
title = {{Turing Test: 50 Years Later}},
year = {2003}
}
@inproceedings{Riba2018,
author = {Riba, P. and Fischer, A. and Llad{\'{o}}s, J. and Forn{\'{e}}s, A.},
booktitle = {ICPR},
doi = {10.1136/jme.2006.019810},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Riba et al. - Learning Graph Distances with Message Passing Neural Networks.pdf:pdf},
isbn = {1473-4257 (Electronic)$\backslash$r0306-6800 (Linking)},
issn = {10514651},
pmid = {18448726},
title = {{Learning Graph Distances with Message Passing Neural Networks}},
year = {2018}
}
@article{Fursin2014,
abstract = {Empirical auto-tuning and machine learning techniques have been showing high potential to improve execution time, power consumption, code size, reliability and other important metrics of various applications for more than two decades. However, they are still far from widespread production use due to lack of native support for auto-tuning in an ever changing and complex software and hardware stack, large and multi-dimensional optimization spaces, excessively long exploration times, and lack of unified mechanisms for preserving and sharing of optimization knowledge and research material. We present a possible collaborative approach to solve above problems using Collective Mind knowledge management system. In contrast with previous cTuning framework, this modular infrastructure allows to preserve and share through the Internet the whole auto-tuning setups with all related artifacts and their software and hardware dependencies besides just performance data. It also allows to gradually structure, systematize and describe all available research material including tools, benchmarks, data sets, search strategies and machine learning models. Researchers can take advantage of shared components and data with extensible meta-description to quickly and collaboratively validate and improve existing auto-tuning and benchmarking techniques or prototype new ones. The community can now gradually learn and improve complex behavior of all existing computer systems while exposing behavior anomalies or model mispredictions to an interdisciplinary community in a reproducible way for further analysis. We present several practical, collaborative and model-driven auto-tuning scenarios. We also decided to release all material at c-mind.org/repo to set up an example for a collaborative and reproducible research as well as our new publication model in computer engineering where experimental results are continuously shared and validated by the community.},
annote = {This paper presents a collaborative and big-data driven approach to auto-tuning, which allows for researchers to share and evaluate ML and DM results results. The paper argues that the inability to develop auto-tuning and machine learning tools can be attributable to:
* Lack of common, diverse benchmarks and data sets.
* Lack of common experimental methodology.
* Problems with continuously changing hardware/software stack.
* Difficulty to reproduce results due to lack of full HW/SW dependencies.
* Difficulty to validate techniques due to lack of sharing in publications.
Cited by 5.},
author = {Fursin, G. and Miceli, R. and Lokhmotov, A. and Gerndt, M. and Baboulin, M. and Malony, A. D. and Chamski, Z. and Novillo, D. and {Del Vento}, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Fursin et al. - Collective Mind Towards practical and collaborative auto-tuning.pdf:pdf},
journal = {Scientific Programming},
keywords = {NoSQL repository,agile development,big data driven optimization,code and data sharing,collaborative experimentation,collaborative knowledge management,data mining,high performance computing,machine learning,model driven optimization,modeling of computer behavior,multi-objective optimization,open access publication model,performance prediction,performance regression buildbot,plugin-based tuning,public repository of knowledge,reproducible research,specification sharing,systematic auto-tuning,systematic benchmarking},
number = {4},
publisher = {IOS Press},
title = {{Collective Mind: Towards practical and collaborative auto-tuning}},
volume = {22},
year = {2014}
}
@article{Johnston2015a,
annote = {NULL},
author = {Johnston, T. and Alsulmi, M. and Cicotti, P. and Taufer, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Johnston et al. - Performance Tuning of MapReduce Jobs Using Surrogate-Based Modeling.pdf:pdf},
journal = {ICCS},
title = {{Performance Tuning of MapReduce Jobs Using Surrogate-Based Modeling}},
year = {2015}
}
@article{Aaronson2005,
abstract = {Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and "anthropic computing." The section on soap bubbles even includes some "experimental" results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {quant-ph/0502072},
author = {Aaronson, S.},
eprint = {0502072},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Aaronson - NP-complete Problems and Physical Reality.pdf:pdf},
journal = {arXiv:quant-ph/0502072},
month = {feb},
primaryClass = {quant-ph},
title = {{NP-complete Problems and Physical Reality}},
url = {http://arxiv.org/abs/quant-ph/0502072},
year = {2005}
}
@article{Wang2014c,
abstract = {Compiler-based auto-parallelization is a much studied area, but has yet to find wide-spread application. This is largely due to the poor identification and exploitation of application parallelism, resulting in disap- pointing performance far below that which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach, resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection we overcome the limitations of static analysis, enabling the identification of more application parallelism and only rely on the user for final approval. We then replace the traditional target-specific and inflexible mapping heuristics with a machine-learning based prediction mechanism, resulting in better mapping decisions while automating adaptation to different target architectures. We have evaluated our parallelization strategy on the NAS and SPEC CPU2000 benchmarks and two different multi-core plat- forms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade).We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers, but comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96{\%} of the performance of the hand-tuned OpenMP NAS and SPEC parallel bench- marks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning based parallelization for complex multi-core platforms.},
annote = {NULL},
author = {Wang, Z. and Tournavitis, G. and Franke, B. and O'Boyle, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wang et al. - Integrating Profile-driven Parallelism Detection and Machine-learning-based Mapping.pdf:pdf},
journal = {TACO},
title = {{Integrating Profile-driven Parallelism Detection and Machine-learning-based Mapping}},
year = {2014}
}
@article{Chen2013,
abstract = {Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.},
author = {Chen, Y. and Groce, A. and Zhang, C. and Wong, W. and Fern, X. and Eide, E. and Regehr, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Chen et al. - Taming Compiler Fuzzers.pdf:pdf},
journal = {PLDI},
keywords = {automated testing,bug reporting,compiler defect,compiler testing,fuzz testing,random testing,test-case reduction},
title = {{Taming Compiler Fuzzers}},
year = {2013}
}
@techreport{OBoyle1992,
author = {O'Boyle, M. F. P.},
file = {:Users/cec/Google Drive/Mendeley Library/1992 - O'Boyle - Program and Data Transformations for Efficient Execution on Distributed Memory Architectures.pdf:pdf},
title = {{Program and Data Transformations for Efficient Execution on Distributed Memory Architectures}},
year = {1992}
}
@article{Cownie2000,
annote = {NULL},
author = {Cownie, James and Moore, Shirley},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Cownie, Moore - Portable OpenMP debugging with TotalView.pdf:pdf},
journal = {EWOMP},
title = {{Portable OpenMP debugging with TotalView}},
year = {2000}
}
@inproceedings{Bohm2011,
abstract = {Dynamic Binary Translation (DBT) is the key technology behind cross-platform virtualization and allows software compiled for one Instruction Set Architecture (ISA) to be executed on a processor supporting a different ISA. Under the hood, DBT is typically im- plemented using Just-In-Time (JIT) compilation of frequently ex- ecuted program regions, also called traces. The main challenge is translating frequently executed program regions as fast as possi- ble into highly efficient native code. As time for JIT compilation adds to the overall execution time, the JIT compiler is often de- coupled and operates in a separate thread independent from the main simulation loop to reduce the overhead of JIT compilation. In this paper we present two innovative contributions. The first con- tribution is a generalized trace compilation approach that consid- ers all frequently executed paths in a program for JIT compilation, as opposed to previous approaches where trace compilation is re- stricted to paths through loops. The second contribution reduces JIT compilation cost by compiling several hot traces in a concurrent task farm. Altogether we combine generalized light-weight trac- ing, large translation units, parallel JIT compilation and dynamic work scheduling to ensure timely and efficient processing of hot traces. We have evaluated our industry-strength, LLVM-based par- allel DBT implementing the ARCompact ISA against three bench- mark suites (EEMBC, BIOPERF and SPEC CPU2006) and demon- strate speedups of up to 2.08 on a standard quad-core Intel Xeon machine. Across short- and long-running benchmarks our scheme is robust and never results in a slowdown. In fact, using four proces- sors total execution time can be reduced by on average 11.5{\%} over state-of-the-art decoupled, parallel (or asynchronous) JIT compilation.},
address = {New York, New York, USA},
annote = {Dynamic Binary Translation (DBT) allows software compiled for one ISA to be executed on another. This paper describes an approach to trace compilation with 100{\%} coverage of hot paths, and a concurrent task farm JIT compiler.




Could be useful when considering runtime dynamic optimisation.},
author = {B{\"{o}}hm, I. and {Edler von Koch}, T. J. K. and Kyle, S. C. and Franke, B. and Topham, N.},
booktitle = {PLDI},
doi = {10.1145/1993498.1993508},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - B{\"{o}}hm et al. - Generalized just-in-time trace compilation using a parallel task farm in a dynamic binary translator.pdf:pdf},
isbn = {9781450306638},
keywords = {Dynamic binary translation,dynamic work scheduling,just-in-time compilation,parallelization,task farm},
publisher = {ACM Press},
title = {{Generalized just-in-time trace compilation using a parallel task farm in a dynamic binary translator}},
url = {http://portal.acm.org/citation.cfm?doid=1993498.1993508},
year = {2011}
}
@inproceedings{Shazeer2017,
annote = {NULL},
author = {Shazeer, N. and Mirhoseini, A. and Maziarz, K. and Davis, A. and Le, Q. and Dean, J.},
booktitle = {ICLR},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Shazeer et al. - Outrageously Large Neural Networks the Sparsely Gated Mixture-of-Experts Layer.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Outrageously Large Neural Networks: the Sparsely Gated Mixture-of-Experts Layer}},
year = {2017}
}
@inproceedings{Eastep2011,
abstract = {As multicores become prevalent, the complexity of program- ming is skyrocketing. One major difficulty is efficiently or- chestrating collaboration among threads through shared data structures. Unfortunately, choosing and hand-tuning data structure algorithms to get good performance across a va- riety of machines and inputs is a herculean task to add to the fundamental difficulty of getting a parallel program cor- rect. To help mitigate these complexities, this work develops a new class of parallel data structures called Smart Data Structures that leverage online machine learning to adapt automatically. We prototype and evaluate an open source library of Smart Data Structures for common parallel pro- gramming needs and demonstrate significant improvements over the best existing algorithms under a variety of condi- tions. Our results indicate that learning is a promising tech- nique for balancing and adapting to complex, time-varying tradeoffs and achieving the best performance available.},
address = {New York, NY, USA},
annote = {The authors use Reinforcement Learning to automatically tune the scancounts of a parallel data structure. They use the Natural Actor-Critic algorithm to achieve {\textless}= 1.5x speedups over static parameters.




The paper has a few "student work" smells, but I really like the idea. They seem incredibly limited by only considering a single "knob".},
author = {Eastep, J. and Wingate, D. and Agarwal, A.},
booktitle = {ICAC},
doi = {10.1145/1998582.1998587},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Eastep, Wingate, Agarwal - Smart Data Structures An Online Machine Learning Approach to Multicore Data Structures.pdf:pdf},
isbn = {9781450306072},
keywords = {auto-tuning,autonomic,concurrent data structures,performance optimization,self-aware,synchronization},
publisher = {ACM},
title = {{Smart Data Structures: An Online Machine Learning Approach to Multicore Data Structures}},
url = {http://doi.acm.org/10.1145/1998582.1998587},
year = {2011}
}
@inproceedings{Oda2015,
abstract = {—Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural lan-guages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.},
annote = {NULL},
author = {Oda, Y. and Fudaba, H. and Neubig, G. and Hata, H. and Sakti, S. and Toda, T. and Nakamura, S.},
booktitle = {ASE},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Oda et al. - Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation.pdf:pdf},
keywords = {TOREAD,algorithms,but is more readable,comprehension of beginners because,doing,education,it explicitly describes,statistical approach,than an,unfamiliar programming language,what the program is},
mendeley-tags = {TOREAD},
publisher = {IEEE},
title = {{Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation}},
year = {2015}
}
@article{Yin2018,
abstract = {We introduce the problem of learning distributed representations of edits. By combining a "neural editor" with an "edit encoder", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem.},
archivePrefix = {arXiv},
arxivId = {1810.13337},
author = {Yin, P. and Neubig, G. and Allamanis, M. and Brockschmidt, M. and Gaunt, A. L.},
doi = {10.1080/02688697.2016.1244252},
eprint = {1810.13337},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Yin et al. - Learning to Represent Edits.pdf:pdf},
isbn = {0268-8697},
journal = {arXiv:1810.13337},
pmid = {27760466},
title = {{Learning to Represent Edits}},
url = {http://arxiv.org/abs/1810.13337},
year = {2018}
}
@techreport{Patra2016,
author = {Patra, J. and Pradel, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Patra, Pradel - Learning to Fuzz Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data.pdf:pdf},
institution = {TU Darmstadt},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data}},
year = {2016}
}
@article{Goli2016,
annote = {NULL},
author = {Goli, Mehdi and Gonz{\'{a}}lez–V{\'{e}}lez, Horacio},
doi = {10.1007/s10766-016-0419-4},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Goli, Gonz{\'{a}}lez–V{\'{e}}lez - Autonomic Coordination of Skeleton-Based Applications Over CPUGPU Multi-Core Architectures.pdf:pdf},
issn = {0885-7458},
journal = {IJPP},
keywords = {Algorithmic skeletons,Multicore architectures,Parallel architectures,Parallel computing,Parallel patterns,Software development methods,Structured parallelism},
publisher = {Springer US},
title = {{Autonomic Coordination of Skeleton-Based Applications Over CPU/GPU Multi-Core Architectures}},
url = {http://link.springer.com/10.1007/s10766-016-0419-4},
year = {2016}
}
@article{Rossum2012a,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Logging HOWTO.pdf:pdf},
title = {{Logging HOWTO}},
year = {2016}
}
@inproceedings{Brown2011a,
abstract = {Computing systems are becoming increasingly parallel and heterogeneous, and therefore new applications must be capable of exploiting parallelism in order to continue achieving high performance. However, targeting these emerging devices often requires using multiple disparate programming models and making decisions that can limit forward scalability. In previous work we proposed the use of domain-specific languages (DSLs) to provide high-level abstractions that enable transformations to high performance parallel code without degrading programmer productivity. In this paper we present a new end-to-end system for building, compiling, and executing DSL applications on parallel heterogeneous hardware, the Delite Compiler Framework and Runtime. The framework lifts embedded DSL applications to an intermediate representation (IR), performs generic, parallel, and domain-specific optimizations, and generates an execution graph that targets multiple heterogeneous hardware devices. Finally we present results comparing the performance of several machine learning applications written in OptiML, a DSL for machine learning that utilizes Delite, to C++ and MATLAB implementations. We find that the implicitly parallel OptiML applications achieve single-threaded performance comparable to C++ and outperform explicitly parallel MATLAB in nearly all cases.},
annote = {Delite programming model. Cited by 93.},
author = {Brown, K. J. and Sujeeth, A. K. and Lee, H. J. and Rompf, T. and Chafi, H. and Odersky, M. and Olukotun, K.},
booktitle = {PACT},
doi = {10.1109/PACT.2011.15},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Brown et al. - A heterogeneous parallel framework for domain-specific languages.pdf:pdf},
isbn = {9780769545660},
issn = {1089795X},
keywords = {Computer Languages,Multicore processing,Parallel programming},
publisher = {ACM},
title = {{A heterogeneous parallel framework for domain-specific languages}},
year = {2011}
}
@inproceedings{Trachsel2010,
abstract = {Competitive parallel execution (CPE) is a simple yet attrac- tive technique to improve the performance of sequential pro- grams on multi-core and multi-processor systems. A sequen- tial program is transformed into a CPE-enabled program by introducing multiple variants for parts of the program. The performance of different variants depends on runtime condi- tions, such as program input or the execution platform, and the execution time of a CPE-enabled program is the sum of the shortest variants. Variants compete at run-time under the control of a CPE- aware run-time system. The run-time system ensures that the behavior and outcome of a CPE-enabled program is not distinguishable from the one of its original sequential coun- terpart. We present and evaluate a run-time system that is implemented as a user-space library and that closely inter- acts with the operating system. The paper discusses two strategies for the generation of variants and investigates the applicability of CPE for two usage scenarios: i) computation-driven CPE: a simple and straightforward parallelization of heuristic algorithms, and ii) compiler-driven CPE: generation of CPE-enabled pro- grams as part of the compilation process using different op- timization strategies. Using a state-of-the-art SAT solver as an illustrative example, we report for compiler-based CPE speedups of 4–6{\%}for many data sets, with a maximumslow- down of 2{\%}. Computation-driven CPE provides super-linear speedups for 5 out of 31 data sets (with a maximum speedup of 7.4) and at most a slow-down of 1{\%} for two data sets.},
annote = {Competitive parallel execution transforms sequential programs for parallel execution by introducing variants into the program which are suited for different run-time conditions.








Their results are weak. The related work section is particularly relevant for auto-parallelisation.},
author = {Trachsel, Oliver and Gross, Thomas R},
booktitle = {CF},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Trachsel, Gross - Variant-based Competitive Parallel Execution of Sequential Programs.pdf:pdf},
isbn = {9781450300445},
keywords = {Design,Experimentation,Performance},
publisher = {ACM},
title = {{Variant-based Competitive Parallel Execution of Sequential Programs}},
year = {2010}
}
@incollection{Ng2018a,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 47-49).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 47-49)}},
year = {2018}
}
@article{Gupta2001,
abstract = {Since the early days of logic programming, researchers in the field realized the potential for exploitation of parallelism present in the execution of logic programs. Their high-level nature, the presence of nondeterminism, and their referential transparency, among other characteristics, make logic programs interesting candidates for obtaining speedups through parallel execution. At the same time, the fact that the typical applications of logic programming frequently involve irregular computations, make heavy use of dynamic data structures with logical variables, and involve search and speculation, makes the techniques used in the corresponding parallelizing compilers and run-time systems potentially interesting even outside the field. The objective of this article is to provide a comprehensive survey of the issues arising in parallel execution of logic programming languages along with the most relevant approaches explored to date in the field. Focus is mostly given to the challenges emerging from the parallel execution of Prolog programs. The article describes the major techniques used for shared memory implementation of Or-parallelism, And-parallelism, and combinations of the two. We also explore some related issues, such as memory management, compile-time analysis, and execution visualization.},
annote = {Skim read. Cited by 193.},
author = {Gupta, G. and Pontelli, E. and Ali, K. A. M. and Carlsson, M. and Hermenegildo, M. V.},
doi = {10.1145/504083.504085},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Gupta et al. - Parallel execution of prolog programs a survey.pdf:pdf},
issn = {01640925},
journal = {TOPLAS},
number = {4},
publisher = {ACM},
title = {{Parallel execution of prolog programs: a survey}},
volume = {23},
year = {2001}
}
@misc{AndersonP.W.1972,
abstract = {Broken symmetry ande the nature of the hierachial structure of science.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Anderson P. W.}},
booktitle = {Science},
doi = {10.1126/science.177.4047.393},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/1972 - Anderson P. W. - More is Different.pdf:pdf},
isbn = {8028675581},
issn = {0036-8075},
number = {4047},
pmid = {17796623},
title = {{More is Different}},
volume = {177},
year = {1972}
}
@article{Bird2015,
annote = {NULL},
author = {Bird, C. and Sutton, C. and Allamanis, M. and Barr, E. T. and Bird, C. and Sutton, C.},
doi = {10.1145/2786805.2786849},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Bird et al. - Suggesting Accurate Method and Class Names.pdf:pdf},
isbn = {9781450336758},
journal = {FSE},
keywords = {Coding conventions,naturalness of software},
publisher = {ACM},
title = {{Suggesting Accurate Method and Class Names}},
year = {2015}
}
@article{Feautrier1991,
abstract = {Given a program written in a simple imperative language (assignment statements, for loops, affine indices and loop limits), this paper presents an algorithm for analyzing the patterns along which values flow as the execution proceeds. For each array or scalar reference, the result is the name and iteration vector of the source statement as a function of the iteration vector of the referencing statement. The paper discusses several applications of the method: conversion of a program to a set of recurrence equations, array and scalar expansion, program verification and parallel program construction.},
annote = {Cited by 712.},
author = {Feautrier, Paul},
doi = {10.1007/BF01407931},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Feautrier - Dataflow analysis of array and scalar references.pdf:pdf},
issn = {08857458},
journal = {IJPP},
keywords = {Dataflow analysis,array expansion,semantics analysis},
number = {I},
title = {{Dataflow analysis of array and scalar references}},
volume = {20},
year = {1991}
}
@inproceedings{Le2015,
abstract = {Compiler testing is important and challenging. Equivalence Modulo Inputs (EMI) is a recent promising approach for compiler validation. It is based on mutating the unexecuted statements of an existing program under some inputs to produce new equivalent test programs w.r.t. these inputs. Orion is a simple realization of EMI by only randomly deleting unexecuted statements. Despite its success in finding many bugs in production compilers, Orion's effectiveness is still limited by its simple, blind mutation strategy. To more effectively realize EMI, this paper introduces a guided, advanced mutation strategy based on Bayesian optimization. Our goal is to generate diverse programs to more thoroughly exercise compilers. We achieve this with two techniques: (1) the support of both code deletions and insertions in the unexecuted regions, leading to a much larger test program space; and (2) the use of an objective function that promotes control-flow-diverse programs for guiding Markov Chain Monte Carlo (MCMC) optimization to explore the search space. Our technique helps discover deep bugs that require elaborate mutations. Our realization, Athena, targets C compilers. In 19 months, Athena has found 72 new bugs-many of which are deep and important bugs-in GCC and LLVM. Developers have confirmed all 72 bugs and fixed 68 of them.},
author = {Le, V. and Sun, C. and Su, Z.},
booktitle = {OOPSLA},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Le, Sun, Su - Finding Deep Compiler Bugs via Guided Stochastic Program Mutation.pdf:pdf},
keywords = {automated testing,compiler testing,equivalent program variants,markov chain monte carlo},
title = {{Finding Deep Compiler Bugs via Guided Stochastic Program Mutation}},
year = {2015}
}
@misc{UniversityofEdinburgh2014z,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 1.pdf:pdf},
title = {{IAML 1}},
year = {2014}
}
@inproceedings{Bergstra2011,
abstract = {In this paper, we present Theano 1 , a framework in the Python programming language for defining, optimizing and evaluating expressions involving high-level operations on ten-sors. Theano offers most of NumPy's functionality, but adds automatic symbolic differen-tiation, GPU support, and faster expression evaluation. Theano is a general mathematical tool, but it was developed with the goal of facilitating research in deep learning. The Deep Learning Tutorials 2 introduce recent advances in deep learning, and showcase how Theano makes such algorithms compact, elegant, and fast.},
annote = {NULL},
author = {Bergstra, J. and Bastien, F. and Breuleux, O. and Lamblin, P. and Pascanu, R. and Delalleau, O. and Desjardins, G. and Warde-Farley, D. and Goodfellow, I. and Bergeron, A. and Bengio, Y.},
booktitle = {BigLearning Workshop},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Bergstra et al. - Theano Deep Learning on GPUs with Python.pdf:pdf},
keywords = {GPU,Python,compiler,computer algebra system,convolutional networks,deep learning,symbolic differentiation},
title = {{Theano: Deep Learning on GPUs with Python}},
year = {2011}
}
@article{Finnsson2009,
author = {Bjornsson, Y. and Finnsson, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Bjornsson, Finnsson - CadiaPlayer A Simulation-Based General Game Player.pdf:pdf},
journal = {T-CIAIG},
number = {1},
title = {{CadiaPlayer: A Simulation-Based General Game Player}},
volume = {1},
year = {2009}
}
@article{Meuleau1998,
abstract = {We present a technique for computing approximately optimal solutions to stochastic resource allocation problems modeled as Markov decision processes (MDPs). We exploit two key properties to avoid explicitly enumerating the very large state and action spaces associated with these problems. First, the problems are composed of multiple tasks whose utilities are independent. Second, the actions taken with respect to (or resources allocated to) a task do not influence the status of any other task. We can therefore view each task as an MDP.How- ever, these MDPs are weakly coupled by resource constraints: actions selected for one MDP restrict the actions available to others. We describe heuristic techniques for dealing with sev- eral classes of constraints that use the solutions for individual MDPs to construct an approximate global solution. We demon- strate this techniqueon problems involving thousandsof tasks, approximating the solution to problems that are far beyondthe reach of standardmethods.},
annote = {NULL},
author = {Meuleau, N. and Hauskrecht, M. and Kim, K. and Peshkin, L. and Kaelbling, L. P. and Dean, T. and Boutilier, C.},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Meuleau et al. - Solving Very Large Weakly Coupled Markov Decision Processes.pdf:pdf},
journal = {AAAI},
keywords = {Copyright {\textcopyright}1998 American Association for Artificia},
title = {{Solving Very Large Weakly Coupled Markov Decision Processes}},
year = {1998}
}
@inproceedings{Karpathy2016,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, A. and Johnson, J. and Fei-Fei, L.},
booktitle = {ICLR},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1506.02078v1},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Karpathy, Johnson, Fei-Fei - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pmid = {26353135},
title = {{Visualizing and Understanding Recurrent Networks}},
year = {2016}
}
@inproceedings{Zaremba2015a,
abstract = {We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using {\$}Q{\$}-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by {\$}Q{\$}-learning.},
annote = {NULL},
author = {Zaremba, W. and Mikolov, T. and Joulin, A. and Fergus, R.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zaremba et al. - Learning Simple Algorithms from Examples.pdf:pdf},
title = {{Learning Simple Algorithms from Examples}},
year = {2016}
}
@misc{UniversityofEdinburgh2014w,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 2.pdf:pdf},
title = {{IAML 2}},
year = {2014}
}
@inproceedings{Lattner2008a,
abstract = {This talk gives a gentle introduction to LLVM, talking about the benefits of using llvm-gcc and including updated compile-time and run-time performance numbers. This talk is high level and particularly suited for those without a deep compiler hacker background.},
annote = {LLVM is a collection of compiler technologies: optimiser and code generator, llvm-gcc and Clang front-ends, MSIL and .NET virtual machines. Existing compilers have stagnated.},
author = {Lattner, Chris},
booktitle = {ACAT},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Lattner - Introduction to the LLVM Compiler System.pdf:pdf},
title = {{Introduction to the LLVM Compiler System}},
year = {2008}
}
@book{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Y.},
booktitle = {Foundations and Trends in Machine Learning},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Bengio - Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
number = {1},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@inproceedings{Pacheco2007,
abstract = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
author = {Pacheco, C. and Lahiri, S. K. and Ernst, M. D. and Ball, T.},
booktitle = {ICSE},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Pacheco et al. - Feedback-directed Random Test Generation.pdf:pdf},
title = {{Feedback-directed Random Test Generation}},
year = {2007}
}
@inproceedings{Ashouri2016a,
annote = {NULL},
author = {Ashouri, A. H. and Bignoli, A. and Palermo, G. and Silvano, C.},
booktitle = {PARMA-DITAM},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ashouri et al. - Predictive Modeling Methodology for Compiler Phase-Ordering.pdf:pdf},
isbn = {9781450340526},
keywords = {autotuning,compilers,machine learning,phase-ordering},
title = {{Predictive Modeling Methodology for Compiler Phase-Ordering}},
url = {http://home.deib.polimi.it/ashouri/publications/PARMA-DITAM-2016{\_}AmirHossein{\_}Ashouri.pdf},
year = {2016}
}
@inproceedings{Iped2008,
abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical is- sue in designing cost-effective, high-performance chip multipro- cessors (CMPs). Conventional memory controllers deliver rela- tively low performance in part because they often employ fixed, rigid access scheduling policies designed for average-case appli- cation behavior. As a result, they cannot learn and optimize the long-term performance impact of their scheduling decisions, and cannot adapt their scheduling policies to dynamic workload behavior. We propose a new, self-optimizing memory controller design that operates using the principles of reinforcement learning (RL) to overcome these limitations. Our RL-based memory controller observes the system state and estimates the long-term perfor- mance impact of each action it can take. In this way, the con- troller learns to optimize its scheduling policy on the fly to maxi- mize long-term performance. Our results show that an RL-based memory controller improves the performance of a set of paral- lel applications run on a 4-core CMP by 19{\%} on average (up to 33{\%}), and it improves DRAM bandwidth utilization by 22{\%} compared to a state-of-the-art controller.},
annote = {This paper presents a self-optimising memory controller which uses Reinforcement Learning to maximise utilisation of off-chip DRAM bandwidth.




This highly cited paper gives an excellent overview of the big concepts in RL and DRAM memory controllers. Many of the ideas and details would be highly applicable to autotuning Algorithmic Skeletons.},
author = {Iped, E. and Mutlu, O. and Martinez, J. F. and Caruana, R.},
booktitle = {ISCA},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Iped et al. - Self-Optimizing Memory Controllers A Reinforcement Learning Approach.pdf:pdf},
number = {June},
publisher = {IEEE},
title = {{Self-Optimizing Memory Controllers: A Reinforcement Learning Approach}},
year = {2008}
}
@inproceedings{Yviquel2011,
abstract = {Although multi-core processors are now available everywhere, few applications are able to truly exploit their multi- processing capabilities. Dataflow programming attempts to solve this problem by expressing explicit parallelism within an application. In this paper, we describe two scheduling strategies for executing a dataflow program on a single-core processor. We also describe an extension of these strate- gies on multi-core architectures using distributed schedulers and lock-free communications. We show the efficiency of these scheduling strategies on MPEG-4 Simple Profile and MPEG-4 Advanced Video Coding decoders.},
annote = {NULL},
author = {Yviquel, Herve and Casseau, Emmanuel and Wipliez, Matthieu and Raulet, Mickael},
booktitle = {SiPS},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Yviquel et al. - Efficient multicore scheduling of dataflow process networks.pdf:pdf},
keywords = {Dataflow computing,Distributed algorithm,Lock-free multithreading,Multicore process-,Scheduling algorithm,ing},
publisher = {IEEE},
title = {{Efficient multicore scheduling of dataflow process networks}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6088974},
year = {2011}
}
@inproceedings{Gulwani2011,
abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.},
annote = {NULL},
author = {Gulwani, S.},
booktitle = {POPL},
doi = {10.1145/1925844.1926423},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Gulwani - Automating string processing in spreadsheets using input-output examples.pdf:pdf},
isbn = {9781450304900},
issn = {0362-1340},
keywords = {ample,pbe,program synthesis,programming by ex-,spreadsheet programming,user intent,version space algebra},
title = {{Automating string processing in spreadsheets using input-output examples}},
year = {2011}
}
@inproceedings{Shi2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.09136v1},
author = {Moren, K. and Gohringer, D.},
booktitle = {ICCS},
doi = {10.1007/978-3-319-93701-4},
eprint = {arXiv:1803.09136v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Moren, Gohringer - Automatic Mapping for OpenCL-Programs on CPUGPU Heterogeneous Platforms.pdf:pdf},
isbn = {978-3-319-93700-7},
keywords = {code analysis,compilers,heterogeneous computing,machine learning,opencl,workload scheduling},
title = {{Automatic Mapping for OpenCL-Programs on CPU/GPU Heterogeneous Platforms}},
year = {2018}
}
@misc{UniversityofEdinburgh2014q,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 1. Compiler Optimisation.pdf:pdf},
title = {{1. Compiler Optimisation}},
year = {2015}
}
@article{Rossum2010a,
abstract = {If you do muchwork on computers, eventually you find that theres some task youd like to automate. For example, you may wish to perform a search-and-replace over a large number of text files, or rename and rearrange a bunch of photo files in a complicated way. Perhaps youd like to write a small custom database, or a specialized GUI application, or a simple game. If youre a professional software developer, you may have to work with several C/C++/Java libraries but find the usual write/compile/test/re-compile cycle is too slow. Perhaps youre writing a test suite for such a library and find writing the testing code a tedious task. Or maybe youve written a program that could use an extension language, and you dont want to design and implement a whole new language for your application.},
annote = {NULL},
author = {Rossum, Guido Van},
doi = {10.1111/j.1094-348X.2008.00203_7.x},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Python Tutorial.pdf:pdf},
isbn = {9789004155008},
issn = {0169-118X},
number = {4},
title = {{Python Tutorial}},
url = {http://docs.python.org/tutorial/},
volume = {42},
year = {2016}
}
@phdthesis{Ungureanu2013,
abstract = {In the past decade we have witnessed an abrupt shift to parallel computing subsequent to the increasing demand for performance and functionality that can no longer be satisfied by conventional paradigms. As a consequence, the abstraction gab between the applications and the underlying hardware increased, triggering both industry and academia in several research directions. This thesis project aims at analyzing some of these directions in order to offer a solution for bridging the abstraction gap between the description of a problem at a functional level and the implementation on a heterogeneous parallel platform using ForSyDe – a formal design methodology. This report treats applications employing data-parallel and time-parallel computation, regards nvidia CUDA-enabled GPGPUs as the main backend platform. The report proposes a heuristic transformation-and-refinement process based on analysis methods and design decisions to automate and aid in a correct-by-design backend code synthesis. Its purpose is to identify potential data parallelism and time parallelism in a high-level system. Furthermore, based on a basic platform model, the algorithm load-balances and maps the execution onto the best computation resources in an automated design flow. This design flow will be embedded into an already existing tool, f2cc (ForSyDe-to-CUDA C) and tested for correctness on an industrial-scale image processing application aimed at monitoring inkjet print-heads reliability. Keywords: system design flow, high abstraction-level models, ForSyDe, GPGPU, CUDA, time- parallel, data-parallel.},
annote = {Correct-by-design backend code synthesis for GPUs using CUDA.},
author = {Ungureanu, George},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Ungureanu - Automatic Software Synthesis from High-Level ForSyDe Models Targeting Massively Parallel Processors.pdf:pdf},
school = {KTH Royal Institute of Technology},
title = {{Automatic Software Synthesis from High-Level ForSyDe Models Targeting Massively Parallel Processors}},
year = {2013}
}
@article{Corbalan2005,
abstract = {In current multiprogrammed multiprocessor systems, to take into account the performance of parallel applications is critical to decide an efficient processor allocation. In this paper, we present the Performance-Driven Processor Allocation policy (PDPA). PDPA is a new scheduling policy that implements a processor allocation policy and a multiprogramming-level policy, in a coordinated way, based on the measured application performance. With regard to the processor allocation, PDPA is a dynamic policy that allocates to applications the maximum number of processors to reach a given target efficiency. With regard to the multiprogramming level, PDPA allows the execution of a new application when free processors are available and the allocation of all the running applications is stable, or if some applications show bad performance. Results demonstrate that PDPA automatically adjusts the processor allocation of parallel applications to reach the specified target efficiency, and that it adjusts the multiprogramming level to the workload characteristics. PDPA is able to adjust the processor allocation and the multiprogramming level without human intervention, which is a desirable property for self-configurable systems, resulting in a better individual application response time.},
annote = {NULL},
author = {Corbalan, J. and Martorell, X. and Labarta, J.},
doi = {10.1109/TPDS.2005.85},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Corbalan, Martorell, Labarta - Performance-driven processor allocation.pdf:pdf},
issn = {1045-9219},
journal = {TPDS},
keywords = {multiprocessor scheduling,openmp,operating system algorithmis,performance analysis,runtime analysis},
month = {jul},
number = {7},
title = {{Performance-driven processor allocation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1435338},
volume = {16},
year = {2005}
}
@inproceedings{Sakellariou2001,
abstract = {The Cactus software is representative for a whole class of scientific applications; typically those that are tightly coupled, have reg- ular space decomposition, and huge memory and processor time require- ments. Cactus proved to be a valuable tool for astrophysicists, who first initiated its development. However, today's fastest supercomputers are not powerful enough to perform realistically large astrophysics simula- tions with Cactus. The emergence of innovative resource environments like Grids satisfies this need for computational power. Our paper ad- dresses issues related to the execution of applications like Cactus in Grid environments. We focus on two types of Grids: a set of geographically distributed supercomputers and a collection of the scale of one million Internet-connected workstations. We study the application performance on traditional systems, validate the theoretical results against experi- mental data, and predict performance in the two new environments.},
annote = {Cited by 53.},
author = {Ripeanu, M. and Iamnitchi, A. and Foster, I.},
booktitle = {Euro-Par},
doi = {10.1007/3-540-44681-8},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Ripeanu, Iamnitchi, Foster - Cactus application Performance predictions in grid environments.pdf:pdf},
isbn = {9783540424956},
title = {{Cactus application: Performance predictions in grid environments}},
url = {http://www.springerlink.com/content/flddyth66lrqdk36/},
year = {2001}
}
@techreport{Corporation2011,
annote = {NULL},
author = {{Intel Corporation}},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Intel Corporation - Intel(R) 64 and IA-32 Architectures Software Developer's Manual, Combined Volumes.pdf:pdf},
number = {January},
title = {{Intel(R) 64 and IA-32 Architectures Software Developer's Manual, Combined Volumes}},
year = {2011}
}
@inproceedings{Hellendoorn2017,
author = {Hellendoorn, V. J. and Devanbu, P.},
booktitle = {ESEC/FSE},
doi = {10.1145/3106237.3106290},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Hellendoorn, Devanbu - Are deep neural networks the best choice for modeling source code.pdf:pdf},
isbn = {9781450351058},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Are deep neural networks the best choice for modeling source code?}},
year = {2017}
}
@inproceedings{Baldassin,
annote = {NULL},
author = {Baldassin, Alexandro and Araujo, Guido},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Baldassin, Araujo - Performance Implications of Dynamic Memory Allocators on Transactional Memory Systems.pdf:pdf},
isbn = {9781450332057},
keywords = {dynamic memory allocation,performance evaluation,transactional memory},
title = {{Performance Implications of Dynamic Memory Allocators on Transactional Memory Systems}},
year = {2015}
}
@article{Stone2010,
annote = {NULL},
author = {Stone, John E. and Gohara, David and Shi, Guochun},
journal = {CS{\&}E},
number = {3},
title = {{OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems}},
volume = {12},
year = {2010}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
annote = {Breakthrough success of deep model on ImageNet.},
author = {Krizhevsky, A. and Sutskever, I. and Hinton, G. E.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Krizhevsky, Sutskever, Hinton - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{JaredHoberock2014a,
annote = {NULL},
author = {Hoberock, Jared},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Hoberock - N4505 Working Draft, Technical Specification for C Extensions for Parallelism.pdf:pdf},
journal = {ISO-N4310},
title = {{N4505: Working Draft, Technical Specification for C++ Extensions for Parallelism}},
year = {2014}
}
@inproceedings{Sampson2017,
abstract = {Mainstream programming models for heterogeneous architectures sacrifice safety and expressiveness to offer low-level con- trol over performance details. The interfaces between hardware units consist of verbose, unsafe APIs; hardware-specific languages make it difficult to move code between units; and brittle preprocessor macros complicate the task of specializing general code for efficient accelerated execution. We propose a newlow-level programming model for heterogeneous systems that offers safe communication constructs, cross-device code portability, and hygienic metaprogramming for specialization. The language extends constructs from multi-stage programming to separate code for different hardware units, to commu- nicate between them, and to express compile-time code optimization. In our new approach, called static staging, compiler statically generates code for all units and communication in a heterogeneous system. We implement static staging in BraidGL, a real-time graphics programming system for CPU–GPU systems. Real-time graphics applications in OpenGL use stringly-typed APIs for communication and unsafe preprocessing to generate special- ized GPU code variants. In BraidGL, programmers instead write hybrid CPU–GPU software in a unified language. The compiler statically generates target-specific code and guarantees safe communication between the CPU and the graphics pipeline stages. Example scenes demonstrate the language's productivity advantages and real-time performance: BraidGL eliminates the safety and expressiveness pitfalls of OpenGL, makes common specialization techniques easy to apply, and delivers real-time frame rates. Humans require 30 fps or higher for an interactive graphics experience. While our demostra- tion uses graphics programming, the requirements match or exceed those of other heterogenous systems and suggest our approach is general enough to meet programmer needs for saftey and performance on heterogeneous systems. 1.},
author = {Sampson, A. and McKinley, K. S. and Mytkowicz, T.},
booktitle = {OOPSLA},
doi = {10.1145/3133895},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Sampson, McKinley, Mytkowicz - Static Stages for Heterogeneous Programming.pdf:pdf},
keywords = {Multi-stage programming,OpenGL,graphics programming,heterogeneous programming},
title = {{Static Stages for Heterogeneous Programming}},
year = {2017}
}
@article{Domingos2012a,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, P.},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Domingos - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
volume = {55},
year = {2012}
}
@inproceedings{Bastani2017,
abstract = {We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.},
archivePrefix = {arXiv},
arxivId = {1608.01723},
author = {Bastani, O. and Sharma, R. and Aiken, A. and Liang, P.},
booktitle = {PLDI},
doi = {10.1145/3062341.3062349},
eprint = {1608.01723},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Bastani et al. - Synthesizing Program Input Grammars.pdf:pdf},
isbn = {9781450349888},
issn = {0362-1340},
keywords = {fuzzing,grammar synthesis},
title = {{Synthesizing Program Input Grammars}},
url = {http://arxiv.org/abs/1608.01723},
year = {2017}
}
@inproceedings{Fursin2015,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.06256v1},
author = {Fursin, G. and Memon, A. and Guillon, C. and Lokhmotov, A.},
booktitle = {CPC},
eprint = {arXiv:1506.06256v1},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Fursin et al. - Collective Mind, Part II Towards Performance- and Cost-Aware Software Engineering as a Natural Science.pdf:pdf},
title = {{Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science}},
year = {2015}
}
@inproceedings{Hsu2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Hsu, Aaron W},
booktitle = {ARRAY},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Hsu - The Key to a Data Parallel Compiler.pdf:pdf},
isbn = {9781450343848},
keywords = {apl,compilers,key,node coordinates,parallel},
title = {{The Key to a Data Parallel Compiler}},
year = {2016}
}
@inproceedings{Ramanathan2015,
abstract = {The increasing adoption of GPUs as mainstream computing devices, coupled with the imminent availability of large high-bandwidth caches based on die-stacked memory makes it important to analyze and understand modern GPU compute applications from the perspective of their memory access and data reuse characteristics. This paper presents detailed workload characterization studies on four GPU compute applications that process large data sets. The applications studied include tree traversal and search algorithms, a partial differential equation (PDE) solver, and a synthetic array processing application. Our studies indicate that while the memory footprint consumed by these applications can be very large, the effectiveness of several GB worth of cache may vary significantly across workloads. This suggests that provisioning cache resources in a die-stacked memory based system needs to be done very carefully, through detailed characterization of target workloads. An added benefit of our work was the discovery that accurate memory characterization data can lead to a significantly more optimized strategy for scheduling GPU threads by taking advantage of a workload's access characteristics. In particular, for the PDE solver, our analysis led to an optimization that achieved 30{\%} measured gain in application performance. This paper also describes our analysis methodology for conducting these types of studies. The methodology is based on trace analysis, where the traces capture memory traffic and calls to the GPU compute API. For each application we highlight the characterization metrics and analysis techniques that were most useful in generating insights about their memory access patterns.},
annote = {NULL},
author = {Ramanathan, Srividya and Hazari, Gautam and Lahiri, Kanishka and Spadini, Francesco},
booktitle = {HiPC},
doi = {10.1109/HiPC.2015.25},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ramanathan et al. - Characterizing Large Dataset GPU Compute Workloads Targeting Systems with Die-Stacked Memory.pdf:pdf},
isbn = {978-1-4673-8488-9},
title = {{Characterizing Large Dataset GPU Compute Workloads Targeting Systems with Die-Stacked Memory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7397635},
year = {2015}
}
@misc{Edinburgh2012,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - University of Edinburgh - COPT 2012 exam.pdf:pdf},
number = {Level 10},
title = {{COPT 2012 exam}},
year = {2012}
}
@inproceedings{Caromel2008,
abstract = {This paper addresses the issue of type safe algorithmic skeletons. From a theoretical perspective we contribute by: formally specifying a type system for algorithmic skeletons, and proving that the type system guarantees type safety. From an implementation point of view, we show how it is possible to enforce the type system on an Java based algo- rithmic skeleton library. The enforcement takes place at the composition of the skeleton program, by typing each skele- ton with respect to its construction parameters: sequential functions, and other skeletons. As a result, hierarchical skeleton nesting can be per- formed safely, since type errors can be detected by the skele- ton type system.},
annote = {The paper describes how you can prove type safe skeletons so that you don't have to rely on runtime casts + type errors. Doesn't seem especially useful.},
author = {Caromel, Denis and Henrio, Ludovic and Leyton, Mario},
booktitle = {PDP},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Caromel, Henrio, Leyton - Type Safe Algorithmic Skeletons.pdf:pdf},
keywords = {Type systems,algorithmic skeletons},
title = {{Type Safe Algorithmic Skeletons}},
year = {2008}
}
@article{Dong2016,
abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparsecoding- based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1501.00092},
author = {Dong, C. and Loy, C. C. and He, K. and Tang, X.},
doi = {10.1109/TPAMI.2015.2439281},
eprint = {1501.00092},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Dong et al. - Image Super-Resolution Using Deep Convolutional Networks.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {TPAMI},
keywords = {Super-resolution,deep convolutional neural networks,sparse coding},
number = {2},
pmid = {26761735},
publisher = {IEEE},
title = {{Image Super-Resolution Using Deep Convolutional Networks}},
volume = {38},
year = {2016}
}
@inproceedings{Benson,
annote = {NULL},
author = {Benson, Austin R},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Benson - A Framework for Practical Parallel Fast Matrix Multiplication.pdf:pdf},
isbn = {9781450332057},
keywords = {dense linear algebra,fast matrix multiplication,lel linear algebra,paral-,shared memory},
title = {{A Framework for Practical Parallel Fast Matrix Multiplication}},
year = {2015}
}
@article{Burgess1996,
author = {Burgess, C J and Saidi, M},
doi = {http://dx.doi.org/10.1016/0950-5849(95)01055-6},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Burgess, Saidi - The automatic generation of test cases for optimizing {\{}Fortran{\}} compilers.pdf:pdf},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Automated testing,Compiler testing,Test cases},
number = {2},
pages = {111--119},
title = {{The automatic generation of test cases for optimizing {\{}Fortran{\}} compilers}},
url = {http://www.sciencedirect.com/science/article/pii/0950584995010556},
volume = {38},
year = {1996}
}
@misc{Silver2015a,
abstract = {Problem Set},
author = {Silver, D},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Reinforcement Learning Exam.pdf:pdf},
pages = {2--6},
title = {{Reinforcement Learning Exam}},
year = {2015}
}
@inproceedings{Majeed2013,
abstract = {SkePU is a C++ template library with a simple and unified interface for expressing data parallel computa- tions in terms of generic components, called skeletons, on multi-GPU systems using CUDA and OpenCL. The smart containers in SkePU, such as Matrix and Vector, perform data management with a lazy memory copying mechanism that reduces redundant data communication. SkePU pro- vides programmability, portability and even performance portability, but up to now application written using SkePU could only run on a single multi-GPU node. We present the extension of SkePU for GPU clusters without the need to modify the SkePU application source code. With our prototype implementation, we performed two experiments. The first experiment demonstrates the scalability with regular algorithms for N-body simulation and electric field calcula- tion over multiple GPU nodes. The results for the second experiment show the benefit of lazy memory copying in terms of speedup gained for one level of Strassen's algorithm and another synthetic matrix sum application.},
annote = {Extending SkePU for GPU clusters using MPI.


Cited by 5.},
author = {Majeed, M. and Dastgeer, U. and Kessler, C.},
booktitle = {PDPTA},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Majeed, Dastgeer, Kessler - Cluster-SkePU A multi-backend skeleton programming library for GPU clusters.pdf:pdf},
keywords = {GPU Cluster,Scalability,Scientific Applications,SkePU,Skeleton Program- ming,Structured parallel programming},
title = {{Cluster-SkePU: A multi-backend skeleton programming library for GPU clusters}},
url = {http://www.ida.liu.se/{~}usmda/skepu/publications/ClusterSkepUPDPTA.pdf},
year = {2013}
}
@inproceedings{Gulwani2012,
abstract = {In this paper, we study the problem of automatically solving ruler/compass based geometry construction problems. We first introduce a logic and a programming language for describing such constructions and then phrase the automation problem as a program synthesis problem. We then describe a new program synthesis technique based on three key insights: (i) reduction of symbolic reasoning to concrete reasoning (based on a deep theoretical result that reduces verification to random testing), (ii) extending the instruction set of the programming language with higher level primitives (representing basic constructions found in textbook chapters, inspired by how humans use their experience and knowledge gained from chapters to perform complicated constructions), and (iii) pruning the forward exhaustive search using a goal-directed heuristic (simulating backward reasoning performed by humans). Our tool can successfully synthesize constructions for various geometry problems picked up from high-school textbooks and examination papers in a reasonable amount of time. This opens up an amazing set of possibilities in the context of making classroom teaching interactive. {\textcopyright} 2011 ACM.},
annote = {NULL},
author = {Gulwani, S. and Korthikanti, V. A. and Tiwari, A.},
booktitle = {PLDI},
doi = {10.1145/2345156.1993505},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Gulwani, Korthikanti, Tiwari - Synthesizing geometry constructions.pdf:pdf},
isbn = {9781450306638},
issn = {03621340},
keywords = {abstraction,forward and backward analysis,program synthesis,ruler-compass geometry con-,structions},
title = {{Synthesizing geometry constructions}},
year = {2011}
}
@article{Collingbourne2012,
abstract = {We present an effective technique for crosschecking a C or C++ program against an accelerated OpenCL version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a symbolic execution engine based on KLEE and KLEE-FP that supports symbolic reasoning on the equivalence between symbolic values. Our approach is to symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. Using this model we are able to run OpenCL programs symbolically, keeping track of memory accesses for the purpose of race detection. We then compare the symbolic result against the plain C or C++ implementation in order to detect mismatches between the two versions. We applied KLEE-CL to the Parboil benchmark suite, the Bullet physics library and the OP2 library, in which we were able to find a total of seven errors: two mismatches between the OpenCL and C implementations, three memory errors, one OpenCL compiler bug and one race condition.},
author = {Collingbourne, P. and Cadar, C. and Kelly, P. H. J.},
doi = {10.1007/978-3-642-34188-5_18},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Collingbourne, Cadar, Kelly - Symbolic testing of OpenCL code.pdf:pdf},
isbn = {9783642341878},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
title = {{Symbolic testing of OpenCL code}},
year = {2012}
}
@inproceedings{Alan1991,
annote = {NULL},
author = {Alan, J.},
booktitle = {PPoPP},
doi = {http://doi.acm.org/10.1145/122759.122765},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Alan - Debugging Parallel Programs.pdf:pdf},
isbn = {0897914570},
keywords = {debugging,multicore,parallel programming,python,ruby},
title = {{Debugging Parallel Programs}},
year = {2015}
}
@article{Pane2001,
abstract = {Programming may be more difficult than necessary because it requires solutions to be expressed in ways that are not familiar or natural for beginners. To identify what is natural, this article examines the ways that non-programmers express solutions to problems that were chosen to be representative of common programming tasks. The vocabulary and structure in these solutions is compared with the vocabulary and structure in modern programming languages, to identify the features and paradigms that seem to match these natural tendencies as well as those that do not. This information can be used by the designers of future programming languages to guide the selection and generation of language features. This design technique can result in languages that are easier to learn and use, because the languages will better match beginners' existing problem solving abilities. Introduction},
annote = {NULL},
author = {Pane, J. F. and Ratanamahatana, C. A. and Myers, B. A.},
doi = {10.1006/ijhc.2000.0410},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Pane, Ratanamahatana, Myers - Studying the language and structure in non-programmers' solutions to programming problems.pdf:pdf},
issn = {10715819},
journal = {IJHCS},
keywords = {end-user programming,natural programming,novice programming,psychology of programming,user},
number = {2},
title = {{Studying the language and structure in non-programmers' solutions to programming problems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1071581900904105},
volume = {54},
year = {2001}
}
@inproceedings{Yaneva2017,
abstract = {Embedded software is found everywhere from our highly visible mobile devices to the confines of our car in the form of smart sensors. Embedded software companies are under huge pressure to produce safe applications that limit risks, and testing is absolutely critical to alleviate concerns regarding safety and user privacy. This requires using large test suites throughout the development process, increasing time-to-market and ultimately hindering competitivity. Speeding up test execution is, therefore, of paramount impor-tance for embedded software developers. This is traditionally achieved by running, in parallel, multiple tests on large-scale clusters of com-puters. However, this approach is costly in terms of infrastructure maintenance and energy consumed, and is at times inconvenient as developers have to wait for their tests to be scheduled on a shared resource. We propose to look at exploiting GPUs (Graphics Processing Units) for running embedded software testing. GPUs are readily available in most computers and offer tremendous amounts of paral-lelism, making them an ideal target for embedded software testing. In this paper, we demonstrate, for the first time, how test execu-tions of embedded C programs can be automatically performed on a GPU, without involving the end user. We take a compiler-assisted approach which automatically compiles the C program into GPU kernels for parallel execution of the input tests. Using this tech-nique, we achieve an average speedup of 16× when compared to CPU execution of input tests across nine programs from an industry standard embedded benchmark suite.},
author = {Yaneva, V. and Rajan, A. and Dubach, C.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Yaneva, Rajan, Dubach - Compiler-Assisted Test Acceleration on GPUs for Embedded Software.pdf:pdf},
keywords = {Automated testing,Compilers,Embedded software,GPUs,KEYWORDS Functional testing,Source code generation},
title = {{Compiler-Assisted Test Acceleration on GPUs for Embedded Software}},
year = {2017}
}
@inproceedings{Lee2013,
annote = {NULL},
author = {Lee, J. and Samadi, M. and Park, Y. and Mahlke, S.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Lee et al. - Transparent CPU-GPU Collaboration for Data-Parallel Kernels on Heterogeneous Systems.pdf:pdf},
publisher = {ACM},
title = {{Transparent CPU-GPU Collaboration for Data-Parallel Kernels on Heterogeneous Systems}},
year = {2013}
}
@article{Kessler2014,
abstract = {In this survey paper, we review recent work on frameworks for the high-level, portable programming of heterogeneous multi-/manycore systems (especially, GPU-based systems) using high-level constructs such as annotated user- level software components, skeletons (i.e., predefined generic components) and containers, and discuss the optimization problems that need to be considered in selecting among mul- tiple implementation variants, generating code and providing runtime support for efficient execution on such systems.},
annote = {NULL},
author = {Kessler, C and Dastgeer, U and Li, L},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Kessler, Dastgeer, Li - Optimized Composition Generating Efficient Code for Heterogeneous Systems from Multi-Variant Components,.pdf:pdf},
journal = {arXiv:1405.2915},
title = {{Optimized Composition: Generating Efficient Code for Heterogeneous Systems from Multi-Variant Components, Skeletons and Containers}},
url = {http://arxiv.org/abs/1405.2915},
year = {2014}
}
@inproceedings{Massalin1987,
abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the func- tions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole op- timizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
annote = {Massalin's superoptimizer finds the smallest possible program which performs a specific function by enumerating the entire search space of all possible programs and testing each against a set of tests. Starting with a program of length 1, the superoptimizer tests to see if any possible instruction passes the tests, if not, it increases the program length by 1 and repeats. This causes the search space to increase in size exponentially, so is impractical for any long programs (usually programs are {\textless} 13 instructions). The optimizer also only performs register to register instructions, with no support for pointers. This can have a large impact on program efficiency due to the memory latency of loading everything into registers.




Cited by 175.},
author = {Massalin, H.},
booktitle = {ASPLOS},
doi = {10.1145/36206.36194},
file = {:Users/cec/Google Drive/Mendeley Library/1987 - Massalin - Superoptimizer -- A Look at the Smallest Program.pdf:pdf},
isbn = {0897912381},
publisher = {ACM},
title = {{Superoptimizer -- A Look at the Smallest Program}},
year = {1987}
}
@inproceedings{Hu2002,
annote = {NULL},
author = {Hu, Z and Iwasaki, H and Takeichi, M},
booktitle = {ESOP},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Hu, Iwasaki, Takeichi - An accumulative parallel skeleton for all.pdf:pdf},
title = {{An accumulative parallel skeleton for all}},
url = {http://link.springer.com/chapter/10.1007/3-540-45927-8{\_}7},
year = {2002}
}
@article{Balduzzi2017,
abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skip-connections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new "looks linear" (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections.},
annote = {NULL},
author = {Balduzzi, D. and Frean, M. and Leary, L. and Lewis, J. and Ma, K. W. and McWilliams, B.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Balduzzi et al. - The Shattered Gradients Problem If resnets are the answer, then what is the question.pdf:pdf},
journal = {arXiv:1702.08591},
title = {{The Shattered Gradients Problem: If resnets are the answer, then what is the question?}},
year = {2017}
}
@inproceedings{Cooper2003,
author = {Cooper, K. D. and Harvey, T. J. and Kennedy, K.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library//2003 - Cooper, Harvey, Kennedy - Iterative Data-flow Analysis, Revisited.pdf:pdf},
title = {{Iterative Data-flow Analysis, Revisited}},
year = {2003}
}
@article{Chen2015b,
abstract = {In recent years the performance of deep learning algorithms has been demonstrated in a variety of application domains. The goal of this paper is to enrich deep learning to be able to predict a set of random variables while taking into account their dependencies. Towards this goal, we propose an efficient algorithm that is able to learn structured models with non-linear functions. We demonstrate the effectiveness of our algorithm in the tasks of predicting words as well as codes from noisy images, and show that by jointly learning multilayer perceptrons and pairwise features, significant gains in performance can be obtained.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1407.2538},
author = {Chen, L. and Schwing, A. G. and Yuille, A. L. and Urtasun, R.},
eprint = {1407.2538},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chen et al. - Learning Deep Structured Models.pdf:pdf},
journal = {JMLR},
title = {{Learning Deep Structured Models}},
url = {http://arxiv.org/abs/1407.2538},
volume = {37},
year = {2015}
}
@inproceedings{Spiegelman2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Spiegelman, Alexander and Golan-gueta, Guy and Keidar, Idit},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Spiegelman, Golan-gueta, Keidar - Transactional Data Structure Libraries.pdf:pdf},
isbn = {9781450342612},
keywords = {concurrency,data structures,semantics,trans-},
title = {{Transactional Data Structure Libraries}},
year = {2016}
}
@article{Gulwani2015,
annote = {NULL},
author = {Gulwani, S. and Hern{\'{a}}ndez-Orallo, J. and Kitzelmann, E. and Muggleton, S. H. and Schmid, U. and Zorn, B.},
doi = {10.1145/2736282},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gulwani et al. - Inductive Programming Meets the Real World.pdf:pdf},
issn = {0001-0782},
journal = {Communications of the ACM},
number = {11},
title = {{Inductive Programming Meets the Real World}},
url = {http://doi.acm.org/10.1145/2736282},
volume = {58},
year = {2015}
}
@techreport{Nvidia2007,
annote = {NULL},
author = {Nvidia},
title = {{Compute unified device architecture programming guide}},
year = {2007}
}
@article{Brin1998,
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
annote = {Cited by 13,086.},
author = {Brin, Sergey and Page, Lawrence},
doi = {10.1016/S0169-7552(98)00110-X},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Brin, Page - The anatomy of a large-scale hypertextual Web search engine BT - Computer Networks and ISDN Systems.pdf:pdf},
isbn = {01697552},
issn = {01697552},
journal = {Computer Networks and ISDN Systems},
keywords = {google,information retrieval,pagerank,search engines,world wide web},
number = {1},
title = {{The anatomy of a large-scale hypertextual Web search engine BT - Computer Networks and ISDN Systems}},
url = {http://dx.doi.org/10.1016/S0169-7552(98)00110-X},
volume = {30},
year = {1998}
}
@misc{CarnegieMellonUniversity2005,
annote = {NULL},
author = {{Carnegie Mellon University}},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Carnegie Mellon University - 5. Statistical Independence, Discrete Random Variables.pdf:pdf},
number = {September},
title = {{5. Statistical Independence, Discrete Random Variables}},
year = {2005}
}
@inproceedings{Radoi2015,
abstract = {In recent years, web applications have become pervasive. Their backbone is JavaScript, the only programming language supported by all major web browsers. Most browsers run on desktop or mobile devices with parallel hardware. However, JavaScript is by design sequential, and current web applications make little use of hardware parallelism. Are web applications ready to exploit parallel hardware? We answer the question in two steps: First, we survey 174 web developers about the potential and challenges of using parallelism. Then, we study the performance and computation shape of a set of web applications that are representative for the emerging web. Our findings indicate that emerging web applications do have latent data parallelism, and JavaScript developers' programming style is not a significant impediment to exploiting this parallelism.},
annote = {NULL},
author = {Radoi, C. and Herhut, S.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Radoi, Herhut - Are Web Applications Ready for Parallelism.pdf:pdf},
isbn = {9781450332057},
keywords = {javascript,performance,survey,web},
title = {{Are Web Applications Ready for Parallelism?}},
year = {2015}
}
@inproceedings{Kalibera2013,
abstract = {Experimental evaluation is key to systems research. Because modern$\backslash$nsystems are complex and non-deterministic, good experimental methodology demands$\backslash$nthat researchers account for uncertainty. To obtain valid results, they are$\backslash$nexpected to run many iterations of benchmarks, invoke virtual machines (VMs)$\backslash$nseveral times, or even rebuild VM or benchmark binaries more than once. All this$\backslash$nrepetition costs time to complete experiments. Currently, many evaluations give$\backslash$nup on sufficient repetition or rigorous statistical methods, or even run$\backslash$nbenchmarks only in training sizes. The results reported often lack proper$\backslash$nvariation estimates and, when a small difference between two systems is$\backslash$nreported, some are simply unreliable.$\backslash$nIn contrast, we provide a statistically rigorous methodology for repetition and$\backslash$nsummarising results that makes efficient use of experimentation time. Time$\backslash$nefficiency comes from two key observations. First, a given benchmark on a given$\backslash$nplatform is typically prone to much less non-determinism than the common$\backslash$nworst-case of published corner-case studies. Second, repetition is most needed$\backslash$nwhere most uncertainty arises (whether between builds, between executions or$\backslash$nbetween iterations). We capture experimentation cost with a novel mathematical$\backslash$nmodel, which we use to identify the number of repetitions at each level of an$\backslash$nexperiment necessary and sufficient to obtain a given level of precision.$\backslash$nWe present our methodology as a cookbook that guides researchers on the number$\backslash$nof repetitions they should run to obtain reliable results. We also show how to$\backslash$npresent results with an effect size confidence interval. As an example, we show$\backslash$nhow to use our methodology to conduct throughput experiments with the DaCapo and$\backslash$nSPEC CPU benchmarks on three recent platforms.},
annote = {NULL},
author = {Kalibera, Tomas and Jones, Richard},
booktitle = {ISMM},
doi = {10.1145/2464157.2464160},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Kalibera, Jones - Rigorous benchmarking in reasonable time.pdf:pdf},
isbn = {978-1-4503-2100-6},
issn = {0362-1340},
keywords = {benchmarking methodology,dacapo,spec cpu,statistical methods},
number = {11},
title = {{Rigorous benchmarking in reasonable time}},
url = {http://dl.acm.org/citation.cfm?id=2491894.2464160},
volume = {48},
year = {2013}
}
@article{Fallis2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Fallis - GPU Multisplit.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PPoPP},
keywords = {icle},
number = {9},
pmid = {25246403},
title = {{GPU Multisplit}},
volume = {53},
year = {2016}
}
@book{Nichols1996,
annote = {NULL},
author = {Nichols, Bradford and Buttlar, Dick and Farrel, Jacqueline},
publisher = {O'Reilly},
title = {{Pthreads programming: A POSIX standard for better multiprocessing}},
year = {1996}
}
@article{Box,
annote = {NULL},
author = {Sarkar, Vivek},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Sarkar - Determining Average Program Execution Times and their Variance.pdf:pdf},
title = {{Determining Average Program Execution Times and their Variance}}
}
@inproceedings{Goli2016a,
annote = {NULL},
author = {Goli, M.},
booktitle = {IWOCL},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Goli - VisionCPP A SYCL-based Computer Vision Framework.pdf:pdf},
isbn = {9781450343381},
title = {{VisionCPP: A SYCL-based Computer Vision Framework}},
year = {2016}
}
@article{Kushilevitz1998,
abstract = {We address the problem ofdesigning data structures that allow efficient search for approximate nearest neighbors. More specifically, given a database consisting ofa set ofvectors in some high dimensional Euclidean space, we want to construct a space-efficient data structure that would allow us to search, given a query vector, for the closest or nearly closest vector in the database. We also address this problem when distances are measured by the L1 norm and in the Hamming cube. Significantly improving and extending recent results ofKleinberg, we construct data structures whose size is polynomial in the size ofthe database and search algorithms that run in time nearly linear or nearly quadratic in the dimension. (Depending on the case, the extra factors are polylogarithmic in the size ofthe database.)},
annote = {Cited by 403.},
author = {Kushilevitz, E and Kushilevitz, E and Ostrovsky, R and Ostrovsky, R and Rabani, Y and Rabani, Y},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Kushilevitz et al. - Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces.pdf:pdf},
journal = {SICOMP},
keywords = {data structures,nearest neighbor search,random projections},
number = {2},
title = {{Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces}},
volume = {30},
year = {2000}
}
@article{Matsuzaki2004,
annote = {NULL},
author = {Matsuzaki, K and Kakehi, K and Iwasaki, H},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Matsuzaki, Kakehi, Iwasaki - A fusion-embedded skeleton library.pdf:pdf},
journal = {Euro-Par},
title = {{A fusion-embedded skeleton library}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-27866-5{\_}85},
year = {2004}
}
@article{Ioffe2015a,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, S. and Szegedy, C.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ioffe, Szegedy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv:1502.03167},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@misc{Goddarda,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 5.pdf:pdf},
title = {{IAML 5}},
year = {2014}
}
@inproceedings{Voronenko2009,
abstract = {The development of high-performance libraries has become extraordinarily difficult due to multiple processor cores, vector instruction sets, and deep memory hierarchies. Often, the library has to be reimplemented and reoptimized, when a new platform is released. In this paper we show how to automatically generate general input-size libraries for the domain of linear transforms. The input to our generator is a formal specification of the transform and the recursive algorithms the library should use; the output is a library that supports general input size, is vectorized and multithreaded, provides an adaptation mechanism for the memory hierarchy, and has excellent performance, comparable to or better than the best human-written libraries. Further, we show that our library generator enables various customizations; one example is the generation of Java libraries.},
annote = {NULL},
author = {Voronenko, Y. and {De Mesmay}, F. and P{\"{u}}schel, M.},
booktitle = {CGO},
doi = {10.1109/CGO.2009.33},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Voronenko, De Mesmay, P{\"{u}}schel - Computer Generation of General Size Linear Transform Libraries.pdf:pdf},
isbn = {9780769535760},
publisher = {IEEE},
title = {{Computer Generation of General Size Linear Transform Libraries}},
year = {2009}
}
@article{Gao2015a,
abstract = {In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long-Short Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, a LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 120,000 images and 250,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset are evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7{\%} of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human).},
annote = {NULL},
author = {Gao, H. and Mao, J. and Zhou, J. and Huang, Z. and Wang, L. and Xu, W.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gao et al. - Are You Talking to a Machine Dataset and Methods for Multilingual Image Question Answering.pdf:pdf},
journal = {arXiv:1505.05612},
title = {{Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering}},
year = {2015}
}
@inproceedings{Nugteren2015,
annote = {A generic iterative search autotuner for OpenCL kernels. Basically OpenTuner for OpenCL, but only has particle swarm and simulated annealing search. Doesn't talk about sample counts.},
author = {Nugteren, C. and Codreanu, V.},
booktitle = {MCSoC},
doi = {10.1109/MCSoC.2015.10},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Nugteren, Codreanu - CLTune A Generic Auto-Tuner for OpenCL Kernels.pdf:pdf},
isbn = {978-1-4799-8670-5},
title = {{CLTune: A Generic Auto-Tuner for OpenCL Kernels}},
year = {2015}
}
@inproceedings{Anzt,
annote = {NULL},
author = {Anzt, H. and Dongarra, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Anzt, Dongarra - Energy Efficiency and Performance Frontiers for Sparse Computations on GPU Supercomputers.pdf:pdf},
isbn = {9781450334044},
keywords = {blocked sparse matrix vector,energy efficiency,gpu supercomputer,lobpcg,product,sparse eigensolver},
title = {{Energy Efficiency and Performance Frontiers for Sparse Computations on GPU Supercomputers}},
year = {2015}
}
@inproceedings{Fletcher2013,
abstract = {This paper presents a light-weight dynamic optimization framework for homogeneous multicores. Our system profiles applications at runtime to detect hot program paths, and offloads the optimization of these paths to a Partner core. Our work contributes two insights: (1) that the dynamic optimization process is highly insensitive to runtime factors in homogeneous multicores and (2) that the Partner core's view of application hot paths can be noisy, allowing the entire optimization process to be implemented with very little dedicated hardware in a multicore.},
annote = {This paper presents a dynamic optimiser for homogeneous multicores that profiles hot paths, which are optimised on a partner core.},
author = {Fletcher, C. W. and Harding, R. and Khan, O. and Devadas, S.},
booktitle = {VLSI},
doi = {10.1109/VLSI-SoC.2013.6673306},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Fletcher et al. - A framework to accelerate sequential programs on homogeneous multicores.pdf:pdf},
isbn = {978-1-4799-0524-9},
month = {oct},
publisher = {Ieee},
title = {{A framework to accelerate sequential programs on homogeneous multicores}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6673306},
year = {2013}
}
@article{Nash,
annote = {NULL},
author = {Nash, J.},
doi = {10.2307/1969529},
file = {:Users/cec/Google Drive/Mendeley Library/1951 - Nash - Non-Cooperative Games.pdf:pdf},
issn = {0003486X},
journal = {Annals of Mathematics},
number = {2},
title = {{Non-Cooperative Games}},
volume = {54},
year = {1951}
}
@inproceedings{Hind2001,
annote = {Performing accurate pointer analysis is a difficult and generally undecidable problem. Without accurate information about pointer behaviour, we must make conservative assumptions which adversely affect the precision of compiler optimisations and other tools for which pointer behaviour must be known. There is a large collection of approximation algorithms have been developed, with worst-case performance from O(n) to O(n{\^{}}2{\^{}}2). These algorithms can be classed as flow insenstivie or flow sensitive, where flow insenstivie consider all paths of a program, and flow sensitive are typically based on profiling information. The paper assumes prior knowledge about pointer analysis. Cited by 516.},
author = {Hind, Michael},
booktitle = {PASTE},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Hind - Pointer Analysis Haven't We Solved This Problem Yet.pdf:pdf},
isbn = {1581134134},
publisher = {ACM},
title = {{Pointer Analysis: Haven't We Solved This Problem Yet?}},
year = {2001}
}
@inproceedings{Cohen2013,
annote = {NULL},
author = {Cohen, Albert and Grosser, Tobias and Kelly, Paul H J and Ramanujam, J and Verdoolaege, Sven and Cohen, Albert and Grosser, Tobias and Kelly, Paul H J and Ramanujam, J and Sadayappan, P},
booktitle = {GPGPU},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Cohen et al. - Split Tiling for GPUs Automatic Parallelization Using Trapezoidal Tiles to Reconcile Parallelism and Locality , a.pdf:pdf},
title = {{Split Tiling for GPUs : Automatic Parallelization Using Trapezoidal Tiles to Reconcile Parallelism and Locality , avoiding Divergence and Load Imbalance}},
year = {2013}
}
@article{Bielik2015,
annote = {NULL},
author = {Bielik, P. and Raychev, V. and Vechev, M.},
doi = {10.4230/LIPIcs.SNAPL.2015.41},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Bielik, Raychev, Vechev - Programming with “Big Code” Lessons, Techniques and Applications.pdf:pdf},
isbn = {9783939897804},
issn = {18688969},
journal = {SNAPL},
keywords = {and phrases probabilistic tools,open-source software,probabilistic inference and learning,program analysis},
title = {{Programming with “Big Code”: Lessons, Techniques and Applications}},
year = {2015}
}
@misc{Bundy2014c,
annote = {NULL},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bundy - The Nature of Informatics.pdf:pdf},
title = {{The Nature of Informatics}},
url = {http://www.inf.ed.ac.uk/teaching/courses/irm/notes/nature.html},
year = {2014}
}
@inproceedings{Chambers2010,
abstract = {MapReduce and similar systems significantly ease the task of writing data-parallel code. However, many real-world computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient data-parallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, high-level, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently, FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.},
annote = {NULL},
author = {Chambers, C. and Raniwala, A. and Perry, F. and Adams, S. and Henry, R. R. and Bradshaw, R. and Weizenbaum, N.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Chambers et al. - FlumeJava Easy, Efficient Data-parallel Pipelines.pdf:pdf},
title = {{FlumeJava: Easy, Efficient Data-parallel Pipelines}},
year = {2010}
}
@article{Lam1991,
abstract = {Blocking is a well-known optimization technique for improving the effectiveness of memory hierarchies. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory ...},
annote = {Cited by 1051.},
author = {Lam, M. D. and Rothberg, E. E. and Wolf, M. E.},
doi = {10.1145/106975.106981},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Lam, Rothberg, Wolf - The cache performance and optimizations of blocked algorithms.pdf:pdf},
isbn = {0897913809},
issn = {01635964},
journal = {OSR},
number = {Special Issue},
publisher = {ACM},
title = {{The cache performance and optimizations of blocked algorithms}},
volume = {25},
year = {1991}
}
@article{Canny1986,
abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
annote = {NULL},
author = {Canny, J.},
doi = {10.1109/TPAMI.1986.4767851},
file = {:Users/cec/Google Drive/Mendeley Library/1986 - Canny - A Computational Approach to Edge Detection.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Edge detection,feature extraction,image processing,machine vision,multiscale image analysis},
number = {6},
pmid = {21869365},
title = {{A Computational Approach to Edge Detection}},
volume = {PAMI-8},
year = {1986}
}
@inproceedings{Wang2014a,
abstract = {Technology scaling enables the integration of both the CPU and the GPU into a single chip for higher throughput and energy efficiency. In such a single-chip heterogeneous processor (SCHP), its memory bandwidth is the most critically shared resource, requiring judicious management to maximize the throughput. Previous studies on memory scheduling for SCHPs have focused on the scenario where multiple applications are running on the CPU and the GPU respectively, which we denote as a multitasking scenario. However, another increasingly important usage scenario for SCHPs is cooperative heterogeneous computing, where a single parallel application is partitioned between the CPU and the GPU such that the overall throughput is maximized. In previous studies on memory scheduling techniques for chip multi-processors (CMPs) and SCHPs, the first-ready firstcome-first-service (FR-FCFS) scheduling policy was used as an inept baseline due to its fairness issue. However, in a cooperative heterogeneous computing scenario, we first demonstrate that FRFCFS actually offers nearly 10{\%} higher throughput than two recently proposed memory scheduling techniques designed for a multi-tasking scenario. Second, based on our analysis on memory access characteristics in a cooperative heterogeneous computing scenario, we propose various optimization techniques that enhance the row-buffer locality by 10{\%}, reduce the service latency of CPU memory requests by 26{\%}, and improve the overall throughput by up to 8{\%} compared to FR-FCFS.},
annote = {NULL},
author = {Wang, Hao and Singh, Ripudaman and Schulte, Michael J. and Kim, Nam Sung},
booktitle = {PACT},
doi = {10.1145/2628071.2628096},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wang et al. - Memory scheduling towards high-throughput cooperative heterogeneous computing.pdf:pdf},
isbn = {9781450328098},
issn = {1089795X},
keywords = {heterogeneous processor,memory scheduling},
publisher = {ACM},
title = {{Memory scheduling towards high-throughput cooperative heterogeneous computing}},
url = {http://dl.acm.org/citation.cfm?doid=2628071.2628096},
year = {2014}
}
@article{McDonagh2014,
abstract = {Distributed compute clusters allow the computing power of heterogeneous (and homogeneous) resources to be utilised to solve large-scale science and engineering problems. One class of problem that has attractive scalability properties, and is therefore often implemented using compute clusters, is task farming (or parameter sweep) applications. A typical char- acteristic of such applications is that no communication is needed between distributed subtasks during the overall com- putation. However, interesting large-scale task farming problem instances that do require global communication between subtask sets also exist. We propose a framework called semi-synchronised task farming in order to address problems requiring distributed formulations containing subtasks that alternate between independence and synchronisation. We apply this framework to several large-scale contemporary computer vision problems and present a detailed performance analysis to demonstrate framework scalability. Semi-synchronised task farming splits a given problem into a number of stages. Each stage involves distributing inde- pendent subtasks to be completed in parallel and then making a set of synchronised global operations, based on informa- tion retrieved from the distributed results. The results influence the following subtask distribution stage. This subtask distribution followed by result collation process is iterated until overall problem solutions are obtained.We construct a simplified Bulk Synchronous Parallel (BSP) model to formalise this framework and with this formalisation, we develop a predictive model for overall task completion time. We present experimental benchmark results comparing the perfor- mance observed by applying our framework to solve real-world problems on compute clusters with that of solving the tasks in a serial fashion. Furthermore by assessing the predicted time savings that our framework provides in simulation and validating these predictions on a range of complex problems drawn from real-world computer vision tasks, we are able to reliably predict the performance gain obtained when using a compute cluster to tackle resource intensive com- puter vision tasks.},
annote = {NULL},
author = {StevenMcDonagh and Beyan, Cigdem and Huang, Phoenix X and Fisher, Robert B},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - StevenMcDonagh et al. - Applying semi-synchronised task farming to large-scale computer vision problems.pdf:pdf},
journal = {IJHPCA},
keywords = {Vision,semi-synchronised,task farming},
title = {{Applying semi-synchronised task farming to large-scale computer vision problems}},
url = {http://hpc.sagepub.com/content/early/2014/05/13/1094342014532965.abstract},
year = {2014}
}
@inproceedings{Lau2006,
abstract = {As hardware complexity increases and virtualization is added at more layers of the execution stack, predicting the performance impact of optimizations becomes increasingly difficult. Production compilers and virtual machines invest substantial development effort in perfor- mance tuning to achieve good performance for a range of bench- marks. Although optimizations typically perform well on average, they often have unpredictable impact on running time, sometimes degrading performance significantly. Today's VMs perform sophis- ticated feedback-directed optimizations, but these techniques do not address performance degradations, and they actually make the situa- tion worse by making the system more unpredictable. This paper presents an online framework for evaluating the ef- fectiveness of optimizations, enabling an online system to automat- ically identify and correct performance anomalies that occur at run- time. This work opens the door for a fundamental shift in the way optimizations are developed and tuned for online systems, and may allow the body of work in offline empirical optimization search to be applied automatically at runtime.We present our implementation and evaluation of this system in a product Java VM.},
annote = {This paper presents an approach to online optimisation evaluation which advocates empirical data over performance modelling, using a "Performance Auditor". The performance auditor compares the performane of two method variants in a "bakeoff".








By only comparing two versions of a method, they are severly restricting the optimisation space size.},
author = {Lau, J. and Arnold, M. and Hind, M. and Calder, B.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Lau et al. - Online Performance Auditing Using Hot Optimizations Without Getting Burned.pdf:pdf},
keywords = {Feedback-directed optmizations,Java,Virtual machines},
publisher = {ACM},
title = {{Online Performance Auditing: Using Hot Optimizations Without Getting Burned}},
year = {2006}
}
@techreport{McNally2012,
author = {McNally, R. and Yiu, K. and Grove, D. and Gerhardy, D.},
booktitle = {Defense Science and Technology Organisation (Australia)},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - McNally et al. - Fuzzing The State of the Art.pdf:pdf},
title = {{Fuzzing: The State of the Art}},
year = {2012}
}
@misc{Shivers1991,
abstract = {Programswritten in powerful, higher-order languages like Scheme,ML, and CommonLisp should run as fast as their FORTRAN and C counterparts. They should, but they don't. A major reason is the level of optimisation applied to these two classes of languages. Many FORTRAN and C compilers employ an arsenal of sophisticated global optimisations that depend upon data-flow analysis: common-subexpression elimination, loop-invariant detection, induction-variable elimination, and many, many more. Compilers for higher- order languages do not provide these optimisations. Without them, Scheme, LISP and ML compilers are doomed to produce code that runs slower than their FORTRAN and C counterparts. The problem is the lack of an explicit control-flow graph at compile time, somethingwhich traditional data-flow analysis techniques require. In this dissertation, I present a technique for recovering the control-flowgraph of aScheme programat compile time. I give examples of how this information can be used to perform several data-flow analysis optimisations, including copy propagation, induction-variable elimination, useless-variable elimination, and type recovery. The analysis is defined in termsof a non-standard semantic interpretation. The denotational semantics is carefully developed, and several theorems establishing the correctness of the semantics and the implementing algorithms are proven.},
annote = {NULL},
author = {Shivers, Olin},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Shivers - Control-Flow Analysis of Higher-Order Languages.pdf:pdf},
number = {May},
title = {{Control-Flow Analysis of Higher-Order Languages}},
year = {1991}
}
@article{Kuchen2002,
abstract = {We describe a skeletal parallel programming library which integrates task and data parallel constructs within an API for C++. Traditional skeletal requirements for higher orderness and polymorphism are achieved through exploitation of operator overloading and templates, while the underlying parallelism is provided by MPI. We present a case study describing two algorithms for the travelling salesman problem.},
annote = {The paper describes a C++ skeleton library which uses templates and operator overloading to implement polymorphic skeleton behaviour. The library supports distributed processing using MPI. Both data- and task-parallel skeletons are implemented, and the differences betweeen the two discussed. These skeletons are then used in combination to implement a solution to the Travelling Salesman problem, with mixed success. They discover that by using a data parallel skeleton within one of the task skeletons, the performance overhead of distributing the data is greater than the performance gained by the distribution, and so it performs slower than a serial implementation.




The paper covers the theory and background in a competent manner, however there is no literature survey to emphasise the need for this work. The paper does not seem to have a definite hypothesis, and this creates a "so what?" reaction to their findings. There is also little explanation of their experimental method, and no tabulated / graphical results. The source code for the library is not published, which when combined with the lack of a results table, seriously hampers the usefulness of the paper.




Of particular interest is the result in which using a data-parallel skeleton where not strictly necessary caused a significant slow down of a program, by a factor of 20-40. This implies that the skeletons are 'blindly' parallelising tasks and data structures, without performing any cost benefit analysis. This is alluded to in the further work section, in which the authors suggest a "skeleton-based cost analyser and a corresponding optimiser". Such a tool would be able to overcome the overhead problems by determining *when* to parallelise, not just *what* to parallelise.},
author = {Kuchen, H. and Cole, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Kuchen, Cole - The integration of task and data parallel skeletons.pdf:pdf},
journal = {Parallel Processing Letters},
keywords = {Data parallelism,algorithmic skeletons,task parallelism,two-tier model},
number = {02},
title = {{The integration of task and data parallel skeletons}},
volume = {12},
year = {2002}
}
@article{Bhupatiraju2017a,
abstract = {We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domain-specific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.},
author = {Bhupatiraju, S. and Singh, R. and Mohamed, A. and Kohli, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Bhupatiraju et al. - Deep API Programmer Learning to Program with APIs.pdf:pdf},
journal = {arXiv:1704.04327},
title = {{Deep API Programmer: Learning to Program with APIs}},
year = {2017}
}
@inproceedings{Donaldson2008,
abstract = {We describe an approach to automatic parallelisation of programs written in Sieve C++ (Codeplays C++ extension), using the Sieve compiler and runtime system. In Sieve C++, the programmer encloses a performance-critical region of code in a sieve block, thereby instructing the compiler to delay side-effects until the end of the block. The Sieve system partitions code inside a sieve block into independent fragments and speculatively distributes them among multiple cores. We present implementation details and experimental results for the Sieve system on the Cell BE processor.},
annote = {NULL},
author = {Donaldson, A. and Riley, C. and Lokhmotov, A. and Cook, A.},
booktitle = {Euro-Par},
doi = {10.1007/978-3-540-78474-6_5},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Donaldson et al. - Auto-parallelisation of sieve C programs.pdf:pdf},
isbn = {3540784721},
issn = {03029743},
title = {{Auto-parallelisation of sieve C++ programs}},
year = {2008}
}
@techreport{Portable2014,
annote = {NULL},
author = {{Khronos OpenCL Group Inc}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Khronos OpenCL Group Inc - The SPIR Specification Version 1.2.pdf:pdf},
title = {{The SPIR Specification Version 1.2}},
year = {2014}
}
@article{Nguyen,
annote = {NULL},
author = {Nguyen, A. and Yosinski, J. and Dosovitskiy, A. and Clune, J.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Nguyen et al. - Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space.pdf:pdf},
journal = {arXiv:1612.00005},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Plug {\&} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}}
}
@article{Deshpandea,
annote = {NULL},
author = {Deshpande, A. and Rock, J. and Forsyth, D.},
doi = {10.1109/ICCV.2015.72},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Deshpande, Rock, Forsyth - Learning Large-Scale Automatic Image Colorization.pdf:pdf},
isbn = {978-1-4673-8391-2},
title = {{Learning Large-Scale Automatic Image Colorization}}
}
@inproceedings{Spampinato2014,
abstract = {Many applications in media processing, control, graphics, and other domains require efficient small-scale linear alge- bra computations. However, most existing high performance libraries for linear algebra, such as ATLAS or Intel MKL are more geared towards large-scale problems (matrix sizes in the hundreds and larger) and towards specific interfaces (e.g., BLAS). In this paper we present LGen: a compiler for small-scale, basic linear algebra computations. The input to LGen is a fixed-size linear algebra expression; the output is a corresponding C function optionally including intrinsics to efficiently use SIMD vector extensions. LGen generates code using two levels of mathematical domain-specific lan- guages (DSLs). The DSLs are used to perform tiling, loop fusion, and vectorization at a high level of abstraction, be- fore the final code is generated. In addition, search is used to select among alternative generated implementations. We show benchmarks of code generated by LGen against Intel MKL and IPP as well as against alternative generators, such as the C++ template-based Eigen and the BTO compiler. The achieved speed-up is typically about a factor of two to three.},
annote = {A compiler for small-scale linear algebra computations which generates SIMD C code.},
author = {Spampinato, D. G. and P{\"{u}}schel, M.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Spampinato, P{\"{u}}schel - A Basic Linear Algebra Compiler.pdf:pdf},
isbn = {9781450326704},
keywords = {basic linear algebra,dsl,program synthesis,simd vectorization,small matrices,tiling},
publisher = {IEEE},
title = {{A Basic Linear Algebra Compiler}},
year = {2014}
}
@inproceedings{Gilmer2017,
abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
archivePrefix = {arXiv},
arxivId = {1704.01212},
author = {Gilmer, J. and Schoenholz, S. S. and Riley, P. F. and Vinyals, O. and Dahl, G. E.},
booktitle = {ICML},
doi = {10.1002/nme.2457},
eprint = {1704.01212},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Gilmer et al. - Neural Message Passing for Quantum Chemistry.pdf:pdf},
isbn = {978-1-4577-0079-8},
issn = {00295981},
pmid = {260949200001},
title = {{Neural Message Passing for Quantum Chemistry}},
url = {http://arxiv.org/abs/1704.01212},
year = {2017}
}
@article{Zoran2017,
abstract = {Nearest neighbor (kNN) methods have been gaining popularity in recent years in light of advances in hardware and efficiency of algorithms. There is a plethora of methods to choose from today, each with their own advantages and disadvantages. One requirement shared between all kNN based methods is the need for a good representation and distance measure between samples. We introduce a new method called differentiable boundary tree which allows for learning deep kNN representations. We build on the recently proposed boundary tree algorithm which allows for efficient nearest neighbor classification, regression and retrieval. By modelling traversals in the tree as stochastic events, we are able to form a differentiable cost function which is associated with the tree's predictions. Using a deep neural network to transform the data and back-propagating through the tree allows us to learn good representations for kNN methods. We demonstrate that our method is able to learn suitable representations allowing for very efficient trees with a clearly interpretable structure.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1702.08833},
author = {Zoran, D. and Lakshminarayanan, B. and Blundell, C.},
doi = {1702.08833},
eprint = {1702.08833},
journal = {arXiv:1702.08833},
title = {{Learning Deep Nearest Neighbor Representations Using Differentiable Boundary Trees}},
year = {2017}
}
@article{Rossum2012,
abstract = {The API of python/C},
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - The Python C API.pdf:pdf},
keywords = {API,python},
title = {{The Python / C API}},
year = {2016}
}
@inproceedings{Christen2011,
abstract = {Stencil calculations comprise an important class of kernels in many scientific computing applications ranging from simple PDE solvers to constituent kernels in multigrid methods as well as image processing applications. In such types of solvers, stencil kernels are often the dominant part of the com- putation, and an efficient parallel implementation of the kernel is therefore crucial in order to reduce the time to solution. However, in the current complex hardware microarchitectures, meticulous architecture-specific tuning is required to elicit the machine's full compute power. We present a code generation and auto-tuning framework PATUS for stencil computations targeted at multi- and manycore processors, such as multicore CPUs and graphics processing units, which makes it possible to generate compute kernels from a specification of the stencil operation and a parallelization and optimization strategy, and leverages the autotuning methodology to optimize strategy- dependent parameters for the given hardware architecture.},
annote = {Cited by 113. PATUS consists of a DSL for expressing stencil codes, a C code generator, and an autotuner for exploring the optimisation space, using blocking and vectorisation strategies. Pro: Supports arbitrarily high dimensional grids. They introduce *2* new DSLs without comment on why they were needed. This is a huge price of entry for anyone who actually wants to *use* PATUS to solve problems, and a decision that I think they could have justified better. They provide nice and concise explanations of stencil codes and the types of optimisation methods used by other autotuners (ATLAS, FLAME, FFTW, SPIRAL). However, they *barely* explain their own, saying only that they perform *either* an exhaustive, multi-run Powell, Nelder Mead, or evolutionary algorithms.
Their evaluation uses 6 benchmarks, and 3 architectures. Only 1 of those is a GPU. It would have been nice to shown performance across GPU architectures. From the perspective of autotuning, the paper comes as across quite weak: They do not present an "oracle" performance, so we can't compare the quality of their autotuner compared to it. They don't show how the optimal tunable parameter values vary across results, so they don't demonstrate how autotuning is *necessary* (maybe there's one single value which works well for all results?). They do not give any indication of convergence time of their autotuner. They do not report the number of different combinations that their autotuner tries.},
author = {Christen, M. and Schenk, O. and Burkhart, H.},
booktitle = {PDPS},
doi = {10.1109/IPDPS.2011.70},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Christen, Schenk, Burkhart - PATUS A Code Generation and Autotuning Framework for Parallel Iterative Stencil Computations on Mode.pdf:pdf},
isbn = {978-1-61284-372-8},
keywords = {autotuning,code generation,high performance computing,stencil computations},
month = {may},
publisher = {IEEE},
title = {{PATUS: A Code Generation and Autotuning Framework for Parallel Iterative Stencil Computations on Modern Microarchitectures}},
year = {2011}
}
@article{Bacci1999,
abstract = {Technological directions for innovative HPC software environments are discussed in this paper. We focus on industrial user requirements of heterogeneous multidisciplinary applications, performance portability, rapid prototyping and software reuse, integration and interoperability of standard tools. The various issues are demonstrated with reference to the PQE2000project and its programming environment Skeleton-based Integrated Environment (). includes a coordination language, , allowing the designers to express, in a primitive and structured way, efficient combinations of data parallelism and task parallelism. The goal is achieving fast development and good efficiency for applications in different areas. Modules developed with standard languages and tools are encapsulated into structures to form the global application. Performance models associated to the coordination language allow powerful optimizations to be introduced both at run time and at compile time without the direct intervention of the programmer. The paper also discusses the features of the environment related to debugging, performance analysis tools, visualization and graphical user interface. A discussion of the results achieved in some applications developed using the environment concludes the paper."},
annote = {NULL},
author = {Bacci, B. and Danelutto, M. and Pelagatti, S. and Vanneschi, M.},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Bacci et al. - SkIE A heterogeneous environment for HPC applications.pdf:pdf},
journal = {Parallel Computing},
keywords = {Parallel programming environments,Parallel programming models,Structured parallel programming},
number = {13-14},
title = {{SkIE: A heterogeneous environment for HPC applications}},
volume = {25},
year = {1999}
}
@article{Owens2008,
abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
annote = {NULL},
author = {Owens, Jd and Houston, M},
doi = {10.1109/JPROC.2008.917757},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Owens, Houston - GPU computing.pdf:pdf},
isbn = {0769527000},
issn = {00189219},
journal = {Proceedings of the IEEE},
pmid = {21776805},
title = {{GPU computing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4490127},
volume = {96},
year = {2008}
}
@article{Langdon2017,
abstract = {We survey genetic improvement (GI) of general purpose computing on graphics cards. We summarise several experiments which demonstrate four themes. Experiments with the gzip program show that genetic programming can automatically port sequential C code to parallel code. Experiments with the StereoCamera program show that GI can upgrade legacy parallel code for new hardware and software. Experiments with NiftyReg and BarraCUDA show that GI can make substantial improvements to current parallel CUDA applications. Finally, experiments with the pknotsRG program show that with semi-automated approaches, enormous speed ups can sometimes be had by growing and grafting new code with genetic programming in combination with human input.},
author = {Langdon, W. B. and Lam, B. Y. H. and Modat, M. and Petke, J. and Harman, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Langdon et al. - Genetic improvement of GPU software.pdf:pdf},
journal = {Genetic Programming and Evolvable Machines},
keywords = {Dynamic programming,GGGP,GI-GPGPU,GPGPU,Genetic programming,Grammar based genetic programming,Metaprogramming,NVidia CUDA,Parallel computing,SBSE,TOSKIM},
mendeley-tags = {TOSKIM},
number = {1},
title = {{Genetic improvement of GPU software}},
volume = {18},
year = {2017}
}
@inproceedings{Parallelization1988,
annote = {NULL},
author = {Aiken, A. and Nicolau, A.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/1988 - Aiken, Nicolau - Optimal Loop Parallelization.pdf:pdf},
publisher = {ACM},
title = {{Optimal Loop Parallelization}},
year = {1988}
}
@article{Xiong2016,
abstract = {Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9{\%} for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3{\%} for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training.},
annote = {NULL},
author = {Xiong, W. and Droppo, J. and Huang, X. and Seide, F. and Seltzer, M. and Stolcke, A. and Yu, D. and Zweig, G.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Xiong et al. - Achieving Human Parity in Conversational Speech Recognition.pdf:pdf},
journal = {arXiv:1610.05256},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Achieving Human Parity in Conversational Speech Recognition}},
year = {2016}
}
@phdthesis{Leather2010,
annote = {NULL},
author = {Leather, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Leather - Machine Learning in Compilers.pdf:pdf},
school = {University of Edinburgh},
title = {{Machine Learning in Compilers}},
year = {2010}
}
@article{Raghu2016,
annote = {NULL},
author = {Raghu, M. and Poole, B. and Kleinberg, J. and Ganguli, S. and Sohl-Dickstein, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Raghu et al. - On the expressive power of deep neural networks.pdf:pdf},
journal = {arXiv:1606.05336},
title = {{On the expressive power of deep neural networks}},
year = {2016}
}
@article{Fernandes2018,
abstract = {Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.},
archivePrefix = {arXiv},
arxivId = {1811.01824},
author = {Fernandes, Patrick and Allamanis, Miltiadis and Brockschmidt, Marc},
eprint = {1811.01824},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Fernandes, Allamanis, Brockschmidt - Structured Neural Summarization.pdf:pdf},
isbn = {0036-7273},
issn = {0036-7273},
pages = {1--22},
pmid = {6623005},
title = {{Structured Neural Summarization}},
url = {http://arxiv.org/abs/1811.01824},
year = {2018}
}
@article{Lewis2013,
annote = {NULL},
author = {Lewis, S. and Zamith, R. and Hermida, A.},
doi = {10.1080/08838151.2012.76170},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Lewis, Zamith, Hermida - Content Analysis in an Era of Big Data A Hybrid Approach to Computational Manual Methods.pdf:pdf},
journal = {Journal of Broadcasting {\&} Electronic Media},
title = {{Content Analysis in an Era of Big Data: A Hybrid Approach to Computational Manual Methods}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08838151.2012.761702},
year = {2013}
}
@article{Allamanis2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.06182v1},
author = {Allamanis, M. and Barr, E. T. and Devanbu, P. and Sutton, C.},
eprint = {arXiv:1709.06182v1},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Allamanis et al. - A Survey of Machine Learning for Big Code and Naturalness.pdf:pdf},
journal = {arXiv:1709.06182},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{A Survey of Machine Learning for Big Code and Naturalness}},
year = {2017}
}
@misc{Choi2010,
abstract = {We present a performance model-driven framework for automated performance tuning (autotuning) of sparse matrix-vector multiply (SpMV) on systems accelerated by graphics processing units (GPU). Our study consists of two parts. First, we describe several carefully hand-tuned SpMV implementations for GPUs, identifying key GPU-specific performance limitations, enhancements, and tuning opportunities. These implementations, which include variants on classical blocked compressed sparse row (BCSR) and blocked ELLPACK (BELLPACK) storage formats, match or exceed state-of-the-art implementations. For instance, our best BELLPACK implementation achieves up to 29.0 Gflop/s in single-precision and 15.7 Gflop/s in double-precision on the NVIDIA T10P multiprocessor (C1060), enhancing prior state-of-the-art unblocked implementations (Bell and Garland, 2009) by up to 1.8× and 1.5× for single-and double-precision respectively. However, achieving this level of performance requires input matrix-dependent parameter tuning. Thus, in the second part of this study, we develop a performance model that can guide tuning. Like prior autotuning models for CPUs (e.g., Im, Yelick, and Vuduc, 2004), this model requires offline measurements and run-time estimation, but more directly models the structure of multithreaded vector processors like GPUs. We show that our model can identify the implementations that achieve within 15{\%} of those found through exhaustive search.},
annote = {Cited by 281.},
author = {Choi, Jee W. and Singh, Amik and Vuduc, Richard W.},
booktitle = {PPoPP},
doi = {10.1145/1837853.1693471},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Choi, Singh, Vuduc - Model-driven autotuning of sparse matrix-vector multiply on GPUs.pdf:pdf},
isbn = {9781605587080},
issn = {03621340},
keywords = {gpu,performance,sparse matrix-vector multiplication},
title = {{Model-driven autotuning of sparse matrix-vector multiply on GPUs}},
year = {2010}
}
@article{Hochreiter1997,
abstract = {Pedagogical strategies are policies for a tutor to decide the next$\backslash$naction when there are multiple actions available. When the content$\backslash$nis controlled to be the same across experimental conditions, there$\backslash$nhas been little evidence that tutorial decisions have an impact on$\backslash$nstudents' learning. In this paper, we applied Reinforcement Learning$\backslash$n(RL) to induce two sets of pedagogical policies from pre-existing$\backslash$nhuman interaction data. The NormGain set was derived with the goal$\backslash$nof enhancing tutorial decisions that contribute to learning while$\backslash$nthe InvNormGain set was derived with the goal of enhancing those$\backslash$ndecisions that contribute less or even nothing to learning. The two$\backslash$nsets were then tested with human students. Our results show that$\backslash$nwhen the content was controlled to be the same, different pedagogical$\backslash$npolicies did make a difference in learning and more specifically,$\backslash$nthe NormGain students outperformed their peers. Overall our results$\backslash$nsuggest that content exposure and practice opportunities can help$\backslash$nstudents to learn even when tutors have poor pedagogical tutorial$\backslash$ntactics. However, with effective tutorial tactics, students can learn$\backslash$neven more. (Contains 9 tables, 6 figures, and 2 footnotes.)},
annote = {NULL},
author = {Chi, M. and Vanlehn, K. and Litman, D. and Jordan, P.},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Hochreiter, Schmidhuber - Long Short-Term Memory.pdf:pdf},
journal = {IJAIED},
keywords = {Reinforcement learning,human learning,intelligent tutoring systems,pedagogical strategy},
number = {1-2},
title = {{An evaluation of pedagogical tutorial tactics for a natural language tutoring system: A reinforcement learning approach}},
volume = {21},
year = {2011}
}
@article{Banerjee1993,
abstract = {An overview of automatic program parallelization techniques is presented. It covers dependence analysis techniques, followed by a discussion of program transformations, including straight-line code parallelization, do-loop transformations, and parallelization of recursive routines. Several experimental studies on the effectiveness of parallelizing compilers are surveyed},
annote = {From Duplicate 1 (Automatic program parallelization - Banerjee, Utpal; Eigenmann, Rudolf; Nicolau, Alexandru)







A survey paper of auto-parallelization in 1993. Interestingly, it's trying to "win over" people to parallelism, since it was still exotic. The paper is structured as follows: 2 - Dependence analysis theory, 3 - parallelizing code transformations, 4 - a review of the effectiveness of implementations. Cited by 363.},
author = {Banerjee, Utpal and Eigenmann, Rudolf and Nicolau, Alexandru},
doi = {10.1109/5.214548},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Banerjee, Eigenmann, Nicolau - Automatic program parallelization.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {2},
title = {{Automatic program parallelization}},
volume = {81},
year = {1993}
}
@inproceedings{Keramidas2007,
abstract = {Several cache management techniques have been proposed that indirectly try to base their decisions on cacheline reuse-distance, like Cache Decay which is a postdiction of reuse-distances: if a cacheline has not been accessed for some "decay interval" we know that its reuse-distance is at least as large as this decay interval. In this work, we propose to directly predict reuse-distances via instruction-based (PC) prediction and use this information for cache level optimizations. In this paper, we choose as our target for optimization the replacement policy of the L2 cache, because the gap between the LRU and the theoretical optimal replacement algorithm is comparatively large for L2 caches. This indicates that, in many situations, there is ample room for improvement. We evaluate our reuse- distance based replacement policy using a subset of the most memory intensive SPEC2000 and our results show significant benefits across the board.},
annote = {One of Pavlos' papers, it describes reuse-distance prediction for L2 caches.},
author = {Keramidas, G. and Petoumenos, P. and Kaxiras, S.},
booktitle = {ICCD},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Keramidas, Petoumenos, Kaxiras - Cache Replacement Based on Reuse-Distance Prediction Pavlos Petoumenos Stefanos Kaxiras.pdf:pdf},
isbn = {1424412587},
title = {{Cache Replacement Based on Reuse-Distance Prediction Pavlos Petoumenos Stefanos Kaxiras}},
year = {2007}
}
@phdthesis{Dastgeer2014,
abstract = {This thesis adresses issues associated with efficiently programming modern heterogeneous GPU-based systems, containingmulticore CPUs and one or more programmable Graphics Processing Units (GPUs). We use ideas from component-based programming to address pro- gramming, performance and portability issues of these heterogeneous systems. Specifically, we present three approaches that all use the idea of having multiple implementations for each computation; per- formance is achieved/retained either a) by selecting a suitable imple- mentation for each computation on a given platform or b) by dividing the computation work across different implementations running on CPU and GPU devices in parallel. In the first approach, we work on a skeleton programming library (SkePU) that provides high-level abstraction while making intelligent implementation selection decisions underneath either before or during the actual program execution. In the second approach, we develop a composition tool that parses extra information (metadata) from XML files, makes certain decisions offline, and, in the end, generates code for making the final decisions at runtime. The third approach is a framework that uses source-code annotations and program analysis to generate code for the runtime library to make the selection decision at runtime. With a generic performance modeling API alongside pro- gram analysis capabilities, it supports online tuning as well as complex program transformations. These approaches differ in terms of genericity, intrusiveness, capa- bilities and knowledge about the program source-code; however, they all demonstrate usefulness of component programming techniques for programming GPU-based systems. With experimental evaluation, we demonstrate how all three approaches, although different in their own way, provide good performance on different GPU-based systems for a variety of applications.},
annote = {NULL},
author = {Dastgeer, U},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Dastgeer - Performance-aware Component Composition for GPU-based systems.pdf:pdf},
title = {{Performance-aware Component Composition for GPU-based systems}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:712422},
year = {2014}
}
@phdthesis{Ogilvie2017a,
author = {Ogilvie, W. F.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Ogilvie - Reducing the Cost of Heuristic Generation with Machine Learning.pdf:pdf},
school = {University of Edinburgh},
title = {{Reducing the Cost of Heuristic Generation with Machine Learning}},
year = {2017}
}
@article{Hall2007,
annote = {NULL},
author = {Hall, Brian Beej},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Hall - Beej's Guide to C Programming.pdf:pdf},
title = {{Beej's Guide to C Programming}},
year = {2007}
}
@article{Strobelt2016,
abstract = {Recurrent neural networks, and in particular long short-term memory networks (LSTMs), are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows a user to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with domain specific structural annotations. We further show several use cases of the tool for analyzing specific hidden state properties on datasets containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.07461},
author = {Strobelt, H. and Gehrmann, S. and Huber, B. and Pfister, H. and Rush, A. M.},
eprint = {1606.07461},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Strobelt et al. - Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks.pdf:pdf},
journal = {arXiv:1606.07461},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
month = {jun},
title = {{Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1606.07461},
year = {2016}
}
@article{She2018,
abstract = {Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even the state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Coverage-guided evolutionary fuzzers, while fast and scalable, often get stuck at fruitless sequences of random mutations. By contrast, more systematic techniques like symbolic and concolic execution incur significant performance overhead and struggle to scale to larger programs. We design, implement, and evaluate NEUZZ, an efficient fuzzer that guides the fuzzing input generation process using deep neural networks. NEUZZ efficiently learns a differentiable neural approximation of the target program logic. The differentiability of the surro-gate neural program, unlike the original target program, allows us to use efficient optimization techniques like gradient descent to identify promising mutations that are more likely to trigger hard-to-reach code in the target program. We evaluate NEUZZ on 10 popular real-world programs and demonstrate that NEUZZ consistently outperforms AFL, a state-of-the-art evolutionary fuzzer, both at finding new bugs and achieving higher edge coverage. In total, NEUZZ found 36 previously unknown bugs that AFL failed to find and achieved, on average, 70× more edge coverage than AFL. Our results also demonstrate that NEUZZ can achieve average 9× more edge coverage while taking 16× less training time than other learning-enabled fuzzers.},
author = {She, D. and Pei, K. and Epstein, D. and Yang, J. and Ray, B. and Jana, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - She et al. - NEUZZ Efficient Fuzzing with Neural Program Learning.pdf:pdf},
journal = {arXiv:1807.05620},
title = {{NEUZZ: Efficient Fuzzing with Neural Program Learning}},
year = {2018}
}
@inproceedings{Lu,
annote = {NULL},
author = {Lu, Yanchao},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lu - Parallelism vs . Speculation Exploiting Speculative Genetic Algorithm on GPU.pdf:pdf},
isbn = {9781450334044},
keywords = {genetic algorithm,gpgpu,perfor-,speculative execution},
title = {{Parallelism vs . Speculation : Exploiting Speculative Genetic Algorithm on GPU}},
year = {2015}
}
@inproceedings{Chiu2015,
abstract = {We describe Genesis, a language for the generation of syn- thetic programs for use in machine learning-based perfor- mance auto-tuning. The language allows users to annotate a template program to customize its code using statistical distributions and to generate program instances based on those distributions. This effectively allows users to generate training programs whose characteristics or features vary in a statistically controlled fashion. We describe the language constructs, a prototype preprocessor for the language, and three case studies that show the ability of Genesis to ex- press a range of training programs in different domains. We evaluate the preprocessor's performance and the statistical quality of the samples it generates. We believe that Gen- esis is a useful tool for generating large and diverse sets of programs, a necessary component when training machine learning models for auto-tuning.},
annote = {From Duplicate 2 (Genesis: A Language for Generating Synthetic Training Programs for Machine Learning - Chiu, Alton; Garvey, Joseph; Abdelrahman, Tarek S)

0 cites.},
author = {Chiu, A. and Garvey, J. and Abdelrahman, T. S.},
booktitle = {CF},
doi = {10.1145/2742854.2742883},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chiu, Garvey, Abdelrahman - Genesis A Language for Generating Synthetic Training Programs for Machine Learning.pdf:pdf},
isbn = {9781450333580},
keywords = {macro languages,synthetic program generation},
publisher = {ACM},
title = {{Genesis: A Language for Generating Synthetic Training Programs for Machine Learning}},
year = {2015}
}
@article{ClintWhaley2001,
abstract = {This paper describes the automatically tuned linear algebra sotware (ATLAS) project, as well as the fundamental principles that underly it.},
annote = {This paper describes ATLAS, an approach to "automated empirical optimisation of software". ATLAS generates performance kernels for basic linear algebra subprograms (BLAS). Cited by},
author = {{Clint Whaley}, R. and Petitet, A. and Dongarra, J. J.},
doi = {10.1016/S0167-8191(00)00087-9},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Clint Whaley, Petitet, Dongarra - Automated empirical optimizations of software and the ATLAS project.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {AEOS,ATLAS,BLAS,portable performance},
month = {jan},
number = {1},
title = {{Automated empirical optimizations of software and the ATLAS project}},
volume = {27},
year = {2001}
}
@article{Benini2000,
abstract = {This tutorial surveys design methods for energy-efficient system-level design. We consider electronic systems consisting of a hardware platform and software layers. We consider the three major constituents of hardware that consume energy, namely computation, communica- tion, and storage units, and we review methods for reducing their energy consumption. We also study models for analyzing the energy cost of software, and methods for energy-efficient software design and compilation. This survey is organized around three main phases of a system design: conceptualization and modeling, design and implementation, and runtime management. For each phase, we review recent techniques for energy-efficient design of both hardware and software.},
annote = {NULL},
author = {Benini, Luca and Micheli, Giovanni De},
doi = {10.1145/335043.335044},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Benini, Micheli - System-level power optimization techniques and tools.pdf:pdf},
issn = {10844309},
journal = {TODAES},
number = {2},
title = {{System-level power optimization: techniques and tools}},
volume = {5},
year = {2000}
}
@article{Wilson2014,
abstract = {We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.},
author = {Wilson, G. and Aruliah, D. A. and Brown, C. T. and {Chue Hong}, N. P. and Davis, M. and Guy, R. T. and Haddock, S. H. D. and Huff, K. D. and Mitchell, I. M. and Plumbley, M. D. and Waugh, B. and White, E. P. and Wilson, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wilson et al. - Best Practices for Scientific Computing.pdf:pdf},
journal = {PLoS Biology},
keywords = {TOPRINT,TOSTUDY},
mendeley-tags = {TOPRINT,TOSTUDY},
number = {1},
title = {{Best Practices for Scientific Computing}},
volume = {12},
year = {2014}
}
@article{Magni2014a,
abstract = {Graphics Processing Units (GPUs) are efficient devices capa- ble of delivering high performance for general purpose com- putation. Realizing their full performance potential often requires extensive compiler tuning. This process is partic- ularly expensive since it has to be repeated for each target program and platform. In this paper we study the utilization of GPU hardware re- sources across multiple input sizes and compiler options. In this context we introduce the notion of hardware saturation. Saturation is reached when an application is executed with a number of threads large enough to fully utilize the available hardware resources. We give experimental evidence of hard- ware saturation and describe its properties using 16 OpenCL kernels on 3 GPUs from Nvidia and AMD.We show that in- put sizes that saturates the GPU show performance stability across compiler transformations. Using the thread-coarsening transformation as an exam- ple, we show that compiler settings maintain their relative performance across input sizes within the saturation region. Leveraging these hardware and software properties we pro- pose a technique to identify the input size at the lower bound of the saturation zone, we call it Minimum Saturation Point (MSP). By performing iterative compilation on the MSP input size we obtain results effectively applicable for much large input problems reducing the overhead of tuning by an order of magnitude on average.},
annote = {NULL},
author = {Magni, A. and Dubach, C. and O'Boyle, M.},
doi = {10.1145/2588768.2576791},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Magni, Dubach, O'Boyle - Exploiting GPU Hardware Saturation for Fast Compiler Optimization.pdf:pdf},
isbn = {9781450327664},
journal = {GPGPU},
keywords = {Experimentation,Measurement,Performance},
title = {{Exploiting GPU Hardware Saturation for Fast Compiler Optimization}},
url = {http://dl.acm.org/citation.cfm?doid=2588768.2576791},
year = {2014}
}
@inproceedings{White2016,
abstract = {Code clone detection is an important problem for software maintenance and evolution. Many approaches consider ei-ther structure or identifiers, but none of the existing detec-tion techniques model both sources of information. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based de-tection techniques where everything for representing terms and fragments in source code is mined from the repository. Our code analysis supports a framework, which relies on deep learning, for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level. We evaluated our novel learning-based approach for code clone detection with respect to feasibility from the point of view of software maintainers. We sampled and manually evaluated 398 file-and 480 method-level pairs across eight real-world Java systems; 93{\%} of the file-and method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types. We compared our approach to a traditional structure-oriented technique and found that our learning-based approach de-tected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our results affirm that our learning-based approach is suitable for clone detec-tion and a tenable technique for researchers.},
annote = {NULL},
author = {White, M. and Tufano, M. and Vendome, C. and Poshyvanyk, D.},
booktitle = {ASE},
doi = {10.1145/2970276.2970326},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - White et al. - Deep Learning Code Fragments for Code Clone Detection.pdf:pdf},
isbn = {9781450321389},
keywords = {Keywords code clone detection,TOSTUDY,abstract syntax trees,deep learning,language models,machine learning,neu-ral networks},
mendeley-tags = {TOSTUDY},
title = {{Deep Learning Code Fragments for Code Clone Detection}},
year = {2016}
}
@unpublished{Atre,
annote = {NULL},
author = {Atre, R. and Jannesari, A.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Atre, Jannesari - The Basic Building Blocks of Parallel Tasks.pdf:pdf},
title = {{The Basic Building Blocks of Parallel Tasks}}
}
@article{Donahue2017,
abstract = {Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.},
author = {Donahue, C. and Lipton, Z. C. and McAuley, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Donahue, Lipton, McAuley - Dance Dance Convolution.pdf:pdf},
journal = {arXiv:1703.06891},
title = {{Dance Dance Convolution}},
year = {2017}
}
@inproceedings{Bielik2016,
abstract = {We introduce a new generative model for code called probabilistic higher order grammar (PHOG). PHOG generalizes probabilistic context free grammars (PCFGs) by allowing conditioning of a production rule beyond the parent non-terminal, thus capturing rich contexts relevant to programs. Even though PHOG is more powerful than a PCFG, it can be learned from data just as efficiently. We trained a PHOG model on a large JavaScript code corpus and show that it is more precise than existing models, while similarly fast. As a result, PHOG can immediately benefit existing programming tools based on probabilistic models of code.},
author = {Bielik, P. and Raychev, V. and Vechev, M.},
booktitle = {ICML},
doi = {10.1126/science.7.179.764},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Bielik, Raychev, Vechev - PHOG Probabilistic Model for Code.pdf:pdf},
isbn = {9781510829008},
issn = {0036-8075},
pmid = {17813410},
title = {{PHOG: Probabilistic Model for Code}},
url = {http://proceedings.mlr.press/v48/bielik16.html},
year = {2016}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Y.},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1701.07274},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Li - Deep Reinforcement Learning An Overview.pdf:pdf},
isbn = {9781509011216},
issn = {1701.07274},
journal = {arXiv:1701.07274},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@inproceedings{Aldinucci2011a,
abstract = {FastFlow is a programming framework specifically targeting cache-coherent shared-memory multi- cores. It is implemented as a stack of C++ template libraries built on top of lock-free (and memory fence free) synchronization mechanisms. Its philosophy is to combine programmability with performance. In this paper a new FastFlow programming methodology aimed at supporting parallelization of existing sequential code via offloading onto a dynamically created software accelerator is presented. The new methodology has been validated using a set of simple micro-benchmarks and some real applications.},
annote = {NULL},
author = {Aldinucci, M. and Danelutto, M. and Kilpatrick, P. and Meneghin, M. and Torquati, M.},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Aldinucci et al. - Accelerating code on multi-cores with FastFlow.pdf:pdf},
keywords = {C++,lock-free synchronization,multi-core,offload,patterns},
publisher = {Springer},
title = {{Accelerating code on multi-cores with FastFlow}},
year = {2011}
}
@inproceedings{Garvey2015b,
annote = {NULL},
author = {Garvey, J. D. and Abdelrahman, T. S.},
booktitle = {ICPP},
doi = {10.1109/ICPP.2015.39},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Garvey, Abdelrahman - Automatic Performance Tuning of Stencil Computations on GPUs.pdf:pdf},
isbn = {978-1-4673-7587-0},
issn = {01903918},
keywords = {auto-tuning,gpgpu,machine learning,stencil},
publisher = {IEEE},
title = {{Automatic Performance Tuning of Stencil Computations on GPUs}},
year = {2015}
}
@misc{Etessamic,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 25. Discrete Probability.pdf:pdf},
number = {Chapter 7},
title = {{25. Discrete Probability}},
year = {2015}
}
@misc{Keras,
annote = {NULL},
title = {{Keras}},
url = {https://keras.io/}
}
@misc{Griswold2014,
annote = {NULL},
author = {Griswold, William G},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Griswold - How to Read an Engineering Research Paper.pdf:pdf},
title = {{How to Read an Engineering Research Paper}},
url = {http://www.inf.ed.ac.uk/teaching/courses/irm/notes/hypotheses.html},
year = {2014}
}
@unpublished{Belikov,
abstract = {This paper presents a profiling-based characterisation of eight small and medium-sized semi-explicitly parallel functional divide-and- conquer and data parallel applications on a server-class multi-core and on a cluster of multi-cores focusing on thread granularity, communica- tion, and memory management profiles, which appear highly relevant for dynamic and adaptive parallelism control at run-time system level. The results confirm that the parallel Haskell implementations cope well with large numbers of potential threads, quantify the impact of the communi- cation rate on performance, and identify memory management overhead as a major limiting factor to scalability on shared-memory machines.We find that a message-passing-based implementation outperforms a shared- memory-based implementation in terms of run time and scalability. The characterisation improves our understanding of the behaviour of paral- lel functional programs and hints at attributes that can be exploited to optimise application performance by improving parallelism manage- ment policies. In particular, thread subsumption mechanism appears key to controlling thread granularity whilst too aggressive thread creation increases garbage collection overhead. Additionally in the distributed- memory implementation, the amount of sharing determines the size of the global address table and hence the communication overhead associ- ated with fragmentation of the virtual shared heap.},
address = {Edinburgh},
annote = {NULL},
author = {Belikov, E and Loidl, HW and Michaelson, G},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Belikov, Loidl, Michaelson - Characterisation of Parallel Functional Applications.pdf:pdf},
institution = {School of Mathematical and Computer Sciences, Heriot-Watt University},
title = {{Characterisation of Parallel Functional Applications}},
url = {http://www.macs.hw.ac.uk/{~}eb96/pdf/tfp2014{\_}draft.pdf},
year = {2014}
}
@article{Zhang2018b,
abstract = {Many-core accelerators, as represented by the XeonPhi coprocessors and GPGPUs, allow software to exploit spatial and temporal sharing of computing resources to improve the overall system performance. To unlock this performance potential requires software to effectively partition the hardware resource to maximize the overlap between hostdevice communication and accelerator computation, and to match the granularity of task parallelism to the resource partition. However, determining the right resource partition and task parallelism on a per program, per dataset basis is challenging. This is because the number of possible solutions is huge, and the benefit of choosing the right solution may be large, but mistakes can seriously hurt the performance. In this paper, we present an automatic approach to determine the hardware resource partition and the task granularity for any given application, targeting the Intel XeonPhi architecture. Instead of hand-crafting the heuristic for which the process will have to repeat for each hardware generation, we employ machine learning techniques to automatically learn it. We achieve this by first learning a predictive model offline using training programs; we then use the learned model to predict the resource partition and task granularity for any unseen programs at runtime. We apply our approach to 23 representative parallel applications and evaluate it on a CPU-XeonPhi mixed heterogenous many-core platform. Our approach achieves, on average, a 1.6x (upto 5.6x) speedup, which translates to 94.5{\%} of the performance delivered by a theoretically perfect predictor.},
author = {Zhang, P. and Fang, J. and Tang, T. and Yang, C. and Wang, Z.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Zhang et al. - Tuning Streamed Applications on Intel Xeon Phi A Machine Learning Based Approach.pdf:pdf},
journal = {arXiv:1802.02760},
title = {{Tuning Streamed Applications on Intel Xeon Phi: A Machine Learning Based Approach}},
year = {2018}
}
@misc{Schulman2016,
annote = {NULL},
author = {Schulman, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Schulman - The Nuts and Bolts of Deep RL Research.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{The Nuts and Bolts of Deep RL Research}},
year = {2016}
}
@inproceedings{Maclaurin2015,
abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1502.03492},
author = {Maclaurin, D. and Duvenaud, D. and Adams, R. P.},
booktitle = {ICML},
eprint = {1502.03492},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Maclaurin, Duvenaud, Adams - Gradient-based Hyperparameter Optimization through Reversible Learning.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Gradient-based Hyperparameter Optimization through Reversible Learning}},
url = {http://arxiv.org/abs/1502.03492},
volume = {37},
year = {2015}
}
@article{Mendis2018a,
author = {Mendis, C. and Amarasinghe, S. and Carbin, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Mendis, Amarasinghe, Carbin - Ithemal Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks.pdf:pdf},
journal = {arXiv:1808.07412},
title = {{Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks}},
year = {2018}
}
@inproceedings{Sura2005,
abstract = {The rise of Java, C{\{}$\backslash${\#}{\}}, and other explicitly parallel languages has increased the importance of compiling for different software memory models. This paper describes co-operating escape, thread structure, and delay set analyses that enable high performance ...},
annote = {NULL},
author = {Sura, Z and Fang, X and Wong, CL and Midkiff, SP},
booktitle = {PPoPP},
doi = {10.1145/1065944.1065947},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Sura et al. - Compiler techniques for high performance sequentially consistent java programs.pdf:pdf},
isbn = {1595930809},
keywords = {java,memory consistency,synchronization},
title = {{Compiler techniques for high performance sequentially consistent java programs}},
year = {2005}
}
@inproceedings{Contreras2008,
abstract = {The Intel Threading Building Blocks (TBB) run- time library [1] is a popular C++ parallelization environment [2][3] that offers a set of methods and templates for creating parallel applications. Through support of parallel tasks rather than parallel threads, the TBB runtime library offers improved performance scalability by dynamically redistributing parallel tasks across available processors. This not only creates more scalable, portable parallel applications, but also increases pro- gramming productivity by allowing programmers to focus their efforts on identifying concurrency rather than worrying about its management. While many applications benefit from dynamic management of parallelism, dynamic management carries parallelization over- head that increases with increasing core counts and decreasing task sizes. Understanding the sources of these overheads and their implications on application performance can help program- mers make more efficient use of available parallelism. Clearly understanding the behavior of these overheads is the first step in creating efficient, scalable parallelization environments targeted at future CMP systems. In this paper we study and characterize some of the overheads of the Intel Threading Building Blocks through the use of real-hardware and simulation performance measurements. Our results show that synchronization overheads within TBB can have a significant and detrimental effect on parallelism performance. Random stealing, while simple and effective at low core counts, becomes less effective as application heterogeneity and core counts increase. Overall, our study provides valuable insights that can be used to create more robust, scalable runtime libraries.},
annote = {This paper profiles the performance of Intel TBB by adapting a set of benchmarks to use TBB, and profiling the overhead of parallelisation functions.




This is a generally poor paper. There are large gaps in their exerimental method explanation, and the conclusions they draw are somewhat weak.},
author = {Contreras, G. and Martonosi, M.},
booktitle = {IISWC},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Contreras, Martonosi - Characterizing and improving the performance of Intel Threading Building Blocks.pdf:pdf},
month = {oct},
publisher = {IEEE},
title = {{Characterizing and improving the performance of Intel Threading Building Blocks}},
year = {2008}
}
@inproceedings{K2010,
abstract = {As core counts in HPC clusters grow, almost every application user is trying to run software at higher scale than before. It is not always an easy task and can end in failure as the limitations of existing software are discovered. Users and developers quickly find new (and old) bugs as scale increases: software can be complex when it seeks to use more threads or processes to exploit the hardware. In this document, we show how using a debugger at the scale of the bug is the most effectiveway to tackle parallel software problems today.We introduceAllinea DDT – theworld's only scalable parallel debugger – and showhowit is fast, capable, and lets you debug your parallel or multithreaded application, no matter how big or small a system you use, easily. Allinea DDT has been setting standards for usability for many years and has torn up scalability records. It is used on the world's largest systems – debugging over 220,000 processes simultaneously in some cases. Bugs can be fixed easily for all developers – not just those with extreme scale – by using Allinea DDT at your scale. 1},
annote = {NULL},
author = {K, Thomas and Klausecker, Christof and Kranzlm, Dieter},
booktitle = {HLRS},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - K, Klausecker, Kranzlm - Debugging at Scale with Allinea DDT.pdf:pdf},
title = {{Debugging at Scale with Allinea DDT}},
year = {2013}
}
@article{Sellappa2004,
abstract = {Multigrid is widely used as an efficient solver for sparse linear systems arising from the discretization of elliptic boundary value problems. Linear relaxation methods such as Gauss-Seidel and Red-Black Gauss-Seidel form the principal computational component of multigrid, and thus affect its efficiency. In the context of multigrid, these iterative solvers are executed for a small number of iterations (2-8). We exploit this property of the algorithm to develop a cache-efficient multigrid method, by focusing on improving the memory behavior of the linear relaxation methods. The efficiency in our cache-efficient linear relaxation algorithm comes from two sources: reducing the number of data cache and TLB misses, and reducing the number of memory references by keeping values registerresident. Our optimizations are applicable to multigrid applied to linear systems arising from constant coefficient elliptic PDEs on structured grids. Experiments on five modern computing platforms show a performance improvement of 1.15-2.7 times over a standard implementation of Full Multigrid V-Cycle. 10.1177/1094342004041295},
annote = {Exploiting low number of iterations by trying to cram everything in cache. Cited by 42.},
author = {Sellappa, S. and Chatterjee, S.},
doi = {10.1177/1094342004041295},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Sellappa, Chatterjee - Cache-Efficient Multigrid Algorithms.pdf:pdf},
issn = {10943420},
journal = {IJHPCA},
keywords = {cache,hierarchical computation,locality,tlb},
title = {{Cache-Efficient Multigrid Algorithms}},
volume = {18},
year = {2004}
}
@inproceedings{Wang2013b,
abstract = {This paper studies an emerging class of software bugs called optimization-unstable code: code that is unexpect- edly discarded by compiler optimizations due to unde- ned behavior in the program. Unstable code is present in many systems, including the Linux kernel and the Post- gres database. The consequences of unstable code range from incorrect functionality to missing security checks. To reason about unstable code, this paper proposes a novel model, which views unstable code in terms of optimizations that leverage unde ned behavior. Using this model, we introduce a new static checker called Stack that precisely identi es unstable code. Applying Stack to widely used systems has uncovered 160 new bugs that have been con rmed and xed by developers.},
author = {Wang, X. and Zeldovich, N. and Kaashoek, M. F. and Solar-Lezama, A.},
booktitle = {SOSP},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Wang et al. - Towards Optimization-Safe Systems Analyzing the Impact of Undefined Behavior.pdf:pdf},
title = {{Towards Optimization-Safe Systems: Analyzing the Impact of Undefined Behavior}},
year = {2013}
}
@misc{UniversityofEdinburgh2014f,
annote = {NULL},
author = {{University of Edinburgh}},
booktitle = {Parallel Architectures},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - 6. Parallel Architectures.pdf:pdf},
title = {{6. Parallel Architectures}},
year = {2014}
}
@inproceedings{Zhangb,
abstract = {—Code clones are common in software. When apply-ing similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacer-bated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, GRAFTER transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, GRAFTER supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, GRAFTER successfully reuses tests in 94{\%} of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of GRAFTER, we systemati-cally inject faults using a mutation testing tool, MAJOR, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, GRAFTER detects 31{\%} more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that GRAFTER should effectively complement static cloning bug finders.},
author = {Zhang, T. and Kim, M.},
booktitle = {ICSE},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Zhang, Kim - Automated Transplantation and Differential Testing for Clones.pdf:pdf},
title = {{Automated Transplantation and Differential Testing for Clones}},
year = {2017}
}
@inproceedings{Ying2018,
abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10{\%} accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1806.08804},
author = {Ying, R. and You, J. and Morris, C. and Ren, X. and Hamilton, W. L. and Leskovec, J.},
booktitle = {NeurIPS},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {1806.08804},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ying et al. - Hierarchical Graph Representation Learning with Differentiable Pooling.pdf:pdf},
isbn = {1558606041},
issn = {18160948},
pmid = {397311},
title = {{Hierarchical Graph Representation Learning with Differentiable Pooling}},
url = {http://arxiv.org/abs/1806.08804},
year = {2018}
}
@inproceedings{Zhang2014,
abstract = {Scaling symbolic execution to large programs or programs with complex inputs remains difficult due to path explosion and complex constraints, as well as external method calls. Additionally, creating an effective test structure with symbolic inputs can be difficult. A popular symbolic execution strategy in practice is to perform symbolic execution not “from scratch” but based on existing test cases. This paper proposes that the effectiveness of this approach to symbolic execution can be enhanced by (1) reducing the size of seed test cases and (2) prioritizing seed test cases to maximize exploration efficiency. The proposed test case reduction strategy is based on a recently introduced generalization of delta debugging, and our prioritization techniques include novel methods that, for this purpose, can outperform some traditional regression testing algorithms. We show that applying these methods can significantly improve the effectiveness of symbolic execution based on existing test cases.},
author = {Zhang, C. and Groce, A. and Alipour, M. A.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Zhang, Groce, Alipour - Using Test Case Reduction and Prioritization to Improve Symbolic Execution.pdf:pdf},
keywords = {Symbolic execution,Test case reduction,Test prioritization},
publisher = {ACM},
title = {{Using Test Case Reduction and Prioritization to Improve Symbolic Execution}},
year = {2014}
}
@inproceedings{Terence2016,
author = {Terence, P. and Vinju, J.},
booktitle = {SLE},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Terence, Vinju - Towards a Universal Code Formatter through Machine Learning.pdf:pdf},
isbn = {9781450344470},
keywords = {a particular code formatting,a subjective notion,because the value of,configurable,discussions,for exam-,for-,formatting algorithms,matters must be highly,often leading to heated,pretty-printer,style is,this allows},
title = {{Towards a Universal Code Formatter through Machine Learning}},
year = {2016}
}
@article{Rossum2016,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Porting Extension Modules to Python.pdf:pdf},
title = {{Porting Extension Modules to Python}},
year = {2016}
}
@article{Miller1991a,
annote = {NULL},
author = {Miller, Barton P and Netzer, Robert H B},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Miller, Netzer - Techniques for Debugging with Flowback Analysis Parallel Programs.pdf:pdf},
journal = {TOPLAS},
keywords = {Debugging,flowback analysis,incremental program,program dependence graph,semantic analysis},
number = {4},
title = {{Techniques for Debugging with Flowback Analysis Parallel Programs}},
volume = {13},
year = {1991}
}
@misc{Haenel2015,
annote = {NULL},
author = {Haenel, V. and Rougier, N. P. and Gommers, R. and Pedregosa, F. and Zbigniew, J. and Virtanen, P. and Combelles, C. and Pinte, D. and Cimrman, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Haenel et al. - Scipy Lecture Notes.pdf:pdf},
title = {{Scipy Lecture Notes}},
year = {2015}
}
@article{Falcou2006,
abstract = {We present Quaff, a new skeleton-based parallel programming library. Its main originality is to rely on C++ template meta-programming techniques to achieve high efficiency. In particular, by performing most of skeleton instantiation and optimization at compile-time, Quaff can keep the overhead traditionally associated to object-oriented implementations of skeleton-based parallel programming libraries very small. This is not done at the expense of expressivity. This is demonstrated in this paper by several applications, including a full-fledged, realistic real-time vision application. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
annote = {Quaff is a C++ template library of algorithmic skeletons.},
author = {Falcou, J. and S{\'{e}}rot, J. and Chateau, T. and Laprest{\'{e}}, J. T.},
doi = {10.1016/j.parco.2006.06.001},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Falcou et al. - Quaff efficient C design for parallel skeletons.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {C++,Computer vision,MPI,Parallel skeletons,Template meta-programming},
number = {7},
publisher = {Elsevier},
title = {{Quaff: efficient C++ design for parallel skeletons}},
volume = {32},
year = {2006}
}
@misc{Coudarcher2001,
annote = {NULL},
author = {Coudarcher, R and S{\'{e}}rot, J and D{\'{e}}rutin, JP},
booktitle = {HIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Coudarcher, S{\'{e}}rot, D{\'{e}}rutin - Implementation of a skeleton-based parallel programming environment supporting arbitrary nesting.pdf:pdf},
title = {{Implementation of a skeleton-based parallel programming environment supporting arbitrary nesting}},
url = {http://link.springer.com/chapter/10.1007/3-540-45401-2{\_}6},
year = {2001}
}
@book{Instructions2001,
author = {Kennedy, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Kennedy - Optimizing Compilers for Modern Architectures A Dependence-Based Approach.pdf:pdf},
isbn = {1-55860-286-0},
title = {{Optimizing Compilers for Modern Architectures: A Dependence-Based Approach}},
year = {2001}
}
@misc{Patterson2002,
annote = {An amusing guide to having a bad career in academia, followed by some positive advice for achieving the opposite, starting at slide 17.},
author = {Patterson, David A},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Patterson - How to Have a Bad Career in Research Academia.pdf:pdf},
title = {{How to Have a Bad Career in Research / Academia}},
year = {2002}
}
@article{Chiu2016,
annote = {NULL},
author = {Chiu, A. and Garvey, J. and Abdelrahman, T. S.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Chiu, Garvey, Abdelrahman - A Language and Preprocessor for User-Controlled Generation of Synthetic Programs.pdf:pdf},
journal = {Scientific Programming},
title = {{A Language and Preprocessor for User-Controlled Generation of Synthetic Programs}},
year = {2016}
}
@article{Drozd2018,
abstract = {Fuzzing is a commonly used technique designed to test software by automatically crafting program inputs. Currently, the most successful fuzzing algorithms emphasize simple, low-overhead strategies with the ability to efficiently monitor program state during execution. Through compile-time instrumentation, these approaches have access to numerous aspects of program state including coverage, data flow, and heterogeneous fault detection and classification. However, existing approaches utilize blind random mutation strategies when generating test inputs. We present a different approach that uses this state information to optimize mutation operators using reinforcement learning (RL). By integrating OpenAI Gym with libFuzzer we are able to simultaneously leverage advancements in reinforcement learning as well as fuzzing to achieve deeper coverage across several varied benchmarks. Our technique connects the rich, efficient program monitors provided by LLVM Santizers with a deep neural net to learn mutation selection strategies directly from the input data. The cross-language, asynchronous architecture we developed enables us to apply any OpenAI Gym compatible deep reinforcement learning algorithm to any fuzzing problem with minimal slowdown.},
author = {Drozd, W. and Wagner, M. D.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Drozd, Wagner - FuzzerGym A Competitive Framework for Fuzzing and Learning.pdf:pdf},
journal = {arXiv:1807.07490},
title = {{FuzzerGym: A Competitive Framework for Fuzzing and Learning}},
year = {2018}
}
@inproceedings{Tallent2015,
annote = {NULL},
author = {Tallent, Nathan R and Vishnu, Abhinav and Dam, Hubertus Van and Daily, Jeff and Kerbyson, Darren J and Hoisie, Adolfy},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Tallent et al. - Diagnosing the Causes and Severity of One-Sided Message Contention.pdf:pdf},
isbn = {9781450332057},
keywords = {congestion,network contention,one-sided messages,performance analysis,performance modeling},
title = {{Diagnosing the Causes and Severity of One-Sided Message Contention}},
year = {2015}
}
@article{Nishihara2017,
abstract = {Machine learning applications are increasingly deployed not only to serve predictions using static models, but also as tightly-integrated components of feedback loops involving dynamic, real-time decision making. These applications pose a new set of requirements, none of which are difficult to achieve in isolation, but the combination of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proof-of-concept architecture that achieves a 63x performance improvement over a state-of-the-art execution framework for a representative application.},
author = {Nishihara, R. and Moritz, P. and Wang, S. and Tumanov, A. and Paul, W. and Schleier-Smith, J. and Liaw, R. and Jordan, M. I. and Stoica, I.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Nishihara et al. - Real-Time Machine Learning The Missing Pieces.pdf:pdf},
journal = {arXiv:1703.03924},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Real-Time Machine Learning: The Missing Pieces}},
year = {2017}
}
@misc{Silver2015c,
abstract = {Problem Set},
author = {Silver, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Reinforcement Learning Exam Answers.pdf:pdf},
title = {{Reinforcement Learning Exam Answers}},
year = {2015}
}
@inproceedings{Amato,
annote = {NULL},
author = {Amato, N. M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Amato - A Hierarchical Approach to Reducing Communication in Parallel Graph Algorithms.pdf:pdf},
isbn = {9781450332057},
keywords = {Big Data,Distributed Computing,Graph Analytics,Parallel Graph Processing},
title = {{A Hierarchical Approach to Reducing Communication in Parallel Graph Algorithms}},
year = {2015}
}
@inproceedings{Long2007,
abstract = {Parallelism is one of the main sources for performance improvement in modern computing environment, but the efficient exploitation of the available parallelism depends on a number of parameters. Determining the optimum number of threads for a given data parallel loop, for example, is a difficult problem and dependent on the specific parallel platform. This paper presents a learning-based approach to parallel workload allocation in a cost- aware manner. This approach uses static program features to classify programs, before deciding the best workload allocation scheme based on its prior experience with similar programs. Experimental results on 12 Java benchmarks (76 test cases with different workloads in total) show that it can efficiently allocate the parallel workload among Java threads and achieve an efficiency of 86{\%} on average.},
annote = {NULL},
author = {Long, S. and Fursin, G. and Franke, B.},
booktitle = {NPC},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Long, Fursin, Franke - A Cost-Aware Parallel Workload Allocation Approach Based on Machine Learning Techniques.pdf:pdf},
keywords = {cost,instance-based learning,parallelism,workload allocation},
publisher = {Springer},
title = {{A Cost-Aware Parallel Workload Allocation Approach Based on Machine Learning Techniques}},
year = {2007}
}
@inproceedings{Gatys2015a,
abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, tex-tures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object informa-tion more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1505.07376},
author = {Gatys, L. A. and Ecker, A. S. and Bethge, M.},
booktitle = {NIPS},
eprint = {1505.07376},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gatys, Ecker, Bethge - Texture Synthesis Using Convolutional Neural Networks.pdf:pdf},
title = {{Texture Synthesis Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1505.07376},
year = {2015}
}
@article{Creswell2017,
abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
author = {Creswell, A. and White, T. and Dumoulin, V. and Arulkumaran, K. and Sengupta, B. and Bharath, A. A.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Creswell et al. - Generative Adversarial Networks An Overview.pdf:pdf},
journal = {arXiv:1710.07035},
title = {{Generative Adversarial Networks: An Overview}},
year = {2017}
}
@inproceedings{Neil,
annote = {NULL},
author = {Neil, M. A. O. and Burtscher, M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Neil, Burtscher - Rethinking the Parallelization of Random-Restart Hill Climbing A Case Study in Optimizing a 2-Opt TSP Solver fo.pdf:pdf},
isbn = {9781450334075},
keywords = {code optimization,cuda,gpgpu,hill climbing,iterative,local search,program parallelization,tsp},
title = {{Rethinking the Parallelization of Random-Restart Hill Climbing A Case Study in Optimizing a 2-Opt TSP Solver for GPU Execution}},
year = {2015}
}
@inproceedings{Lutz2015,
abstract = {State of the art automatic optimization of OpenCL applications fo- cuses on improving the performance of individual compute kernels. Programmers address opportunities for inter-kernel optimization in specific applications by ad-hoc hand tuning: manually fusing ker- nels together. However, the complexity of interactions between host and kernel code makes this approach weak or even unviable for applications involving more than a small number of kernel in- vocations or a highly dynamic control flow, leaving substantial po- tential opportunities unexplored. It also leads to an over complex, hard to maintain code base. We present Helium, a transparent OpenCL overlay which dis- covers, manipulates and exploits opportunities for inter-and intra- kernel optimization. Helium is implemented as preloaded library and uses a delay-optimize-replay mechanism in which kernel calls are intercepted, collectively optimized, and then executed accord- ing to an improved execution plan. This allows us to benefit from composite optimizations, on large, dynamically complex applica- tions, with no impact on the code base. Our results show that He- lium obtains at least the same, and frequently even better perfor- mance, than carefully handtuned code. Helium outperforms hand- optimized code where the exact dynamic composition of compute kernel cannot be known statically. In these cases, we demonstrate speedups of up to 3x over unoptimized code and an average speedup of 1.4x over hand optimized code.},
annote = {NULL},
author = {Lutz, T. and Fensch, C. and Cole, M.},
booktitle = {GPGPU},
doi = {10.1145/2716282.2716284},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lutz, Fensch, Cole - Helium A Transparent Inter-kernel Optimizer for OpenCL.pdf:pdf},
isbn = {978-1-4503-3407-5},
keywords = {GPGPU,JIT compilation,OpenCL,gpgpu,inter-kernel optimization,jit compi-,opencl,profiling,staging},
title = {{Helium: A Transparent Inter-kernel Optimizer for OpenCL}},
url = {http://doi.acm.org/10.1145/2716282.2716284},
year = {2015}
}
@article{Howison2004,
annote = {NULL},
author = {Howison, James and Crowston, Kevin},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Howison, Crowston - The Perils and Pitfalls of Mining SourceForge The perils and pitfalls of mining SourceForge.pdf:pdf},
title = {{The Perils and Pitfalls of Mining SourceForge The perils and pitfalls of mining SourceForge}},
year = {2004}
}
@inproceedings{Filipovic2017,
author = {Filipovi{\v{c}}, J. and Petrovi{\v{c}}, F. and Benkner, S.},
booktitle = {ANDARE},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Filipovi{\v{c}}, Petrovi{\v{c}}, Benkner - Autotuning of OpenCL Kernels with Global Optimizations.pdf:pdf},
title = {{Autotuning of OpenCL Kernels with Global Optimizations}},
year = {2017}
}
@misc{Brown2010,
annote = {An implementation of Orbit algorithm in Eden skeleton language, derived from Haskell.},
author = {Brown, C and Hammond, K},
booktitle = {TFP},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Brown, Hammond - Ever-decreasing circles a skeleton for parallel orbit calculations in Eden.pdf:pdf},
title = {{Ever-decreasing circles: a skeleton for parallel orbit calculations in Eden}},
url = {http://chrisb.host.cs.st-andrews.ac.uk/publications/eden{\_}tfp2010.pdf},
year = {2010}
}
@inproceedings{Bienia2008,
abstract = {This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a bench- mark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on high- performance computing applications and used a limited number of synchronization methods. PARSEC includes emerging appli- cations in recognition, mining and synthesis (RMS) as well as sys- tems applications which mimic large-scale multithreaded commer- cial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, syn- chronization and off-chip traffic. The benchmark suite has been made available to the public.},
annote = {PARSEC is a benchmark for CMPs in recognition, mining and synthesis.},
author = {Bienia, Christian and Kumar, Sanjeev and Singh, Jaswinder Pal and Li, Kai},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Bienia et al. - The PARSEC Benchmark Suite Characterization and Architectural Implications.pdf:pdf},
isbn = {9781605582825},
keywords = {benchmark suite,multithreading,performance measurement,shared-memory computers},
publisher = {ACM},
title = {{The PARSEC Benchmark Suite: Characterization and Architectural Implications}},
year = {2008}
}
@article{Smith1999,
abstract = {The task of the referee is to evaluate in a timely manner a paper for publication in a specific journal or conference proceedings. This involves determining if the work presented is correct, if the problem studied and the results obtained are new and significant, if the quality of the presentation is satisfactory or can be made so, and what revisions and changes to the paper are necessary and/or desirable. The evaluation must be with regard to the coverage and degree of selec- tivity of the specific publication.},
annote = {NULL},
author = {Smith, Alan Jay},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Smith - The Task of the Referee.pdf:pdf},
journal = {Computer},
number = {4},
title = {{The Task of the Referee}},
volume = {23},
year = {1999}
}
@inproceedings{Ginsbach2018,
abstract = {Heterogeneous accelerators often disappoint. They provide the prospect of great performance, but only deliver it when using vendor specific optimized libraries or domain specific languages. This requires considerable legacy code modifications, hindering the adoption of heterogeneous computing. This paper develops a novel approach to automatically detect opportunities for accelerator exploitation. We focus on calculations that are well supported by established APIs: sparse and dense linear algebra, stencil codes and generalized reductions and histograms. We call them idioms and use a custom constraint-based Idiom Description Language (IDL) to discover them within user code. Detected idioms are then mapped to BLAS libraries, cuSPARSE and clSPARSE and two DSLs: Halide and Lift. We implemented the approach in LLVM and evaluated it on the NAS and Parboil sequential C/C++ benchmarks, where we detect 60 idiom instances. In those cases where idioms are a significant part of the sequential execution time, we generate code that achieves 1.26× to over 20× speedup on integrated and external GPUs.},
author = {Ginsbach, P. and Remmelg, T. and Steuwer, M. and Bodin, B. and Dubach, C. and O'Boyle, M.},
booktitle = {ASPLOS},
doi = {10.1145/3173162.3173182},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ginsbach et al. - Automatic Matching of Legacy Code to Heterogeneous APIs.pdf:pdf},
isbn = {9781450349116},
keywords = {computer systems organization},
title = {{Automatic Matching of Legacy Code to Heterogeneous APIs}},
url = {http://dl.acm.org/citation.cfm?doid=3173162.3173182},
year = {2018}
}
@article{Lee2010,
abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important as- pect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for to- day's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (be- tween 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after ap- plying optimizations appropriate for both CPUs and GPUs the per- formance gap between an Nvidia GTX280 processor and the Intel Core i7 960 processor narrows to only 2.5x on average. In this pa- per, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance dif- ferences between the two architectures, and recommend a set of architectural features which provide significant improvement in ar- chitectural efficiency for throughput kernels.},
annote = {Lee presents a performance analysis of optimised throughput computing applications for GPUs and CPUs. Of the 14 applications tested, they found GPU performance to be 0.7×-14.9× that of multi-threaded CPU code, with an average of only 2.5×. This is much lower than the 100×-1000× values reported by other studies, a fact that they attribute to uneven comparison of optimised GPU code to unoptimised CPU code, or vice versa. Lee et al. found{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}that multithreading, cache blocking, reordering of memory accesses and use of SIMD instructions to contribute most to CPU performance. For GPUs, the most effective optimisations are reducing synchronization costs, and exploiting local shared memory. It is unclear whether this relative performance still holds after 5 years. Cited by 504.},
author = {Lee, V. W. and Hammarlund, P. and Singhal, R. and Dubey, P. and Kim, C. and Chhugani, J. and Deisher, M. and Kim, D. and Nguyen, A. D. and Satish, S. and Smelyanskiy, M. and Chennupaty, S.},
doi = {10.1145/1816038.1816021},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Lee et al. - Debunking the 100X GPU vs. CPU myth.pdf:pdf},
isbn = {9781450300537},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {cpu architecture,gpu architecture,mance measurement,perfor-,performance analysis,software optimization,throughput comput-},
title = {{Debunking the 100X GPU vs. CPU myth}},
volume = {38},
year = {2010}
}
@article{Fernando2017a,
annote = {NULL},
author = {Fernando, C. and Banarse, D. and Blundell, C. and Zwols, Y. and Ha, D. and Rusu, A. A. and Pritzel, A. and Wierstra, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Fernando et al. - PathNet Evolution Channels Gradient Descent in Super Neural Networks.pdf:pdf},
journal = {arXiv:1701.08734},
keywords = {basal ganglia,continual learning,evolution and,giant networks,learning,multitask,path evolution algorithm,transfer learning},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
year = {2017}
}
@inproceedings{Tobergte2013a,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Aviv, Rotem and Wang, Guohui},
booktitle = {IWOCL},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Aviv, Wang - OpenCL-Based Mobile GPGPU Benchmarking Methods and Challenges.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{OpenCL-Based Mobile GPGPU Benchmarking: Methods and Challenges}},
year = {2016}
}
@article{Rossum2014b,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Sorting HOW TO.pdf:pdf},
title = {{Sorting HOW TO}},
year = {2016}
}
@inproceedings{Majo,
annote = {NULL},
author = {Majo, Z. and Gross, T. R.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Majo, Gross - A Library for Portable and Composable Data Locality Optimizations for NUMA Systems.pdf:pdf},
isbn = {9781450332057},
keywords = {data placement,numa,scheduling},
title = {{A Library for Portable and Composable Data Locality Optimizations for NUMA Systems}},
year = {2015}
}
@article{Lutz2013,
abstract = {GPGPUs are a powerful and energy-efficient solution for many problems.$\backslash$nFor higher performance or larger problems, it is necessary to distribute$\backslash$nthe problem across multiple GPUs, increasing the already high$\backslash$nprogramming complexity.$\backslash$nIn this article, we focus on abstracting the complexity of multi-GPU$\backslash$nprogramming for stencil computation. We show that the best strategy$\backslash$ndepends not only on the stencil operator, problem size, and GPU, but$\backslash$nalso on the PCI express layout. This adds nonuniform characteristics to$\backslash$na seemingly homogeneous setup, causing up to 23{\%} performance loss. We$\backslash$naddress this issue with an autotuner that optimizes the distribution$\backslash$nacross multiple GPUs.},
annote = {An exhaustive search of the optimisation space for stencil benchmark border regions using features: PCI-type, halo size, compute granularity, {\#} of GPUs. Evaluated on 2 different mobos and GPUs. Optimisations evaluated: rotating volume so that largest dimension is first dimension, adjusting halo size dynamically, and computing halo region first so that swap can occur concurrently with the rest of the computation. They concluded that the optimal setting depends on problem size, stencil shape GPU, and PCI. This is well worth studying in detail as a case of successfully applied autotuning to GPUs. Cited by 19.},
author = {Lutz, T. and Fensch, C. and Cole, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Lutz, Fensch, Cole - PARTANS An Autotuning Framework for Stencil Computation on Multi-GPU Systems.pdf:pdf},
issn = {15443566},
journal = {TACO},
number = {4},
title = {{PARTANS: An Autotuning Framework for Stencil Computation on Multi-GPU Systems}},
volume = {9},
year = {2013}
}
@inproceedings{Cooper2005,
abstract = {Research over the past five years has shown significant performance improvements using a technique called adaptive compilation. An adaptive compiler uses a compile-execute-analyze feedback loop to find the combination of optimizations and parameters that mini- mizes some performance goal, such as code size or execution time. Despite its ability to improve performance, adaptive compila- tion has not seen widespread use because of two obstacles: the large amounts of time that such systems have used to perform the many compilations and executions prohibits most users from adopting these systems, and the complexity inherent in a feedback-driven adaptive system has made it difficult to build and hard to use. A significant portion of the adaptive compilation process is devoted to multiple executions of the code being compiled. We have developed a technique called virtual execution to address this problem. Virtual execution runs the program a single time and preserves information that allows us to accurately predict the performance of different optimization sequences without running the code again. Our prototype implementation of this technique significantly reduces the time required by our adaptive compiler. In conjunction with this performance boost, we have developed a graphical-user interface (GUI) that provides a controlled view of the compilation process. By providing appropriate defaults, the in- terface limits the amount of information that the user must provide to get started. At the same time, it lets the experienced user exert fine-grained control over the parameters that control the system.},
annote = {ACME is an adaptive compilation tool which reduces the number of iterations needed to search the optimisation space by using a "virtual execution" technique. The tool has a GUI front-end for easier knob-twiddling.




Some interesting ideas, but I would argue that simply wrapping a program which requires 14 parameters in a GUI is not reducing the complexity, just disguising it. No one wants to have to twiddle knobs and look at graphs just to compile a piece of code.},
author = {Cooper, K. D. and Grosul, A. and Harvey, T. J. and Reeves, S. and Subramanian, D. and Torczon, L. and Waterman, T.},
booktitle = {LCTES},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Cooper et al. - ACME adaptive compilation made efficient.pdf:pdf},
isbn = {1595930183},
keywords = {adaptive compilation},
publisher = {ACM},
title = {{ACME: adaptive compilation made efficient}},
year = {2005}
}
@misc{Fastest,
annote = {NULL},
author = {NVIDIA},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - NVIDIA - Kepler GK110 Whitepaper.pdf:pdf},
title = {{Kepler GK110 Whitepaper}}
}
@article{Allamanis,
abstract = {The field of big code relies on mining large corpora of code to perform some learning task. A significant threat to this approach has been recently identified by Lopes et al. [19] who found a large amount of code duplication on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this article, we study the effect of code duplication to machine learning models showing that reported metrics are sometimes inflated by up to 100{\%} when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present an "errata" for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them, and release tools to help the community avoid this problem in future research. !},
archivePrefix = {arXiv},
arxivId = {1812.06469v1},
author = {Allamanis, M.},
eprint = {1812.06469v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Allamanis - The Adverse Effects of Code Duplication in Machine Learning Models of Code.pdf:pdf},
isbn = {1812.06469v1},
journal = {arXiv:1812.06469},
title = {{The Adverse Effects of Code Duplication in Machine Learning Models of Code}},
url = {https://github.com/Microsoft/dpu-utils.},
year = {2018}
}
@inproceedings{Spieker2017,
author = {Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Spieker et al. - Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration.pdf:pdf},
title = {{Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration}},
year = {2017}
}
@inproceedings{Wong2013,
abstract = {—Code comments improve software maintainability. To address the comment scarcity issue, we propose a new automatic comment generation approach, which mines comments from a large programming Question and Answer (Q{\&}A) site. Q{\&}A sites allow programmers to post questions and receive solutions, which contain code segments together with their descriptions, referred to as code-description mappings. We develop AutoComment to extract such mappings, and leverage them to generate description comments automatically for similar code segments matched in open-source projects. We apply AutoCom-ment to analyze Java and Android tagged Q{\&}A posts to extract 132,767 code-description mappings, which help AutoComment to generate 102 comments automatically for 23 Java and Android projects. The user study results show that the majority of the participants consider the generated comments accurate, adequate, concise, and useful in helping them understand the code.},
annote = {NULL},
author = {Wong, E. and Yang, J. and Tan, L.},
booktitle = {ASE},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Wong, Yang, Tan - AutoComment Mining Question and Answer Sites for Automatic Comment Generation.pdf:pdf},
keywords = {Index Terms—automated comment generation,documentation,program comprehension},
publisher = {IEEE},
title = {{AutoComment: Mining Question and Answer Sites for Automatic Comment Generation}},
year = {2013}
}
@article{Ashouri2016,
annote = {Completely uninspiring paper. It would be groudbreaking were it published 10 years before.},
author = {Ashouri, A. H. and Milano, G. and Palermo, G. and Park, E. and Cavazos, J. and Silvano, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ashouri et al. - COBAYN Compiler Autotuning Framework Using Bayesian Networks.pdf:pdf},
journal = {TACO},
number = {2},
title = {{COBAYN: Compiler Autotuning Framework Using Bayesian Networks}},
volume = {13},
year = {2016}
}
@inproceedings{Zhang2013a,
abstract = {This paper develops and evaluates search and optimization techniques for auto-tuning 3D stencil (nearest-neighbor) computations on GPUs. Observations indicate that parameter tuning is necessary for heterogeneous GPUs to achieve optimal performance with respect to a search space. Our proposed framework takes a most concise specification of stencil behavior from the user as a single formula, auto-generates tunable code from it, systematically searches for the best configuration and generates the code with optimal parameter configurations for different GPUs. This auto-tuning approach guarantees adaptive performance for different generations of GPUs while greatly enhancing programmer productivity. Experimental results show that the delivered floating point performance is very close to previous handcrafted work and outperforms other auto-tuned stencil codes by a large margin.},
annote = {A DSL for 3D stencil codes, with an autotuner to search for optimal values of parameters. Parameters are: block size (i.e. decomposing the grid into smaller sections), block dimension (i.e. decomposing blocks into workgroups), and whether to use texture memory or not. This provides a search space of worse case 200 permutations, which are exhaustively evaluated to find the optimum. Since CUDA is statically compiled, this means compiling and evaluating the performance of up to 200 versions of a kernel. Cited by 51.},
author = {Zhang, Y. and Mueller, F.},
booktitle = {CGO},
doi = {10.1109/TPDS.2012.160},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Zhang, Mueller - Auto-generation and Auto-tuning of 3D Stencil Codes on GPU clusters.pdf:pdf},
isbn = {9781450312066},
issn = {10459219},
publisher = {IEEE},
title = {{Auto-generation and Auto-tuning of 3D Stencil Codes on GPU clusters}},
year = {2012}
}
@inproceedings{Che2009,
abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.},
annote = {From Duplicate 1 (Rodinia: A benchmark suite for heterogeneous computing - Che, Shuai; Boyer, Michael; Meng, Jiayuan; Tarjan, David; Sheaffer, Jeremy W.; Lee, Sang Ha; Skadron, Kevin)

Rodinia is a heterogeneous benchmark suite with a number of workloads.

From Duplicate 2 (Rodinia: A benchmark suite for heterogeneous computing - Che, Shuai; Boyer, Michael; Meng, Jiayuan; Tarjan, David; Sheaffer, Jeremy W.; Lee, Sang-Ha Ha; Skadron, Kevin)

From Duplicate 1 (Rodinia: A benchmark suite for heterogeneous computing - Che, Shuai; Boyer, Michael; Meng, Jiayuan; Tarjan, David; Sheaffer, Jeremy W.; Lee, Sang-Ha; Skadron, Kevin)

Rodinia is a heterogeneous benchmark suite with a number of workloads. Worth keeping an eye on.

From Duplicate 2 (Rodinia: A benchmark suite for heterogeneous computing - Che, Shuai; Boyer, Michael; Meng, Jiayuan; Tarjan, David; Sheaffer, Jeremy W.; Lee, Sang Ha; Skadron, Kevin)

Rodinia is a heterogeneous benchmark suite with a number of workloads.},
author = {Che, S. and Boyer, M. and Meng, J. and Tarjan, D. and Sheaffer, J. W. and Lee, S. H. and Skadron, K.},
booktitle = {IISWC},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Che et al. - Rodinia A Benchmark Suite for Heterogeneous Computing.pdf:pdf},
month = {oct},
publisher = {IEEE},
title = {{Rodinia: A Benchmark Suite for Heterogeneous Computing}},
year = {2009}
}
@inproceedings{Biswas,
author = {Biswas, R. and Lu, X. and Panda, D.},
booktitle = {BPOE},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Biswas, Lu, Panda - Designing a Micro-Benchmark Suite to Evaluate gRPC for TensorFlow Early Experiences.pdf:pdf},
keywords = {deep learn-,grpc,micro-benchmark,tensorflow},
title = {{Designing a Micro-Benchmark Suite to Evaluate gRPC for TensorFlow: Early Experiences}},
year = {2018}
}
@inproceedings{Lewis2015,
annote = {NULL},
author = {Lewis, T James and Sastry, Shankar P and Kirby, Robert M and Whitaker, Ross T},
booktitle = {HiPC},
doi = {10.1109/HiPC.2015.38},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lewis et al. - A GPU-Based MIS Aggregation Strategy Algorithms , Comparisons , and Applications Within AMG.pdf:pdf},
isbn = {9781467384889},
title = {{A GPU-Based MIS Aggregation Strategy : Algorithms , Comparisons , and Applications Within AMG}},
year = {2015}
}
@inproceedings{VanCraeynest2012,
abstract = {Single-ISA heterogeneous multi-core processors are typically composed of small (e.g., in-order) power-efficient cores and big (e.g., out-of-order) high-performance cores. The effectiveness of heterogeneous multi-cores depends on how well a scheduler can map workloads onto the most appropriate core type. In general, small cores can achieve good performance if the workload inherently has high levels of ILP. On the other hand, big cores provide good performance if the workload exhibits high levels of MLP or requires the ILP to be extracted dynamically. This paper proposes Performance Impact Estimation (PIE) as a mechanism to predict which workload-to-core mapping is likely to provide the best performance. PIE collects CPI stack, MLP and ILP profile information, and estimates performance if the workload were to run on a different core type. Dynamic PIE adjusts the scheduling at runtime and thereby exploits fine-grained time-varying execution behavior. We show that PIE requires limited hardware support and can improve system performance by an average of 5.5{\%} over recent state-of-the-art scheduling proposals and by 8.7{\%} over a sampling-based scheduling policy.},
annote = {NULL},
author = {{Van Craeynest}, K. and Jaleel, A. and Eeckhout, L. and Narvaez, P. and Emer, J.},
booktitle = {ISCA},
doi = {10.1109/ISCA.2012.6237019},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Van Craeynest et al. - Scheduling heterogeneous multi-cores through performance impact estimation (PIE).pdf:pdf},
isbn = {9781467304757},
issn = {10636897},
title = {{Scheduling heterogeneous multi-cores through performance impact estimation (PIE)}},
year = {2012}
}
@article{Compton2002,
abstract = {Due to its potential to greatly accelerate a wide variety of applications, reconfigurable computing has become a subject of a great deal of research. Its key feature is the ability to perform computations in hardware to increase performance, while retaining much of the flexibility of a software solution. In this survey, we explore the hardware aspects of reconfigurable computing machines, from single chip architectures to multi-chip systems, including internal structures and external coupling. We also focus on the software that targets these machines, such as compilation tools that map high-level algorithms directly to the reconfigurable substrate. Finally, we consider the issues involved in run-time reconfigurable systems, which reuse the configurable hardware during program execution.},
annote = {NULL},
author = {Compton, K. and Hauck, S.},
doi = {10.1145/508352.508353},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Compton, Hauck - Reconfigurable computing a survey of systems and software.pdf:pdf},
isbn = {9780123705228},
issn = {03600300},
journal = {CSUR},
number = {2},
title = {{Reconfigurable computing: a survey of systems and software}},
volume = {34},
year = {2002}
}
@techreport{Torquati,
abstract = {FastFlow is an open-source, structured parallel programming framework orig- inally conceived to support highly efficient stream parallel computation while targeting shared memory multi-core. Its efficiency comes mainly from the op- timised implementation of the base communication mechanisms and from its layered design. FastFlow provides the parallel applications programmer with a set of ready-to-use, parametric algorithmic skeletons modelling the most com- mon parallelism exploitation patterns. The algorithmic skeletons provided by FastFlow may be freely nested to model more and more complex parallelism exploitation patterns.},
annote = {An OCaml implementation of 2 data parallel operations. Big woop. *sigh* I'm sure they enjoyed it.},
author = {Torquati, M},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Torquati - Parallel Programming Using FastFlow.pdf:pdf},
institution = {Computer Science Department, University of Pisa},
title = {{Parallel Programming Using FastFlow}},
url = {http://calvados.di.unipi.it/storage/tutorial/fftutorial.pdf},
year = {2014}
}
@inproceedings{Miceli2012,
abstract = {Performance analysis and tuning is an important step in programming multicore- and manycore-based parallel architectures. While there are several tools to help developers analyze application performance, no tool provides recommendations about how to tune the code. The AutoTune project is extending Periscope, an automatic distributed performance analysis tool developed by Technische Universit{\"{a}}t M{\"{u}}nchen, with plugins for performance and energy efficiency tuning. The resulting Periscope Tuning Framework will be able to tune serial and parallel codes for multicore and manycore architectures and return tuning recommendations that can be integrated into the production version of the code. The whole tuning process - both performance analysis and tuning - will be performed automatically during a single run of the application. {\textcopyright} 2013 Springer-Verlag.},
annote = {NULL},
author = {Miceli, R. and Civario, G. and Sikora, A. and C{\'{e}}sar, E. and Gerndt, M. and Haitof, H. and Navarrete, C. and Benkner, S. and Sandrieser, M. and Morin, L. and Bodin, F.},
booktitle = {PARA},
doi = {10.1007/978-3-642-36803-5_24},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Miceli et al. - AutoTune A Plugin-Driven Approach to the Automatic Tuning of Parallel Applications.pdf:pdf},
isbn = {9783642368028},
issn = {03029743},
title = {{AutoTune: A Plugin-Driven Approach to the Automatic Tuning of Parallel Applications}},
year = {2012}
}
@inproceedings{Cummins2018a,
author = {Cummins, C. and Petoumenos, P. and Murray, A. and Leather, H.},
booktitle = {ACACES},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Cummins et al. - DeepSmith Compiler Fuzzing through Deep Learning.pdf:pdf},
keywords = {compilers,deep learning,differential testing,opencl},
title = {{DeepSmith: Compiler Fuzzing through Deep Learning}},
year = {2018}
}
@misc{Arapinis2014,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 02. Propositional Logic.pdf:pdf},
title = {{02. Propositional Logic}},
year = {2015}
}
@article{Prelec2004,
abstract = {Subjective judgments, an essential information source for science and policy, are problematic because there are no public criteria for assessing judgmental truthfulness. I present a scoring method for eliciting truthful subjective data in situations where objective truth is unknowable. The method assigns high scores not to the most common answers but to the answers that are more common than collectively predicted, with predictions drawn from the same population. This simple adjustment in the scoring criterion removes all bias in favor of consensus: Truthful answers maximize expected score even for respondents who believe that their answer represents a minority view.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.},
author = {Prelec, D.},
doi = {10.1126/science.1102081},
eprint = {arXiv:1011.},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Prelec - A Bayesian truth serum for subjective data.pdf:pdf},
isbn = {0036-8075$\backslash$r1095-9203},
issn = {0036-8075},
journal = {Science},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
number = {5695},
pmid = {15486294},
title = {{A Bayesian truth serum for subjective data.}},
volume = {306},
year = {2004}
}
@inproceedings{Han2013a,
abstract = {Branch divergence can incur a high performance penalty on GPGPU programs. We propose a software optimization, called loop merging, that aims to reduce divergence due to varying trip-count of a loop across warp threads. This optimization merges the divergent loop with one or more outer surrounding loops into one loop. In this way, warp threads do not have to wait for each other in each outer loop iter- ation, thus improving execution efficiency. We implement loop merging in LLVM. Our evaluation on a Fermi GPU shows that it improves the performance of a synthetic benchmark and five application benchmarks by up to 1.6× and 4.3× respectively.},
annote = {NULL},
author = {Han, T. D. and Abdelrahman, T. S.},
booktitle = {GPGPU},
doi = {10.1145/2458523.2458525},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Han, Abdelrahman - Reducing divergence in GPGPU programs with loop merging.pdf:pdf},
isbn = {9781450320177},
keywords = {Branch divergence,Loop optimizations,Perfor- mance evaluation},
title = {{Reducing divergence in GPGPU programs with loop merging}},
url = {http://dl.acm.org/citation.cfm?doid=2458523.2458525{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2458525},
year = {2013}
}
@inproceedings{Dagum1998,
abstract = {OpenMP, the portable alternative to message passing, offers a powerful new way to achieve scalability in software. This article compares OpenMP to existing parallel-programming models.},
annote = {NULL},
author = {Dagum, Leonardo and Enon, Rameshm},
booktitle = {CSE},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Dagum, Enon - OpenMP an industry standard API for shared-memory programming.pdf:pdf},
title = {{OpenMP: an industry standard API for shared-memory programming}},
year = {1998}
}
@misc{UniversityofEdinburgh2014j,
annote = {NULL},
author = {{University of Edinburgh}},
booktitle = {Parallel Architectures},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - 3. Parallel Architectures.pdf:pdf},
title = {{3. Parallel Architectures}},
year = {2014}
}
@inproceedings{Bhat,
abstract = {The increasing heterogeneity, dynamism, and uncertainty of emerging DCE (Distributed Computing Environment) systems imply that an application must be able to detect and adapt to changes in its state, its requirements, and the state of the system to meet its desired QoS constraints. As system and application scales increase, ad hoc heuristic-based approaches to application adaptation and self-management quickly become insufficient. This paper builds on the Accord programming system for rule-based self-management, and extends it with model-based control and optimization strategies. This paper also presents the development of a self-managing data streaming service based on online control using Accord. This service is part of a Grid-based fusion simulation workflow consisting of long-running simulations, executing on remote supercomputing sites and generating several terabytes of data, which must then be streamed over a wide-area network for live analysis and visualization. The self-managing data streaming service minimize data streaming overheads on the simulations, adapt to dynamic network bandwidth, and prevent data loss. An evaluation of the service demonstrating its feasibility is presented.},
annote = {This paper enables applications to adapt using model-based control and optimisation strategies, built on the Accord programming system. It enables applicatoin requirements to be dynamically specified, and sesntivie to system/application state.},
author = {Bhat, V. and Parashar, M. and Khandekar, M. and Kandasamy, N. and Abdelwahed, S.},
booktitle = {ICCAC},
doi = {10.1109/ICAC.2006.1662377},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Bhat et al. - Enabling Self-Managing Applications using Model-based Online Control Strategies.pdf:pdf},
isbn = {1-4244-0175-5},
keywords = {Autonomic computing,Grid workflows,model-based control,programming systems,self-managed data streaming.},
publisher = {Ieee},
title = {{Enabling Self-Managing Applications using Model-based Online Control Strategies}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1662377},
year = {2006}
}
@book{Banerjee1990,
annote = {NULL},
author = {Banerjee, Utpal},
file = {:Users/cec/Google Drive/Mendeley Library/1990 - Banerjee - Unimodular transformations of double loops.pdf:pdf},
publisher = {University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development},
title = {{Unimodular transformations of double loops}},
year = {1990}
}
@inproceedings{Ellis2015,
abstract = {We introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis. We apply our tech- niques to both a visual learning domain and a language learning problem, showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some English inflectional morphology. Taken together, these results give both a new approach to unsupervised learning of symbolic composi- tional structures, and a technique for applying program synthesis tools to noisy data.},
annote = {NULL},
author = {Ellis, K. and Solar-lezama, A. and Tenenbaum, J. B.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ellis, Solar-lezama, Tenenbaum - Unsupervised Learning by Program Synthesis.pdf:pdf},
issn = {10495258},
title = {{Unsupervised Learning by Program Synthesis}},
year = {2015}
}
@inproceedings{Rompf2013,
abstract = {High level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user- or library-defined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts. Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops. We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments.},
annote = {NULL},
author = {Rompf, T. and Sujeeth, A. K. and Amin, N. and Brown, K. J. and Jovanovic, V. and Lee, H. and Jonnalagedda, M. and Olukotun, K. and Odersky, M.},
booktitle = {POPL},
doi = {10.1145/2480359.2429128},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Rompf et al. - Optimizing Data Structures in High-Level Programs New Directions for Extensible Compilers based on Staging.pdf:pdf},
isbn = {9781450318327},
issn = {0362-1340},
keywords = {code generation,data structures,extensible,staging},
title = {{Optimizing Data Structures in High-Level Programs New Directions for Extensible Compilers based on Staging}},
url = {http://doi.acm.org/10.1145/2429069.2429128},
year = {2013}
}
@inproceedings{Chen2015a,
annote = {NULL},
author = {Chen, Yifeng and Mei, Hong},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chen, Mei - Tiles A New Language Mechanism for Heterogeneous Parallelism.pdf:pdf},
isbn = {9781450332057},
keywords = {parallel programming},
title = {{Tiles : A New Language Mechanism for Heterogeneous Parallelism}},
year = {2015}
}
@article{Steuwer2014,
abstract = {Algorithmic skeletons simplify software development: they abstract typi- cal patterns of parallelism and provide their efficient implementations, allowing the application developer to focus on the structure of algorithms, rather than on imple- mentation details.This becomes especially important formodern parallel systems with multiple graphics processing units (GPUs) whose programming is complex and error- prone, because state-of-the-art programming approaches likeCUDAandOpenCLlack high-level abstractions. We define a new algorithmic skeleton for allpairs computa- tions which occur in real-world applications, ranging from bioinformatics to physics. We develop the skeleton's generic parallel implementation for multi-GPU Systems in OpenCL. To enable the automatic use of the fastGPUmemory, we identify and imple- ment an optimized version of the allpairs skeleton with a customizing function that follows a certain memory access pattern.We use matrix multiplication as an applica- tion study for the allpairs skeleton and its two implementations and demonstrate that the skeleton greatly simplifies programming, saving up to 90{\%} of lines of code as compared to OpenCL. The performance of our optimized implementation is up to 6.8 times higher as compared with the generic implementation and is competitive to the performance of a manually written optimized OpenCL code.},
annote = {The allpairs pattern operates on two matrices of sizes 'n * d' and 'm * d', producing a matrix of size 'n * m'. It operates on each row vector in turn: 'c{\_}{\{}i,j{\}} = A{\_}i + B{\_}j', where '+' is the user function binary operator. An example use is Matrix Multiplication, where the '+' operator represents dot product.},
author = {Steuwer, M. and Friese, M. and Albers, S. and Gorlatch, S.},
doi = {10.1007/s10766-013-0265-6},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Steuwer et al. - Introducing and implementing the allpairs skeleton for programming multi-GPU Systems.pdf:pdf},
issn = {08857458},
journal = {IJPP},
keywords = {Algorithmic skeletons,Allpairs computation,GPU computing,High-level programming models,SkelCL},
title = {{Introducing and implementing the allpairs skeleton for programming multi-GPU Systems}},
volume = {42},
year = {2014}
}
@article{Turing1937,
annote = {NULL},
author = {Turing, A. M.},
doi = {10.1112/plms/s2-42.1.230},
file = {:Users/cec/Google Drive/Mendeley Library/1937 - Turing - On computable numbers, with an application to the Entscheidungsproblem.pdf:pdf},
journal = {Proceedings of the London Mathematical Society},
number = {1931},
title = {{On computable numbers, with an application to the Entscheidungsproblem}},
volume = {42},
year = {1937}
}
@article{Toit2013,
annote = {NULL},
author = {Toit, Stefanus Du and Corporation, Intel},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Toit, Corporation - N4594 Working Draft, Standard for Programming Language C.pdf:pdf},
title = {{N4594: Working Draft, Standard for Programming Language C ++}},
year = {2013}
}
@article{Mantripragada2014,
abstract = {High intensive computation applications can usually take days to months to finish an execution. During this time, it is common to have variations of the available resources when considering that such hardware is usually shared among a plurality of researchers/departments within an organization. On the other hand, High Performance Clusters can take advantage of Cloud Computing bursting techniques for the execution of applications together with the on-premise resources. In order to meet deadlines, high intensive computational applications can use the Cloud to boost their performance when they are data and task parallel. This article presents an ongoing work towards the use of extended resources of an HPC execution platform together with Cloud. We propose an unified view of such heterogeneous environments and a method that monitors, predicts the application execution time, and dynamically shifts part of the domain -- previously running in local HPC hardware -- to be computed on the Cloud, meeting then a specific deadline. The method is exemplified along with a seismic application that, at runtime, adapts itself to move part of the processing to the Cloud (in a movement called bursting) and also auto-scales (the moved part) over cloud nodes. Our preliminary results show that there is an expected overhead for performing this movement and for synchronizing results, but our outcomes demonstrate it is an important feature for meeting deadlines in the case an on-premise cluster is overloaded or cannot provide the capacity needed for a particular project.},
annote = {Won NVIDIA best-paper award.},
archivePrefix = {arXiv},
arxivId = {1412.6392},
author = {Mantripragada, K. and Binotto, A. and Tizzei, L. P.},
eprint = {1412.6392},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Mantripragada, Binotto, Tizzei - A Self-adaptive Auto-scaling Method for Scientific Applications on HPC Environments and Clouds.pdf:pdf},
keywords = {cloud,cloud computing,is granted without fee,load-balancing,or hard copies of,part or all of,permission to make digital,personal or classroom use,provided that copies,self-adaptation,this work for},
title = {{A Self-adaptive Auto-scaling Method for Scientific Applications on HPC Environments and Clouds}},
url = {http://arxiv.org/abs/1412.6392},
year = {2014}
}
@inproceedings{Rewari2015,
annote = {NULL},
author = {Rewari, Gaurav and Kapoor, Rahul},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Rewari, Kapoor - Analytics Applications on the Cloud Business Potential , Solution Requirements , and Research Opportunities ( I.pdf:pdf},
isbn = {9781450334051},
title = {{Analytics Applications on the Cloud : Business Potential , Solution Requirements , and Research Opportunities ( Invited Talk )}},
year = {2015}
}
@inproceedings{McCool2010,
abstract = {Many-core processors target improved computational performance by making available various forms of architectural parallelism, including but not limited to multiple cores and vector instructions. However, approaches to parallel programming based on targeting these low-level parallel mechanisms directly leads to overly complex, non-portable, and often unscalable and unreliable code. A more structured approach to designing and implementing parallel algorithms is useful to reduce the complexity of developing software for such processors, and is particularly relevant for many-core processors with a large amount of parallelism and multiple parallelism mechanisms. In particular, efficient and reliable parallel programs can be designed around the composition of deterministic algorithmic skeletons, or patterns. While improving the productivity of experts, specific patterns and fused combinations of patterns can also guide relatively inexperienced users to developing efficient algorithm implementations that have good scalability. The approach to parallelism described in this document includes both collective “data-parallel” patterns such as map and reduce as well as structured “task-parallel” patterns such as pipelining and superscalar task graphs. The structured pattern based approach, like data-parallel models, addresses issues of both data access and parallel task distribution in a common framework. Optimization of data access is important for both many-core processors with shared memory systems and accelerators with their own memories not directly attached to the host processor. A catalog of useful structured serial and parallel patterns will be presented. Serial patterns are presented because structured parallel programming can be considered an extension of structured control flow in serial programming. We will emphasize deterministic patterns in order to support the development of systems that automatically avoid unsafe race conditions and deadlock.},
annote = {NULL},
author = {McCool, MD},
booktitle = {HotPar},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - McCool - Structured parallel programming with deterministic patterns.pdf:pdf},
keywords = {Deterministic parallel computing,many-core computing,patterns,software engineering,structured programming},
title = {{Structured parallel programming with deterministic patterns}},
url = {https://www.usenix.org/event/hotpar10/tech/full{\_}papers/McCool.pdf},
year = {2010}
}
@article{Kaelbling1996,
annote = {print me




Discounting future rewards: used to represent "interest" earned on rewards, so that an action that generates an immediate reward will be preferred over one that generates the same reward in the future. Only necessary in cyclical tasks.},
author = {Kaelbling, L.},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Kaelbling - Average reward reinforcement learning Foundations, algorithms, and empirical results.pdf:pdf},
journal = {Machine Learning},
keywords = {markov decision processes,reinforcement learning},
number = {1-3},
publisher = {Springer},
title = {{Average reward reinforcement learning: Foundations, algorithms, and empirical results}},
volume = {22},
year = {1996}
}
@article{Zadeh1965,
abstract = {A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (charac- teristic) function which assigns to each object a grade of member- ship ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint.},
annote = {NULL},
author = {Zadeh, L. A.},
file = {:Users/cec/Google Drive/Mendeley Library/1965 - Zadeh - Fuzzy Sets.pdf:pdf},
journal = {Information and Control},
title = {{Fuzzy Sets}},
volume = {8},
year = {1965}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
annote = {NULL},
author = {Doersch, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Doersch - Tutorial on Variational Autoencoders.pdf:pdf},
journal = {arXiv:1606.05908},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
title = {{Tutorial on Variational Autoencoders}},
year = {2016}
}
@inproceedings{Ben-nun2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1806.07336v3},
author = {Ben-nun, T. and Jakobovits, A. S. and Hoefler, T.},
booktitle = {NeurIPS},
eprint = {arXiv:1806.07336v3},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ben-nun, Jakobovits, Hoefler - Neural Code Comprehension A Learnable Representation of Code Semantics(2).pdf:pdf},
title = {{Neural Code Comprehension: A Learnable Representation of Code Semantics}},
year = {2018}
}
@article{Watters2010,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - python Curses Programming with Python.pdf:pdf},
isbn = {15558514848},
title = {{python Curses Programming with Python}},
year = {2016}
}
@inproceedings{Astorga2016,
abstract = {Since free performance lunch of processors is over, paral- lelism has become the new trend in hardware and architec- ture design. However, parallel resources deployed in data centers are underused in many cases, given that sequential programming is still deeply rooted in current software de- velopment. To face this problem, new methodologies and techniques for parallel programming have been progressively developed. For instance, parallel frameworks offer program- ming skeletons that allow expressing parallelism and concur- rency in applications to better exploit concurrent hardware. Nevertheless, it remains a large portion of production soft- ware, coming from a broad range of scientific and industrial areas, that still execute sequential legacy codes. Taking into account that these software modules contain thousands, or even millions, of code lines, the effort needed to identify par- allel regions is extremely high. To pave the way in this area, this paper presents Parallel Pattern Analyzer Tool (PPAT), a software component that aids discovering and annotating parallel patterns in source codes. Hence, facilitating the transformation of sequential code into parallel. We evalu- ate this tool for the special case of parallel pipelines using a series of well-known sequential benchmark suites. Categories},
annote = {NULL},
author = {Astorga, Rio and Dolz, Manuel F and Sanchez, Luis Miguel and Garc{\'{i}}a, J Daniel},
booktitle = {PMAM},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Astorga et al. - Discovering Pipeline Parallel Patterns in Sequential Legacy C Codes.pdf:pdf},
isbn = {9781450341967},
keywords = {Algorithmic Skeletons,C++11,Parallel Design Patterns},
title = {{Discovering Pipeline Parallel Patterns in Sequential Legacy C ++ Codes}},
year = {2016}
}
@article{Devlin1993,
annote = {NULL},
author = {Devlin, Keith and Gouvea, Fernando and Granville, Andrew},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Devlin, Gouvea, Granville - Fermat's Last Theorem, a theorem at last.pdf:pdf},
journal = {Focus},
number = {3},
title = {{Fermat's Last Theorem, a theorem at last}},
volume = {13},
year = {1993}
}
@article{Hitaj2017,
abstract = {In recent years, a branch of machine learning called Deep Learning has become incredibly popular thanks to the ability of a new class of algorithms to model and interpret a large quantity of data in a similar way to humans. Their success is due to a combination of recent algorithmic breakthroughs, in-creasingly powerful computers, and access to significant amounts of data. The future for computer science looks bright, and in particular for areas such as image/video/speech recognition, computer vision, natural language processing, security, and many others. Researchers have also considered privacy implications of deep learning. Properly training models involve collecting a vast amount of users' private data, including habits, geographical po-sitions, interests, and much more. Another major issue is that it is possible to extract from trained models useful information about the training set and this hinders collaboration among distrustful participants or parties that deal with sensitive information. To tackle this problem, collaborative deep learning models have recently been proposed where parties share only a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy to make information extraction even more challenging, as shown by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collabo-rative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversar-ial Network (GAN) that generates valid samples of the targeted training set that was meant to be private. Interestingly, we show that differential privacy applied to the shared parameters of the model as suggested at CCS'15 and CCS'16 is utterly futile. In our generative model attack, all techniques adopted to scramble or obfuscate shared parameters in collaborative deep learning are rendered ineffective with no possibility of a remedy under the threat model considered.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1702.07464},
author = {Hitaj, B. and Ateniese, G. and Perez-Cruz, F.},
eprint = {1702.07464},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Hitaj, Ateniese, Perez-Cruz - Deep Models Under the GAN Information Leakage from Collaborative Deep Learning.pdf:pdf},
journal = {arXiv:1702.07464},
title = {{Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning}},
year = {2017}
}
@article{Wang2017,
abstract = {This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks, and extends to popular recent models like variational autoencoder and generative adversarial nets. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.},
annote = {This is a really good summary. Beware of typos and grammatical errors.},
archivePrefix = {arXiv},
arxivId = {1702.07800},
author = {Wang, H. and Raj, B. and Xing, E. P.},
eprint = {1702.07800},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Wang, Raj, Xing - On the Origin of Deep Learning.pdf:pdf},
journal = {arXiv:1702.07800},
title = {{On the Origin of Deep Learning}},
year = {2017}
}
@inproceedings{Ciechanowicz2010,
abstract = {Algorithmic skeletons encapsulate typical parallel programming patterns such that they can be easily applied by users. Existing skeleton libraries usually work on distributed memory machines. We present an extension of our skeleton library Muesli which now allows to use the same application without modifications on a variety of parallel machines ranging from multi-processor distributed memory to many-core shared memory machines and combinations of those such as clusters of multi-core nodes. Internally, the skeletons are based on MPI and OpenMP. We demonstrate the efficiency of our approach by providing experimental results},
annote = {This paper extends Muesli for multi-core parallelism. It's a "yet another extension of Skeletons" paper, the claim for novelty being that existing multi-core Skeleton libraries (Skandium and SkeTo) are either exclusively data or task parallel.},
author = {Ciechanowicz, P and Kuchen, H},
booktitle = {HPCC},
doi = {10.1109/HPCC.2010.23},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Ciechanowicz, Kuchen - Enhancing Muesli's Data Parallel Skeletons for Multi-core Computer Architectures.pdf:pdf},
isbn = {978-1-4244-8335-8},
keywords = {distributed computing,distributed memory systems,message,multiprocessing,parallel programming,passing,programming environments,shared memory systems},
month = {sep},
publisher = {Ieee},
title = {{Enhancing Muesli's Data Parallel Skeletons for Multi-core Computer Architectures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5581334 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5581334},
year = {2010}
}
@inproceedings{Smith,
annote = {NULL},
author = {Smith, C.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Smith - MapReduce Program Synthesis.pdf:pdf},
isbn = {9781450342612},
keywords = {average computer user,capable of synthesizing programs,data analysis,general mapreduce paradigm,in the,our technique uses the,program,program synthesis,simple in-,synthesis technique that is,to that end,verification,we present a novel},
title = {{MapReduce Program Synthesis}},
year = {2016}
}
@inproceedings{Zhang2017a,
abstract = {A program can be viewed as a syntactic structure P (syntactic skele-ton) parameterized by a collection of the identifiers V (variable names). This paper introduces the skeletal program enumeration (SPE) problem: Given a fixed syntactic skeleton P and a set of variables V , enumerate a set of programs P exhibiting all possible variable usage patterns within P. It proposes an effective realiza-tion of SPE for systematic, rigorous compiler testing by leverag-ing three important observations: (1) Programs with different vari-able usage patterns exhibit diverse control-and data-dependence information, and help exploit different compiler optimizations and stress-test compilers; (2) most real compiler bugs were revealed by small tests (i.e., small-sized P) — this " small-scope " observation opens up SPE for practical compiler validation; and (3) SPE is ex-haustive w.r.t. a given syntactic skeleton and variable set, and thus can offer a level of guarantee that is absent from all existing com-piler testing techniques. The key challenge of SPE is how to eliminate the enormous amount of equivalent programs w.r.t. $\alpha$-conversion. Our main tech-nical contribution is a novel algorithm for computing the canoni-cal (and smallest) set of all non-$\alpha$-equivalent programs. To demon-strate its practical utility, we have realized our SPE technique and evaluated it using syntactic skeletons derived from GCC's test-suite. Our evaluation results on testing GCC and Clang are ex-tremely promising. In less than six months, our approach has led to 217 confirmed bug reports, 104 of which have already been fixed, and the majority are long latent bugs despite the extensive prior efforts of automatically testing both compilers (e.g., Csmith and EMI). The results also show that our algorithm for enumerating non-$\alpha$-equivalent programs provides six orders of magnitude re-duction, enabling processing the GCC test-suite in under a month versus 40K+ years with a na{\"{i}}ve enumeration.},
author = {Zhang, Q. and Sun, C. and Su, Z.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Zhang, Sun, Su - Skeletal Program Enumeration for Rigorous Compiler Testing.pdf:pdf},
keywords = {TOPRINT,TOREAD},
mendeley-tags = {TOPRINT,TOREAD},
title = {{Skeletal Program Enumeration for Rigorous Compiler Testing}},
year = {2017}
}
@inproceedings{Joshi,
author = {Joshi, A. M. and Eeckhout, L. and John, L. K.},
booktitle = {SPEC Benchmark Workshop},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Joshi, Eeckhout, John - The Return of Synthetic Benchmarks.pdf:pdf},
title = {{The Return of Synthetic Benchmarks}},
year = {2008}
}
@inproceedings{Spafford2010,
abstract = {As heterogeneous computing platforms become more prevalent, the programmer must account for complex memory hierarchies in addition to the difficulties of parallel programming. OpenCL is an open standard for parallel computing that helps alleviate this difficulty by providing a portable set of abstractions for device memory hierarchies. However, OpenCL requires that the programmer explicitly controls data transfer and device synchronization, two tedious and error-prone tasks. This paper introduces Maestro, an open source library for data orchestration on OpenCL devices. Maestro provides automatic data transfer, task decomposition across multiple devices, and autotuning of dynamic execution parameters for some types of problems.},
annote = {Cited by 39.},
author = {Spafford, Kyle and Meredith, Jeremy and Vetter, Jeffrey},
booktitle = {Euro-Par},
doi = {10.1007/978-3-642-15291-7_26},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Spafford, Meredith, Vetter - Maestro Data orchestration and tuning for OpenCL devices.pdf:pdf},
isbn = {3642152902},
issn = {03029743},
title = {{Maestro: Data orchestration and tuning for OpenCL devices}},
year = {2010}
}
@inproceedings{Lin2013,
abstract = {This paper proposes a new FPGA-based embedded computer architecture, which fo- cuses on how to construct an application-specific memory access network capable of extracting the maximum amount of memory-level parallelism on a per-application basis. Specifically, through perform- ing dynamic memory analysis and utilizing the ca- pabilities of modern FPGA devices: abundant dis- tributed block RAMs and programmability, the pro- posed reconfigurable architecture synthesizes highly efficient accelerators that enable parallelized mem- ory accesses, and therefore accomplish effective data orchestration by maximally extracting the target ap- plication's instruction, loop and memory-level paral- lelism. To validate our proposed architecture, we im- plemented a baseline embedded processor platform, a conventional CPU+accelerator with a centralized single memory, and a prototype based on Xilinx Mi- croBlaze technology. Our experimental results have shown that on average for 5 benchmark applica- tions from SPEC2006 and MiBench [1], our pro- posed architecture achieves 8.6 times speedup com- pared to the baseline embedded processor platform and 1.7 times speedup compared to a conventional CPU+accelerator platform. More interestingly, the proposed platform achieves more than 40{\%} reduction in energy-delay product compared to a conventional CPU+accelerator with a centralized memory.},
annote = {Cited by 0.},
author = {Lin, M. and Cheng, S. and Wawrzynek, J.},
booktitle = {ReConFig},
doi = {10.1109/ReConFig.2013.6732290},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Lin, Cheng, Wawrzynek - Extracting memory-level parallelism through reconfigurable hardware traces.pdf:pdf},
isbn = {978-1-4799-2079-2},
month = {dec},
publisher = {Ieee},
title = {{Extracting memory-level parallelism through reconfigurable hardware traces}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6732290},
year = {2013}
}
@article{System2004,
abstract = {Many opportunities exist to improve micro-architectural performance due to perfor- mance events that are difficult to optimize at static compile time. Cache misses and branch mis-prediction patterns may vary for different micro-architectures using different inputs. Dynamic optimization provides an approach to address these and other performance events at runtime. This paper describes a software system of real implementation that detects per- formance problems of running applications and deploys optimizations to increase execution efficiency. We discuss issues of detecting performance bottlenecks, generating optimized traces and redirecting execution from the original code to the dynamically optimized code. Our current system speeds up many of the CPU2000 benchmark programs having large numbers of D-Cache misses through dynamically deployed cache prefetching. For other applications that don't benefit from our runtime optimization, the average cost is only 2{\%} of execution time. We present this lightweight system as an example of using existing hardware and software to deploy speculative optimizations to improve a program's runtime performance.},
annote = {This paper presents ADORE, a runtime optimiser which uses standard profiling tools to identify hot spots {\&} phase detection.




The results seem week.},
author = {System, Optimization},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - System - Design and Implementation of a Lightweight Dynamic Optimization System.pdf:pdf},
journal = {Journal of Instruction-Level Parallelism},
number = {4},
title = {{Design and Implementation of a Lightweight Dynamic Optimization System}},
volume = {6},
year = {2004}
}
@inproceedings{Kaiser2017,
abstract = {Despite recent advances, memory-augmented deep neural networks are still lim- ited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural net- work. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can success- fully generalize from them. We match best previous results for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.},
archivePrefix = {arXiv},
arxivId = {1703.03129},
author = {Kaiser, {\L}. and Nachum, O. and Roy, A. and Bengio, S.},
booktitle = {ICLR},
eprint = {1703.03129},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Kaiser et al. - Learning to Remember Rare Events.pdf:pdf},
title = {{Learning to Remember Rare Events}},
year = {2017}
}
@inproceedings{Marlow2013a,
abstract = {In this paper we describe a qualitative investigation of impression formation in an online distributed software development community with social media functionality. We find that users in this setting seek out additional information about each other to explore the project space, inform future interactions, and understand the potential future value of a new person. They form impressions around other users' expertise based on history of activity across projects, and successful collaborations with key high status projects in the community. These impressions influence their receptivity to strangers' work contributions.},
annote = {NULL},
author = {Marlow, Jennifer and Dabbish, Laura and Herbsleb, Jim},
booktitle = {CSCW},
doi = {10.1145/2441776.2441792},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Marlow, Dabbish, Herbsleb - Impression Formation in Online Peer Production Activity Traces and Personal Profiles in GitHub.pdf:pdf},
isbn = {9781450313315},
keywords = {Activity traces,Collaborative software development,Impression formation,Peer production},
title = {{Impression Formation in Online Peer Production : Activity Traces and Personal Profiles in GitHub}},
year = {2013}
}
@article{Spector2002,
abstract = {Push is a programming language designed for the$\backslash$nexpression of evolving programs within an evolutionary$\backslash$ncomputation system. This article describes Push and$\backslash$nillustrates some of the opportunities that it presents$\backslash$nfor evolutionary computation. Two evolutionary$\backslash$ncomputation systems, PushGP and Pushpop, are described$\backslash$nin detail. PushGP is a genetic programming system that$\backslash$nevolves Push programs to solve computational problems.$\backslash$nPushpop, an ?autoconstructive evolution? system, also$\backslash$nevolves Push programs but does so while simultaneously$\backslash$nevolving its own evolutionary mechanisms.},
author = {Spector, Lee and Robinson, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Spector, Robinson - Genetic Programming and Autoconstructive Evolution with the Push Programming Language.pdf:pdf},
journal = {GPEM},
keywords = {artificial life,genetic algorithms,genetic programming,modularity,programming languages,self-adaptation},
number = {1},
title = {{Genetic Programming and Autoconstructive Evolution with the Push Programming Language}},
volume = {3},
year = {2002}
}
@article{Shieber1994,
abstract = {We report on the recent Loebner prize competition inspired by Turing's test of intelligent behavior. The presentation covers the structure of the competition and the outcome of its first instantiation in an actual event, and an analysis of the purpose, design, and appropriateness of such a competition. We argue that the competition has no clear purpose, that its design prevents any useful outcome, and that such a competition is inappropriate given the current level of technology. We then speculate as to suitable alternatives to the Loebner prize.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9404002},
author = {Shieber, S.},
doi = {10.1145/175208.175217},
eprint = {9404002},
file = {:Users/cec/Google Drive/Mendeley Library/1994 - Shieber - Lessons from a Restricted Turing Test.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {6},
primaryClass = {cmp-lg},
title = {{Lessons from a Restricted Turing Test}},
url = {http://arxiv.org/abs/cmp-lg/9404002},
volume = {37},
year = {1994}
}
@inproceedings{Baskaran2010,
abstract = {Graphics Processing Units (GPUs) offer tremendous computational power. CUDA (Compute Unified Device Architecture) provides a multi-threaded parallel programming model, facilitating high performance implementations of general-purpose computations. However, the explicitly managed memory hierar- chy and multi-level parallel viewmakemanual development of high-performance CUDA code rather complicated. Hence the automatic transformation of sequen- tial input programs into efficient parallel CUDA programs is of considerable in- terest. This paper describes an automatic code transformation system that gener- ates parallel CUDA code from input sequential C code, for regular (affine) pro- grams. Using and adapting publicly available tools that have made polyhedral compiler optimization practically effective, we develop a C-to-CUDA transfor- mation system that generates two-level parallel CUDA code that is optimized for efficient data access. The performance of automatically generated code is compared with manually optimized CUDA code for a number of benchmarks. The performance of the automatically generated CUDA code is quite close to hand-optimized CUDA code and considerably better than the benchmarks' per- formance on a multicore CPU. 1},
annote = {NULL},
author = {Baskaran, Muthu Manikandan and Ramanujam, J. and Sadayappan, P.},
booktitle = {CC},
doi = {10.1007/978-3-642-11970-5_14},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Baskaran, Ramanujam, Sadayappan - Automatic C-to-CUDA code generation for affine programs.pdf:pdf},
isbn = {3642119697},
issn = {03029743},
title = {{Automatic C-to-CUDA code generation for affine programs}},
year = {2010}
}
@inproceedings{Hall1995,
annote = {NULL},
author = {Hall, M. W. and Amarasinghe, S. and Murphy, B. R. and Liao, S. and Lam, M. S.},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Hall et al. - Detecting coarse-grain parallelism using an interprocedural parallelizing compiler.pdf:pdf},
publisher = {IEEE},
title = {{Detecting coarse-grain parallelism using an interprocedural parallelizing compiler}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1383185},
year = {1995}
}
@misc{Knuth,
annote = {A nice scathing quote:


"In a relatively new field such as computing there is bound to be a lot of trash published since there are too few people available to recognize the poor quality of much of the material."},
author = {Knuth, Donald E},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Knuth - Hints for referees.pdf:pdf},
title = {{Hints for referees}}
}
@book{Keith2012,
abstract = {Compilers are a rich area of study, drawing together the whole world of computer science in one, elegant construction. Cooper and Torczon have succeeded in creating a welcoming guide to these software systems, enhancing this new edition with clear lessons and the details you simply must get right, all the while keeping the big picture firmly in view. Engineering a Compiler is an invaluable companion for anyone new to the subject. The Second Edition of Engineering a Compiler is an excellent introduction to the construction of modern optimizing compilers. The authors draw from a wealth of experience in compiler construction in order to help students grasp the big picture while at the same time guiding them through many important but subtle details that must be addressed to construct an effec-tive optimizing compiler. In particular, this book contains the best introduction to Static Single Assignment Form that I've seen.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cooper, K. D. and Torczon, L.},
booktitle = {Engineering a Compiler},
doi = {10.1016/B978-0-12-088478-0.00005-0},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Cooper, Torczon - Engineering a Compiler Second Edition.pdf:pdf},
isbn = {9780120884780},
issn = {03043835},
pmid = {25246403},
title = {{Engineering a Compiler Second Edition}},
year = {2012}
}
@article{Mark2003,
abstract = {The latest real-time graphics architectures include programmable floating-point vertex and fragment processors, with support for data-dependent control flow in the vertex processor. We present a programming language and a supporting system that are designed for programming these stream processors. The language follows the philosophy of C, in that it is a hardware-oriented, general-purpose language, rather than an application-specific shading language. The language includes a variety of facilities designed to support the key architectural features of programmable graphics processors, and is designed to support multiple generations of graphics architectures with different levels of functionality. The system supports both of the major 3D graphics APIs: OpenGL and Direct3D. This paper identifies many of the choices that we faced as we designed the system, and explains why we made the decisions that we did.},
annote = {Cited by 710.},
author = {Mark, W. R. and Glanville, R. S. and Akeley, K. and Kilgard, M. J.},
doi = {10.1145/1201775.882362},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Mark et al. - Cg a system for programming graphics hardware in a C-like language.pdf:pdf},
isbn = {1581137095},
issn = {07300301},
journal = {TOG},
number = {3},
publisher = {ACM},
title = {{Cg: a system for programming graphics hardware in a C-like language}},
url = {http://portal.acm.org/citation.cfm?doid=1201775.882362},
volume = {22},
year = {2003}
}
@article{Michaelson2001,
abstract = {Algorithmic skeletons provide a promising basis for the automatic utilisation of parallelism at sites of higher-order function use through static program analysis. However, decisions about whether or not to realise particular higher-order function instances as skeletons must be based on information about available processing resources, and such resources may change subsequent to program analysis. In principle, nested higher-order functions may be realised as nested skeletons. However, where higher-order function arguments result from partially applied functions, free-variable bindings must be identified and communicated through the corresponding skeleton hierarchy to where those arguments are actually applied. Here, a skeleton based parallelising compiler from Standard ML to native code is presented. Hybrid skeletons, which can change from parallel to serial evaluation at run-time, are considered and mechanisms for their nesting are discussed. Compilation stages are illustrated through a simple nested higher-order function based algorithm for multiplying matrices of arbitrary length integers and performance figures for compiled code running on a Fujitsu AP3000 are discussed.},
annote = {From Duplicate 1 (Nested Algorithmic Skeletons from Higher Order Functions - Michaelson, Greg; Scaife, Norman; Bristow, Paul; King, Peter)

An initial skim indicates that this paper is too unrelated and dated to be worth a detailed read.},
author = {Michaelson, GREG and Scaife, NORMAN and BRISTOW, Paul and KING, Peter},
doi = {10.1080/01495730108935271},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Michaelson et al. - Nested Algorithmic Skeletons from Higher Order Functions.pdf:pdf},
issn = {1063-7192},
journal = {Journal of Parallel Algorithms and Applications},
keywords = {higher order functions,parallelising compilation,skeletons},
number = {3},
title = {{Nested Algorithmic Skeletons from Higher Order Functions}},
volume = {16},
year = {2001}
}
@inproceedings{Georgiou2018,
abstract = {This paper presents the interesting observation that by performing fewer of the optimizations available in a standard compiler optimization level such as -O2, while preserving their original ordering, significant savings can be achieved in both execution time and energy consumption. This observation has been validated on two embedded processors, namely the ARM Cortex-M0 and the ARM Cortex-M3, using two different versions of the LLVM compilation framework; v3.8 and v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated performance gains for at least half of the benchmarks for both processors. An average execution time reduction of 2.4{\%} and 5.3{\%} was achieved across all the benchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with execution time improvements ranging from 1{\%} up to 90{\%} over the -O2. The savings that can be achieved are in the same range as what can be achieved by the state-of-the-art compilation approaches that use iterative compilation or machine learning to select flags or to determine phase orderings that result in more efficient code. In contrast to these time consuming and expensive to apply techniques, our approach only needs to test a limited number of optimization configurations, less than 64, to obtain similar or even better savings. Furthermore, our approach can support multi-criteria optimization as it targets execution time, energy consumption and code size at the same time.},
author = {Georgiou, K. and Blackmore, C. and Xavier-de-Souza, S. and Eder, K.},
booktitle = {SCOPES},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Georgiou et al. - Less is More Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption.pdf:pdf},
title = {{Less is More: Exploiting the Standard Compiler Optimization Levels for Better Performance and Energy Consumption}},
year = {2018}
}
@inproceedings{Eeckhout2002,
abstract = {Having a representative workload of the target domain of a microprocessor is extremely important throughout its design. The composition of a workload involves two issues: (i) which benchmarks to select and (ii) which input data sets to select per benchmark. Unfortunately, it is impossible to select a huge number of benchmarks and respective input sets due to the large instruction counts per benchmark and due to limitations on the available simulation time. In this paper, we use statistical data analysis techniques such as principal components analysis (PCA) and cluster analysis to efficiently explore the workload space. Within this workload space, different input data sets for a given benchmark can be displayed, a distance can be measured between program-input pairs that gives us an idea about their mutual behavioral differences and representative input data sets can be selected for the given benchmark. This methodology is validated by showing that program-input pairs that are close to each other in this workload space indeed exhibit similar behavior. The final goal is to select a limited set of representative benchmark-input pairs that span the complete workload space. Next to workload composition, there are a number of other possible applications, namely getting insight in the impact of input data sets on program behavior and profile-guided compiler optimizations.},
annote = {NULL},
author = {Eeckhout, L. and Vandierendonck, H. and {De Bosschere}, K.},
booktitle = {PACT},
doi = {10.1109/PACT.2002.1106006},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Eeckhout, Vandierendonck, De Bosschere - Workload design Selecting representative program-input pairs.pdf:pdf},
isbn = {0769516203},
issn = {1089795X},
keywords = {Data analysis,Design optimization,Extraterrestrial measurements,Information systems,Microprocessors,Optimizing compilers,Principal component analysis,Program processors,Space exploration,Time to market},
number = {i},
publisher = {ACM},
title = {{Workload design: Selecting representative program-input pairs}},
volume = {2002-Janua},
year = {2002}
}
@misc{UniversityofEdinburgh2014g,
annote = {NULL},
author = {{University of Edinburgh}},
doi = {10.1145/365719.366426},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 5. Compiling techniques.pdf:pdf},
issn = {00010782},
title = {{5. Compiling techniques}},
volume = {9},
year = {2015}
}
@article{Kessenich2015,
annote = {NULL},
author = {Kessenich, John and Ouriel, Boaz},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kessenich, Ouriel - SPIR-V Specification (Provisional).pdf:pdf},
title = {{SPIR-V Specification (Provisional)}},
year = {2015}
}
@inproceedings{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5298v1},
author = {Kingma, D. P. and Rezende, D. J. and Mohamed, S. and Welling, M.},
booktitle = {NIPS},
eprint = {arXiv:1406.5298v1},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Kingma et al. - Semi-supervised Learning with Deep Generative Models.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Semi-supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@phdthesis{Breuer2013,
abstract = {Graphics Processing Units (GPUs) are a powerful hardware to perform compu- tationally challenging tasks. However, the programming of a GPU and, espe- cially, multiple GPUs, is an error-prone and difficult task. Multiple approaches have been made in order to simplify GPU programming, such as developing new languages or high-level programming libraries. An OpenCL-based high-level programming library for multiple GPUs is SkelCL. SkelCL provides algorithmic skeletons, which capture recurring patterns of parallel computation and commu- nication, and two abstract containers, which provide a unified abstraction to contiguous data on both GPU and (multi-core) Central Processing Unit (CPU). Stencil computations, or nearest neighbor computations, are one of the most fundamental computational patterns in numerical algorithms. In a stencil com- putation, each point of a multi-dimensional grid of data is updated as a function of its neighboring points in the grid. Generally, these updates are performed iteratively. The introduction of iterations is an innovation for SkelCL. Possible application domains of stencil computations are image processing algorithms, the solving of partial differential equations (PDE) or weather predictions. When introducing iterative stencil computations, new challenges in program- ming arise. Generally, in multi-GPU systems the work and data is split disjointly among devices. Therefore, a data transfer between devices is necessary eventually so that the data is up-to-date on each device. Currently, the SkelCL library offers a specialized skeleton for stencil compu- tations, the MapOverlap skeleton. As it offers support for single iterations it is capable of dealing with simple stencil computations. The objective of this master's thesis is to design and implement a dedicated skeleton for stencil computations in SkelCL. The developed Stencil skeleton sup- ports complex stencil computations in single- and multi-GPU systems equally well. The evaluation shows the competitive behavior in both environments. This thesis is structured as follows. The fundamentals including stencil com- putations, OpenCL, SkelCL, and the MapOverlap skeleton, are presented in the first chapter. The second chapter describes the requirements for a Stencil skeleton in SkelCL. Choices regarding the design of the Stencil skeleton are discussed in this chapter. The third chapter deals with the implementation of the Stencil skeleton for single-GPU systems including the user interface and the internal implementation. The chapter concludes with an evaluation of the Stencil skeleton for single GPUs regarding the user comfort and performance. Chapter four describes the extensions to SkelCL and the Stencil skeleton in order to efficiently execute the Stencil skeleton on multiple GPUs. An evaluation regarding the performance, especially the scalability, concludes this chapter. The final chapter concludes this master's thesis with a presentation of related work and a discussion about possible further enhancements.},
annote = {NULL},
author = {Breuer, Stefan},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Breuer - Introducing a Skeleton for Stencil Computations to the SkelCL Library.pdf:pdf},
school = {Westf{\"{a}}lische Wilhelms-Universit{\"{a}}t M{\"{u}}nster},
title = {{Introducing a Skeleton for Stencil Computations to the SkelCL Library}},
year = {2013}
}
@inproceedings{Cummins2015a,
abstract = {Selecting an appropriate workgroup size is critical for the performance of OpenCL kernels, and requires knowledge of the underlying hardware, the data being operated on, and the implementation of the kernel. This makes portable performance of OpenCL programs a challenging goal, since simple heuristics and statically chosen values fail to exploit the available performance. To address this, we propose the use of machine learning-enabled autotuning to automatically predict workgroup sizes for stencil patterns on CPUs and multi-GPUs. We present three methodologies for predicting workgroup sizes. The first, using classifiers to select the optimal workgroup size. The second and third proposed methodologies employ the novel use of regressors for performing classification by predicting the runtime of kernels and the relative performance of different workgroup sizes, respectively. We evaluate the effectiveness of each technique in an empirical study of 429 combinations of architecture, kernel, and dataset, comparing an average of 629 different workgroup sizes for each. We find that autotuning provides a median 3.79x speedup over the best possible fixed workgroup size, achieving 94{\%} of the maximum performance.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1511.02490},
author = {Cummins, C. and Petoumenos, P. and Steuwer, M. and Leather, H.},
booktitle = {ADAPT},
eprint = {1511.02490},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Cummins et al. - Autotuning OpenCL Workgroup Size for Stencil Patterns.pdf:pdf},
title = {{Autotuning OpenCL Workgroup Size for Stencil Patterns}},
year = {2016}
}
@misc{Silver2015d,
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 2 Markov Decision Processes.pdf:pdf},
title = {{Lecture 2 : Markov Decision Processes}},
url = {http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching{\_}files/MDP.pdf},
year = {2015}
}
@misc{Bundy2014b,
abstract = {To collect experimental evidence to support a hypothesis, we need to design and implement an experiment, and then analyse the results. We may need multiple passes to ensure that our experiment collects evidence capable of addressing our hypothesis. We may need to conduct exploratory data analysis in order to understand why our initial experiments are not meeting our needs and to help us adjust our experimental design accordingly. In these notes, we address these issues using an example experiment as a vehicle. The example and the analysis are based on a half-day tutorial by Paul Cohen, to whom I am grateful for his original slides and his helpful clarification of several issues.},
annote = {A fantastic write-up of empirical methods using a real-world example. It covers t-tests, stastical certainty, and independent and dependent variables.},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bundy - Empirical methods.pdf:pdf},
title = {{Empirical methods}},
url = {http://www.inf.ed.ac.uk/teaching/courses/irm/notes/empirical.html},
year = {2014}
}
@inproceedings{Chen2017,
author = {Chen, J. and Bai, Y. and Hao, D. and Xiong, Y. and Zhang, H. and Xie, B.},
booktitle = {ICSE},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Chen et al. - Learning to Prioritize Test Programs for Compiler Testing.pdf:pdf},
title = {{Learning to Prioritize Test Programs for Compiler Testing}},
year = {2017}
}
@article{Kohavi1997,
abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.},
annote = {NULL},
author = {Kohavi, R. and John, G. H.},
doi = {10.1016/S0004-3702(97)00043-X},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Kohavi, John - Wrappers for feature subset selection.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Classification,Feature selection,Filter,TOREAD,Wrapper},
mendeley-tags = {TOREAD},
number = {1-2},
pmid = {356583},
title = {{Wrappers for feature subset selection}},
url = {http://www.sciencedirect.com/science/article/pii/S000437029700043X},
volume = {97},
year = {1997}
}
@article{Giryes2016,
author = {Vidal, R. and Bruna, J. and Giryes, R. and Soatto, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Vidal et al. - Mathematics of Deep Learning.pdf:pdf},
journal = {arXiv:1712.04741},
title = {{Mathematics of Deep Learning}},
year = {2017}
}
@inproceedings{Covington2016,
abstract = {YouTube represents one of the largest scale and most sophis-ticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and fo-cus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a sepa-rate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintain-ing a massive recommendation system with enormous user-facing impact.},
annote = {NULL},
author = {Covington, P. and Adams, J. and Sargin, E.},
booktitle = {RecSys},
doi = {10.1145/2959100.2959190},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Covington, Adams, Sargin - Deep Neural Networks for YouTube Recommendations.pdf:pdf},
isbn = {9781450340359},
keywords = {deep learning,recommender system,scalability},
title = {{Deep Neural Networks for YouTube Recommendations}},
url = {http://dl.acm.org/citation.cfm?doid=2959100.2959190},
year = {2016}
}
@inproceedings{Kulkarni2012,
abstract = {Today's compilers have a plethora of optimizations to choose from, and the correct choice of optimizations can have a significant impact on the performance of the code being optimized. Furthermore, choosing the correct order in which to apply those optimizations has been a long standing problem in compilation research. Each of these optimizations interacts with the code and in turn with all other optimizations in complicated ways. Traditional compilers typically apply the same set of optimization in a fixed order to all functions in a program, without regard the code being optimized. Understanding the interactions of optimizations is very important in determining a good solution to the phase-ordering problem. This paper develops a new approach that automatically selects good optimization orderings on a per method basis within a dynamic compiler. Our approach formulates the phase-ordering problem as a Markov process and uses a characterization of the current state of the code being optimized to creating a better solution to the phase ordering problem. Our technique uses neuro-evolution to construct an artificial neural network that is capable of predicting beneficial optimization ordering for a piece of code that is being optimized. We implemented our technique in Jikes RVM and achieved significant improvements on a set of standard Java benchmarks over a well-engineered fixed order.},
annote = {NULL},
author = {Kulkarni, S. and Cavazos, J.},
booktitle = {OOPSLA},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kulkarni, Cavazos - Mitigating the Compiler Optimization Phase-Ordering Problem using Machine Learning.pdf:pdf},
keywords = {chine learning,code,compiler optimization,java,jikes rvm,ma-,neural networks,phase ordering},
publisher = {ACM},
title = {{Mitigating the Compiler Optimization Phase-Ordering Problem using Machine Learning}},
year = {2012}
}
@misc{Arapinis2014b,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 14. Algorithms.pdf:pdf},
title = {{14. Algorithms}},
year = {2015}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v1},
author = {Andor, D. and Alberti, C. and Weiss, D. and Severyn, A. and Presta, A. and Ganchev, K. and Petrov, S. and Collins, M.},
eprint = {arXiv:1603.06042v1},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Andor et al. - Globally Normalized Transition-Based Neural Networks.pdf:pdf},
keywords = {()},
title = {{Globally Normalized Transition-Based Neural Networks}},
year = {2016}
}
@article{Wang2013a,
abstract = {The past decade has seen a growing research interest in using large-scale graphs to analyze complex data sets from social networks, simulations, bioinformatics, and other applications. As the size of these data sets increases as we move into the petascale and beyond, we see a need for a more efficient method for large- scale graph analysis. Modern graphics processors (GPUs) are high-performance, highly parallel, fully programmable architectures and could be a good fit for this task. Initial research efforts in this area are promising, although widespread use has not yet arrived. However, there are several challenges in graph processing, including dependencies between vertices in the graph, irregular memory accesses during graph processing, and scalability to larger data sets and clusters.},
author = {Wang, Y. and Owens, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Wang, Owens - Large-Scale Graph Processing Algorithms on the GPU.pdf:pdf},
title = {{Large-Scale Graph Processing Algorithms on the GPU}},
year = {2013}
}
@article{Kober2017,
abstract = {In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remarkably well.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1702.06696},
author = {Kober, T. and Weeds, J. and Wilkie, J. and Reffin, J. and Weir, D.},
eprint = {1702.06696},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Kober et al. - One Representation per Word - Does it make Sense for Composition.pdf:pdf},
journal = {arXiv:1702.06696},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{One Representation per Word - Does it make Sense for Composition?}},
url = {http://arxiv.org/abs/1702.06696},
year = {2017}
}
@inproceedings{Ranzato2007a,
abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
annote = {NULL},
author = {Ranzato, M. A. and Poultney, C. and Chopra, S. and Lecun, Y.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Ranzato et al. - Efficient Learning of Sparse Representations with an Energy-Based Model.pdf:pdf},
isbn = {9780262195683},
issn = {10495258},
title = {{Efficient Learning of Sparse Representations with an Energy-Based Model}},
volume = {19},
year = {2007}
}
@inproceedings{Alglave2015,
abstract = {Concurrency is pervasive and perplexing, particularly on graphics processing units (GPUs). Current specifications of languages and hardware are inconclusive; thus programmers often rely on folklore assumptions when writing software. To remedy this state of affairs, we conducted a large empirical study of the concurrent behaviour of deployed GPUs. Armed with litmus tests (i.e. short concurrent programs), we questioned the assumptions in programming guides and vendor documentation about the guarantees provided by hardware. We developed a tool to generate thousands of litmus tests and run them under stressful workloads. We observed a litany of previously elusive weak behaviours, and exposed folklore beliefs about GPU programming---often supported by official tutorials---as false. As a way forward, we propose a model of Nvidia GPU hardware, which correctly models every behaviour witnessed in our experiments. The model is a variant of SPARC Relaxed Memory Order (RMO), structured following the GPU concurrency hierarchy.},
author = {Alglave, J. and Batty, M. and Donaldson, A. and Gopalakrishnan, G. and Ketema, J. and Poetzl, D. and Sorensen, T. and Wickerson, J.},
booktitle = {ASPLOS},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Alglave et al. - GPU Concurrency Weak Behaviours and Programming Assumptions.pdf:pdf},
keywords = {GPU,Nvidia PTX,formal model,litmus testing,memory consistency,openCL,test generation},
title = {{GPU Concurrency: Weak Behaviours and Programming Assumptions}},
year = {2015}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Ioannidis - Why most published research findings are false.pdf:pdf},
isbn = {3540239081},
issn = {15491277},
journal = {PLOS Medicine},
number = {8},
pmid = {16060722},
primaryClass = {gr-qc},
title = {{Why most published research findings are false}},
volume = {2},
year = {2005}
}
@inproceedings{Tian,
annote = {NULL},
author = {Tian, Y. and Greathouse, J. L. and Beckmann, B. M. and Jim{\'{e}}nez, D. A.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Tian et al. - Adaptive GPU Cache Bypassing Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334075},
keywords = {bypassing,graphics processing unit cache,prediction},
title = {{Adaptive GPU Cache Bypassing Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Coons2008,
abstract = {Communication overheads are one of the fundamental chal- lenges in a multiprocessor system. As the number of proces- sors on a chip increases, communication overheads and the distribution of computation and data become increasingly important performance factors. Explicit Dataflow Graph Execution (EDGE) processors, in which instructions com- municate with one another directly on a distributed sub- strate, give the compiler control over communication over- heads at a fine granularity. Prior work shows that compilers can effectively reduce fine-grained communication overheads in EDGE architectures using a spatial instruction placement algorithm with a heuristic-based cost function. While this algorithm is effective, the cost function must be painstak- ingly tuned. Heuristics tuned to perform well across a va- riety of applications leave users with little ability to tune performance-critical applications, yet we find that the best placement heuristics vary significantly with the application. First, we suggest a systematic feature selection method that reduces the feature set size based on the extent to which features affect performance. To automatically dis- cover placement heuristics, we then use these features as input to a reinforcement learning technique, called Neuro- Evolution of Augmenting Topologies (NEAT), that uses a genetic algorithm to evolve neural networks. We show that NEAToutperforms simulated annealing, themost commonly used optimization technique for instruction placement. We use NEAT to learn general heuristics that are as effective as hand-tuned heuristics, but we find that improving over highly hand-tuned general heuristics is difficult. We then suggest a hierarchical approach to machine learning that classifies segments of code with similar characteristics and learns heuristics for these classes. This approach performs closer to the specialized heuristics. Together, these results suggest that learning compiler heuristics may benefit from both improved feature selection and classification.},
address = {New York, New York, USA},
annote = {Neuro-Evolution of Augmenting Topologies (NEAT) is used to optimise instruction placement in compilers for Explicit Dataflow Graph Execution (EDGE) processors.},
author = {Coons, K. E. and Robatmili, B. and Taylor, M. E. and Maher, B. A. and Burger, D. and McKinley, K. S.},
booktitle = {PACT},
doi = {10.1145/1454115.1454122},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Coons et al. - Feature selection and policy optimization for distributed instruction placement using reinforcement learning.pdf:pdf},
isbn = {9781605582825},
keywords = {compiler heuristics,genetic algorithms,instruction scheduling,machine learning,neural networks},
publisher = {ACM},
title = {{Feature selection and policy optimization for distributed instruction placement using reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1454115.1454122},
year = {2008}
}
@article{Liu1973,
abstract = {The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed ser- vice. It is shown that an optimum fixed priority scheduler possesses an upper bound to proces- sor utihzation which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadhnes. A combination of these two scheduling techmques is also discussed.},
annote = {NULL},
author = {Liu, C. L. and Layland, J. W.},
file = {:Users/cec/Google Drive/Mendeley Library/1973 - Liu, Layland - Scheduling algorithms for multiprogramming in a hard-real-time environment.pdf:pdf},
journal = {JACM},
keywords = {deadline driven scheduling,dynamic scheduling,muttiprogram scheduling,priority assignment,processor utdlzation,real-time multiprogramming,scheduling},
number = {1},
title = {{Scheduling algorithms for multiprogramming in a hard-real-time environment}},
volume = {20},
year = {1973}
}
@article{Shasha1988,
abstract = {In this paper we consider an optimization problem that arises in the execution of parallel programs on shared-memory multiple-instruction-stream, multiple-data-stream (MIMD) computers. A program on such machines consists of many sequential program segments, each executed by a single processor. These segments interact as they access shared variables. Access to memory is asynchronous, and memory accesses are not necessarily executed in the order they were issued. An execution is correct if it is sequentially consistent: It should seem as if all the instructions were executed sequentially, in an order obtained by interleaving the instruction streams of the processors. Sequential consistency can be enforced by delaying each access to shared memory until the previous access of the same processor has terminated. For performance reasons, however, we want to allow several accesses by the same processor to proceed concurrently. Our analysis finds a minimal set of delays that enforces sequential consistency. The analysis extends to interprocessor synchronization constraints and to code where blocks of operations have to execute atomically. We use a conflict graph similar to that used to schedule transactions in distributed databases. Our graph incorporates the order on operations given by the program text, enabling us to do without locks even when database conflict graphs would suggest that locks are necessary. Our work has implications for the design of multiprocessors; it offers new compiler optimization techniques for parallel languages that support shared variables.},
annote = {NULL},
author = {Shasha, Dennis and Snir, Marc},
doi = {10.1145/42190.42277},
file = {:Users/cec/Google Drive/Mendeley Library/1988 - Shasha, Snir - Efficient and Correct Execution of Parallel Programs that Share Memory.pdf:pdf},
issn = {01640925},
journal = {TOPLAS},
keywords = {and phrases,networks},
number = {2},
title = {{Efficient and Correct Execution of Parallel Programs that Share Memory}},
volume = {10},
year = {1988}
}
@inproceedings{Fursin2005,
abstract = {This article aims at making iterative optimization practical and usable by speeding up the evaluation of a large range of optimizations. Instead of us- ing a full run to evaluate a single program optimization, we take advantage of periods of stable performance, called phases. For that purpose, we propose a low- overhead phase detection scheme geared toward fast optimization space pruning, using code instrumentation and versioning implemented in a production compiler. Our approach is driven by simplicity and practicality. We show that a simple phase detection scheme can be sufficient for optimization space pruning.We also show it is possible to search for complex optimizations at run-time without re- sorting to sophisticated dynamic compilation frameworks. Beyond iterative opti- mization, our approach also enables one to quickly design self-tuned applications. Considering 5 representative SpecFP2000 benchmarks, our approach speeds up iterative search for the best program optimizations by a factor of 32 to 962. Phase prediction is 99.4{\%} accurate on average, with an overhead of only 2.6{\%}. The resulting self-tuned implementations bring an average speed-up of 1.4.},
annote = {This paper describes a massive reduction in the time required to perform an iterative search of the optimisation space by taking advantage of performance stability in programs. Instead of executing an entire benchmark with a single set of optimisations, multiple versions are compiled and switched between during run-time phases, which are detected automatically based on instrumentation added by hand. This is a pretty solid paper, overall. The paper starts strongly but starts to lose itself around the 3rd section. They clearly hand-picked a set of benchmarks which would provide the best results, and so it's hard to say whether the results are repeatable, or applicable to other problems.},
author = {Fursin, G. and Cohen, A. and O'Boyle, M. and Temam, O.},
booktitle = {HiPEAC},
doi = {10.1007/11587514_4},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Fursin et al. - A Practical Method for Quickly Evaluating Program Optimizations.pdf:pdf},
publisher = {Springer Berlin Heidelberg},
title = {{A Practical Method for Quickly Evaluating Program Optimizations}},
url = {http://dx.doi.org/10.1007/11587514{\_}4},
volume = {3793},
year = {2005}
}
@inproceedings{Prabhu2003,
annote = {A study of thread-level speculation (TLS) for manual pa XXXXX Cited by 104.},
author = {Prabhu, Manohar K. and Olukotun, Kunle},
booktitle = {PPoPP},
doi = {10.1145/966049.781500},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Prabhu, Olukotun - Using thread-level speculation to simplify manual parallelization.pdf:pdf},
isbn = {1581135882},
issn = {03621340},
keywords = {chip multiprocessor,data speculation,feedback-driven optimization,manual parallel,multithreading,programming},
month = {oct},
number = {10},
title = {{Using thread-level speculation to simplify manual parallelization}},
url = {http://portal.acm.org/citation.cfm?doid=966049.781500},
volume = {38},
year = {2003}
}
@inproceedings{Maddison,
abstract = {We study the problem of building generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary con-tribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih {\&} Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive log likelihoods on held-out data.},
author = {Maddison, C. J. and Tarlow, D.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Maddison, Tarlow - Structured Generative Models of Natural Source Code.pdf:pdf},
title = {{Structured Generative Models of Natural Source Code}},
year = {2014}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, A. and Wayne, G. and Danihelka, I.},
eprint = {arXiv:1410.5401v2},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Graves, Wayne, Danihelka - Neural Turing Machines.pdf:pdf},
journal = {arXiv:1410.5401},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Steuwer2013a,
abstract = {Application development for modern high-performance sys- tems with Graphics Processing Units (GPUs) currently relies on low-level programming approaches like CUDA and OpenCL, which leads to com- plex, lengthy and error-prone programs. In this paper, we present SkelCL – a high-level programming ap- proach for systems with multiple GPUs and its implementation as a library on top of OpenCL. SkelCL provides three main enhancements to the OpenCL standard: 1) computations are conveniently expressed using parallel algorithmic patterns (skeletons); 2) memory management is simplified using parallel container data types (vectors and matrices); 3) an automatic data (re)distribution mechanism allows for implicit data movements between GPUs and ensures scalability when using multiple GPUs. We demonstrate how SkelCL is used to implement parallel ap- plications on one- and two-dimensional data. We report experimental results to evaluate our approach in terms of programming effort and performance.},
annote = {This paper describes SkelCL. The 3 main advantages of SkelCL:
















1) Abstraction through skeletons
2) Parallel container data types (vectors {\&} matrices)
3) Auotomatic data redistribution
















There is a large crossover between this paper and the first SkelCL publication [1].
















[1] M. Steuwer, P. Kegel, and S. Gorlatch, “SkelCL - A Portable Skeleton Library for High-Level GPU Programming,” in Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on, 2011, pp. 1176–1182.},
author = {Steuwer, Michel and Gorlatch, Sergei},
doi = {10.1007/978-3-642-39958-9_24},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Steuwer, Gorlatch - SkelCL Enhancing OpenCL for High-Level Programming of Multi-GPU Systems.pdf:pdf},
journal = {Parallel Computing Technologies},
publisher = {Springer Berlin Heidelberg},
title = {{SkelCL: Enhancing OpenCL for High-Level Programming of Multi-GPU Systems}},
url = {http://dx.doi.org/10.1007/978-3-642-39958-9{\_}24},
volume = {7979},
year = {2013}
}
@inproceedings{Han2017,
author = {Han, T. D. and Abdelrahman, T. S.},
booktitle = {IPDPSW},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Han, Abdelrahman - Use of Synthetic Benchmarks for Machine- Learning-based Performance Auto-tuning.pdf:pdf},
publisher = {IEEE},
title = {{Use of Synthetic Benchmarks for Machine- Learning-based Performance Auto-tuning}},
year = {2017}
}
@article{Li2018a,
author = {Ziwei, Z. and Cui, P. and Zhu, W.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ziwei, Cui, Zhu - Deep Learning on Graphs A Survey.pdf:pdf},
journal = {arXiv:1812.04202},
title = {{Deep Learning on Graphs: A Survey}},
year = {2018}
}
@inproceedings{Groce2017,
author = {Groce, A. and Holmes, J. and Kellar, K.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Groce, Holmes, Kellar - One Test to Rule Them All.pdf:pdf},
publisher = {ACM},
title = {{One Test to Rule Them All}},
year = {2017}
}
@article{Jaderberg2016a,
abstract = {Training directed neural networks typically requires forwardpropagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feedforward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass amounting to independent networks which colearn such that they can be composed into a single functioning corporation.},
annote = {Using a neural network to predict training errors on neural network layer.},
author = {Jaderberg, M. and Czarnecki, W. M. and Osindero, S. and Vinyals, O. and Graves, A. and Kavukcuoglu, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Jaderberg et al. - Decoupled Neural Interfaces using Synthetic Gradients.pdf:pdf},
journal = {arXiv:1608.05343},
keywords = {toread},
mendeley-tags = {toread},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
year = {2016}
}
@inproceedings{Ojika2018,
author = {Ojika, D. and Gordon-ross, A. and Lam, H. and Patel, B. and Kaul, G. and Strayer, J.},
booktitle = {BPOE},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ojika et al. - Using FPGAs as Microservices Technology, Challenges and Case Study.pdf:pdf},
keywords = {and composability across an,cloud computing,compression,container,deployment,fpga,fpga-as-a-service,maintainability,significant resource management challenge,virtualization,which includes},
title = {{Using FPGAs as Microservices: Technology, Challenges and Case Study}},
year = {2018}
}
@inproceedings{Ioffe2015,
annote = {NULL},
author = {Ioffe, Robert and Sharma, Sonal and Stoner, Michael},
booktitle = {IWOCL},
doi = {10.1145/2791321.2791324},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ioffe, Sharma, Stoner - Achieving Performance with OpenCL 2.0 on Intel{\textregistered} Processor Graphics.pdf:pdf},
isbn = {9781450334846},
title = {{Achieving Performance with OpenCL 2.0 on Intel{\textregistered} Processor Graphics}},
year = {2015}
}
@article{Steuwer2015,
abstract = {Computing systems have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computa- tional power at the cost of increased programming effort. This re- sults in a tension between achieving performance and code porta- bility. Code is either tuned using device-specific optimizations to achieve maximum performance or is written in a high-level lan- guage to achieve portability at the expense of performance. We propose a novel approach that offers high-level program- ming, code portability and high-performance. It is based on algo- rithmic pattern composition coupled with a powerful, yet simple, set of rewrite rules. This enables systematic transformation and op- timization of a high-level program into a low-level hardware spe- cific representation which leads to high performance code. We test our design in practice by describing a subset of the OpenCL programming model with low-level patterns and by im- plementing a compiler which generates high performance OpenCL code. Our experiments show that we can systematically derive high-performance device-specific implementations from simple high-level algorithmic expressions. The performance of the gen- erated OpenCL code is on par with highly tuned implementations for multicore CPUs and GPUs written by experts.},
annote = {A system which generates code for programs using high-level patterns (e.g. map, zip, reduce), by translating into highlow-level expressions (i.e. OpenCL patterns), and then to OpenCL programs using hardware paradigms. The translation is performed using a set of rewrite rules and an explorative search which tests the performance of trial translations. Seems very similar to the Denali, the super-optimizer, which used re-write rules to translate high-level programs into instructions.},
author = {Steuwer, M. and Fensch, C. and Dubach, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Steuwer, Fensch, Dubach - Patterns and Rewrite Rules for Systematic Code Generation From High-Level Functional Patterns to High-P.pdf:pdf},
journal = {arXiv:1502.02389},
keywords = {algorithmic patterns,code generation,gpu,opencl,performance,portability,rewrite rules},
title = {{Patterns and Rewrite Rules for Systematic Code Generation From High-Level Functional Patterns to High-Performance OpenCL Code}},
year = {2015}
}
@inproceedings{Ryoo2008,
abstract = {Program optimization for highly-parallel systems has his- torically been considered an art, with experts doing much of the performance tuning by hand. With the introduction of inexpensive, single-chip, massively parallel platforms, more developers will be creating highly-parallel applications for these platforms, who lack the substantial experience and knowledge needed to maximize their performance. This cre- ates a need for more structured optimization methods with means to estimate their performance effects. Furthermore these methods need to be understandable by most program- mers. This paper shows the complexity involved in opti- mizing applications for one such system and one relatively simple methodology for reducing the workload involved in the optimization process. This work is based on one such highly-parallel system, the GeForce 8800 GTX using CUDA. Its flexible allocation of resources to threads allows it to extract performance from a range of applications with varying resource requirements, but places new demands on developers who seek to maxi- mize an application's performance. We show how optimiza- tions interact with the architecture in complex ways, initially prompting an inspection of the entire configuration space to find the optimal configuration. Even for a seemingly sim- ple application such as matrix multiplication, the optimal configuration can be unexpected. We then present metrics derived from static code that capture the first-order factors of performance. We demonstrate how these metrics can be used to prune many optimization configurations, down to those that lie on a Pareto-optimal curve. This reduces the optimization space by as much as 98{\%} and still finds the optimal configuration for each of the studied applications.},
annote = {This paper lists some of the goals of optimizing for GPUs, as well as some metrics for calculating performance estimates. Good paper. Cited by 231.},
author = {Ryoo, S. and Rodrigues, C. I. and Stone, S. S. and Baghsorkhi, S. S. and Ueng, S. and Stratton, J. A. and Hwu, W. W.},
booktitle = {CGO},
doi = {10.1145/1356058.1356084},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Ryoo et al. - Program optimization space pruning for a multithreaded GPU.pdf:pdf},
isbn = {9781595939784},
keywords = {gpgpu,optimization,parallel computing},
publisher = {IEEE},
title = {{Program optimization space pruning for a multithreaded GPU}},
year = {2008}
}
@inproceedings{Hammond2013,
abstract = {This paper describes the ParaPhrase project, a new 3-year targeted research project funded under EU Framework 7 Objective 3.4 (Computer Systems), starting in October 2011. ParaPhrase aims to follow a new approach to introducing parallelism using advanced refac- toring techniques coupled with high-level parallel design patterns. The refactoring approach will use these design patterns to restructure pro- grams defined as networks of software components into other forms that are more suited to parallel execution. The programmer will be aided by high-level cost information that will be integrated into the refactor- ing tools. The implementation of these patterns will then use a well- understood algorithmic skeleton approach to achieve good parallelism. A key ParaPhrase design goal is that parallel components are intended to match heterogeneous architectures, defined in terms of CPU/GPU combinations, for example. In order to achieve this, the ParaPhrase approach will map components at link time to the available hardware, and will then re-map them during program execution, taking account of multiple applications, changes in hardware resource availability, the desire to reduce communication costs etc. In this way, we aim to develop a new approach to programming that will be able to produce software that can adapt to dynamic changes in the system environment. More- over, by using a strong component basis for parallelism, we can achieve potentially significant gains in terms of reducing sharing at a high level of abstraction, and so in reducing or even eliminating the costs that are usually associated with cache management, locking, and synchronisation.},
annote = {NULL},
author = {Hammond, K. and Aldinucci, M. and Brown, C.},
booktitle = {FMCO},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Hammond, Aldinucci, Brown - The paraphrase project Parallel patterns for adaptive heterogeneous multicore systems.pdf:pdf},
publisher = {Springer},
title = {{The paraphrase project: Parallel patterns for adaptive heterogeneous multicore systems}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35887-6{\_}12},
year = {2013}
}
@article{Collins2012,
abstract = {Parallel skeletons are a structured parallel programming abstraction that provide programmers with a predefined set of algorithmic templates that can be combined, nested and parameterized with sequential code to produce complex programs. The implementation of these skeletons is currently a manual process, requiring human expertise to choose suitable implementation parameters that provide good performance. This paper presents an empirical exploration of the optimization space of the FastFlow parallel skeleton framework. We performed this using a Monte Carlo search of a random subset of the space, for a representative set of platforms and programs. The results show that the space is program and platform dependent, non-linear, and that automatic search achieves a significant average speedup in program execution time of 1.6× over a human expert. An exploratory data analysis of the results shows a linear dependence between two of the parameters, and that another two parameters have little effect on performance. These properties are then used to reduce the size of the space by a factor of 6, reducing the cost of the search. This provides a starting point for automatically optimizing parallel skeleton programs without the need for human expertise, and with a large improvement in execution time compared to that achievable using human expert tuning.},
annote = {Using a Monte Carlo search to explore the optimisation space of FastFlow on multiple architectures. Using PCA to reduce dimensionality of search space.},
author = {Collins, A. and Fensch, C. and Leather, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Collins, Fensch, Leather - Auto-Tuning Parallel Skeletons.pdf:pdf},
journal = {Parallel Processing Letters},
keywords = {fastflow,multicore,optimization space exploration,parallel skeletons},
month = {jun},
number = {02},
title = {{Auto-Tuning Parallel Skeletons}},
volume = {22},
year = {2012}
}
@inproceedings{Park2011,
abstract = {Iterative compilation techniques, which involve iterating over different sets of optimizations, have proven useful in helping compilers choose the right set of optimizations for a given program. However, compilers typically have a large number of optimizations to choose from, making it impossible to iterate over a significant fraction of the entire optimization search space. Recent research has proposed to iterate over the optimization search space using predictive methods. In particular, state-the-art methods in iterative compilation use characteristics of the code being optimized to predict good optimization sequences to evaluate. Thus, an important step in developing predictive methods for compilation is deciding how to model the problem of choosing the right optimizations. In this paper, we evaluate three different ways of modeling the problem of choosing the right optimization sequences using machine learning techniques. We evaluate a novel prediction modeling technique, namely a tournament predictor, that is able to effectively predict good optimization sequences. We show that our tournament predictor can outperform current state-of-the-art predictors and the most aggressive setting of the Open64 compiler (-Ofast) on an average by 75{\%} in just 10 iterations over a set of embedded and scientific kernels. Moreover, using our tournament predictor, we achieved on average 10{\%} improvement over -Ofast for a set of MiBench applications.},
annote = {NULL},
author = {Park, E. and Kulkarni, S. and Cavazos, J.},
booktitle = {CASES},
doi = {10.1145/2038698.2038711},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Park, Kulkarni, Cavazos - An evaluation of different modeling techniques for iterative compilation.pdf:pdf},
isbn = {9781450307130},
keywords = {compiler optimization,iterative compilation,machine learn-},
title = {{An evaluation of different modeling techniques for iterative compilation}},
year = {2011}
}
@incollection{Yearning-draftb,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 33-35).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 33-35)}},
year = {2018}
}
@techreport{Allamanis2016b,
annote = {NULL},
author = {Allamanis, M. and Barr, E. T. and Bird, C. and Marron, M. and Sutton, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Allamanis et al. - Mining Semantic Loop Idioms from Big Code.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Mining Semantic Loop Idioms from Big Code}},
year = {2016}
}
@article{Narasimhan2015,
abstract = {In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.},
archivePrefix = {arXiv},
arxivId = {1506.08941},
author = {Narasimhan, K. and Kulkarni, T. D. and Barzilay, R.},
doi = {10.18653/v1/D15-1001},
eprint = {1506.08941},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Narasimhan, Kulkarni, Barzilay - Language Understanding for Text-based Games Using Deep Reinforcement Learning.pdf:pdf},
isbn = {9781941643327},
journal = {arXiv:1506.08941},
title = {{Language Understanding for Text-based Games Using Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1506.08941},
year = {2015}
}
@article{Purdom1972,
author = {Purdom, P.},
journal = {BIT Numerical Mathematics},
number = {3},
title = {{A Sentence Generator for Testing Parsers}},
volume = {12},
year = {1972}
}
@inproceedings{Park2012,
abstract = {Using machine learning has proven effective at choosing the right set of optimizations for a particular program. For machine learning techniques to be most effective, compiler writers have to develop expressive means of characterizing the program being optimized. The current state-of-the-art techniques for characterizing programs include using a fixed-length feature vector of either source code features extracted during compile time or performance counters collected when running the program. For the problem of identifying optimizations to apply, models constructed using performance counter characterizations of a program have been shown to outperform models constructed using source code features. However, collecting performance counters requires running the program multiple times, and this "dynamic" method of characterizing programs can be specific to inputs of the program. It would be preferable to have a method of characterizing programs that is as expressive as performance counter features, but that is "static" like source code features and therefore does not require running the program. In this paper, we introduce a novel way of characterizing programs using a graph-based characterization, which uses the program's intermediate representation and an adapted learning algorithm to predict good optimization sequences. To evaluate different characterization techniques, we focus on loop-intensive programs and construct prediction models that drive polyhedral optimizations, such as auto-parallelism and various loop transformation. We show that our graph-based characterization technique outperforms three current state-of-the-art characterization techniques found in the literature. By using the sequences predicted to be the best by our graph-based model, we achieved up to 73{\%} of the speedup achievable in our search space for a particular platform, whereas we could only achieve up to 59{\%} by other state-of-the-art techniques we evaluated.},
annote = {NULL},
author = {Park, E. and Cavazos, J. and Alvarez, M. A.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Park, Cavazos, Alvarez - Using Graph-Based Program Characterization for Predictive Modeling.pdf:pdf},
publisher = {IEEE},
title = {{Using Graph-Based Program Characterization for Predictive Modeling}},
year = {2012}
}
@misc{Silver2015f,
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 3 Planning by Dynamic Programming Outline.pdf:pdf},
title = {{Lecture 3 : Planning by Dynamic Programming Outline}},
year = {2015}
}
@techreport{Huselius2002,
annote = {NULL},
author = {Huselius, Joel},
booktitle = {MRTC Report no 63},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Huselius - Debugging parallel systems A state of the art report.pdf:pdf},
title = {{Debugging parallel systems: A state of the art report}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Debugging+Parallel+Systems+:+A+State+of+the+Art+Report{\#}0},
year = {2002}
}
@inproceedings{Aldinucci2014,
abstract = {Get an overview of FastFlow's parallel patterns can be used to design parallel applications for execution on both CPUs and GPGPUs while avoiding most of the complex low-level detail needed to make them efficient, portable and rapid to prototype. For a more detailed and technical review of FastFlow's parallel patterns as well as a use case where we will show the design and effectiveness of a novel universal image filtering template based on the variational approach.},
annote = {NULL},
author = {Aldinucci, M. and Torquati, M. and Drocco, M. and {Peretti Pezzi}, G. and Spampinato, C.},
booktitle = {GTC},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Aldinucci et al. - An Overview of FastFlow Combining Pattern-Level Abstraction and Efficiency in GPGPUs.pdf:pdf},
keywords = {fastflow,gpu},
title = {{An Overview of FastFlow: Combining Pattern-Level Abstraction and Efficiency in GPGPUs}},
url = {http://calvados.di.unipi.it/storage/talks/2014{\_}S4585-Marco-Aldinucci.pdf},
year = {2014}
}
@article{Poggio2017,
abstract = {A main puzzle of deep networks revolves around the absence of overfitting despite overparametrization and despite the large capacity demonstrated by zero training error on randomly labeled data. In this note, we show that the dynamical systems associated with gradient descent minimization of nonlinear networks behave near zero stable minima of the empirical error as gradient system in a quadratic potential with degenerate Hessian. The proposition is supported by theoretical and numerical results, under the assumption of stable minima of the gradient. Our proposition provides the extension to deep networks of key properties of gradient descent methods for linear networks, that as, suggested in (1), can be the key to understand generalization. Gradient descent enforces a form of implicit regularization controlled by the number of iterations, and asymptotically converging to the minimum norm solution. This implies that there is usually an optimum early stopping that avoids overfitting of the loss (this is relevant mainly for regression). For classification, the asymptotic convergence to the minimum norm solution implies convergence to the maximum margin solution which guarantees good classification error for "low noise" datasets. The implied robustness to overparametrization has suggestive implications for the robustness of deep hierarchically local networks to variations of the architecture with respect to the curse of dimensionality.},
author = {Poggio, T. and Kawaguchi, K. and Liao, Q. and Miranda, B. and Rosasco, L. and Boix, X. and Hidary, J. and Mhaskar, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Poggio et al. - Theory of Deep Learning III explaining the non-overfitting puzzle.pdf:pdf},
journal = {arXiv:1801.00173},
title = {{Theory of Deep Learning III: explaining the non-overfitting puzzle}},
year = {2018}
}
@article{Breuer2014,
abstract = {The paper proposes a high-productivity framework formulti- GPU computation of mesh-based applications. In order to achieve high performance on these applications, we have to introduce complicated optimized techniques for GPU com- puting, which requires relatively-high cost of implementa- tion. Our framework automatically translates user-written functions that update a grid point and generates both GPU and CPU code. In order to execute user code on multiple GPUs, the framework parallelizes this code by using MPI and OpenMP. The framework also provides C++ classes to write GPU-GPU communication effectively. The pro- grammers write user code just in the C++ language and can develop program code optimized for GPU supercomput- ers without introducing complicated optimizations for GPU computation and GPU-GPU communication. As an exper- iment evaluation, we have implemented multi-GPU compu- tation of a diffusion equation by using this framework and achieved good weak scaling results. By using peer-to-peer access between GPUs in this framework, the framework- based diffusion computation using two NVIDIA Tesla K20X GPUs is 1.4 times faster than manual implementation code. We also show computational results of the Rayleigh-Taylor instability obtained by 3D compressible flow computation written by this framework.},
annote = {NULL},
author = {Steuwer, Michel and Haidl, Michael and Breuer, Stefan and Gorlatch, Sergei and Steuwer, Michel and Gorlatch, Sergei},
doi = {10.1142/S0129626414410059},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Steuwer et al. - High-level programming of stencil computations on multi-GPU systems using the SkelCL library.pdf:pdf},
isbn = {0129626414},
issn = {0129-6264},
journal = {HiStencils},
keywords = {gpu,manycores,opencl,skelcl,skeletons,stencils},
number = {03},
title = {{High-level programming of stencil computations on multi-GPU systems using the SkelCL library}},
volume = {24},
year = {2014}
}
@inproceedings{Brook,
annote = {NULL},
author = {Brook, Stony and Brook, Stony},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Brook, Brook - Cache-Oblivious Wavefront Improving Parallelism of Recursive Dynamic Programming Algorithms without Losing Cache-.pdf:pdf},
isbn = {9781450332057},
keywords = {cache-oblivious,cache-oblivious parallel algorithm,dynamic programming,multi-core,nested parallel com-,wavefront},
title = {{Cache-Oblivious Wavefront : Improving Parallelism of Recursive Dynamic Programming Algorithms without Losing Cache-Efficiency}},
year = {2015}
}
@article{Herrmann2000,
abstract = {We propose the higher-order functional style for the parallel programming of al- gorithms. The functional language HVC, a subset of the language Haskell, facilitates the clean integration of skeletons into a functional program. Skeletons are predefined programming schemata with an efficient parallel implementation. We report on our com- piler, which translates {\%}VC programs into C+MPI, especially on the design decisions we made. Two small examples, the n queens problem and Karatsuba's polynomial mul- tiplication, are presented to demonstrate the programming comfort and the speedup one can obtain.},
annote = {NULL},
author = {Herrmann, Christoph A and Lengauer, Christian},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Herrmann, Lengauer - HDC a higher-order language for divide-and-conquer.pdf:pdf},
journal = {Parallel Processing Letters},
keywords = {drvide-and-conquer,functional program,haskell,parallelization,skeleton},
title = {{HDC: a higher-order language for divide-and-conquer}},
volume = {10},
year = {2000}
}
@misc{Tamrawi2012a,
abstract = {Build process is crucial in software development. However, the analysis$\backslash$nsupport for build code is still limited. In this paper, we present$\backslash$nSYMake, an infrastructure and tool for the analysis of build code in$\backslash$nmake. Due to the dynamic nature of make language, it is challenging to$\backslash$nunderstand and maintain complex Makefiles. SYMake provides a symbolic$\backslash$nevaluation algorithm that processes Makefiles and produces a symbolic$\backslash$ndependency graph (SDG), which represents the build dependencies (i.e.$\backslash$nrules) among files via commands. During the symbolic evaluation, for$\backslash$neach resulting string value in an SDG that represents a part of a file$\backslash$nname or a command in a rule, SYMake provides also an acyclic graph$\backslash$n(called T-model) to represent its symbolic evaluation trace. We have$\backslash$nused SYMake to develop algorithms and a tool 1) to detect several types$\backslash$nof code smells and errors in Makefiles, and 2) to support build code$\backslash$nrefactoring, e. g. renaming a variable/target even if its name is$\backslash$nfragmented and built from multiple substrings. Our empirical evaluation$\backslash$nfor SYMake's renaming on several real-world systems showed its high$\backslash$naccuracy in entity renaming. Our controlled experiment showed that with$\backslash$nSYMake, developers were able to understand Makefiles better and to$\backslash$ndetect more code smells as well as to perform refactoring more$\backslash$naccurately.},
annote = {NULL},
author = {Tamrawi, Ahmed and Nguyen, Hoan Anh and Nguyen, Hung Viet and Nguyen, Tien N.},
booktitle = {ICSE},
doi = {10.1109/ICSE.2012.6227152},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Tamrawi et al. - Build code analysis with symbolic evaluation.pdf:pdf},
isbn = {9781467310673},
issn = {02705257},
keywords = {build code analysis,build code maintenance},
title = {{Build code analysis with symbolic evaluation}},
year = {2012}
}
@article{Hsu2015,
abstract = {This paper presents a neural network-based end-to-end clustering framework. We design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data, in addition to learning a feature embedding suitable for such clustering. The network is trained with weak labels, specifically partial pairwise relationships between data instances. The cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data. The framework has the interesting characteristic that no cluster centers need to be explicitly specified, thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined. The experiments show that the proposed approach beats the conventional two-stage method (feature embedding with k-means) by a significant margin. It also compares favorably to the performance of the standard cross entropy loss for classification. Robustness analysis also shows that the method is largely insensitive to the number of clusters. Specifically, we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering.},
author = {Hsu, Y. and Kira, Z.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Hsu, Kira - Neural Network-based Clustering Using Pairwise Constraints.pdf:pdf},
journal = {arXiv:1511.06321},
title = {{Neural Network-based Clustering Using Pairwise Constraints}},
year = {2016}
}
@inproceedings{Jablin2014,
annote = {NULL},
author = {Jablin, James A and Jablin, Thomas B and Herlihy, Maurice},
booktitle = {PACT},
doi = {10.1145/2628071.2628101},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Jablin, Jablin, Herlihy - Warp-Aware Trace Scheduling for GPUs.pdf:pdf},
isbn = {9781450328098},
issn = {1089795X},
keywords = {gpus,p-aware trace scheduling for},
publisher = {ACM},
title = {{Warp-Aware Trace Scheduling for GPUs}},
year = {2014}
}
@article{Scaife2005,
abstract = {Algorithmic skeletons are abstractions from common patterns of parallel activity which offer a high degree of reusability for developers of parallel algorithms. Their close association with higher order functions (HOFs) makes functional languages, with their strong transformational properties, excellent vehicles for skeleton-based parallel program development. However, using HOFs in this way raises substantial problems of identification of useful HOFs within a given application and of resource allocation on target architectures. We present the design and implementation of a parallelising compiler for Standard ML which exploits parallelism in the familiar map and fold HOFs through skeletons for processor farms and processor trees, respectively. The compiler extracts parallelism automatically and is target architecture independant. HOF execution within a functional language can be nested in the sense that one HOF may be passed and evaluated during the execution of another HOF. We are able to exploit this by nesting our parallel skeletons in a processor topology which matches the structure of the Standard ML source. However, where HOF arguments result from partially applied functions, free variable bindings must be identified and communicated through the corresponding skeleton hierarchy to where those arguments are actually applied. We describe the analysis leading from input Standard ML through HOF instantiation and backend compilation to an executable parallel program. We also present an overview of the runtime system and the execution model. Finally, we give parallel performance figures for several example programs, of varying computational loads, on the Linux-based Beowulf, IBM SP/2, Fujitsu AP3000 and Sun StarCat 15000 MIMD parallel machines. These demonstrate good cross-platform consistency of parallel code behaviour.},
annote = {A compiler which automatically extracts parallelism using the map and fold algorithmic skeletons, based on Standard ML.},
author = {Scaife, N. and Horiguchi, S. and Michaelson, G. and Bristow, P.},
doi = {10.1017/S0956796804005489},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Scaife et al. - A parallel SML compiler based on algorithmic skeletons.pdf:pdf},
isbn = {0956-7968},
issn = {0956-7968},
journal = {Journal of Functional Programming},
number = {4},
title = {{A parallel SML compiler based on algorithmic skeletons}},
volume = {15},
year = {2005}
}
@inproceedings{Qawasmeh,
annote = {NULL},
author = {Qawasmeh, Ahmad and Chapman, Barbara and Hugues, Maxime and Calandra, Henri},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Qawasmeh et al. - GPU Technology Applied to Reverse Time Migration and Seismic Modeling via OpenACC Categories and Subject Descri.pdf:pdf},
isbn = {9781450334044},
keywords = {gpu,openacc,reverse time migration,seismic imaging},
title = {{GPU Technology Applied to Reverse Time Migration and Seismic Modeling via OpenACC Categories and Subject Descriptors}},
year = {2015}
}
@phdthesis{Nugteren2014,
annote = {NULL},
author = {Nugteren, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Nugteren - Improving the Programmability of GPU Architectures.pdf:pdf},
title = {{Improving the Programmability of GPU Architectures}},
url = {http://alexandria.tue.nl/extra2/771987.pdf},
year = {2014}
}
@article{Dietterich1995,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9905014v1},
author = {Dietterich, T. G.},
eprint = {9905014v1},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Dietterich - Hierarchical reinforcement learning with the MAXQ value function decomposition.pdf:pdf},
primaryClass = {arXiv:cs},
title = {{Hierarchical reinforcement learning with the MAXQ value function decomposition}},
year = {1999}
}
@inproceedings{Fursin2008,
abstract = {Tuning hardwired compiler optimizations for rapidly evolving hardware makes porting an optimizing com- piler for each new platform extremely challenging. Our radical approach is to develop a modular, extensible, self-optimizing compiler that automatically learns the best optimization heuristics based on the behavior of the platform. In this paper we describeMILEPOST1 GCC, a machine-learning-based compiler that automatically adjusts its optimization heuristics to improve the exe- cution time, code size, or compilation time of specific programs on different architectures. Our preliminary experimental results show that it is possible to consider- ably reduce execution time of the MiBench benchmark suite on a range of platforms entirely automatically.},
annote = {MILEPOST GCC is an extension of GCC which uses machine learning to automically set optimisation heuristics by predicting pass runs based on a number of program features.},
author = {Fursin, G. and Miranda, C. and Temam, O. and Yom-tov, E. and Bonilla, E. and Thomson, J. and Leather, H. and Williams, C. and O'Boyle, M.},
booktitle = {GCC Developers' Summit},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Fursin et al. - MILEPOST GCC machine learning based research compiler.pdf:pdf},
title = {{MILEPOST GCC: machine learning based research compiler}},
year = {2008}
}
@article{Chen2018,
abstract = {This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a system, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 commits, carefully curated from open-source repositories. We evaluate it on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4711 testing samples. It captures a wide range of repair operators without any domain-specific top-down design.},
archivePrefix = {arXiv},
arxivId = {1901.01808},
author = {Chen, Z. and Kommrusch, S. and Tufano, M. and Pouchet, L. and Poshyvanyk, D. and Monperrus, M.},
eprint = {1901.01808},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Chen et al. - SequenceR Sequence-to-Sequence Learning for End-to-End Program Repair.pdf:pdf},
journal = {arXiv:1901.01808},
title = {{SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair}},
url = {http://arxiv.org/abs/1901.01808},
year = {2018}
}
@inproceedings{Ogilvie2015,
abstract = {Building effective optimization heuristics is a challenging task which often takes developers several months if not years to complete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data. However, obtaining this data can take months per platform. This is becoming an ever more critical problem and if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines. In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which significantly reduces this training overhead. Typically in supervised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry. This wastes effort on parts of the space that contribute little to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples. We demonstrate this technique by automatically constructing a model to determine on which device to execute four parallel programs at differing problem dimensions for a representative CPU--GPU based heterogeneous system. Our methodology is remarkably simple and yet effective, making it a strong candidate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state-of-the-art.},
annote = {NULL},
author = {Ogilvie, W. F. and Petoumenos, P. and Wang, Z. and Leather, H.},
booktitle = {CPC},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ogilvie et al. - Intelligent Heuristic Construction with Active Learning.pdf:pdf},
title = {{Intelligent Heuristic Construction with Active Learning}},
year = {2015}
}
@inproceedings{Nandi2017,
author = {Nandi, C. and Grossman, D. and Sampson, A. and Mytkowicz, T. and McKinley, K. S.},
booktitle = {MAPL},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Nandi et al. - Debugging Probabilistic Programs.pdf:pdf},
keywords = {debugging,probabilistic programming,program analy-,sis,statistical inference},
title = {{Debugging Probabilistic Programs}},
year = {2017}
}
@article{Battaglia2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1806.01261v3},
author = {Battaglia, P. and Hamrick, J. and Bapst, V. and Sanchez-Gonzalez, A. and Zambaldi, V. and Malinowski, M. and Tacchetii, A. and Raposo, D. and Santoro, A. and Faulkner, R. and Gulcehre, C. and Song, F. and Ballard, A. and Gilmer, J. and Dahl, G. and Vaswani, A. and Allen, K. and Nash, C. and Langston, V. and Dyer, C. and Heess, N. and Wierstra, D. and Kohli, P. and Botvinick, M. and Vinyals, O. and Li, Y. and Pascanu, R.},
eprint = {arXiv:1806.01261v3},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Battaglia et al. - Relational inductive biases, deep learning, and graph networks.pdf:pdf},
journal = {arXiv:1806.01261},
title = {{Relational inductive biases, deep learning, and graph networks}},
year = {2018}
}
@article{John2004,
annote = {NULL},
author = {John, Lizy Kurian},
doi = {10.1145/991124.991126},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - John - More on finding a single number to indicate overall performance of a benchmark suite.pdf:pdf},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
number = {1},
title = {{More on finding a single number to indicate overall performance of a benchmark suite}},
url = {http://dl.acm.org/citation.cfm?id=991124.991126},
volume = {32},
year = {2004}
}
@book{Lindley,
annote = {NULL},
author = {Lindley, Sam and Mcbride, Conor and Trinder, Phil and Sannella, Don and Hutchison, David},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Lindley et al. - A List of Successes That Can Change the World Essays Dedicated to Philip Wadler on the Occasion of His 60th Birt.pdf:pdf},
isbn = {9783319309354},
title = {{A List of Successes That Can Change the World: Essays Dedicated to Philip Wadler on the Occasion of His 60th Birthday}},
year = {2016}
}
@article{Ward2008,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Installing Python Modules.pdf:pdf},
title = {{Installing Python Modules}},
year = {2016}
}
@inproceedings{Heilig,
annote = {NULL},
author = {Heilig, Brian and Gao, Guang R},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Heilig, Gao - Design and Evaluation of a Novel DataFlow based BigData Solution.pdf:pdf},
isbn = {9781450334044},
keywords = {bigdata,dataflow,fine-grain,in-memory processing},
title = {{Design and Evaluation of a Novel DataFlow based BigData Solution}},
year = {2015}
}
@article{Conway1970,
annote = {NULL},
author = {Conway, J.},
journal = {Scientific American},
number = {4},
title = {{The Game of Life}},
volume = {223},
year = {1970}
}
@inproceedings{Georges2007,
abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer- based method sampling, thread scheduling, garbage collection, and various system effects. There exist a wide variety of Java performance evaluation methodologies used by researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance ob- served; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider mul- tiple VM invocations and iterate a single benchmark execution; yet others consider multipleVMinvocations and iterate the benchmark multiple times. This paper shows that prevalent methodologies can be mis- leading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this pa- per, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate ap- proaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
address = {New York, NY, USA},
annote = {This paper describes a methodology for performing rigurous performance evaluation, focused on Java programs.








This is a great paper to reference in your methodology section. Of particular interest is the discussion on calculating confidence intervals, using a normal distribution when n {\textgreater}= 30, else a t-distribution. This is the basis of srtime [1].








[1] https://github.com/ChrisCummins/srtime},
author = {Georges, A. and Buytaert, D. and Eeckhout, L.},
booktitle = {OOPSLA},
doi = {10.1145/1297027.1297033},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Georges, Buytaert, Eeckhout - Statistically Rigorous Java Performance Evaluation.pdf:pdf},
isbn = {9781595937865},
issn = {03621340},
keywords = {benchmarking,data analysis,java,methodolgy,statistics},
month = {oct},
publisher = {ACM},
title = {{Statistically Rigorous Java Performance Evaluation}},
year = {2007}
}
@inproceedings{Zhang2013,
abstract = {We study the performance portability of OpenCL across diverse architectures including NVIDIA GPU, Intel Ivy Bridge CPU, and AMD Fusion APU. We present detailed performance analysis at assem- bly level on three exemplar OpenCL benchmarks: SGEMM, SpMV, and FFT. We also identify a number of tuning knobs that are critical to performance portability, including threads-data mapping, data layout, tiling size, data caching, and operation-specific factors. We further demonstrate that proper tuning could improve the OpenCL portable performance from the current 15{\%} to a potential 67{\%} of the state-of-the-art performance on the Ivy Bridge CPU. Finally, we evaluate the current OpenCL programming model, and propose a list of extensions that improve performance portability.},
annote = {Cited by 12.},
author = {Zhang, Yao and {Sinclair II}, Mark and Chien, Andrew A},
booktitle = {SC},
doi = {10.1007/978-3-642-38750-0_11},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Zhang, Sinclair II, Chien - Improving Performance Portability in OpenCL Programs.pdf:pdf},
isbn = {978-3-642-38750-0},
publisher = {Springer},
title = {{Improving Performance Portability in OpenCL Programs}},
volume = {7905},
year = {2013}
}
@inproceedings{Nagai2013,
author = {Nagai, E. and Hashimoto, A. and Ishiura, N.},
booktitle = {SASIMI},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Nagai, Hashimoto, Ishiura - Scaling up Size and Number of Expressions in Random Testing of Arithmetic Optimization of C Compilers.pdf:pdf},
title = {{Scaling up Size and Number of Expressions in Random Testing of Arithmetic Optimization of C Compilers}},
year = {2013}
}
@inproceedings{Kulkarni2013,
annote = {NULL},
author = {Kulkarni, S. and Cavazos, J. and Wimmer, C. and Simon, D.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Kulkarni et al. - Automatic Construction of Inlining Heuristics using Machine Learning.pdf:pdf},
isbn = {9781467355254},
title = {{Automatic Construction of Inlining Heuristics using Machine Learning}},
year = {2013}
}
@article{Touati2013a,
abstract = {In the area of high performance computing and embedded systems, numerous code optimisation methods exist to accelerate the speed of the computation (or optimise another performance criteria). They are usually experimented by doing multiple observations of the initial and the optimised execution times of a program in order to declare a speedup. Even with fixed input and execution environment, program execution times vary in general. Hence different kinds of speedups may be reported: the speedup of the average execution time, the speedup of the minimal execution time, the speedup of the median, etc. Many published speedups in the literature are observations of a set of experiments. In order to improve the reproducibility of the experimental results, this article presents a rigorous statistical methodology regarding program performance analysis. We rely on well known statistical tests (Shapiro-wilk's test, Fisher's F-test, Student's t-test, Kolmogorov-Smirnov's test, Wilcoxon-Mann-Whitney's test) to study if the observed speedups are statistically significant or not. By fixing 0Y]{\textgreater}1/2, the probability that an individual execution of the optimised code is faster than the individual execution of the initial code. In addition, we can compute the confidence interval of the probability to get a speedup on a randomly selected benchmark that does not belong to the initial set of tested benchmarks. Our methodology defines a consistent improvement compared to the usual performance analysis method in high performance computing. We explain in each situation what are the hypothesis that must be checked to declare a correct risk level for the statistics. The Speedup-Test protocol certifying the observed speedups with rigorous statistics is implemented and distributed as an open source tool based on R software.},
annote = {NULL},
author = {Touati, Sid Ahmed Ali and Worms, Julien and Briais, S{\'{e}}bastien},
doi = {10.1002/cpe.2939},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Touati, Worms, Briais - The Speedup-Test A statistical methodology for programme speedup analysis and computation.pdf:pdf},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {code optimisation,programme performance evaluation and analysis,statistics},
number = {10},
title = {{The Speedup-Test: A statistical methodology for programme speedup analysis and computation}},
volume = {25},
year = {2013}
}
@article{Baker2016,
abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modelling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with an {\$}\backslashepsilon{\$}-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing network design meta-modelling approaches on image classification.},
annote = {NULL},
author = {Baker, B. and Gupta, O. and Naik, N. and Raskar, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Baker et al. - Designing Neural Network Architectures using Reinforcement Learning.pdf:pdf},
isbn = {2857825749},
journal = {arXiv:1611.02167v2},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Designing Neural Network Architectures using Reinforcement Learning}},
year = {2016}
}
@article{Kraska2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.01208v2},
author = {Kraska, T. and Beutel, A. and Chi, E. H. and Dean, J. and Polyzotis, N.},
eprint = {arXiv:1712.01208v2},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Kraska et al. - The Case for Learned Index Structures.pdf:pdf},
journal = {arXiv:1712.01208},
title = {{The Case for Learned Index Structures}},
year = {2017}
}
@article{Goes2012,
annote = {NULL},
author = {Goes, L. F. W. and Ioannou, N. and Xekalakis, P. and Cole, M. and Cintra, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Goes et al. - Autotuning Skeleton-Driven Optimizations for Transactional Worklist Applications.pdf:pdf},
journal = {TPDS},
title = {{Autotuning Skeleton-Driven Optimizations for Transactional Worklist Applications}},
year = {2012}
}
@inproceedings{Grillo2013,
annote = {NULL},
author = {Grillo, Lucas and {De Sande}, Francisco and Fumero, Juan J. and Reyes, Ruyman},
booktitle = {3PGCIC},
doi = {10.1109/3PGCIC.2013.106},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Grillo et al. - Programming for GPUs The Directive-Based Approach.pdf:pdf},
isbn = {978-0-7695-5094-7},
number = {November 2011},
title = {{Programming for GPUs: The Directive-Based Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6681300},
year = {2013}
}
@misc{Oliva2010,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 01. Course Overview and Policies.pdf:pdf},
isbn = {9780132834872},
title = {{01. Course Overview and Policies}},
url = {https://scholar.vt.edu/access/content/group/eea644f3-57fe-4a69-8a24-76f51f59555e/Fall2010/Admin/Course Overview and Policies-1},
year = {2015}
}
@inproceedings{Tournavitis2009,
abstract = {Integrating Profile-Driven Parallelism Detection and Research centre identities Machine-Learning Based Mapping},
annote = {NULL},
author = {Tournavitis, G. and Wang, Z. and Franke, B. and O'Boyle, M.},
booktitle = {MPSoCs},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Tournavitis et al. - Towards a holistic approach to auto-parallelization integrating profile-driven parallelism detection and mac.pdf:pdf},
publisher = {ACM},
title = {{Towards a holistic approach to auto-parallelization: integrating profile-driven parallelism detection and machine-learning based mapping}},
url = {http://dl.acm.org/citation.cfm?id=1542496},
year = {2009}
}
@inproceedings{Collins2013,
abstract = {Parallel skeletons provide a predefined set of parallel templates that can be combined, nested and parameterized with sequential code to produce complex parallel programs. The implementation of each skeleton includes parameters that have a significant effect on performance; so carefully tuning them is vital. The optimization space formed by these parameters is complex, non-linear, exhibits multiple local optima and is program dependent. This makes manual tuning impractical. Ef- fective automatic tuning is therefore essential for the performance of parallel skeleton programs. In this paper we present MaSiF, a novel tool to auto-tune the parallelization parameters of skeleton parallel programs. It reduces the size of the parameter space using a combination of machine learning, via nearest neighbor classification, and linear dimensionality reduction using Principal Components Analysis. To auto-tune a new program, a set of program features is determined statically and used to compute k nearest neighbors from a set of training programs. Previously collected performance data for the nearest neighbors is used to reduce the size of the search space using Principal Components Analysis. Good parallelization parameters are found quickly by searching this smaller search space.We evaluate MaSiF for two existing parallel frameworks: Threading Building Blocks and FastFlow. MaSiF achieves 89{\%} of the performance of the oracle on average. This exploration requires just 45 parameters values on average, which is {\~{}}0.05{\%} of the optimization space. In contrast, a state-of-the- art machine learning approach achieves 51{\%}. MaSiF achieves an average speedup of 1.32× over parallelization parameters chosen by human experts.},
annote = {MaSiF is a tool that selects optimal paramaters for a skeleton program, by using machine learning to compare its feature vector against k-nearest neighbours of a training dataset. PCA is used to reduce the parameter optimisation space for Thread Building Blocks and FastFlow, and the performance is compared against a human expert, oracle, and best competing method. The paper is well written and contains a susbstial experimental results section. The assumption of statically unknowable properties is a bit of an oversight. Could be improved using dynamic autotuning?},
author = {Collins, A. and Fensch, C. and Leather, H. and Cole, M.},
booktitle = {HiPC},
doi = {10.1109/HiPC.2013.6799098},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Collins et al. - MaSiF Machine Learning Guided Auto-tuning of Parallel Skeletons.pdf:pdf},
isbn = {978-1-4799-0730-4},
publisher = {IEEE},
title = {{MaSiF: Machine Learning Guided Auto-tuning of Parallel Skeletons}},
year = {2013}
}
@article{Fachada,
abstract = {OpenCL is an open standard for parallel programming of heteroge-neous compute devices, such as GPUs, CPUs, DSPs or FPGAs. However, the verbosity of its C host API can hinder application development. In this paper we present cf4ocl, a software library for rapid development of OpenCL programs in pure C. It aims to reduce the verbosity of the OpenCL API, offering straightforward memory management, integrated profiling of events (e.g., kernel execution and data transfers), simple but extensible device selection mechanism and user-friendly error manage-ment. We compare two versions of a conceptual application example, one based on cf4ocl, the other developed directly with the OpenCL host API. Results show that the former is simpler to implement and offers more features, at the cost of an effectively negligible computational overhead. Additionally, the tools provided with cf4ocl allowed for a quick analysis on how to optimize the application.},
annote = {NULL},
author = {Fachada, Nuno and Lopes, Vitor V and Martins, Rui C and Rosa, Agostinho C},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Fachada et al. - cf4ocl a C framework for OpenCL.pdf:pdf},
keywords = {C,GPGPU,High-performance computing,OpenCL,Profilin},
title = {{cf4ocl: a C framework for OpenCL}}
}
@inproceedings{Runciman2014,
annote = {NULL},
author = {Trilla, Jose Manuel Calderon and Runciman, Colin},
booktitle = {IFL},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Trilla, Runciman - An Iterative Compiler for Implicit Parallelism.pdf:pdf},
keywords = {Automatic parallelism,Feedback Directed Compilation,Implicit Parallelism,Iterative Compilation,Lazy Functional Languages,Projections,Strictness Analysis},
title = {{An Iterative Compiler for Implicit Parallelism}},
year = {2014}
}
@article{Jimborean2014,
abstract = {We propose a framework based on an original generation and use of algo- rithmic skeletons, and dedicated to speculative parallelization of scientific nested loop kernels, able to apply at run-time polyhedral transformations to the target code in order to exhibit parallelism and data locality. Parallel code generation is achieved almost at no cost by using binary algorithmic skeletons that are generated at compile-time, and that embed the original code and operations devoted to instantiate a polyhedral parallelizing transformation and to verify the speculations on dependences. The skele- tons are patched at run-time to generate the executable code. The run-time process includes a transformation selection guided by online profiling phases on short samples, using an instrumented version of the code. During this phase, the accessed memory addresses are used to compute on-the-fly dependence distance vectors, and are also interpolated to build a predictor of the forthcoming accesses. Interpolating functions and distance vectors are then employed for dependence analysis to select a paral- lelizing transformation that, if the prediction is correct, does not induce any rollback during execution. In order to ensure that the code is executed in successive slices of the outermost original loop of the nest. Each slice can be either a parallel version which instantiates a skeleton, a sequential original version, or an instrumented version. Moreover, such slicing of the execution provides the opportunity of transforming differently the code to adapt to the observed execution phases, by patching differently one of the pre-built skeletons. The framework has been implemented with extensions of the LLVM compiler and an x86-64 runtime system. Significant speed-ups are shown on a set of benchmarks that could not have been handled efficiently by a compiler.},
annote = {An inspector passes over a small fragment of the loop iterations and computes dependence vectors, which are used to predict the loop dependences across all iterations. This prediction is used to generate parallel code using algorithmic skeletons which is executed speculatively. Loops are sliced to minimise rollback effort, and to perform mid-way tuning. Cited by 6.},
author = {Jimborean, A. and Clauss, P. and Loechner, J. D. V. and Manuel, J. and Caama{\~{n}}o, M.},
doi = {10.1007/s10766-013-0259-4},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Jimborean et al. - Dynamic and Speculative Polyhedral Parallelization Using Compiler-Generated Skeletons.pdf:pdf},
journal = {IJPP},
keywords = {Algorithmic skeletons,Automatic parallelization,Compilation,Dynamic parallelization,Loop nests,Polytope model,Speculative parallelization},
number = {4},
publisher = {Springer},
title = {{Dynamic and Speculative Polyhedral Parallelization Using Compiler-Generated Skeletons}},
volume = {42},
year = {2014}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, J. and Gulcehre, C. and Cho, K. and Bengio, Y.},
eprint = {1412.3555v1},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Chung et al. - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
journal = {arXiv:1412.3555},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
year = {2014}
}
@misc{Spolsky2003,
annote = {NULL},
author = {Spolsky, Joel},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Spolsky - The absolute minimum every software developer absolutely, positively must know about unicode and character sets (no exc.pdf:pdf},
title = {{The absolute minimum every software developer absolutely, positively must know about unicode and character sets (no excuses!)}},
url = {http://intra.iam.hva.nl/content/1112/verdieping2/internetstandaarden{\_}2/intro-en-materiaal/06-The-Absolute-Minimum-Every-Software-Developer-Absolutely-Positively-Must-Know-About-Unicode-and-Character-Sets-(No-Excuses)-Joel-on-Software.pdf},
year = {2003}
}
@misc{UniversityofEdinburgh2014c,
annote = {NULL},
author = {{University of Edinburgh}},
booktitle = {Parallel Architectures},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - 5. Parallel Architectures.pdf:pdf},
title = {{5. Parallel Architectures}},
year = {2014}
}
@techreport{Goodman,
annote = {NULL},
author = {Goodman, James R.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Goodman - Cache Consistency and Sequential Consistency.pdf:pdf},
institution = {University of Wisconsin-Madison},
title = {{Cache Consistency and Sequential Consistency}}
}
@misc{IntelTBB,
annote = {NULL},
author = {{Intel Corporation}},
title = {{Intel Thread Building Blocks}},
url = {https://www.threadingbuildingblocks.org/}
}
@book{Wolfe1996,
annote = {NULL},
author = {Wolfe, M. J.},
publisher = {Addison-Wesley},
title = {{High performance compilers for parallel computing}},
year = {1996}
}
@inproceedings{Wang2015,
annote = {NULL},
author = {Wang, Chang and Jiang, Jiang and Zhu, Yongxing and Liu, Xu and Han, Xing},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Wang et al. - CRA A Dynamic Task Allocation Algorithm for Many-core Processor.pdf:pdf},
keywords = {contiguous application allocation,many-core processor,network},
title = {{CRA: A Dynamic Task Allocation Algorithm for Many-core Processor}},
year = {2015}
}
@article{Grelck2005,
abstract = {Classical application domains of parallel computing are dominated by processing large arrays of numerical data. Whereas most functional languages focus on lists and trees rather than on arrays, SaC is tailor-made in design and in implementation for efficient high-level array processing. Advanced compiler optimizations yield performance levels that are often competitive with low-level imperative implementations. Based on SaC, we develop compilation techniques and runtime system support for the compiler-directed parallel execution of high-level functional array processing code on shared memory architectures. Competitive sequential performance gives us the opportu- nity to exploit the conceptual advantages of the functional paradigm for achieving real performance gains with respect to existing imperative implementations, not only in com- parison with uniprocessor runtimes. While the design of SaC facilitates parallelization, the particular challenge of high sequential performance is that realization of satisfying speedups through parallelization becomes substantially more difficult. We present an initial compilation scheme andmulti-threaded execution model, which we step-wise refine to reduce organizational overhead and to improve parallel performance. We close with a detailed analysis of the impact of certain design decisions on runtime performance, based on a series of experiments.},
annote = {NULL},
author = {Grelck, Clemens},
doi = {10.1017/S0956796805005538},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Grelck - Shared memory multiprocessor support for functional array processing in SAC.pdf:pdf},
issn = {0956-7968},
journal = {Journal of Functional Programming},
month = {jun},
number = {3},
title = {{Shared memory multiprocessor support for functional array processing in SAC}},
url = {http://www.journals.cambridge.org/abstract{\_}S0956796805005538},
volume = {15},
year = {2005}
}
@article{Blum1997,
annote = {NULL},
author = {Blum, Avrim},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Blum - On-Line Algorithms in Machine Learning.pdf:pdf},
title = {{On-Line Algorithms in Machine Learning}},
year = {1997}
}
@inproceedings{Neelakantan2016,
abstract = {Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.},
author = {Neelakantan, A. and Le, Q. V. and Sutskever, I.},
booktitle = {ICLR},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Neelakantan, Le, Sutskever - Neural Programmer Inducing Latent Programs with Gradient Descent.pdf:pdf},
title = {{Neural Programmer: Inducing Latent Programs with Gradient Descent}},
year = {2016}
}
@inproceedings{Komatsu2010,
abstract = {Recently, OpenCL, a new open programming standard for GPGPU programming, has become available in addition toCUDA. OpenCL can support various compute devices due to its higher abstraction pro- gramming framework. Since there is a semantic gap between OpenCL and compute devices, the OpenCL C compiler plays important roles to exploit the potential of compute devices and therefore its capabil- ity should be clarified. In this paper, the performance of CUDA and OpenCL programs is quantitatively evaluated. First, several CUDA and OpenCL programs of almost the same computations are developed, and their performances are compared. Then, the main factors causing their performance differences is investigated. The evaluation results suggest that the performances of OpenCL programs are comparable with those of CUDA ones if the kernel codes are appropriately optimized by hand or by the compiler optimizations. This paper also discusses the differences between NVIDIA and AMD OpenCL implementations by comparing the performances of their GPUs for the same programs. The performance comparison shows that the compiler options of the OpenCL C compiler and the execution configuration parameters have to be optimized for each GPU to obtain its best performance. Therefore, automatic param- eter tuning is essential to enable a single OpenCL code to run efficiently on various GPUs.},
annote = {Cited by 85.},
author = {Komatsu, Kazuhiko and Sato, Katsuto and Arai, Yusuke and Koyama, Kentaro and Takizawa, Hiroyuki and Kobayashi, Hiroaki},
booktitle = {iWAPT},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Komatsu et al. - Evaluating performance and portability of OpenCL programs.pdf:pdf},
title = {{Evaluating performance and portability of OpenCL programs}},
year = {2010}
}
@techreport{Hoffmann2009,
abstract = {Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are de- signed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. We present a new general technique, code perforation, for auto- matically augmenting existing computations with the capability of trading off accuracy in return for performance. In contrast to ex- isting approaches, which typically require the manual development of new algorithms, our implemented SpeedPress compiler can au- tomatically apply code perforation to existing computations with no developer intervention whatsoever. The result is a transformed computation that can respond almost immediately to a range of in- creased performance demands while keeping any resulting output distortion within acceptable user-defined bounds. We have used SpeedPress to automatically apply code perfora- tion to applications from the PARSEC benchmark suite. The results show that the transformed applications can run as much as two to three times faster than the original applications while distorting the output by less than 10{\%}. Because the transformed applications can operate successfully at many points in the performance/accuracy tradeoff space, they can (dynamically and on demand) navigate the tradeoff space to either maximize performance subject to a given accuracy constraint, or maximize accuracy subject to a given per- formance constraint. We also demonstrate the SpeedGuard runtime system which uses code perforation to enable applications to au- tomatically adapt to challenging execution environments such as multicore machines that suffer core failures or machines that dy- namically adjust the clock speed to reduce power consumption or to protect the machine from overheating.},
annote = {A technique for automatically trading accuracy for performance without developer intervention.

Cited by 61.},
author = {Hoffmann, H. and Misailovic, S. and Agarwal, A. and Rinard, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Hoffmann et al. - Using Code Perforation to Improve Performance, Reduce Energy Consumption, and Respond to Failures.pdf:pdf},
title = {{Using Code Perforation to Improve Performance, Reduce Energy Consumption, and Respond to Failures}},
year = {2009}
}
@inproceedings{Huo2016,
author = {Huo, X. and Li, M. and Zhou, Z.},
booktitle = {IJCAI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Huo, Li, Zhou - Learning Unified Features from Natural and Programming Languages for Locating Buggy Source Code.pdf:pdf},
keywords = {Machine Learning},
title = {{Learning Unified Features from Natural and Programming Languages for Locating Buggy Source Code}},
year = {2016}
}
@article{Hata,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.07170v1},
author = {Hata, H. and Shihab, E. and Neubig, G.},
eprint = {arXiv:1812.07170v1},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Hata, Shihab, Neubig - Learning to Generate Corrective Patches using Neural Machine Translation.pdf:pdf},
journal = {arXiv:1812.07170},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Learning to Generate Corrective Patches using Neural Machine Translation}}
}
@inproceedings{Bansal2006,
abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all oppor- tunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the po- tential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peep- hole optimizers.We show experimentally that our optimizer is able to exploit performance opportunities not found by existing com- pilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
annote = {NULL},
author = {Bansal, S. and Aiken, A.},
booktitle = {ASPLOS},
doi = {10.1145/1168919.1168906},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Bansal, Aiken - Automatic generation of peephole superoptimizers.pdf:pdf},
isbn = {1-59593-451-0},
issn = {01635964},
publisher = {ACM},
title = {{Automatic generation of peephole superoptimizers}},
year = {2006}
}
@techreport{Pradel2017,
author = {Pradel, M. and Sen, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Pradel, Sen - Deep Learning to Find Bugs.pdf:pdf},
institution = {TU Darmstadt},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Deep Learning to Find Bugs}},
year = {2017}
}
@article{Clark2015a,
abstract = {Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing mil-lions of possible future positions to play well, but intuitively a stronger and more 'humanlike' way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convo-lutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to 'hard code' symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our fi-nal networks are able to achieve move prediction accuracies of 41.1{\%} and 44.4{\%} on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural net-works can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3409v2},
author = {Clark, C. and Storkey, A.},
eprint = {arXiv:1412.3409v2},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Clark, Storkey - Training Deep Convolutional Neural Networks to Play Go.pdf:pdf},
journal = {JMLR},
title = {{Training Deep Convolutional Neural Networks to Play Go}},
year = {2015}
}
@inproceedings{Chen2014,
abstract = {GPU is often equipped with complex memory sys- tems, including global memory, texture memory, shared memory, constant memory, and various levels of cache. Where to place the data is important for the performance of a GPU program. However, the decision is difficult for a programmer to make because of architecture complexity and the sensitivity of suitable data placements to input and architecture changes. This paper presents PORPLE, a portable data placement engine that enables a new way to solve the data placement problem. PORPLE consists of a mini specification language, a source-to-source compiler, and a runtime data placer. The language allows an easy description of a memory system; the compiler transforms a GPU program into a form amenable to runtime profiling and data placement; the placer, based on the memory description and data access patterns, identifies on the fly appropriate placement schemes for data and places them accordingly. PORPLE is distinctive in being adaptive to program inputs and architecture changes, being transparent to programmers (in most cases), and being extensible to new memory architectures. Our experiments on three types of GPU systems show that PORPLE is able to consistently find optimal or near-optimal placement despite the large differences among GPU architectures and program inputs, yielding up to 2.08X (1.59X on average) speedups on a set of regular and irregular GPU benchmarks.},
annote = {The authors have designed a specification language for describing GPU memory; a compiler for transforming GPU programs into a memory-agnostic form; a prediction model for assessing the efficiency of a given memory layout for a program; and a search engine for searching the space of possible memory layouts. Critical-reflection: The paper is generally solid. The requirement for a MSL spec for each GPU to be used limits portability, and how can the user validate that the spec is correct?},
author = {Chen, G. and Wu, B.},
booktitle = {MICRO},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Chen, Wu - PORPLE An Extensible Optimizer for Portable Data Placement on GPU.pdf:pdf},
keywords = {GPU,cache,compiler,data placement,hardware specification language},
publisher = {ACM},
title = {{PORPLE: An Extensible Optimizer for Portable Data Placement on GPU}},
year = {2014}
}
@misc{Muralidhar2014,
annote = {Cited by 8.},
author = {Muralidhar, S and Lloyd, W and Roy, S and Hill, C},
booktitle = {OSDI},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Muralidhar et al. - f4 Facebook's Warm BLOB Storage System.pdf:pdf},
isbn = {9781931971164},
title = {{f4: Facebook's Warm BLOB Storage System}},
year = {2014}
}
@inproceedings{Taigman2014,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ⇒ align ⇒ represent ⇒ clas-sify. We revisit both the alignment step and the representa-tion step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight shar-ing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an iden-tity labeled dataset of four million facial images belong-ing to more than 4,000 identities. The learned representa-tions coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35{\%} on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27{\%}, closely approach-ing human-level performance.},
annote = {Very cool idea.},
author = {Taigman, Y. and Yang, M. and Ranzato, M. A. and Wolf, L.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.220},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Taigman et al. - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
publisher = {IEEE},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, D. P. and Ba, J. L.},
eprint = {1412.6980},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kingma, Ba - Adam a Method for Stochastic Optimization.pdf:pdf},
journal = {ICLR},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@inproceedings{Lee2003,
annote = {Cited by 15.},
author = {Lee, Hhs and Fryman, Jb},
booktitle = {WCED},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Lee, Fryman - The elusive metric for low-power architecture research.pdf:pdf},
title = {{The elusive metric for low-power architecture research}},
url = {http://arch.ece.gatech.edu/pub/wced03.pdf},
year = {2003}
}
@phdthesis{Williams2008,
abstract = {For the last decade, the exponential potential of Moore's Law has been squan- dered in the effort to increase single thread performance, which is now limited by the memory, instruction, and power walls. In response, the computing industry has boldly placed its hopes on the multicore gambit. That is, abandon instruction-level parallelism and frequency-scaling in favor of the exponential scaling of the number of compute cores per microprocessor. The massive thread-level parallelism results in tremendous potential performance, but demands efficient parallel programming — a task existing software tools are ill-equipped for. We desire performance portability — the ability to write a program once and not only have it deliver good performance on the development computer, but on all multicore computers today and tomorrow. This thesis accepts for fact that multicore is the basis for all future computers. Furthermore, we regiment our study by organizing it around the computational patterns and motifs as set forth in the Berkeley View. Although domain experts may be extremely knowledgeable on the mathematics and algorithms of their fields, they often lack the de- tailed computer architecture knowledge required to achieve high performance. Forthcoming heterogeneous architectures will exacerbate the problem for everyone. Thus, we extend the auto-tuning approach to program optimization and performance portability to the menagerie of multicore computers. In an automated fashion, an auto-tuner will explore the optimization space for a particular computational kernel of a motif on a particular com- puter. In doing so, it will determine the best combination of algorithm, implementation, and data structure for the combination of architecture and input data. We implement and evaluate auto-tuners for two important kernels: Lattice Boltz- mann Magnetohydrodynamics (LBMHD) and sparse matrix-vector multiplication (SpMV). They are representative of two of the computational motifs: structured grids and sparse linear algebra. To demonstrate the performance portability that our auto-tuners deliver, we selected an extremely wide range of architectures as an experimental test bed. These include conventional dual- and quad-core superscalar x86 processors both with and without inte- grated memory controllers. We also include the rather unconventional chip multithreaded (CMT) Sun Niagara2 (Victoria Falls) and the heterogeneous, local store-based IBM Cell Broadband Engine. In some experiments we sacrifice the performance portability of a com- mon C representation, by creating ISA-specific auto-tuned versions of these kernels to gain architectural insight. To quantify our success, we created the Roofline model to perform a bound and bottleneck analysis for each kernel-architecture combination. Despite the common wisdom that LBMHD and SpMV are memory bandwidth- bound, and thus nothing can be done to improve performance, we show that auto-tuning consistently delivers speedups in excess of 3× across all multicore computers except the memory-bound Intel Clovertown, where the benefit was as little as 1.5×. The Cell pro- cessor, with its explicitly managed memory hierarchy, showed far more dramatic speedups of between 20× and 130×. The auto-tuners includes both architecture-independent opti- mizations based solely on source code transformations and high-level kernel knowledge, as well as architecture-specific optimizations like the explicit use of single instruction, multiple data (SIMD) extensions or the use Cell's DMA-based memory operations. We observe that the these ISA-specific optimizations are becoming increasingly important as architectures evolve.},
annote = {NULL},
author = {Williams, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Williams - Auto-tuning Performance on Multicore Computers.pdf:pdf},
school = {University of California, Berkeley Professor},
title = {{Auto-tuning Performance on Multicore Computers}},
year = {2008}
}
@article{Veldhuizen1995,
abstract = {Expression Templates is a C++ technique for passing expressions as function arguments. The expression can be inlined into the function body, which results in faster and more convenient code than C-style callback functions. This technique can also be used to evaluate vector and matrix expressions in a single pass without temporaries. In preliminary benchmark results, one compiler evaluates vector expressions at 95-99.5{\%} efficiency of hand- coded C using this technique (for long vectors). The speed is 2-15 times that of a conventional C++ vector class.},
annote = {Cited by 553.},
author = {Veldhuizen, T.},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Veldhuizen - Expression Templates.pdf:pdf},
journal = {C++ Report},
number = {5},
title = {{Expression Templates}},
volume = {7},
year = {1995}
}
@inproceedings{Paper2016,
annote = {NULL},
author = {Donaldson, A. and Lascu, A.},
booktitle = {MET},
doi = {10.1145/2896971.2896978},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Donaldson, Lascu - Metamorphic Testing for (Graphics) Compilers.pdf:pdf},
isbn = {9781450341639},
keywords = {compiler testing,graphics,metamorphic testing,opengl},
title = {{Metamorphic Testing for (Graphics) Compilers}},
year = {2016}
}
@article{Jouppi2017b,
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a ​ Tensor Processing Unit (TPU) ​ — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, {\ldots}) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95{\%} of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. Index terms–DNN, MLP, CNN, RNN, LSTM, neural network, domain-specific architecture, accelerator},
author = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-Luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and Mackean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Jouppi et al. - In-Datacenter Performance Analysis of a Tensor Processing Unit​ TM.pdf:pdf},
pages = {1--17},
title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit​}},
year = {2017}
}
@inproceedings{Breuer2014a,
annote = {NULL},
author = {Breuer, Stefan and Steuwer, Michel and Gorlatch, Sergei},
booktitle = {HiStencils},
title = {{Extending the SkelCL Skeleton Library for Stencil Computations on Multi-GPU Systems}},
year = {2014}
}
@misc{Goddard,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 7.pdf:pdf},
title = {{IAML 7}},
year = {2014}
}
@inproceedings{Gousios2014a,
abstract = {In recent years, GitHub has become the largest code host in the world, with more than 5M developers collaborating across 10M repositories. Numerous popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap, Django or jQuery) have chosen GitHub as their host and have migrated their code base to it. GitHub offers a tremendous research potential. For instance, it is a flagship for current open source development, a place for developers to showcase their expertise to peers or potential recruiters, and the platform where social coding features or pull requests emerged. However, GitHub data is, to date, largely underexplored. To facilitate studies of GitHub, we have created GHTorrent, a scalable, queriable, offline mirror of the data offered through the GitHub REST API. In this paper we present a novel feature of GHTorrent designed to offer customisable data dumps on demand. The new GHTorrent data-on-demand service offers users the possibility to request via a web form up-to-date GHTorrent data dumps for any collection of GitHub repositories. We hope that by offering customisable GHTorrent data dumps we will not only lower the 'barrier for entry' even further for researchers interested in mining GitHub data (thus encourage researchers to intensify their mining efforts), but also enhance the replicability of GitHub studies (since a snapshot of the data on which the results were obtained can now easily accompany each study).},
annote = {NULL},
author = {Gousios, G. and Vasilescu, B. and Serebrenik, A. and Zaidman, A.},
booktitle = {MSR},
doi = {10.1145/2597073.2597126},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Gousios et al. - Lean GHTorrent GitHub data on demand.pdf:pdf},
isbn = {9781450328630},
keywords = {data on demand,dataset,github},
title = {{Lean GHTorrent: GitHub data on demand}},
url = {http://dl.acm.org/citation.cfm?doid=2597073.2597126},
year = {2014}
}
@article{Kusner2017,
abstract = {Deep generative models have been wildly suc-cessful at learning coherent latent representa-tions for continuous data such as video and au-dio. However, generative modeling of discrete data such as arithmetic expressions and molec-ular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent la-tent space in which nearby points decode to sim-ilar discrete outputs. We demonstrate the effec-tiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
annote = {Source code: https://github.com/mkusner/grammarVAE .
Practical concerns: the grammars used are of trivial simplicity compared to a PL - I wonder does the technique scale as the size of the masks increases?},
author = {Kusner, M. J. and Paige, B. and Hern{\'{a}}ndez-Lobato, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Kusner, Paige, Hern{\'{a}}ndez-Lobato - Grammar Variational Autoencoder.pdf:pdf},
journal = {arXiv:1703.01925},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Grammar Variational Autoencoder}},
year = {2017}
}
@article{Wu2018,
abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories. With a focus on graph convolutional networks, we review alternative architectures that have recently been developed; these learning paradigms include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this fast-growing field.},
archivePrefix = {arXiv},
arxivId = {1901.00596v1},
author = {Wu, Z. and Pan, S. and Chen, F. and Long, G. and Zhang, C. and Yu, P. S.},
doi = {arXiv:1901.00596v1},
eprint = {1901.00596v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Wu et al. - A Comprehensive Survey on Graph Neural Networks.pdf:pdf},
journal = {arXiv:1901.00596},
keywords = {Index Terms-Deep Learning,graph autoencoder,graph convolutional networks,graph neural networks,graph representation learning,network embedding !},
title = {{A Comprehensive Survey on Graph Neural Networks}},
year = {2018}
}
@phdthesis{Bash2015b,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lutz, Thibaut},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lutz - Enhancing Productivity and Performance Portability of OpenCL Applications on Heterogeneous Systems using Runtime Optimizat.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
school = {University of Edinburgh},
title = {{Enhancing Productivity and Performance Portability of OpenCL Applications on Heterogeneous Systems using Runtime Optimizations}},
year = {2015}
}
@inproceedings{Pflanzer2016,
abstract = {We report on an extension to the C-Reduce tool, for auto- matic reduction of C test cases, to handle OpenCL kernels. This enables an automated method for detecting bugs in OpenCL compilers, by generating large random kernels us- ing the CLsmith generator, identifying kernels that yield re- sult differences across OpenCL platforms and optimisation levels, and using our novel extension to C-Reduce to auto- matically reduce such kernels to minimal forms that can be filed as bug reports. A major part of our effort involved the design of ShadowKeeper, a new plugin for the Oclgrind sim- ulator that provides accurate detection of accesses to unini- tialised data. We present experimental results showing the effectiveness of our method for finding bugs in a number of OpenCL compilers.},
annote = {NULL},
author = {Pflanzer, M. and Donaldson, A. and Lascu, A.},
booktitle = {IWOCL},
doi = {10.1145/2909437.2909439},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Pflanzer, Donaldson, Lascu - Automatic Test Case Reduction for OpenCL.pdf:pdf},
isbn = {9781450343381},
title = {{Automatic Test Case Reduction for OpenCL}},
year = {2016}
}
@misc{UniversityofEdinburgh2015c,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 10. Vectorisation.pdf:pdf},
title = {{10. Vectorisation}},
year = {2015}
}
@incollection{Potter2014,
annote = {NULL},
author = {Potter, Susan},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Potter - Git.pdf:pdf},
number = {Volume 2},
title = {{Git}},
volume = {2},
year = {2014}
}
@inproceedings{Cummins2018,
abstract = {Random program generation-fuzzing-is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of gen-erative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03× less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing. CCS CONCEPTS • Software and its engineering → Software testing and de-bugging;},
author = {Cummins, C. and Petoumenos, P. and Murray, A. and Leather, H.},
booktitle = {ISSTA},
doi = {10.1145/3213846.3213848},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Cummins et al. - Compiler fuzzing through deep learning.pdf:pdf},
isbn = {9781450356992},
issn = {0301-4215},
title = {{Compiler fuzzing through deep learning}},
url = {http://dl.acm.org/citation.cfm?doid=3213846.3213848},
year = {2018}
}
@misc{CanergieMellonUniversity2005c,
annote = {NULL},
author = {{Canergie Mellon University}},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Canergie Mellon University - 2. Writing About Projects.pdf:pdf},
number = {January},
title = {{2. Writing About Projects}},
year = {2005}
}
@article{Horvitz2016,
annote = {NULL},
author = {Horvitz, E.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Horvitz - Artificial intelligence and life in 2030.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Artificial intelligence and life in 2030}},
year = {2016}
}
@misc{Etessami,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 16. Counting.pdf:pdf},
number = {Chapter 6},
title = {{16. Counting}},
year = {2015}
}
@article{Li2016,
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where allocation of additional resources to an arm corresponds to training a configuration on larger subsets of the data. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. We compare Hyperband with state-of-the-art Bayesian optimization methods and a random search baseline on a comprehensive benchmark including 117 datasets. Our results on this benchmark demonstrate that while Bayesian optimization methods do not outperform random search trained for twice as long, Hyperband in favorable settings offers valuable speedups.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1603.06560},
author = {Li, L. and Jamieson, K. and DeSalvo, G. and Rostamizadeh, A. and Talwalkar, A.},
eprint = {1603.06560},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Li et al. - Hyperband A Novel Bandit-Based Approach to Hyperparameter Optimization.pdf:pdf},
journal = {arXiv:1603.06560},
keywords = {arms,bandits with infinitely many,deep learning,hyperparameter optimization,model selection,online optimization},
title = {{Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization}},
year = {2016}
}
@book{Trask,
annote = {NULL},
author = {Trask, A. W.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Trask - Grokking Deep Learning.pdf:pdf},
publisher = {Manning Publications},
title = {{Grokking Deep Learning}},
year = {2016}
}
@inproceedings{Seo2013,
abstract = {In this paper, we address the effect of the work-group size on the performance of OpenCL kernels. We propose a profiling-based algorithm that finds a good work-group size, in terms of performance, for the target multicore CPU architecture. Our algorithm reduces misses in the private L1 data cache and achieves load balancing between cores. It exploits the polyhedral model to estimate the working-set size and the number of cache misses for a parameterized work-group size of the OpenCL kernel. Based on the profiling information, it heuristically searches the space of parameterized work-group sizes. Our virtually-extended index space helps to increase the probability to find a better work-group size. We implement our work-group size selection algorithm as a development tool that consists of a code generator and a search library. The code generator extracts the polytope of each memory reference from the kernel code and generates a function that simplifies polytopes using the run-time information and invokes search library routines. The search library calculates the working-set size using the polytopes and finds a proper work-group size. We evaluate our approach using 31 OpenCL kernels on four different multicore CPUs. We compare its accuracy and search time to those of an exhaustive search method. Experimental results show that our tool is, on average, 1566 times faster than the exhaustive search and selects a work-group size whose performance is the same as or comparable to that of the exhaustive search.},
annote = {NULL},
author = {Seo, Sangmin and Lee, Jun and Jo, Gangwon and Lee, Jaejin},
booktitle = {PACT},
doi = {10.1109/PACT.2013.6618834},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Seo et al. - Automatic OpenCL work-group size selection for multicore CPUs.pdf:pdf},
isbn = {9781479910212},
issn = {1089795X},
keywords = {OpenCL,automatic selection,multicore CPU,performance portability,profiling,work-group size,working-set},
publisher = {ACM},
title = {{Automatic OpenCL work-group size selection for multicore CPUs}},
year = {2013}
}
@article{Saclay2010,
abstract = {Iterative optimization is a popular and efficient research approach to optimize programs using feedback-directed compilation. However, one of the key limitations that prevented widespread use in production compilers and day-to-day practice is the necessity to perform a large number of program runs with the same dataset and environment (architecture, OS, compiler) to test many different combinations of optimizations. In this article, we propose to overcome such a practical obstacle using collective optimization, where the task of optimizing a program or tuning default compiler optimization heuristic leverages the experience of many other users continuously, rather than being performed in isolation, and often redundantly, by each user. During this unobtrusive approach, performance information is sent to a central database after each run and statistically combined with the data fromall users to suggest most profitable optimizations for a given program and an architecture, or to gradually improve default optimization level of a compiler for a given architecture. In this article, we address two key challenges of collective optimization. We show that it is possible to simultaneously learn and improve performance while avoiding long training phases. We also demonstrate how to use our approach with static compilers to learn optimizations across multiple datasets and architectures without even a reference run normally needed to compute speedups over the baseline optimization by using static function cloning and dynamic adaptation. We present a novel probabilistic approach based on competition among pairs of optimizations (program reaction to optimizations) to enable optimization knowledge reuse and achieve nearly the best possible iterative optimization performance. We implemented our technique in GCC (widespread production open-source compiler that supports multiple architectures) and connected it to a public collective optimization database at cTuning.org to gather profile and optimization data continuously and transparently in realistic environments ranging from desktop PCs and mobile systems to supercomputers and data centers.},
annote = {NULL},
author = {Fursin, G. and Temam, O.},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Fursin, Temam - Collective Optimization A Practical Collaborative Approach.pdf:pdf},
journal = {TACO},
keywords = {Performance of Systems-Measurement techniques,modeling techniques},
number = {4},
title = {{Collective Optimization: A Practical Collaborative Approach}},
volume = {7},
year = {2010}
}
@inproceedings{Beazley2009,
annote = {NULL},
author = {Beazley, David},
booktitle = {Python Concurrency Workshop},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Beazley - Inside the Python GIL.pdf:pdf},
title = {{Inside the Python GIL}},
year = {2009}
}
@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural net-works. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empiri-cally, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
annote = {NULL},
author = {Ba, J. L. and Kiros, J. R. and Hinton, G. E.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ba, Kiros, Hinton - Layer Normalization.pdf:pdf},
journal = {arXiv:1607.06450},
title = {{Layer Normalization}},
year = {2016}
}
@misc{Silver2015g,
abstract = {Lecture of University College London.},
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 4 Model-Free Prediction.pdf:pdf},
isbn = {9780195306613},
title = {{Lecture 4 : Model-Free Prediction}},
year = {2015}
}
@inproceedings{Dubach2010,
abstract = {Adaptive microarchitectures are a promising solu- tion for designing high-performance, power-efficient micropro- cessors. They offer the ability to tailor computational resources to the specific requirements of different programs or program phases. They have the potential to adapt the hardware cost- effectively at runtime to any application's needs. However, one of the key challenges is how to dynamically determine the best architecture configuration at any given time, for any new workload. This paper proposes a novel control mechanism based on a predictivemodel for microarchitectural adaptivity control. This model is able to efficiently control adaptivity by monitoring the behaviour of an application's different phases at runtime. We show that using this model on SPEC 2000, we double the energy/performance efficiency of the processor when compared to the best static configuration tuned for the whole benchmark suite. This represents 74{\%} of the improvement available if we knew the best microarchitecture for each program phase ahead of time. In addition, we show that the overheads associated with the implementation of our scheme have a negligible impact on performance and power.},
annote = {Using adaptive hardware to increase energy efficiency of benchmarks at runtime.

Cited by 29.},
author = {Dubach, C. and Jones, T. M. and Bonilla, E. V. and O'Boyle, M.},
booktitle = {MICRO},
doi = {10.1109/MICRO.2010.14},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Dubach et al. - A Predictive Model for Dynamic Microarchitectural Adaptivity Control.pdf:pdf},
isbn = {978-1-4244-9071-4},
month = {dec},
publisher = {ACM},
title = {{A Predictive Model for Dynamic Microarchitectural Adaptivity Control}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5695560},
year = {2010}
}
@misc{Leroy2013,
author = {Leroy, X.},
booktitle = {INRIA Paris-Rocquencourt},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Leroy - The CompCert C Verified Compiler.pdf:pdf},
title = {{The CompCert C Verified Compiler}},
year = {2017}
}
@inproceedings{DzmitryBahdana2014,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
annote = {NULL},
author = {{Dzmitry B.} and Bahdanau, D. and Cho, K. and Bengio, Y.},
booktitle = {ICLR},
doi = {10.1146/annurev.neuro.26.041002.131047},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Dzmitry B. et al. - Neural Machine Translation By Jointly Learning To Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
year = {2015}
}
@article{Marcus,
author = {Marcus, G.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Marcus - Deep Learning A Critical Appraisal.pdf:pdf},
journal = {arXiv:1801.00631},
title = {{Deep Learning: A Critical Appraisal}},
year = {2018}
}
@article{Ling2016,
abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architec-ture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this frame-work, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong bench-marks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1603.06744},
author = {Ling, W. and Grefenstette, E. and {Moritz Hermann}, K. and Kocisky, T. and Senior, A. and Wang, F. and Blunsom, P.},
eprint = {1603.06744},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ling et al. - Latent Predictor Networks for Code Generation.pdf:pdf},
journal = {ACL},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Latent Predictor Networks for Code Generation}},
year = {2016}
}
@inproceedings{Memarian2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Memarian, Kayvan and Matthiesen, Justus and Lingard, James and Nienhuis, Kyndylan and Chisnall, David and Watson, Robert N. M. and Sewell, Peter},
booktitle = {PLDI},
doi = {10.1145/2908080.2908081},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Memarian et al. - Into the depths of C elaborating the de facto standards.pdf:pdf},
isbn = {9781450342612},
title = {{Into the depths of C: elaborating the de facto standards}},
year = {2016}
}
@article{Heller1978,
abstract = {The existence of parallel and pipeline computers has inspired a new approach to algorithmic analysis. Classical numerical methods are generally unable to exploit multiple processors and powerful vector-oriented hardware. Efficient parallel algorithms can be created by reformulating familiar algorithms or by discovering new ones, and the results are often surprising. A comprehensive survey of parallel techniques for problems in linear algebra is given. Specific topics include: relevant computer models and their consequences for programs, evaluation of arithmetic expressions, solution of general and special linear systems of equations, and computation of eigenvalues.},
annote = {NULL},
author = {Heller, D.},
file = {:Users/cec/Google Drive/Mendeley Library/1978 - Heller - A Survey of Parallel Algorithms in Numerical Linear Algebra.pdf:pdf},
journal = {Siam Review},
number = {4},
publisher = {SIAM},
title = {{A Survey of Parallel Algorithms in Numerical Linear Algebra}},
volume = {20},
year = {1978}
}
@article{Sheridan2007,
author = {Sheridan, F.},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Sheridan - Practical Testing of a C99 Compiler Using Output Comparison.pdf:pdf},
journal = {Software: Practice and Experience},
number = {14},
publisher = {Wiley Online Library},
title = {{Practical Testing of a C99 Compiler Using Output Comparison}},
volume = {37},
year = {2007}
}
@inproceedings{Kegel2012,
abstract = {Modern computer systems are becoming increas- ingly heterogeneous by comprising multi-core CPUs, GPUs, and other accelerators. Current programming approaches for such systems usually require the application developer to use a combination of several programming models (e. g., MPI with OpenCL or CUDA) in order to exploit the full compute capability of a system. In this paper, we present dOpenCL (Distributed OpenCL) – a uniform approach to programming distributed heterogeneous systems with accelerators. dOpenCL extends the OpenCL standard, such that arbitrary computing devices installed on any node of a distributed system can be used together within a single application. dOpenCL allows moving data and program code to these devices in a transparent, portable manner. Since dOpenCL is designed as a fully-fledged implementation of the OpenCL API, it allows running existing OpenCL applications in a heterogeneous distributed environment without any modifi- cations.We describe in detail the mechanisms that are required to implement OpenCL for distributed systems, including a de- vice management mechanism for running multiple applications concurrently. Using three application studies, we compare the performance of dOpenCL with MPI+OpenCL and a standard OpenCL implementation.},
annote = {NULL},
author = {Kegel, Philipp and Steuwer, Michel and Gorlatch, Sergei},
booktitle = {IPDPSW},
doi = {10.1109/IPDPSW.2012.16},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kegel, Steuwer, Gorlatch - dOpenCL Towards a uniform programming approach for distributed heterogeneous multi-many-core systems.pdf:pdf},
isbn = {9780769546766},
keywords = {Distributed Systems,GPU Computing,Heterogeneous Systems,OpenCL,dOpenCL},
publisher = {IEEE},
title = {{dOpenCL: Towards a uniform programming approach for distributed heterogeneous multi-/many-core systems}},
year = {2012}
}
@inproceedings{Lee2009b,
abstract = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
annote = {NULL},
author = {Lee, H. and Largman, Y. and Pham, P. and Ng, A. Y.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Lee et al. - Unsupervised Feature Learning for Audio Classification using Convolutional Deep Belief Networks.pdf:pdf},
title = {{Unsupervised Feature Learning for Audio Classification using Convolutional Deep Belief Networks}},
year = {2009}
}
@phdthesis{Seeker2017,
author = {Seeker, V.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Seeker - User Experience Driven CPU Frequency Scaling On Mobile Devices Towards Better Energy Efficiency Volker Seeker.pdf:pdf},
school = {University of Edinburgh},
title = {{User Experience Driven CPU Frequency Scaling On Mobile Devices Towards Better Energy Efficiency Volker Seeker}},
year = {2017}
}
@inproceedings{You,
annote = {NULL},
author = {You, Yi-ping and Wu, Hen-jung and Tsai, Yeh-ning and Chao, Yen-ting},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - You et al. - VirtCL A Framework for OpenCL Device Abstraction and Management.pdf:pdf},
isbn = {9781450332057},
keywords = {device abstraction,multitasking,opencl},
title = {{VirtCL : A Framework for OpenCL Device Abstraction and Management}},
year = {2015}
}
@misc{Godefroid2008,
abstract = {Fuzz testing is an effective technique for finding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a pro- gram and test the resulting values. We present an alterna- tive whitebox fuzz testing approach inspired by recent ad- vances in symbolic execution and dynamic test generation. Our approach records an actual run of the program un- der test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coveragemaximizing heuristic designed to find defects as fast as possible. We have implemented this algorithm in SAGE (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary file-reading Windows ap- plications. We describe key optimizations needed to make dynamic test generation scale to large input files and long execution traces with hundreds of millions of instructions. We then present detailed experiments with severalWindows applications. Notably, without any format-specific knowl- edge, SAGE detects theMS07-017 ANI vulnerability, which was missed by extensive blackbox fuzzing and static analy- sis tools. Furthermore, while still in an early stage of de- velopment, SAGE has already discovered 30+ new bugs in large shipped Windows applications including image pro- cessors, media players, and file decoders. Several of these bugs are potentially exploitable memory access violations.},
annote = {Cited by 667.},
author = {Godefroid, P. and Levin, M. Y. and Molnar, D.},
booktitle = {NDSS},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Godefroid, Levin, Molnar - Automated Whitebox Fuzz Testing.pdf:pdf},
issn = {1064-3745},
pmid = {7581676},
title = {{Automated Whitebox Fuzz Testing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.151.9430{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@article{Curnow1976,
annote = {Cited by 386.},
author = {Curnow, H. J. and Wichmann, B. A.},
file = {:Users/cec/Google Drive/Mendeley Library/1976 - Curnow, Wichmann - A Syntetic Benchmark.pdf:pdf},
journal = {Computer},
keywords = {benchmarking},
number = {1},
title = {{A Syntetic Benchmark}},
volume = {19},
year = {1976}
}
@techreport{Collins2013a,
abstract = {Functional languages provide a solid foundation on which complex optimization passes can be designed to exploit available parallelism in the underlying system. Their mathematical foundations enable high-level optimizations thatwould be impossible in traditional im- perative languages. This makes them uniquely suited for generation of efficient target code for parallel systems, such as multiple Central Processing Units (CPUs) or highly data-parallel Graphics Process- ing Units (GPUs). Such systems are becoming the mainstream for scientific and ‘desktop' computing. Writing performance portable code for such systems using low- level languages requires significant effort from a human expert. This paper presents NOVA, a functional language and compiler for multi-core CPUs and GPUs. TheNOVAlanguage is a polymorphic, statically-typed functional language with a suite of higher-order functions which are used to express parallelism. These include map, reduce and scan. The NOVA compiler is a light-weight, yet powerful, optimizing compiler. It generates code for a variety of target platforms that achieve performance comparable to competing languages and tools, including hand-optimized code. The NOVA compiler is stand-alone and can be easily used as a target for higher-level or domain specific languages or embedded in other applications. We evaluate NOVA against two competing approaches: the Thrust library and hand-written CUDA C. NOVA achieves com- parable performance to these approaches across a range of bench- marks. NOVA-generated code also scales linearly with the number of processor cores across all compute-bound benchmarks.},
annote = {NOVA is a functional programming language for parallel CPU {\&} GPU programing using data-parallel skeletons. The compiler supports code generation for C, multi-threaded C, and CUDA C.




The performance for a number of benchmarks is comparable with Thrust library and CUDA C. It is unclear what the immediate advantages are, aside from the functional semantics. Assuming that the functional semantics are worth pursuing, why not build a NOVA frontend for LLVM, and so take advantage of the dozens of (highly sophisticated) optimising passes?},
author = {Collins, A. and Grewe, D. and Lee, S. and Susnea, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Collins et al. - NOVA A Functional Language for Data Parallelism.pdf:pdf},
institution = {NVIDIA},
title = {{NOVA: A Functional Language for Data Parallelism}},
year = {2013}
}
@article{Downey2012,
abstract = {Expand your Python skills by working with data structures and algorithms in a refreshing context—through an eye-opening exploration of complexity science. Whether you're an intermediate-level Python programmer or a student of computational modeling, you'll delve into examples of complex systems through a series of exercises, case studies, and easy-to-understand explanations.You'll work with graphs, algorithm analysis, scale-free networks, and cellular automata, using advanced features that make Python such a powerful language. Ideal as a text for courses on Python programming and algorithms, Think Complexity will also help self-learners gain valuable experience with topics and ideas they might not encounter otherwise.Work with NumPy arrays and SciPy methods, basic signal processing and Fast Fourier Transform, and hash tablesStudy abstract models of complex physical systems, including power laws, fractals and pink noise, and Turing machinesGet starter code and solutions to help you re-implement and extend original experiments in complexityExplore the philosophy of science, including the nature of scientific laws, theory choice, realism and instrumentalism, and other topicsExamine case studies of complex systems submitted by students and readers},
annote = {NULL},
author = {Downey, A. B.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Downey - Think Complexity.pdf:pdf},
isbn = {1449331696},
title = {{Think Complexity}},
url = {http://books.google.com/books?id=SaV59o7K0GMC{\&}pgis=1},
year = {2012}
}
@article{Becker2017,
abstract = {In this paper, we present the first-of-its-kind machine learning (ML) system, called AI Programmer, that can automatically generate full software programs requiring only minimal human guidance. At its core, AI Programmer uses genetic algorithms (GA) coupled with a tightly constrained programming language that minimizes the overhead of its ML search space. Part of AI Programmer's novelty stems from (i) its unique system design, including an embedded, hand-crafted interpreter for efficiency and security and (ii) its augmentation of GAs to include instruction-gene randomization bindings and programming language-specific genome construction and elimination techniques. We provide a detailed examination of AI Programmer's system design, several examples detailing how the system works, and experimental data demonstrating its software generation capabilities and performance using only mainstream CPUs.},
archivePrefix = {arXiv},
arxivId = {1709.05703},
author = {Becker, K. and Gottschlich, J.},
eprint = {1709.05703},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Becker, Gottschlich - AI Programmer Autonomously Creating Software Programs Using Genetic Algorithms.pdf:pdf},
journal = {arXiv:1709.05703},
title = {{AI Programmer: Autonomously Creating Software Programs Using Genetic Algorithms}},
year = {2017}
}
@inproceedings{Nobre2016,
annote = {NULL},
author = {Nobre, R. and Cardoso, M. P.},
booktitle = {LCTES},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Nobre, Cardoso - A Graph-Based Iterative Compiler Pass Selection and Phase Ordering Approach.pdf:pdf},
isbn = {9781450343169},
keywords = {compilers,design space exploration,phase-ordering},
publisher = {ACM},
title = {{A Graph-Based Iterative Compiler Pass Selection and Phase Ordering Approach}},
year = {2016}
}
@inproceedings{Shan,
annote = {NULL},
author = {Shan, H. and Williams, S. and Jong, W. D. and Oliker, L.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Shan et al. - Thread-Level Parallelization and Optimization of NWChem for the Intel MIC Architecture Categories and Subject Descr.pdf:pdf},
isbn = {9781450334044},
keywords = {CCSD(T),Fock Matrix Construction,MPI,Manycore Architecture,NWChem,OpenMP,OpenMP Task,Performance,Texas Integral,Thread-level Parallelism},
title = {{Thread-Level Parallelization and Optimization of NWChem for the Intel MIC Architecture Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Darlington1993,
abstract = {Prograxnming parallel machines is notoriously difficult. Factors contributing to this difficulty include the complexity of concurrency, the effect of resource allocation on performance and the current diversity of parallel machine models. The net result is that effective portability, which depends crucially on the predictability of performance, has been lost. Functional programming languages have been put forward as solutions to these problems, because of the availability of implicit parallelism. However, performance will be generally poor unless the issue of resource allocation is addressed explicitly, diminishing the advantage of using a functional language in the first place. We present a methodology which is a compromise between the extremes of explicit imperative programming and implicit functional programming. We use a repertoire of higher-order parallel forms, skeletons, as the basic building blocks for parallel implementations and provide program transformations which can convert between skeletons, giving portability between differing machines. Resource allocation issues are documented for each skeleton/machine pair and are addressed explicitly during implementation in an interactive, selective manner, rather than by explicit programming.},
annote = {This paper describes a methodology for using skeletons as a basic build block for parallel transformations (e.g. DC, PIPE, FARM etc).


It doesn't appear to include any evidence of substantial "engineering", e.g. a comparison of parallel skeleton implementation execution times vs sequential. A brief two paragraph section describes an implementation using a functional language, although "Initial re- sults, in terms of both speed-up and the usability of the methodology , have been promising although we have not, as yet, made direct comparisons with hand- coded versions of the same algorithms."},
author = {Darlington, J and Field, AJ J and Harrison, PG G},
booktitle = {PARLE},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Darlington, Field, Harrison - Parallel Programming Using Skeleton Functions.pdf:pdf},
title = {{Parallel Programming Using Skeleton Functions}},
url = {http://link.springer.com/chapter/10.1007/3-540-56891-3{\_}12},
year = {1993}
}
@misc{Edinburgh2013a,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - University of Edinburgh - COPT 2011 exam.pdf:pdf},
number = {Level 10},
title = {{COPT 2011 exam}},
year = {2013}
}
@techreport{This2016,
abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
author = {Zinkevich, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zinkevich - Rules of Machine Learning Best Practices for ML Engineering.pdf:pdf},
keywords = {TOPRINT,TOSTUDY},
mendeley-tags = {TOPRINT,TOSTUDY},
title = {{Rules of Machine Learning: Best Practices for ML Engineering}},
url = {papers3://publication/uuid/4CC57281-97A2-4896-BC0D-94B065C5B70B},
year = {2016}
}
@inproceedings{Liu2019,
abstract = {Compilers are among the most fundamental programming tools for building software. However, production compilers remain buggy. Fuzz testing is often leveraged with newly-generated, or mutated inputs in order to find new bugs or security vulnerabilities. In this paper, we propose a grammar-based fuzzing tool called DEEPFUZZ. Based on a generative Sequence-to-Sequence model, DEEPFUZZ automatically and continuously generates well-formed C programs. We use this set of new C programs to fuzz off-the-shelf C compilers, e.g., GCC and Clang/LLVM. We present a detailed case study to analyze the success rate and coverage improvement of the generated C programs for fuzz testing. We analyze the performance of DEEPFUZZ with three types of sampling methods as well as three types of generation strategies. Consequently , DEEPFUZZ improved the testing efficacy in regards to the line, function, and branch coverage. In our preliminary study, we found and reported 8 bugs of GCC, all of which are actively being addressed by developers.},
author = {Liu, X. and Li, X. and Prajapati, R. and Wu, D.},
booktitle = {AAAI},
file = {:Users/cec/Google Drive/Mendeley Library/2019 - Liu et al. - DeepFuzz Automatic Generation of Syntax Valid C Programs for Fuzz Testing.pdf:pdf},
title = {{DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing}},
year = {2019}
}
@inproceedings{Danaee2016,
abstract = {Cancer detection from gene expression data continues to pose a challenge due to the high dimensionality and complexity of these data. After decades of research there is still uncertainty in the clinical diagnosis of cancer and the identification of tumor-specific markers. Here we present a deep learning approach to cancer detection, and to the identification of genes critical for the diagnosis of breast cancer. First, we used Stacked Denoising Autoencoder (SDAE) to deeply extract functional features from high dimensional gene expression profiles. Next, we evaluated the performance of the extracted representation through supervised classification models to verify the usefulness of the new features in cancer detection. Lastly, we identified a set of highly interactive genes by analyzing the SDAE connectivity matrices. Our results and analysis illustrate that these highly interactive genes could be useful cancer biomarkers for the detection of breast cancer that deserve further studies.},
author = {Danaee, P. and Ghaeini, R. and Hendrix, D. A.},
booktitle = {PSB},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Danaee, Ghaeini, Hendrix - A Deep Learning Approach for Cancer Detection and Relevant Gene Identification.pdf:pdf},
keywords = {cancer detection,classification,deep learning,dimensionality reduction,rna-seq expression,stacked denoising autoencoder},
title = {{A Deep Learning Approach for Cancer Detection and Relevant Gene Identification}},
year = {2017}
}
@article{Oord2016a,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {Oord, A. and Dieleman, S. and Zen, H. and Simonyan, K. and Vinyals, O. and Graves, A. and Kalchbrenner, N. and Senior, A. and Kavukcuoglu, K.},
eprint = {1609.03499},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Oord et al. - WaveNet A Generative Model for Raw Audio.pdf:pdf},
isbn = {9783901882760},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{Dam2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.00921v1},
author = {Dam, H. K. and Grundy, J. and Kim, T. and Kim, C.},
eprint = {arXiv:1802.00921v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Dam et al. - A deep tree-based model for software defect prediction.pdf:pdf},
journal = {arXiv:1802.00921},
title = {{A deep tree-based model for software defect prediction}},
year = {2018}
}
@article{Rossum2012c,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Logging Cookbook.pdf:pdf},
title = {{Logging Cookbook}},
year = {2016}
}
@inproceedings{Sethia2013,
annote = {NULL},
author = {Sethia, A. and Dasika, G. and Samadi, M. and Mahlke, S.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Sethia et al. - APOGEE Adaptive Prefetching On GPU.pdf:pdf},
publisher = {ACM},
title = {{APOGEE: Adaptive Prefetching On GPU}},
year = {2013}
}
@incollection{Peterson2014,
abstract = {PyPy is a Python implementation and a dynamic language implementation framework. This chapter assumes familiarity with some basic interpreter and compiler concepts like bytecode and constant folding.},
annote = {NULL},
author = {Peterson, Benjamin},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Peterson - PyPy.pdf:pdf},
number = {Volume 2},
title = {{PyPy}},
volume = {2},
year = {2014}
}
@misc{UniversityofEdinburgh2014r,
annote = {NULL},
author = {{University of Edinburgh}},
doi = {10.1145/365719.366426},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 1. Compiling techniques.pdf:pdf},
issn = {00010782},
title = {{1. Compiling techniques}},
volume = {9},
year = {2015}
}
@inproceedings{Gluck2009,
abstract = {The three classic Futamura projections stand as a cornerstone in the development of partial evaluation. The observation by Futamura 1983, that compiler generators produced by his third projection are self-generating, and the insight by Klimov and Romanenko 1987, that Futamura's abstraction scheme can be continued beyond the three projections, are systematically investigated, and several new applications for compiler generators are proposed. Possible applications include the generation of quasi-online compiler generators and of compiler generators for domain-specific languages, and the bootstrapping of compiler generators from program specializers. From a theoretical viewpoint, there is equality between the class of self-generating compiler generators and the class of compiler generators produced by the third Futamura projection. This exposition may lead to new practical applications of compiler generators, as well as deepen our theoretical understanding of program specialization.},
annote = {NULL},
author = {Gl{\"{u}}ck, R},
booktitle = {PEPM},
doi = {10.1145/1480945.1480954},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Gl{\"{u}}ck - Is there a fourth Futamura projection.pdf:pdf},
isbn = {9781605583273},
keywords = {bootstrapping,cogen approach,compiler generators,domain-specific languages,futamura projections,generation,generator self-,program specialization,self-application},
title = {{Is there a fourth Futamura projection?}},
url = {http://dl.acm.org/citation.cfm?id=1480954},
year = {2009}
}
@inproceedings{Parallelism2005,
annote = {NULL},
author = {Parallelism, Bulk Synchronous and Prediction, Performance},
booktitle = {ICCS},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Parallelism, Prediction - Bulk Synchronous Parallel ML Modular Implementation and Performance Prediction.pdf:pdf},
title = {{Bulk Synchronous Parallel ML: Modular Implementation and Performance Prediction}},
year = {2005}
}
@inproceedings{Graves2013a,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1303.5778},
author = {Graves, A. and Mohamed, A. and Hinton, G.},
booktitle = {ICASSP},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {1303.5778},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Graves, Mohamed, Hinton - Speech Recognition with Deep Recurrent Neural Networks.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
pmid = {27295638},
publisher = {IEEE},
title = {{Speech Recognition with Deep Recurrent Neural Networks}},
year = {2013}
}
@inproceedings{Cummins2016c,
author = {Cummins, C. and Petoumenos, P. and Steuwer, M. and Leather, H.},
booktitle = {ACACES},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Cummins et al. - Autotuning OpenCL Workgroup Sizes.pdf:pdf},
keywords = {GPUs,authotuning,machine learning,opencl,stencil codes,synthetic benchmarking},
title = {{Autotuning OpenCL Workgroup Sizes}},
year = {2016}
}
@article{Donahue2016,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to "linearize semantics" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, J. and Kr{\"{a}}henb{\"{u}}hl, P. and Darrell, T.},
eprint = {1605.09782},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Donahue, Kr{\"{a}}henb{\"{u}}hl, Darrell - Adversarial Feature Learning.pdf:pdf},
journal = {arXiv:1605.09782},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2016}
}
@inproceedings{Bakker2001,
author = {Bakker, B.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Bakker - Reinforcement Learning with Long Short-Term Memory.pdf:pdf},
title = {{Reinforcement Learning with Long Short-Term Memory}},
url = {https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.pdf},
year = {2001}
}
@inproceedings{Heo2017,
abstract = {—We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use a machine learning technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.},
author = {Heo, K. and Oh, H. and Yi, K.},
booktitle = {ICSE},
doi = {10.1109/ICSE.2017.54},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Heo, Oh, Yi - Machine-Learning-Guided Selectively Unsound Static Analysis(2).pdf:pdf},
isbn = {978-1-5386-3868-2},
keywords = {-static analysis,machine learning,unsoundness},
title = {{Machine-Learning-Guided Selectively Unsound Static Analysis}},
year = {2017}
}
@inproceedings{Wilson1995,
abstract = {This paper proposes an efficient technique for context-sensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using partial transfer functions. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful—a single PTF per procedure is usually sufficient to obtain completely context-sensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the high-level type system, our algorithm is based on a low-level representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks.},
annote = {The paper uses partial transfer functions to summarize th effects of procedures, which describe the behaviour of procedures given certain alias relationships.


Cited by 666.},
author = {Wilson, R. P. and Lam, M. S.},
booktitle = {PLDI},
doi = {10.1145/223428.207111},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Wilson, Lam - Efficient context-sensitive pointer analysis for C programs.pdf:pdf},
isbn = {0897916972},
issn = {03621340},
number = {6},
publisher = {ACM},
title = {{Efficient context-sensitive pointer analysis for C programs}},
volume = {30},
year = {1995}
}
@article{Conneau2016,
abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.},
annote = {NULL},
author = {Conneau, A. and Schwenk, H. and Lecun, Y. and Barrault, L.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Conneau et al. - Very Deep Convolutional Networks for Natural Language Processing.pdf:pdf},
journal = {arXiv:16006.01781},
title = {{Very Deep Convolutional Networks for Natural Language Processing}},
year = {2016}
}
@misc{Silver2015h,
abstract = {Lecture of University College London.},
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 5 Model-Free Control.pdf:pdf},
isbn = {9780195306613},
title = {{Lecture 5 : Model-Free Control}},
year = {2015}
}
@article{Otoom2015,
annote = {NULL},
author = {Otoom, Mwaffaq and Jordan, Irbid and Jordan, Irbid and Jordan, Irbid},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Otoom et al. - Scalable and Dynamic Global Power Management for Multicore Chips.pdf:pdf},
isbn = {9781450333436},
title = {{Scalable and Dynamic Global Power Management for Multicore Chips}},
year = {2015}
}
@article{Pivirotto1993,
annote = {NULL},
author = {Holzmann, Gerald J},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Holzmann - The Power of Ten - Rules for Developing Safety Critical Code.pdf:pdf},
journal = {NASA},
number = {May},
title = {{The Power of Ten - Rules for Developing Safety Critical Code}},
year = {1993}
}
@inproceedings{Danalis2010,
abstract = {Scalable heterogeneous computing systems, which are composed of a mix of compute devices, such as commodity multicore processors, graphics processors, reconfigurable processors, and others, are gaining attention as one approach to continuing performance improvement while managing the new challenge of energy efficiency. As these systems become more common, it is important to be able to compare and contrast architectural designs and programming systems in a fair and open forum. To this end, we have designed the Scalable HeterOgeneous Computing benchmark suite (SHOC). SHOC's initial focus is on systems containing graphics processing units (GPUs) and multi-core processors, and on the new OpenCL programming standard. SHOC is a spectrum of programs that test the performance and stability of these scalable heterogeneous computing systems. At the lowest level, SHOC uses microbenchmarks to assess architectural features of the system. At higher levels, SHOC uses application kernels to determine system-wide performance including many system features such as intranode and internode communication among devices. SHOC includes benchmark implementations in both OpenCL and CUDA in order to provide a comparison of these programming models.},
annote = {NULL},
author = {Danalis, A. and Marin, G. and McCurdy, C. and Meredith, J. S. and Roth, P. C. and Spafford, K. and Tipparaju, V. and Vetter, J. S.},
booktitle = {GPGPU},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Danalis et al. - The Scalable HeterOgeneous Computing (SHOC) Benchmark Suite.pdf:pdf},
keywords = {Benchmarking,GPGPU,Graphics Processors,Performance},
publisher = {ACM},
title = {{The Scalable HeterOgeneous Computing (SHOC) Benchmark Suite}},
year = {2010}
}
@article{DeBoom2018,
abstract = {Recurrent neural networks are nowadays successfully used in an abundance of applications, going from text, speech and image processing to recommender systems. Backpropagation through time is the algorithm that is commonly used to train these networks on specific tasks. Many deep learning frameworks have their own implementation of training and sampling procedures for recurrent neural networks, while there are in fact multiple other possibilities to choose from and other parameters to tune. In existing literature this is very often overlooked or ignored. In this paper we therefore give an overview of possible training and sampling schemes for character-level recurrent neural networks to solve the task of predicting the next token in a given sequence. We test these different schemes on a variety of datasets, neural network architectures and parameter settings, and formulate a number of take-home recommendations. The choice of training and sampling scheme turns out to be subject to a number of trade-offs, such as training stability, sampling time, model performance and implementation effort, but is largely independent of the data. Perhaps the most surprising result is that transferring hidden states for correctly initializing the model on subsequences often leads to unstable training behavior depending on the dataset.},
author = {{De Boom}, C. and Dhoedt, B. and Demeester, T.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - De Boom, Dhoedt, Demeester - Character-level Recurrent Neural Networks in Practice Comparing Training and Sampling Schemes.pdf:pdf},
journal = {arXiv:1801.00632},
keywords = {backpropagation through time,deep learning,optimization,performance,recurrent neural networks},
title = {{Character-level Recurrent Neural Networks in Practice: Comparing Training and Sampling Schemes}},
year = {2018}
}
@article{Kamp2012,
abstract = {Quality happens only when someone is responsible for it.},
annote = {Hacker news discussion: https://news.ycombinator.com/item?id=12251323},
author = {Kamp, Poul-Henning},
doi = {10.1145/2347736.2347752},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kamp - A generation lost in the bazaar.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
title = {{A generation lost in the bazaar}},
volume = {55},
year = {2012}
}
@article{Aho1976,
abstract = {This paper discusses algorithms which transform expression trees into code for register machines. A necessary and sufficient condition for optimality of such an algorithm is derived, which applies to a broad class of machines. A dynamic programming algorithm is then presented which produces optimal code for any machine in this class; this algorithm runs in time linearly proportional to the size of the input.},
annote = {NULL},
author = {Aho, A. V. and Johnson, S. C.},
doi = {10.1145/321958.321970},
file = {:Users/cec/Google Drive/Mendeley Library/1976 - Aho, Johnson - Optimal Code Generation for Expression Trees.pdf:pdf},
isbn = {0-89791-252-7},
issn = {00045411},
journal = {JACM},
keywords = {algorithms,and phrases code generation,expressions,optlmahty,register machine},
number = {3},
title = {{Optimal Code Generation for Expression Trees}},
url = {http://portal.acm.org/citation.cfm?doid=321958.321970},
volume = {23},
year = {1976}
}
@inproceedings{Amaris2015,
abstract = {—Models are useful to represent abstractions of soft-ware and hardware processes. The Bulk Synchronous Parallel (BSP) is a bridging model for parallel computation that allows algorithmic analysis of programs on parallel computers using performance modeling. The main idea of BSP model is the treatment of communication and computation as abstractions of a parallel system. Meanwhile, the use of GPU devices are becoming more widespread and they are currently capable of performing efficient parallel computation for applications that can be decomposed on thousands of simple threads. However, few models for predicting application execution time on GPUs have been proposed. In this work we present a simple and intuitive BSP-based model for predicting the CUDA application execution times on GPUs. The model is based on the number of computations and memory accesses of the GPU, with additional information on cache usage obtained from profiling. Scalability, divergence, effect of optimizations and differences of architectures are adjusted by a single parameter. We evaluated our model using two applications and six different boards. We showed by using profile information for a single board, that the model is general enough to predict the execution time of an application with different input sizes and on different boards with the same architecture. Our model predictions were within 0.8 to 1.2 times the measured execution times, which are reasonable for such a simple model. These results indicate that the model is good enough to generalize the predictions for different problem sizes and GPU configurations.},
annote = {NULL},
author = {Amar{\'{i}}s, M. and Cordeiro, D. and Goldman, A. and {De Camargo}, R. Y.},
booktitle = {HiPC},
doi = {10.1109/HiPC.2015.34},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Amar{\'{i}}s et al. - A Simple BSP-based Model to Predict Execution Time in GPU Applications.pdf:pdf},
isbn = {978-1-4673-8488-9},
keywords = {BSP model,CUDA,GPGPU,Kepler Architecture,Performance Prediction},
title = {{A Simple BSP-based Model to Predict Execution Time in GPU Applications}},
year = {2015}
}
@incollection{Harris2007a,
abstract = {Parallel prefix sum, also known as parallel Scan, is a useful building block for many parallel algorithms including sorting and building data structures. In this document we introduce Scan and describe step-by-step how it can be implemented efficiently in NVIDIA CUDA. We start with a basic na{\"{i}}ve algorithm and proceed through more advanced techniques to obtain best performance. We then explain how to scan arrays of arbitrary size that cannot be processed with a single block of threads.},
annote = {NULL},
author = {Harris, Mark and Sengupta, Shubhabrata and Owens, John D.},
booktitle = {GPU Gems 3},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Harris, Sengupta, Owens - Parallel Prefix Sum (Scan) with CUDA Mark.pdf:pdf},
isbn = {9780321515261},
title = {{Parallel Prefix Sum (Scan) with CUDA Mark}},
url = {http://dl.acm.org/citation.cfm?id=1407436},
year = {2007}
}
@inproceedings{Xiao,
annote = {NULL},
author = {Xiao, W. and Zhao, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Xiao, Zhao - Parallelizing a Discrete Event Simulation Application Using the Habanero-Java Multicore Library Categories and Subje.pdf:pdf},
isbn = {9781450334044},
title = {{Parallelizing a Discrete Event Simulation Application Using the Habanero-Java Multicore Library Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Newburn2011,
abstract = {Our ability to create systems with large amount of hardware parallelism is exceeding the average software developer's ability to effectively program them. This is a problem that plagues our industry. Since the vast majority of the world's software developers are not parallel programming experts, making it easy to write, port, and debug applications with sufficient core and vector parallelism is essential to enabling the use of multi- and many-core processor architectures. However, hardware architectures and vector ISAs are also shifting and diversifying quickly, making it difficult for a single binary to run well on all possible targets. Because of this, retargetability and dynamic compilation are of growing relevance. This paper introduces Intel{\textregistered} Array Building Blocks (ArBB), which is a retargetable dynamic compilation framework. This system focuses on making it easier to write and port programs so that they can harvest data and thread parallelism on both multi-core and heterogeneous many-core architectures, while staying within standard C++. ArBB interoperates with other programming models to help meet the demands we hear from customers for a solution with both greater programmer productivity and good performance. This work makes contributions in language features, compiler architecture, code transformations and optimizations. It presents performance data from the current beta release of ArBB and quantitatively shows the impact of some key analyses, enabling transformations and optimizations for a variety of benchmarks that are of interest to our customers.},
annote = {NULL},
author = {Newburn, C. J. and So, B. and Liu, Z. and McCool, M. and Ghuloum, A. and Toit, S. D. and Wang, Z. G. and Du, Z. H. and Chen, Y. and Wu, G.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Newburn et al. - Intel's Array Building Blocks A retargetable, dynamic compiler and embedded language.pdf:pdf},
publisher = {IEEE},
title = {{Intel's Array Building Blocks: A retargetable, dynamic compiler and embedded language}},
year = {2011}
}
@book{Cooper2011,
annote = {NULL},
author = {Cooper, K. and Torczon, L.},
publisher = {Elsevier},
title = {{Engineering a compiler}},
year = {2011}
}
@inproceedings{Cavazos2007,
abstract = {Applying the right compiler optimizations to a particular program can have a significant impact on program per- formance. Due to the non-linear interaction of compiler optimizations, however, determining the best setting is non- trivial. There have been several proposed techniques that search the space of compiler options to find good solutions; however such approaches can be expensive. This paper pro- poses a different approach using performance counters as a means of determining good compiler optimization settings. This is achieved by learning amodel off-linewhich can then be used to determine good settings for any new program. We show that such an approach outperforms the state-of- the-art and is two orders of magnitude faster on average. Furthermore, we show that our performance counter-based approach outperforms techniques based on static code fea- tures. Using our technique we achieve a 17{\%} improve- ment over the highest optimization setting of the commer- cial PathScale EKOPath 2.3.1 optimizing compiler on the SPEC benchmark suite on a recent AMD Athlon 64 3700+ platform.},
annote = {NULL},
author = {Cavazos, J. and Fursin, G. and Agakov, F. and Bonilla, E. and O'Boyle, M. and Temam, O.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Cavazos et al. - Rapidly selecting good compiler optimizations using performance counters.pdf:pdf},
publisher = {IEEE},
title = {{Rapidly selecting good compiler optimizations using performance counters}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4145114},
year = {2007}
}
@inproceedings{Fidel2016,
annote = {NULL},
author = {Fidel, A. and Sabido, F. C. and Riedel, C. and Amato, N. M. and Rauchwerger, L.},
booktitle = {LCPC},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Fidel et al. - Fast Approximate Distance Queries in Unweighted Graphs using Bounded Asynchrony.pdf:pdf},
keywords = {approximate algorithms,asynchronous,breadth-first search,distance query,distributed memory,parallel graph algorithms},
title = {{Fast Approximate Distance Queries in Unweighted Graphs using Bounded Asynchrony}},
year = {2016}
}
@article{Greff2015,
abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1503.04069},
author = {Greff, K. and Srivastava, R. K. and Koutn{\'{i}}k, J. and Steunebrink, B. R. and Schmidhuber, J.},
doi = {10.1017/CBO9781107415324.004},
eprint = {1503.04069},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Greff et al. - LSTM A Search Space Odyssey.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {arXiv:1503.04069},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
pmid = {25246403},
title = {{LSTM: A Search Space Odyssey}},
url = {http://arxiv.org/abs/1503.04069},
year = {2015}
}
@article{Yi2016,
abstract = {We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1603.09114},
author = {Yi, K. M. and Trulls, E. and Lepetit, V. and Fua, P.},
eprint = {1603.09114},
keywords = {deep learning,feature descriptors,local features},
title = {{LIFT: Learned Invariant Feature Transform}},
url = {http://arxiv.org/abs/1603.09114},
year = {2016}
}
@inproceedings{Beard2015a,
annote = {NULL},
author = {Beard, J. C. and Li, P. and Chamberlain, R. D.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Beard, Li, Chamberlain - RaftLib A C Template Library for High Performance Stream Parallel Processing.pdf:pdf},
isbn = {9781450334044},
title = {{RaftLib: A C ++ Template Library for High Performance Stream Parallel Processing}},
year = {2015}
}
@article{Zhao2010,
abstract = {Profiling and online analysis are important tasks in program understanding and feedback-directed optimization. However, fine-grained profiling and online analysis tend to seriously slow down the application. To cope with the slowdown, one may have to terminate the process early or resort to sampling. The former tends to distort the result because of warm-up effects. The latter runs the risk of missing important effects because sampling was turned off during the time that these effects appeared. A promising approach is to make use of the parallel processing capabilities of the now ubiquitousmulticore processors to speed up the profiling and analysis process. In this article, we present Pipelined Profiling and Analysis (PiPA), which is a novel technique for parallelizing dynamic program profiling and analysis by taking advantage of multicore systems. In essence, the application under examination is profiled using a dynamic instrumentation tool. Optimized instrumentation code outputs the profile information in a succinct format, that we call the REP format, to buffers.This lightweight trace compression minimizes the processing overhead impinged on the applicationwhenever a buffer is full. Another thread recovers the required information from the REP buffer. The recovered full profile is then divided up and passed to multiple threads for further analysis. To achieve the best performance, the entire system has to be well-balanced. We have implemented prototypes of PiPA using two dynamic instrumentation systems, namely DynamoRIO and Pin, thereby demonstrating its portability. Our experiments show that PiPA is able to speed up the overall profiling and analysis tasks significantly. Compared to the more than 100× slowdown of Cachegrind and the 32× slowdown of Pin dcache, we achieved a mere 10.2× slowdown on an 8-core system. In this paper, we will also describe the insights we gained in obtaining the balance needed for PiPA to perform optimally.},
annote = {NULL},
author = {Zhao, Q. and Cutcutache, I. and Wong, W.},
doi = {10.1145/1880037.1880038},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Zhao, Cutcutache, Wong - PiPA Pipelined profiling and analysis on multicore systems.pdf:pdf},
issn = {15443566},
journal = {TACO},
keywords = {Design,Experimentation,Performance},
month = {dec},
number = {3},
title = {{PiPA: Pipelined profiling and analysis on multicore systems}},
url = {http://portal.acm.org/citation.cfm?doid=1880037.1880038},
volume = {7},
year = {2010}
}
@inproceedings{Berral2010a,
abstract = {As energy-related costs have become a major economical factor for IT infrastructures and data-centers, companies and the research community are being challenged to find better and more efficient power-aware resource management strategies. There is a growing interest in "Green" IT and there is still a big gap in this area to be covered.},
annote = {Cited by 127.},
author = {Berral, Josep Ll and Goiri, {\'{I}}{\~{n}}igo and Nou, Ram{\'{o}}n and Juli{\`{a}}, Ferran and Guitart, Jordi and Gavald{\`{a}}, Ricard and Torres, Jordi},
booktitle = {e-Energy},
doi = {10.1145/1791314.1791349},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Berral et al. - Towards energy-aware scheduling in data centers using machine learning.pdf:pdf},
isbn = {978-1-4503-0042-1},
keywords = {data center,machine learning,power efficiency,scheduling},
title = {{Towards energy-aware scheduling in data centers using machine learning}},
url = {http://portal.acm.org/citation.cfm?doid=1791314.1791349},
year = {2010}
}
@inproceedings{Alnaeli2012a,
annote = {Cited by 4.},
author = {Alnaeli, S. M. and Maletic, J. I.},
booktitle = {WCRE},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Alnaeli, Maletic - Empirically Examining the Parallelizability of Open Source Software Systems.pdf:pdf},
keywords = {Parallelization inhibitors,data dependency,emprical,function calls,reegnineering,study},
title = {{Empirically Examining the Parallelizability of Open Source Software Systems}},
year = {2012}
}
@inproceedings{Ghiya2001,
abstract = {In this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel{\&}reg; Itanium{\&}trade; processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average points-to set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation techniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor.},
annote = {Pointer anlaysis aims to enable more accurate disambiguation of pointer-based indirect memory references. Cited by 89.},
author = {Ghiya, Rakesh and Lavery, Daniel and Sehr, David},
booktitle = {PLDI},
doi = {10.1145/381694.378806},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Ghiya, Lavery, Sehr - On the importance of points-to analysis and other memory disambiguation methods for C programs.pdf:pdf},
isbn = {1581134142},
issn = {03621340},
number = {5},
title = {{On the importance of points-to analysis and other memory disambiguation methods for C programs}},
volume = {36},
year = {2001}
}
@inproceedings{Hong2017,
author = {Hong, C. and Sukumaran-rajam, A. and Kim, J. and Sadayappan, P.},
booktitle = {PACT},
doi = {10.1109/PACT.2017.48},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Hong et al. - MultiGraph Efficient Graph Processing on GPUs.pdf:pdf},
isbn = {9781509067640},
title = {{MultiGraph: Efficient Graph Processing on GPUs}},
year = {2017}
}
@inproceedings{Zhang2015a,
abstract = {Developers often wonder how to implement a certain func- tionality (e.g., how to parse XML files) using APIs. Obtain- ing an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bags-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.},
annote = {NULL},
author = {Gu, X. and Zhang, H. and Zhang, D. and Kim, S.},
booktitle = {FSE},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Gu et al. - Deep API Learning.pdf:pdf},
keywords = {API,API usage,RNN,code search,deep learning},
publisher = {ACM},
title = {{Deep API Learning}},
year = {2016}
}
@inproceedings{Wimmer,
annote = {NULL},
author = {Wimmer, Martin},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Wimmer - The Lock-Free k -LSM Relaxed Priority Queue.pdf:pdf},
isbn = {9781450332057},
keywords = {concur-,priority-queue,rent data structure relaxation,shared memory,task-parallel programming},
title = {{The Lock-Free k -LSM Relaxed Priority Queue}},
year = {2015}
}
@article{Kotsiantis2007,
annote = {NULL},
author = {Kotsiantis, S. B. and Zaharakis, I. and Pintelas, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Kotsiantis, Zaharakis, Pintelas - Supervised Machine Learning A Review of Classification Techniques.pdf:pdf},
issn = {03505596},
journal = {Informatica},
title = {{Supervised Machine Learning: A Review of Classification Techniques}},
volume = {31},
year = {2007}
}
@techreport{Rovatsou2015,
annote = {NULL},
author = {{Khronos OpenCL Group Inc}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Khronos OpenCL Group Inc - SYCL Specification Vesion 1.2.pdf:pdf},
title = {{SYCL Specification Vesion 1.2}},
year = {2015}
}
@inproceedings{Cadar2008,
abstract = {We present a new symbolic execution tool, KLEE, ca- pable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed onmillions of Unix sys- tems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage—on average over 90{\%} per tool (median: over 94{\%}) — and significantly beat the coverage of the developers' own hand-written test suites. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100{\%}coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to cross-check purportedly identical BUSY- BOX and COREUTILS utilities, finding functional cor- rectness errors and a myriad of inconsistencies.},
annote = {NULL},
author = {Cadar, C. and Dunbar, D. and Engler, D.},
booktitle = {OSDI},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Cadar, Dunbar, Engler - KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs.pdf:pdf},
title = {{KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs}},
year = {2008}
}
@article{Wu2016a,
abstract = {We study, for the first time, automated inference on criminality based solely on still face images. Via supervised machine learning, we build four classifiers (logistic regression, KNN, SVM, CNN) using facial images of 1856 real persons controlled for race, gender, age and facial expressions, nearly half of whom were convicted criminals, for discriminating between criminals and non-criminals. All four classifiers perform consistently well and produce evidence for the validity of automated face-induced inference on criminality, despite the historical controversy surrounding the topic. Also, we find some discriminating structural features for predicting criminality, such as lip curvature, eye inner corner distance, and the so-called nose-mouth angle. Above all, the most important discovery of this research is that criminal and non-criminal face images populate two quite distinctive manifolds. The variation among criminal faces is significantly greater than that of the non-criminal faces. The two manifolds consisting of criminal and non-criminal faces appear to be concentric, with the non-criminal manifold lying in the kernel with a smaller span, exhibiting a law of normality for faces of non-criminals. In other words, the faces of general law-biding public have a greater degree of resemblance compared with the faces of criminals, or criminals have a higher degree of dissimilarity in facial appearance than normal people.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1611.04135},
author = {Wu, X. and Zhang, X.},
eprint = {1611.04135},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Wu, Zhang - Automated Inference on Criminality using Face Images.pdf:pdf},
journal = {arXiv:1611.04135},
title = {{Automated Inference on Criminality using Face Images}},
url = {http://arxiv.org/abs/1611.04135},
year = {2016}
}
@article{Henkel,
author = {Henkel, J. and Liblit, B. and Lahiri, S. and Reps, T.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Henkel et al. - Code Vectors Understanding Programs Through Embedded Abstracted Symbolic Traces.pdf:pdf},
journal = {arXiv:1803.06686},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces}}
}
@article{Rossum2014,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Argparse Tutorial.pdf:pdf},
title = {{Argparse Tutorial}},
year = {2016}
}
@article{Remmelg2016,
annote = {NULL},
author = {Remmelg, T. and Lutz, T. and Steuwer, M. and Dubach, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Remmelg et al. - Performance Portable GPU Code Generation for Matrix Multiplication.pdf:pdf},
keywords = {code generation,gpu,high-level parallel programming,matrix multiplication,performance portability},
title = {{Performance Portable GPU Code Generation for Matrix Multiplication}},
year = {2016}
}
@misc{Allen1987,
abstract = {An algorithm for transforming sequential programs into equivalent parallel programs is presented. The method concentrates on finding loops whose separate iterations can be run in parallel without synchronization. Although a simple version of the method can be shown to be optimal, the problem of generating optimal code when loop interchange is employed is shown to be intractable. These methods are implemented in an experimental translation system developed at Rice University.},
annote = {Cited by 210.},
author = {Allen, R. and Callahan, D. and Kennedy, K.},
booktitle = {POPL},
doi = {10.1145/41625.41631},
file = {:Users/cec/Google Drive/Mendeley Library/1987 - Allen, Callahan, Kennedy - Automatic Decomposition of Scientific Programs for Parallel Execution.pdf:pdf},
isbn = {0-89791-215-2},
title = {{Automatic Decomposition of Scientific Programs for Parallel Execution}},
url = {http://portal.acm.org/citation.cfm?doid=41625.41631{\%}5Cnhttp://dl.acm.org/citation.cfm?id=41631},
year = {1987}
}
@article{Collins2012a,
annote = {NULL},
author = {Collins, A. and Fensch, C. and Leather, H.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Collins, Fensch, Leather - Optimization Space Exploration of the FastFlow Parallel Skeleton Framework.pdf:pdf},
keywords = {algorithmic skeletons,multicore,optimization space exploration,performance measurement},
title = {{Optimization Space Exploration of the FastFlow Parallel Skeleton Framework}},
year = {2012}
}
@inproceedings{Cummins2016a,
abstract = {Selecting an appropriate workgroup size is critical for the performance of OpenCL kernels, and requires knowledge of the underlying hardware, the data being operated on, and the implementation of the kernel. This makes portable performance of OpenCL programs a challenging goal, since simple heuristics and statically chosen values fail to exploit the available performance. To address this, we propose the use of machine learning-enabled autotuning to automatically predict workgroup sizes for stencil patterns on CPUs and multi-GPUs. We present three methodologies for predicting workgroup sizes. The first, using classifiers to select the optimal workgroup size. The second and third proposed methodologies employ the novel use of regressors for performing classification by predicting the runtime of kernels and the relative performance of different workgroup sizes, respectively. We evaluate the effectiveness of each technique in an empirical study of 429 combinations of architecture, kernel, and dataset, comparing an average of 629 different workgroup sizes for each. We find that autotuning provides a median 3.79x speedup over the best possible fixed workgroup size, achieving 94{\%} of the maximum performance.},
archivePrefix = {arXiv},
arxivId = {1511.02490},
author = {Cummins, C. and Petoumenos, P. and Steuwer, M. and Leather, H.},
booktitle = {ADAPT},
eprint = {1511.02490},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Cummins et al. - Autotuning OpenCL Workgroup Size for Stencil Patterns.pdf:pdf},
title = {{Autotuning OpenCL Workgroup Size for Stencil Patterns}},
url = {http://arxiv.org/abs/1511.02490},
year = {2015}
}
@article{Frees2015,
abstract = {While there are many server-side frameworks available, node.js is unique in its ability to help solve well-recognized challenges in teaching web development. Using node.js allows for the consolidation of language throughout the application stack. The platform supports a smooth learning curve by allowing students to build knowledge gradually through the use of modular, open source components. This paper details the advantages of using node.js in an undergraduate web development course and examines its value to the CS curriculum as a whole.},
annote = {NULL},
author = {Frees, Scott},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Frees - A place for Node.js in the computer science curriculum.pdf:pdf},
journal = {Journal of Computing Sciences in Colleges},
number = {3},
title = {{A place for Node.js in the computer science curriculum}},
volume = {30},
year = {2015}
}
@article{Padua1986,
annote = {Cited by 890.},
author = {Padua, David a. and Wolfe, Michael J.},
doi = {10.1145/7902.7904},
file = {:Users/cec/Google Drive/Mendeley Library/1986 - Padua, Wolfe - Advanced compiler optimizations for supercomputers.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {12},
title = {{Advanced compiler optimizations for supercomputers}},
volume = {29},
year = {1986}
}
@inproceedings{Theis2015a,
abstract = {A systematic description of low-energy observables in light nuclei is presented. The effective field theory formalism without pions is extended to: i) predictions with next-to-leading-order (non-perturbatively) accuracy for the 4-helium binding energy B({\{}$\backslash$alpha{\}}), the triton charge radius, and the 3-helium-neutron scattering length; ii) phase shifts for neutron-deuteron scattering and {\{}$\backslash$alpha{\}}-neutron low-energy scattering at leading order; iii) the ground states of the 5-helium (with and without Coulomb interaction) and 6-helium isotopes up to next-to-leading order; The convergence from leading- to next-to-leading order of the theory is demonstrated for correlations between: i) the triton binding energy B(t) and the triton charge radius; ii) B(t) and the 4-helium binding energy B({\{}$\backslash$alpha{\}}); Furthermore, a correlation between B(t) and the scattering length in the singlet S-wave channel of neutron-helium-3 scattering is discovered, and a model-independent estimate for the trinucleon binding energy splitting is provided. The results provide evidence for the usefulness of the applied power-counting scheme, treating next-to-leading-order interactions nonperturbatively and four-nucleon interactions as, at least, one order higher. The 5- and 6-helium ground states are analyzed with a power-counting scheme which includes the momentum-dependent next-to-leading order vertices perturbatively. All calculations include a full treatment of the Coulomb interaction. The assessment of numerical uncertainties associated with the solution of the few-body equation of motion through the Resonating Group Method parallels the report of the results for light nuclei in order to establish this method as practical for the analysis of systems with up to six particles interacting via short-range interactions.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03478v1},
author = {Theis, L. and Bethge, M.},
booktitle = {NIPS},
eprint = {arXiv:1506.03478v1},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Theis, Bethge - Generative Image Modeling Using Spatial LSTMs.pdf:pdf},
title = {{Generative Image Modeling Using Spatial LSTMs}},
year = {2015}
}
@inproceedings{Dubach2007,
abstract = {Performance tuning is an important and time consuming task which may have to be repeated for each new application and platform. Although iterative optimisation can automate this process, it still requires many executions of different ver-sions of the program. As execution time is frequently the limiting factor in the number of versions or transformed pro-grams that can be considered, what is needed is a mechanism that can automatically predict the performance of a modi-fied program without actually having to run it. This paper presents a new machine learning based technique to auto-matically predict the speedup of a modified program using a performance model based on the code features of the tuned programs. Unlike previous approaches it does not require any prior learning over a benchmark suite. Furthermore, it can be used to predict the performance of any tuning and is not restricted to a prior seen transformation space. We show that it can deliver predictions with a high correlation coefficient and can be used to dramatically reduce the cost of search.},
annote = {This paper uses a machine learning based model to predict the speedup of a modified program based on code features, without prior learning.},
author = {Dubach, C. and Cavazos, J. and Franke, B. and Fursin, G. and O'Boyle, M. and Temam, O.},
booktitle = {CF},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Dubach et al. - Fast Compiler Optimisation Evaluation Using Code-Feature Based Performance Prediction.pdf:pdf},
keywords = {Architecture,Artificial Neural Networks,Compiler optimisation,Machine learning,Performance Modelling},
publisher = {ACM},
title = {{Fast Compiler Optimisation Evaluation Using Code-Feature Based Performance Prediction}},
year = {2007}
}
@article{Practice2002,
abstract = {Probably one of the most successful interfaces between operations research and computer science has been the development of discrete-event simulation software. The recent integration of optimization techniques into simulation practice, specifically into commercial software, has become nearly ubiquitous, as most discrete-event simulation packages now include some form of "optimization" routine. The main thesis of this article, however, is that there is a disconnect between research in simulation optimization-which has addressed the stochastic nature of discrete-event simulation by concentrating on theoretical results of convergence and specialized algorithms that are mathematically elegant-and the recent software developments, which implement very general algorithms adopted from techniques in the deterministic optimization metaheuristic literature (e.g,, genetic algorithms, tabu search, artificial neural networks), A tutorial exposition that summarizes the approaches found in the research literature is included, as well as a discussion contrasting these approaches with the algorithms implemented in commercial software. The article concludes with the author's speculations on promising research areas and possible future directions in practice.},
annote = {Cited by 800.},
author = {Fu, Michael C},
doi = {10.1287/educ.1080.0050},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Fu - Optimization for Simulation Theory vs. Practice.pdf:pdf},
isbn = {9781877640230},
issn = {10919856},
journal = {IJOC},
pmid = {22016214},
title = {{Optimization for Simulation: Theory vs. Practice}},
url = {http://dx.doi.org/10.1145/268437.268460},
volume = {14},
year = {2002}
}
@inproceedings{Carpen-amarie,
abstract = {In the last few years, managed runtime environments such as the Java Virtual Machine (JVM) are increasingly used on large-scale multicore servers. The garbage collector (GC) represents a critical component of the JVMand has a signif- icant influence on the overall performance and efficiency of the running application. We perform a study on all available Java GCs, both in an academic environment (set of bench- marks), as well as in a simulated real-life situation (client- server application). We mainly focus on the three most widely used collectors: ParallelOld, ConcurrentMarkSweep and G1. We find that they exhibit different behaviours in the two tested environments. In particular, the default Java GC, ParallelOld, proves to be stable and adequate in the first situation, while in the real-life scenario its use results in unacceptable pauses for the application threads. We be- lieve that this is partly due to the memory requirements of the multicore server. G1 GC performs notably bad on the benchmarks when forced to have a full collection between the iterations of the application. Moreover, even though G1 and ConcurrentMarkSweep GCs introduce significantly lower pauses than ParallelOld in the client-server environ- ment, they can still seriously impact the response time on the client. Pauses of around 3 seconds can make a real-time system unusable and may disrupt the communication be- tween nodes in the case of large-scale distributed systems.},
annote = {NULL},
author = {Carpen-amarie, Maria and Marlier, Patrick and Felber, Pascal and Thomas, Ga{\"{e}}l},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Carpen-amarie et al. - A Performance Study of Java Garbage Collectors on Multicore Architectures Categories and Subject Descripto.pdf:pdf},
isbn = {9781450334044},
keywords = {Performance analysis,garbage collection,multicore},
title = {{A Performance Study of Java Garbage Collectors on Multicore Architectures Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Faddegon2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Faddegon, Maarten and Chitil, Olaf},
booktitle = {PLDI},
doi = {10.1145/2908080.2908104},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Faddegon, Chitil - Lightweight computation tree tracing for lazy functional languages.pdf:pdf},
isbn = {9781450342612},
keywords = {algorithmic debugging,lazy evaluation,tracing},
title = {{Lightweight computation tree tracing for lazy functional languages}},
year = {2016}
}
@article{Bernstein2005,
annote = {NULL},
author = {Bernstein, D. J.},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Bernstein - Salsa20 design.pdf:pdf},
title = {{Salsa20 design}},
year = {2005}
}
@article{Kamil2010,
abstract = {Although stencil auto-tuning has shown tremendous potential in effectively utilizing architectural resources, it has hitherto been limited to single kernel instantiations; in addition, the large variety of stencil kernels used in practice makes this computation pattern difficult to assemble into a library. This work presents a stencil auto-tuning framework that significantly advances programmer productivity by automatically converting a straightforward sequential Fortran 95 stencil expression into tuned parallel implementations in Fortran, C, or CUDA, thus allowing performance portability across diverse computer architectures, including the AMD Barcelona, Intel Nehalem, Sun Victoria Falls, and the latest NVIDIA GPUs. Results show that our generalized methodology delivers significant performance gains of up to 22{\~{A}} speedup over the reference serial implementation. Overall we demonstrate that such domain-specific auto-tuners hold enormous promise for architectural efficiency, programmer productivity, performance portability, and algorithmic adaptability on existing and emerging multicore systems.},
annote = {Kamil presents an auto-tuning framework which accepts as input a Fortran 95 stencil expression, and generates tuned parallel implementations in Fortan, C, or CUDA. The system uses an IR to explore auto-tuning transformations, and has an SMP backend code generator. They demonstrate their system on 4 architectures using 3 benchmarks, with speedups of up to x22 over serial. The CUDA code generator *only uses global memory*. Also, there's no real search engine. They randomly enumerate a subset of the optimisation space, and then record only a *single execution time*, reporting the fastest. Cited by 127.},
author = {Kamil, S. and Chan, C. and Oliker, L. and Shall, J. and Williams, S.},
doi = {10.1109/IPDPS.2010.5470421},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Kamil et al. - An auto-tuning framework for parallel multicore stencil computations.pdf:pdf},
isbn = {9781424464432},
issn = {15302075},
journal = {IPDPS},
title = {{An auto-tuning framework for parallel multicore stencil computations}},
year = {2010}
}
@inproceedings{Ertel2018,
author = {Ertel, S. and Goens, A. and Adam, J. and Castrillon, J.},
booktitle = {CC},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ertel et al. - Compiling for Concise Code and Efficient I O.pdf:pdf},
isbn = {9781450356442},
keywords = {2018,acm reference format,and e,and jeronimo castril-,andr{\'{e}}s goens,cient i,compiling for concise code,concurrency,data,i,in pro-,justus adam,lon,o,ow,sebastian ertel},
title = {{Compiling for Concise Code and Efficient I / O}},
year = {2018}
}
@phdthesis{Atkin-granville2013,
abstract = {Contemporary computers are increasingly based around highly parallel architectures through chip multiprocessors, instruction-level parallelism and graphics processing units with potentially thousands of cores. Despite this, many popular programs are based around a sequential programming paradigm. This project investigates the use of the Graal compiler infrastructure in order to dynam- ically profile running programs within the Java Virtual Machine and determine which hot loops are good candidates for automatic parallelism transformations, possibly JIT recompilation to an OpenCL target. We show that, currently, Graal is not yet mature enough to support the transformations required. However, in the future, it will becomes mature enough and our framework is a simple ‘drop-in' to the Graal compilation procedure to add instrumentation to programs. We consider two main approaches to trace collection: exact approaches and probabilis- tic approaches based on bloom filters. A new benchmark has been created that allows for fine-tuning of the number and types of dependencies, as well as the amount of computation associated with each operation (in the form of a number of floating-point operations) which allows us to analyse the efficacy of each approach without the disadvantages of real-world benchmarks (i.e., where we cannot know ahead-of-time how many dependencies there are, and so cannot evaluate the performance of each approach). The findings presented by this dissertation show that not only is it feasible to use bloom filters for these kinds of analysis, but it also advantageous to do so over hash sets. With the correct parameters, bloom filters show a small memory use increase over uninstrumented programs, and a relatively small increase in the execution time of between a factor of 1.4 and 2 times slower. Our framework supports both full dependency analysis, which generates a full set of information regarding dependencies with loops, as well as disabling instrumentation when a single dependency has been detected.},
annote = {NULL},
author = {Atkin-granville, C. E.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Atkin-granville - Parallelism Detection using Dynamic Instrumentation in a Virtual Machine.pdf:pdf},
school = {University of Edinburgh},
title = {{Parallelism Detection using Dynamic Instrumentation in a Virtual Machine}},
year = {2013}
}
@phdthesis{Breiman1999,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
annote = {Cited by {\textgreater} 9000! + dated meme},
author = {Breiman, L.},
doi = {10.1023/A:1010933404324},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Breiman - Random forest.pdf:pdf},
isbn = {9781424444427},
issn = {0885-6125},
pmid = {20142443},
title = {{Random forest}},
year = {1999}
}
@article{Marr2014,
abstract = {With the rise of domain-specific languages (DSLs), research in language implementation techniques regains importance. While DSLs can help to manage the domain's complexity, it is rarely affordable to build highly optimizing compilers or virtual machines, and thus, performance remains an issue. Ideally, one would implement a simple interpreter and still reach acceptable performance levels. RPython and Truffle are two approaches that promise to facilitate language implementation based on simple interpreters, while reaching performance of the same order of magnitude as highly optimizing virtual machines. In this case study, we compare the two approaches to identify commonalities, weaknesses, and areas for further research to improve their utility for language implementations.},
annote = {NULL},
author = {Marr, S. and Pape, T. and Meuter, W. D.},
doi = {10.1109/MS.2014.98.http},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Marr, Pape, Meuter - Are We There Yet Simple Language-Implementation Techniques for the 21st Century.pdf:pdf},
journal = {IEEE Software},
keywords = {Compilers,Interpreters,Language Implementation,Virtual Machines},
number = {5},
title = {{Are We There Yet? Simple Language-Implementation Techniques for the 21st Century}},
volume = {31},
year = {2014}
}
@misc{UniversityofEdinburgh2014e,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 2. Coursework.pdf:pdf},
title = {{2. Coursework}},
year = {2015}
}
@inproceedings{Franke2005,
abstract = {Efficient implementation of DSP applications is critical for many embedded systems. Optimising C compilers for embedded proces- sors largely focus on code generation and instruction scheduling which, with their growing maturity, are providing diminishing re- turns. This paper empirically evaluates another approach, namely source-level transformations and the probabilistic feedback-driven search for “good” transformation sequences within a large optimi- sation space. This novel approach combines two selection methods: one based on exploring the optimisation space, the other focused on localised search of good areas. This technique was applied to the UTDSP benchmark suite on two digital signal and multimedia processors (Analog Devices TigerSHARC TS-101, Philips TriMe- dia TM-1100) and an embedded processor derived from a popular general-purpose processor architecture (Intel Celeron 400). On av- erage, our approach gave a factor of 1.71 times improvement across all platforms equivalent to an average 41{\%} reduction in execution time, outperforming existing approaches. In certain cases a speedup of up to ≈ 7 was found for individual benchmarks.},
annote = {NULL},
author = {Franke, B. and O'Boyle, M. and Thomson, J. and Fursin, G.},
booktitle = {LCTES},
doi = {10.1145/1065910.1065922},
isbn = {1595930183},
keywords = {Source-level optimization,adaptive compilation,digital signal processing,feedback-directed optimization,iterative compilation},
publisher = {ACM},
title = {{Probabilistic source-level optimisation of embedded programs}},
url = {http://portal.acm.org/citation.cfm?doid=1065910.1065922},
year = {2005}
}
@article{Pei2017,
abstract = {Deep learning (DL) systems are increasingly deployed in security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner-case inputs are of great importance. However, systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose different erroneous behaviors for rare inputs. We present DeepXplore, the first whitebox framework for systematically testing real-world DL systems. We address two problems: (1) generating inputs that trigger different parts of a DL system's logic and (2) identifying incorrect behaviors of DL systems without manual effort. First, we introduce neuron coverage for estimating the parts of DL system exercised by a set of test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles and thus avoid manual checking for erroneous behaviors. We demonstrate how finding inputs triggering differential behaviors while achieving high neuron coverage for DL algorithms can be represented as a joint optimization problem and solved efficiently using gradient-based optimization techniques. DeepXplore finds thousands of incorrect corner-case behaviors in state-of-the-art DL models trained on five popular datasets. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved 33.2{\%} higher neuron coverage on average than existing testing methods. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve classification accuracy or identify polluted training data.},
author = {Pei, K. and Cao, Y. and Yang, J. and Jana, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Pei et al. - DeepXplore Automated Whitebox Testing of Deep Learning Systems.pdf:pdf},
journal = {arXiv:1705.06640},
title = {{DeepXplore: Automated Whitebox Testing of Deep Learning Systems}},
year = {2017}
}
@misc{Pfenning2013,
author = {Pfenning, F.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Pfenning - Lecture Notes on Loop Optimizations.pdf:pdf},
title = {{Lecture Notes on Loop Optimizations}},
url = {https://www.cs.cmu.edu/{~}fp/courses/15411-f13/lectures/17-loopopt.pdf},
year = {2013}
}
@inproceedings{Wadler2009,
abstract = {We introduce the blame calculus, which adds the notion of blame from Findler and Felleisen's contracts to a system similar to Siek and Taha's gradual types and Flanagan's hybrid types. We characterise where positive and negative blame can arise by decomposing the usual notion of subtype into positive and negative subtypes, and show that these recombine to yield naive subtypes. Naive subtypes previously appeared in type systems that are unsound, but we believe this is the first time naive subtypes play a role in establishing type soundness.},
annote = {NULL},
author = {Wadler, Philip and Findler, Robert Bruce},
booktitle = {ESOP},
doi = {http://dx.doi.org/10.1007/978-3-642-00590-9_1},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Wadler, Findler - Well-typed programs can't be blamed.pdf:pdf},
isbn = {978-3-642-00589-3},
title = {{Well-typed programs can't be blamed}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-00590-9{\_}1},
year = {2009}
}
@article{Erhan2010,
abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5538v1},
author = {Erhan, D. and Courville, A. and Vincent, P.},
doi = {10.1145/1756006.1756025},
eprint = {arXiv:1206.5538v1},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Erhan, Courville, Vincent - Why Does Unsupervised Pre-training Help Deep Learning.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {JMLR},
number = {2007},
title = {{Why Does Unsupervised Pre-training Help Deep Learning ?}},
url = {http://portal.acm.org/citation.cfm?id=1756025},
volume = {11},
year = {2010}
}
@article{Zhang2016a,
abstract = {Synthesizing photo-realistic images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose stacked Generative Adversarial Networks (StackGAN) to generate photo-realistic images conditioned on text descriptions. The Stage-I GAN sketches the primitive shape and basic colors of the object based on the given text description, yielding Stage-I low resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high resolution images with photo-realistic details. The Stage-II GAN is able to rectify defects and add compelling details with the refinement process. Samples generated by StackGAN are more plausible than those generated by existing approaches. Importantly, our StackGAN for the first time generates realistic 256 x 256 images conditioned on only text descriptions, while state-of-the-art methods can generate at most 128 x 128 images. To demonstrate the effectiveness of the proposed StackGAN, extensive experiments are conducted on CUB and Oxford-102 datasets, which contain enough object appearance variations and are widely-used for text-to-image generation analysis.},
annote = {Text to image synthesis. Incredible!},
archivePrefix = {arXiv},
arxivId = {1612.03242},
author = {Zhang, H. and Xu, T. and Li, H. and Zhang, S. and Huang, X. and Wang, X. and Metaxas, D.},
eprint = {1612.03242},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zhang et al. - StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.pdf:pdf},
journal = {arXiv:1612.03242},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
year = {2016}
}
@inproceedings{Medeiros2007,
abstract = {The state of web security remains troubling as web appli-cations continue to be favorite targets of hackers. Static analysis tools are important mechanisms for programmers to deal with this problem as they search for vulnerabilities automatically in the application source code, allowing pro-grammers to remove them. However, developing these tools requires explicitly coding knowledge about how to discover each kind of vulnerability. This paper presents a new ap-proach in which static analysis tools learn to detect vulnera-bilities automatically using machine learning. The approach uses a sequence model to learn to characterize vulnerabili-ties based on a set of annotated source code slices. This model takes into consideration the order in which the code elements appear and are executed in the slices. The model created can then be used as a static analysis tool to discover and identify vulnerabilities in source code. The approach was implemented in the DEKANT tool and evaluated ex-perimentally with a set of open source PHP applications and WordPress plugins, finding 16 zero-day vulnerabilities.},
author = {Medeiros, I. and Neves, N. and Correia, M.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Medeiros, Neves, Correia - DEKANT A Static Analysis Tool that Learns to Detect Web Application Vulnerabilities.pdf:pdf},
keywords = {@BULLETComputing methodologies → Machine learning,Keywords vulnerabilities,Web application security,machine learning,sequence models,software security,static anal-ysis,web application},
title = {{DEKANT: A Static Analysis Tool that Learns to Detect Web Application Vulnerabilities}},
year = {2016}
}
@inproceedings{Stephenson2003,
abstract = {Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Find- ing a heuristic that performs well on a broad range of ap- plications is a tedious and difficult process. This paper in- troduces Meta Optimization, a methodology for automat- ically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce com- piler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effec- tive compiler heuristics. We present promising experimental results. In onemode of operationMeta Optimization creates application-specific heuristics which often result in impres- sive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23{\%} (up to 73{\%}) for the applications in our suite. Further- more, by evolving a compiler's heuristic over several bench- marks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hy- perblock formation improved performance by an average of 25{\%} on our training set, and 9{\%} on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock for- mation, register allocation, and data prefetching.},
annote = {Cited by 221.},
author = {Stephenson, M. and Martin, M. and Reilly, U. O.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Stephenson, Martin, Reilly - Meta Optimization Improving Compiler Heuristics with Machine Learning.pdf:pdf},
isbn = {1581136625},
title = {{Meta Optimization: Improving Compiler Heuristics with Machine Learning}},
year = {2003}
}
@inproceedings{Strzodka2010,
abstract = {We present a new cache oblivious scheme for iterative stencil computations that performs beyond system bandwidth limitations as though gigabytes of data could reside in an enormous on-chip cache. We compare execution times for 2D and 3D spatial domains with up to 128 million double precision elements for constant and variable stencils against hand-optimized naive code and the automatic polyhedral parallelizer and locality optimizer PluTo and demonstrate the clear superiority of our results. The performance benefits stem from a tiling structure that caters for data locality, parallelism and vectorization simultaneously. Rather than tiling the iteration space from inside, we take an exterior approach with a predefined hierarchy, simple regular parallelogram tiles and a locality preserving parallelization. These advantages come at the cost of an irregular work-load distribution but a tightly integrated load-balancer ensures a high utilization of all resources.},
annote = {Cited by 35.},
author = {Strzodka, Robert and Shaheen, Mohammed and Pajak, D},
booktitle = {SC},
doi = {10.1145/1810085.1810096},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Strzodka, Shaheen, Pajak - Cache oblivious parallelograms in iterative stencil computations.pdf:pdf},
isbn = {9781450300186},
keywords = {cache oblivious,memory bound,memory wall,parallelism and locality,stencil,temporal blocking,time skewing},
publisher = {ACM},
title = {{Cache oblivious parallelograms in iterative stencil computations}},
url = {http://dl.acm.org/citation.cfm?id=1810096},
year = {2010}
}
@inproceedings{Le2012,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Q. V. and Monga, R. and Devin, M. and Corrado, G. and Chen, K. and Ranzato, M. A. and Dean, J. and Ng, A. Y.},
booktitle = {ICML},
doi = {10.1109/MSP.2011.940881},
eprint = {1112.6209},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Le et al. - Building High-level Features Using Large Scale Unsupervised Learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {10535888},
pmid = {20957573},
title = {{Building High-level Features Using Large Scale Unsupervised Learning}},
year = {2012}
}
@article{Czarnecki2017,
abstract = {When training neural networks, the use of Syn-thetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backprop-agated – resulting in Decoupled Neural Inter-faces (DNIs). This unlocked ability of being able to update parts of a neural network asyn-chronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very lit-tle demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gra-dients on feed-forward networks to better under-stand their behaviour and elucidate their effect on optimisation. We show that the incorpora-tion of SGs does not affect the representational strength of the learning system for a neural net-work, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approx-imate the true loss, and, surprisingly, how that leads to drastically different layer-wise represen-tations. Finally, we also expose the relationship of using synthetic gradients to other error ap-proximation techniques and find a unifying lan-guage for discussion and comparison.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1703.00522},
author = {Czarnecki, W. M. and Swirszcz, G. and Jaderberg, M. and Osindero, S. and Vinyals, O. and Kavukcuoglu, K.},
eprint = {1703.00522},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Czarnecki et al. - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf:pdf},
journal = {arXiv:1703.00522},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Understanding Synthetic Gradients and Decoupled Neural Interfaces}},
year = {2017}
}
@misc{Blair-chappell,
annote = {NULL},
author = {Andersson, B. and Blair-Chappell, S. and Mueller-Albrecht, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Andersson, Blair-Chappell, Mueller-Albrecht - Intel Debugger for Linux.pdf:pdf},
title = {{Intel Debugger for Linux}},
url = {https://software.intel.com/sites/default/files/04/0d/idb-linux-12.pdf},
year = {2012}
}
@inproceedings{Cooper1992,
abstract = {Procedure cloning is an interprocedural optimization where the compiler creates specialized copies of proce- dure bodies. To clone a procedure, the compiler repli- cates it and then divides the incoming calls between the original procedure and the copy. By carefully par- titioning the call sites, the compiler can ensure that each clone inherits an environment that allows for bet- ter code optimization. Subsequent optimization tailors the various procedure bodies. This paper examines the problem of procedure cloning. It describes an experiment where cloning was required to enable other transformations. It presents a three-phase algorithm for deciding how to clone a program, and analyzes the algorithm's complexity. Fi- nally, it presents a set of assumptions that bound both the running time of the algorithm and the expansion in code size.},
annote = {Procedure cloning is where a compiler replicates a procedue and divides calls between clones.},
author = {Cooper, K. D. and Hall, M. W. and Kennedy, K.},
booktitle = {Computer Languages},
file = {:Users/cec/Google Drive/Mendeley Library/1992 - Cooper, Hall, Kennedy - Procedure Cloning.pdf:pdf},
publisher = {IEEE},
title = {{Procedure Cloning}},
year = {1992}
}
@article{Hardt2016,
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in su-pervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are avail-able, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individual features. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1610.02413},
author = {Hardt, M.},
eprint = {1610.02413},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Hardt - Equality of Opportunity in Supervised Learning.pdf:pdf},
title = {{Equality of Opportunity in Supervised Learning}},
year = {2016}
}
@article{Hinton2006a,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, G. E and Osindero, S. and Teh, Y.},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Hinton, Osindero, Teh - A fast learning algorithm for deep belief nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {7},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@article{Huo,
author = {Huo, Xuan and Li, Ming},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Huo, Li - Enhancing the Unified Features to Locate Buggy Files by Exploiting the Sequential Nature of Source Code.pdf:pdf},
title = {{Enhancing the Unified Features to Locate Buggy Files by Exploiting the Sequential Nature of Source Code}}
}
@inproceedings{Botorog1996a,
abstract = {Algorithmic skeletons are polymorphic higher-order functions representing common parallelization patterns and implemented in parallel. They can be used as the building blocks of parallel and distributed applications by integrating them into a sequential language. In this paper, we present a new approach to programming with skeletons. We integrate the skeletons into an imperative host language enhanced with higher-order functions and currying, as well as with a polymorphic type system. We thus obtain a high-level programming language which can be implemented very efficiently. After describing a seris of skeletons which work with distributed arrays, we give two examples of parallel algorithms implemented in our language, namely matrix multiplication and a statistical numerical algorithm for solving partial differential equations. Run-time measurements show that we approch the efficiency of massage-passing C up to a factor of between 1 and 1.75.},
annote = {The paper describes a high level language for skeletons programming, with a polmorphic type system {\&} higher order functions, that is 1-1.75x slower than C+MPI.




Too old to be worth caring about.},
author = {Botorog, George Horatiu and Kuchen, Herbert},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Botorog, Kuchen - Efficient Parallel Programming with Algorithmic Skeletons.pdf:pdf},
publisher = {Springer},
title = {{Efficient Parallel Programming with Algorithmic Skeletons}},
year = {1996}
}
@inproceedings{Jim2009,
abstract = {Heterogeneous architectures are currently widespread. With the advent of easy-to-program general purpose GPUs, virtually every re- cent desktop computer is a heterogeneous system. Combining the CPU and the GPU brings great amounts of processing power. However, such architecturesare oftenusedinarestricted way for domain-specific appli- cations like scientific applications and games, and they tend to be used by a single application at a time.We envision future heterogeneous com- puting systems where all their heterogeneous resources are continuously utilized by different applications with versioned critical parts to be able to better adapt their behavior and improve execution time, power con- sumption, response time and other constraints at runtime. Under such a model, adaptive scheduling becomes a critical component. In this paper, we propose a novel predictive user-level scheduler based on past performance history for heterogeneous systems. We developed several scheduling policies and present the study of their impact on system performance. We demonstrate that such scheduler allows mul- tiple applications to fully utilize all available processing resources in CPU/GPU-like systems and consistently achieve speedups ranging from 30{\%} to 40{\%} compared to just using the GPU in a single application mode.},
annote = {This paper describes a user-level scheduler for heterogeneous devices.},
author = {Jim, J. and Fursin, G.},
booktitle = {HiPEAC},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Jim, Fursin - Predictive Runtime Code Scheduling for Heterogeneous Architectures.pdf:pdf},
publisher = {Springer},
title = {{Predictive Runtime Code Scheduling for Heterogeneous Architectures}},
year = {2009}
}
@article{Tram2016,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.02943v1},
author = {Tram, F. and Zhang, F. and Reiter, M. K. and Juels, A. and Tech, C. and Ristenpart, T. and Sep, C. R.},
eprint = {arXiv:1609.02943v1},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Tram et al. - Stealing Machine Learning Models via Prediction APIs.pdf:pdf},
journal = {arXiv:1609.02943},
title = {{Stealing Machine Learning Models via Prediction APIs}},
year = {2016}
}
@article{Legaux2013,
abstract = {Exception handling is a traditional and natural mechanism to manage errors and events that disrupt the normal flow of program instructions. In most concurrent or parallel systems, exception handling is done locally or sequentially, and cannot guarantee the global coherence of the system after an exception is caught. Working with a structured parallel model is an advantage in this respect. Algorithmic skeletons, that are patterns of parallel algorithms on distributed data structures, offer such a structured model. However very few algorithmic skeleton libraries provide a specific parallel exception mechanism, and no C++-based library. In this paper we propose the design of an exception mechanism for the C++ Orl´ eans Skeleton Library that ensures the global coherence of the system after exceptions are caught. We explain our design choices, experiment on the performance penalty of its use, and we illustrate how to purposefully use this mechanism to extract the results in the course of some algorithms. Keywords:},
annote = {The paper describes an extension to the Orleans Skeleton Library, adding support for parallel exceptions using the Boost library. The design and implementation of this exception throwing mechanism is discussed, and experimental results demonstrating the overhead caused by this additional safety is given.},
author = {Legaux, Joeffrey and Loulergue, Fr{\'{e}}d{\'{e}}ric and Jubertie, Sylvain},
doi = {10.1016/j.procs.2013.05.189},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Legaux, Loulergue, Jubertie - OSL An Algorithmic Skeleton Library with Exceptions.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {algorithmic skeletons,c++,exceptions,parallel programming},
month = {jan},
number = {0},
title = {{OSL: An Algorithmic Skeleton Library with Exceptions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050913003323},
volume = {18},
year = {2013}
}
@inproceedings{Frigo1998,
abstract = {FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the pro- cessor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the com- putation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW's self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTWalso compares favorably with machine-specific, vendor-optimized libraries.},
annote = {FFTW is an adaptive FFT program that tunes the copmutation automatically for particular hardware, rather than simply trying to reduce the number of FLOPs executed.},
author = {Frigo, Matteo and Johnson, Steven G},
booktitle = {ICASSP},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Frigo, Johnson - FFTW An adaptive software architecture for the FFT.pdf:pdf},
publisher = {IEEE},
title = {{FFTW: An adaptive software architecture for the FFT}},
year = {1998}
}
@article{Donti2017,
abstract = {As machine learning techniques have become more ubiquitous, it has become common to see machine learning prediction algorithms operating within some larger process. However, the criteria by which we train machine learning algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models within the context of stochastic programming, in a manner that directly captures the ultimate task-based objective for which they will be used. We then present two experimental evaluations of the proposed approach, one as applied to a generic inventory stock problem and the second to a real-world electrical grid scheduling task. In both cases, we show that the proposed approach can outperform both a traditional modeling approach and a purely black-box policy optimization approach.},
author = {Donti, P. L. and Amos, B. and Kolter, J. Z.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Donti, Amos, Kolter - Task-based End-to-end Model Learning.pdf:pdf},
journal = {arXiv:1703.04529},
title = {{Task-based End-to-end Model Learning}},
year = {2017}
}
@inproceedings{McAuley2015b,
abstract = {In a modern recommender system, it is important to understand how products relate to each other. For example, while a user is looking for mobile phones, it might make sense to recommend other phones, but once they buy a phone, we might instead want to recommend batteries, cases, or chargers. These two types of recommendations are referred to as substitutes and complements: substitutes are products that can be purchased instead of each other, while complements are products that can be purchased in addition to each other. Here we develop a method to infer networks of substitutable and complementary products. We formulate this as a supervised link prediction task, where we learn the semantics of substitutes and complements from data associated with products. The primary source of data we use is the text of product reviews, though our method also makes use of features such as ratings, specifications, prices, and brands. Methodologically, we build topic models that are trained to automatically discover topics from text that are successful at predicting and explaining such relationships. Experimentally, we evaluate our system on the Amazon product catalog, a large dataset consisting of 9 million products, 237 million links, and 144 million reviews.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1506.08839},
author = {McAuley, J. and Pandey, R. and Leskovec, J.},
booktitle = {KDD},
doi = {10.1145/2783258.2783381},
eprint = {1506.08839},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - McAuley, Pandey, Leskovec - Inferring Networks of Substitutable and Complementary Products.pdf:pdf},
isbn = {9781450336642},
title = {{Inferring Networks of Substitutable and Complementary Products}},
url = {http://arxiv.org/abs/1506.08839},
year = {2015}
}
@inproceedings{Orso2014,
abstract = {Despite decades ofwork by researchers and practitioners on numerous software quality assurance techniques, testing remains one of the most widely practiced and studied approaches for assessing and improving software quality. Our goal, in this paper, is to provide an accounting of some of the most successful research performed in software testing since the year 2000, and to present what appear to be some of the most significant challenges and opportunities in this area. To be more inclusive in this effort, and to go beyond our own personal opinions and biases, we began by contacting over 50 of our colleagues who are active in the testing research area, and asked them what they believed were (1) the most significant contributions to software testing since 2000 and (2) the greatest open challenges and opportunities for future research in this area. While our col- leagues' input (consisting of about 30 responses) helped guide our choice of topics to cover and ultimately the writing of this paper, we by no means claim that our paper represents all the relevant and noteworthy research performed in the area of software testing in the time period considered—a task that would require far more space and time than we have available. Nevertheless, we hope that the approach we followed helps this paper better reflect not only our views, but also those of the software testing community in general},
author = {Orso, A. and Rothermel, G.},
booktitle = {FOSE},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Orso, Rothermel - Software testing a research travelogue (2000–2014).pdf:pdf},
keywords = {software testing},
title = {{Software testing: a research travelogue (2000–2014)}},
year = {2014}
}
@article{Wangb,
abstract = {This paper proposes the Stacked Approximated Regression Machine (SARM), a novel, simple yet powerful deep learning (DL) baseline. We start by discussing the relationship between regularized regression models and feed-forward networks, with emphasis on the non-negative sparse coding and convolutional sparse coding models. We demonstrate how these models are naturally converted into a unified feed-forward network structure, which coincides with popular DL components. SARM is constructed by stacking multiple unfolded and truncated regression mod-els. Compared to the PCANet, whose feature extraction layers are completely linear, SARM naturally introduces non-linearities, by embedding sparsity regularization. The parameters of SARM are easily obtained, by solving a series of light-weight problems, e.g., PCA or KSVD. Extensive experiments are conducted, which show that SARM outperforms the existing simple deep baseline, PCANet, and is on par with many state-of-the-art deep models, but with much lower computational loads.},
annote = {NULL},
author = {Wang, Z. and Chang, S. and Ling, Q. and Huang, S. and Hu, X. and Shi, H. and Huang, T. S.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Wang et al. - Stacked Approximated Regression Machine A Simple Deep Learning Approach.pdf:pdf},
title = {{Stacked Approximated Regression Machine: A Simple Deep Learning Approach}}
}
@article{Simon1994,
annote = {NULL},
author = {Simon, Daniel R},
doi = {10.1109/sfcs.1994.365701},
file = {:Users/cec/Google Drive/Mendeley Library/1994 - Simon - On the Power of Quantum Computation.pdf:pdf},
isbn = {0-8186-6580-7},
issn = {0097-5397},
journal = {SICOMP},
number = {5},
title = {{On the Power of Quantum Computation}},
volume = {26},
year = {1994}
}
@inproceedings{Baghsorkhi2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Baghsorkhi, S. S. and Vasudevan, N. and Wu, Y.},
booktitle = {PLDI},
doi = {10.1145/2908080.2908111},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Baghsorkhi, Vasudevan, Wu - FlexVec auto-vectorization for irregular loops.pdf:pdf},
isbn = {9781450342612},
keywords = {AVX,SIMD,Vectorization,vector partitioning},
title = {{FlexVec: auto-vectorization for irregular loops}},
url = {http://dl.acm.org/citation.cfm?id=2908080.2908111},
year = {2016}
}
@incollection{FreeSoftwareFoundation2014,
annote = {NULL},
author = {{Free Software Foundation}},
title = {{GNU Documentation v4.9.2: Options That Control Optimization}},
url = {https://gcc.gnu.org/onlinedocs/gcc-4.9.2/gcc/Optimize-Options.html},
year = {2014}
}
@article{Liu2014,
abstract = {MapReduce programming model attracts a lot of enthusiasm among both industry and academia, largely because it simplifies the implementations of many data parallel applications. In spite of the simplicity of the program- ming model, there are many applications that are hard to be implemented by MapReduce, due to their innate characters of computational dependency. In this paper we propose a new approach of using the programming pattern accumulate overMapReduce, to handle a large class of problems that cannot be simply divided into independent sub-computations. Using this accumulate pattern, many problems that have computational dependency can be easily expressed, and then the programs will be transformed to MapReduce programs executed on large clusters. Users without much knowledge of MapReduce can also easily write programs in a sequential manner but finally obtain efficient and scalable MapRe- duce programs. We describe the programming interface of our accumulate framework and explain how to transform a user-specified accumulate computationtoanefficientMapReduce program. Our experiments and evaluations illustrate the usefulness and efficiency of the framework.},
annote = {NULL},
author = {Liu, Yu and Emoto, Kento and Matsuzaki, Kiminori and Hu, Zhenjiang},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Liu et al. - Accumulative Computation on MapReduce.pdf:pdf},
journal = {IPSJ Online Transactions},
keywords = {MapReduce,accumulative computation,algorithmic skeleton,automatic parallelization,parallel programming},
number = {0},
title = {{Accumulative Computation on MapReduce}},
url = {http://jlc.jst.go.jp/DN/JST.JSTAGE/ipsjtrans/7.33?from=Google},
volume = {7},
year = {2014}
}
@inproceedings{Alon2018a,
abstract = {We present a neural model for representing snippets of code as continuous distributed vectors ("code embeddings"). The main idea is to represent a code snippet as a single fixed-length {\$}\backslashtextit{\{}code vector{\}}{\$}, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path {\$}\backslashtextit{\{}simultaneously{\}}{\$} with learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75{\%}, being the first to successfully predict method names based on a large, cross-project, corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data, and trained models are available at https://github.com/tech-srl/code2vec.},
archivePrefix = {arXiv},
arxivId = {1803.09473},
author = {Alon, U. and Zilberstein, M. and Levy, O. and Yahav, E.},
booktitle = {POPL},
doi = {arXiv:1803.09473v5},
eprint = {1803.09473},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Alon et al. - code2vec Learning Distributed Representations of Code(2).pdf:pdf},
keywords = {Big Code,Distributed Representa,Machine Learning},
title = {{code2vec: Learning Distributed Representations of Code}},
url = {http://arxiv.org/abs/1803.09473},
year = {2018}
}
@article{Wu2008,
abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) inDecember 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among themost influential datamining algorithms in the research community.With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and reviewcurrent and further research on the algorithm. These 10 algorithms cover classification clustering, statistical learning, association analysis, and linkmining, which are all among the most important topics in data mining research and development.},
annote = {NULL},
author = {Wu, X. and Kumar, V. and {Ross Quinlan}, J. and Ghosh, J. and Yang, Q. and Motoda, H. and McLachlan, G. J. and Ng, A. and Liu, B. and Yu, P. S. and Zhou, Z. and Steinbach, M. and Hand, D. J. and Steinberg, D.},
doi = {10.1007/s10115-007-0114-2},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Wu et al. - Top 10 algorithms in data mining.pdf:pdf},
isbn = {1011500701},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
number = {1},
title = {{Top 10 algorithms in data mining}},
volume = {14},
year = {2008}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1128v1},
author = {Sak, H. and Senior, A. and Beaufays, F.},
doi = {arXiv:1402.1128},
eprint = {arXiv:1402.1128v1},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Sak, Senior, Beaufays - Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recogniti.pdf:pdf},
journal = {arXiv:1402.1128},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
year = {2014}
}
@article{Allamanis2017,
abstract = {The question of how procedural knowledge is represented and inferred is a funda- mental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters, that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic represen- tations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learn- ing continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1611.01423},
author = {Allamanis, M. and Chanthirasegaran, P. and Kohli, P. and Sutton, C.},
eprint = {1611.01423},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Allamanis et al. - Learning Continuous Semantic Representations of Symbolic Expressions.pdf:pdf},
journal = {arXiv:1611.01423},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Learning Continuous Semantic Representations of Symbolic Expressions}},
year = {2016}
}
@article{Grosenick2013,
abstract = {Multivariate machine learning methods are increasingly used to analyze neuroimaging data, often replacing more traditional "mass univariate" techniques that fit data one voxel at a time. In the functional magnetic resonance imaging (fMRI) literature, this has led to broad application of "off-the-shelf" classification and regression methods. These generic approaches allow investigators to use ready-made algorithms to accurately decode perceptual, cognitive, or behavioral states from distributed patterns of neural activity. However, when applied to correlated whole-brain fMRI data these methods suffer from coefficient instability, are sensitive to outliers, and yield dense solutions that are hard to interpret without arbitrary thresholding. Here, we develop variants of the Graph-constrained Elastic-Net (GraphNet), a fast, whole-brain regression and classification method developed for spatially and temporally correlated data that automatically yields interpretable coefficient maps (Grosenick et al., 2009b). GraphNet methods yield sparse but structured solutions by combining structured graph constraints (based on knowledge about coefficient smoothness or connectivity) with a global sparsity-inducing prior that automatically selects important variables. Because GraphNet methods can efficiently fit regression or classification models to whole-brain, multiple time-point data sets and enhance classification accuracy relative to volume-of-interest (VOI) approaches, they eliminate the need for inherently biased VOI analyses and allow whole-brain fitting without the multiple comparison problems that plague mass univariate and roaming VOI ("searchlight") methods. As fMRI data are unlikely to be normally distributed, we (1) extend GraphNet to include robust loss functions that confer insensitivity to outliers, (2) equip them with "adaptive" penalties that asymptotically guarantee correct variable selection, and (3) develop a novel sparse structured Support Vector GraphNet classifier (SVGN). When applied to previously published data (Knutson et al., 2007), these efficient whole-brain methods significantly improved classification accuracy over previously reported VOI-based analyses on the same data (Grosenick et al., 2008; Knutson et al., 2007) while discovering task-related regions not documented in the original VOI approach. Critically, GraphNet estimates fit to the Knutson et al. (2007) data generalize well to out-of-sample data collected more than three years later on the same task but with different subjects and stimuli (Karmarkar et al., submitted for publication). By enabling robust and efficient selection of important voxels from whole-brain data taken over multiple time points ({\textgreater} 100,000 "features"), these methods enable data-driven selection of brain areas that accurately predict single-trial behavior within and across individuals. {\textcopyright} 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1110.4139},
author = {Grosenick, Logan and Klingenberg, Brad and Katovich, Kiefer and Knutson, Brian and Taylor, Jonathan E.},
doi = {10.1016/j.neuroimage.2012.12.062},
eprint = {1110.4139},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Grosenick et al. - Interpretable whole-brain prediction analysis with GraphNet.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
number = {650},
pages = {304--321},
pmid = {23298747},
title = {{Interpretable whole-brain prediction analysis with GraphNet}},
volume = {72},
year = {2013}
}
@article{Wang2017d,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.07163v4},
author = {Wang, K. and Singh, R. and Su, Z.},
eprint = {arXiv:1711.07163v4},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Wang, Singh, Su - Dynamic Neural Program Embeddings for Program Repair.pdf:pdf},
journal = {arXiv:1711.07163},
title = {{Dynamic Neural Program Embeddings for Program Repair}},
year = {2017}
}
@misc{UniversityofEdinburgh2014a,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 6. Compiling Techniques.pdf:pdf},
number = {1},
title = {{6. Compiling Techniques}},
year = {2015}
}
@misc{Shannon1993,
annote = {NULL},
author = {Shannon, Bill},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Shannon - C Style and Coding Standards for SunOS C Style and Coding Standards for SunOS.pdf:pdf},
title = {{C Style and Coding Standards for SunOS C Style and Coding Standards for SunOS}},
year = {1993}
}
@inproceedings{Thomson2015,
annote = {NULL},
author = {Thomson, P. and Donaldson, A.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Thomson, Donaldson - The Lazy Happens-Before Relation Better Partial-Order Reduction for Systematic Concurrency Testing ∗.pdf:pdf},
isbn = {9781450332057},
keywords = {partial-order reduction,systematic concurrency testing},
title = {{The Lazy Happens-Before Relation : Better Partial-Order Reduction for Systematic Concurrency Testing ∗}},
year = {2015}
}
@inproceedings{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, V. and Hinton, G. E.},
booktitle = {ICML},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Nair, Hinton - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@misc{Silver2015j,
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 6 Value Function Approximation.pdf:pdf},
title = {{Lecture 6: Value Function Approximation}},
year = {2015}
}
@article{Jeffrey2003,
annote = {An important paper which outlines the ideal situation of self-governing agents in complex computing environments.


Cited by 4741.},
author = {Jeffrey, O and David, M},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Jeffrey, David - The Vision of Autonomic Computing.pdf:pdf},
journal = {Computer},
number = {1},
publisher = {IEEE},
title = {{The Vision of Autonomic Computing}},
volume = {36},
year = {2003}
}
@article{Radford2016b,
abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised man-ner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be pos-itive or negative generates samples with the cor-responding positive or negative sentiment.},
archivePrefix = {arXiv},
arxivId = {1704.01444},
author = {Radford, A. and Jozefowicz, R. and Sutskever, I.},
eprint = {1704.01444},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Radford, Jozefowicz, Sutskever - Learning to Generate Reviews and Discovering Sentiment.pdf:pdf;:Users/cec/Google Drive/Mendeley Library/2017 - Radford, Jozefowicz, Sutskever - Learning to Generate Reviews and Discovering Sentiment(2).pdf:pdf},
journal = {arXiv:1704.01444},
title = {{Learning to Generate Reviews and Discovering Sentiment}},
url = {https://arxiv.org/pdf/1704.01444.pdf},
year = {2017}
}
@article{Duan2016,
abstract = {Recently, researchers have made significant progress combining the advances in deep learn-ing for learning feature representations with rein-forcement learning. Some notable examples in-clude training agents to play Atari games based on raw pixel data and to acquire advanced ma-nipulation skills using raw sensory inputs. How-ever, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of contin-uous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning al-gorithms. Both the benchmark and reference im-plementations are released open-source in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
annote = {NULL},
author = {Duan, Y. and Chen, X. and Schulman, J. and Abbeel, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Duan et al. - Benchmarking Deep Reinforcement Learning for Continuous Control.pdf:pdf},
journal = {arXiv:1604.06778},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Benchmarking Deep Reinforcement Learning for Continuous Control}},
year = {2016}
}
@article{Tassa2018,
abstract = {The DeepMind Control Suite is a set of continuous control tasks with a stan-dardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and mod-ify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at github.com/deepmind/dm{\_}control. A video summary of all tasks is available at youtu.be/rAai4QzcYbs.},
author = {Tassa, Y. and Doron, Y. and Muldal, A. and Erez, T. and Li, Y. and De, D. and Casas, L. and Budden, D. and Abdolmaleki, A. and Merel, J. and Lefrancq, A. and Lillicrap, T. and Riedmiller, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Tassa et al. - DeepMind Control Suite.pdf:pdf},
journal = {arXiv:1801.00690},
title = {{DeepMind Control Suite}},
year = {2018}
}
@inproceedings{Steuwer2017,
annote = {NULL},
author = {Steuwer, M. and Dubach, C.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Steuwer, Dubach - Lift A Functional Data-Parallel IR for High-Performance GPU Code Generation.pdf:pdf},
publisher = {IEEE},
title = {{Lift: A Functional Data-Parallel IR for High-Performance GPU Code Generation}},
year = {2017}
}
@inproceedings{Meng2016,
abstract = {Binary code analysis is an enabling technique for many ap-plications. Modern compilers and run-time libraries have introduced significant complexities to binary code, which negatively affect the capabilities of binary analysis tool kits to analyze binary code, and may cause tools to report inaccu-rate information about binary code. Analysts may hence be confused and applications based on these tool kits may have degrading quality. We examine the problem of constructing control flow graphs from binary code and labeling the graphs with accurate function boundary annotations. We identified several challenging code constructs that represent hard-to-analyze aspects of binary code, and show code examples for each code construct. As part of this discussion, we present new code parsing algorithms in our open source Dyninst tool kit that support these constructs, including a new model for describing jump tables that improves our ability to precisely determine the control flow targets, a new interprocedural analysis to determine when a function is non-returning, and techniques for handling tail calls. We evaluated how vari-ous tool kits fare when handling these code constructs with real software as well as test binaries patterned after each challenging code construct we found in real software.},
author = {Meng, X. and Miller, B. P.},
booktitle = {ISSTA},
doi = {10.1145/2931037.2931047},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Meng, Miller - Binary code is not easy.pdf:pdf},
isbn = {9781450343909},
keywords = {challenging code constructs,static binary code analysis},
title = {{Binary code is not easy}},
year = {2016}
}
@inproceedings{Kulkarni2015,
abstract = {Recent progress on probabilistic modeling and statisti- cal learning, coupled with the availability of large training datasets, has led to remarkable progress in computer vision. Generative probabilistic models, or “analysis-by-synthesis” approaches, can capture rich scene structure but have been less widely applied than their discriminative counterparts, as they often require considerable problem-specific engineering in modeling and inference, and inference is typically seen as requiring slow, hypothesize-and-test Monte Carlo meth- ods. Here we present Picture, a probabilistic programming language for scene understanding that allows researchers to express complex generative vision models, while auto- matically solving them using fast general-purpose inference machinery. Picture provides a stochastic scene language that can express generative models for arbitrary 2D/3D scenes, as well as a hierarchy of representation layers for compar- ing scene hypotheses with observed images by matching not simply pixels, but also more abstract features (e.g., contours, deep neural network activations). Inference can flexibly integrate advanced Monte Carlo strategies with fast bottom- up data-driven methods. Thus both representations and inference strategies can build directly on progress in discrim- inatively trained systems to make generative vision more robust and efficient. We use Picture to write programs for 3D face analysis, 3D human pose estimation, and 3D object reconstruction – each competitive with specially engineered baselines.},
annote = {NULL},
author = {Kulkarni, T. D. and Tenenbaum, J. B.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2015.7299068},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kulkarni, Tenenbaum - Picture A Probabilistic Programming Language for Scene Perception.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
publisher = {IEEE},
title = {{Picture: A Probabilistic Programming Language for Scene Perception}},
year = {2015}
}
@inproceedings{Park1991,
abstract = {The Omega test is an integer programming algorithm that can determine whether a dependence exists between two array references, and if so, under what conditions. Conven- tional wisdom holds that integer programming techniques are far too expensive to be used for dependence analysis, except as a method of last resort for situations that cannot be decided by simpler methods. We present evidence that suggests this wisdom is wrong, and that the Omega test is competitive with approximate algorithms used in practice and suitable for use in production compilers. The Omega test is based on an extension of Fourier- Motzkin variable elimination to integer programming, and has worst-case exponential time complexity. However, we show that for many situations in which other (polynomial) methods are accurate, the Omega test has low order poly- nomial time complexity. The Omega test can be used to simplify integer program- ming problems, rather than just deciding them. This has many applications, including accurately and efficiently com- puting dependence direction and distance vectors.},
annote = {From Duplicate 1 (The Omega Test: A Fast and Practical Integer Programming Algorithm for Dependence Analysis - Pugh, William)










Cited by 927.










From Duplicate 2 (The Omega Test: A Fast and Practical Integer Programming Algorithm for Dependence Analysis - Pugh, William)










Using ILP to solve dependence.},
author = {Pugh, W.},
booktitle = {SC},
doi = {10.1145/125826.125848},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Pugh - The Omega Test A Fast and Practical Integer Programming Algorithm for Dependence Analysis.pdf:pdf},
isbn = {0897914597},
issn = {0897914597},
publisher = {ACM},
title = {{The Omega Test: A Fast and Practical Integer Programming Algorithm for Dependence Analysis}},
year = {1991}
}
@inproceedings{Murali,
author = {Murali, V. and Chaudhuri, S. and Jermaine, C.},
booktitle = {FSE},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Murali, Chaudhuri, Jermaine - Bayesian Specification Learning for Finding API Usage Errors.pdf:pdf},
isbn = {9781450351058},
keywords = {2017,TOSKIM,acm reference format,and chris jermaine,anomaly detection,apis,bayesian,bug finding,cation learning,speci,swarat chaudhuri,vijayaraghavan murali},
mendeley-tags = {TOSKIM},
title = {{Bayesian Specification Learning for Finding API Usage Errors}}
}
@inproceedings{Javed2011,
abstract = {Algorithmic skeletons are a high-level approach to parallel programming that can be combined with widely used programming languages such as Java, C and C++. In this paper we show that prototyping such a library with a structured parallel functional language, namely Bulk Syn- chronous Parallel ML, provides a parallel implementation with which experiments can be performed and gives some hints about the formal semantics of the library as well.},
annote = {NULL},
author = {Javed, Noman and Loulergue, Fr{\'{e}}d{\'{e}}ric and Tesson, Julien and Bousdira, Wadoud},
booktitle = {PDPTA},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Javed et al. - Prototyping a Library of Algorithmic Skeletons with Bulk Synchronous Parallel ML.pdf:pdf},
keywords = {algorithmic skeletons,applications,functional programming,parallel programming},
title = {{Prototyping a Library of Algorithmic Skeletons with Bulk Synchronous Parallel ML}},
year = {2011}
}
@article{Peters2008,
annote = {The Natural Actor-Critic algorithm uses natural policy gradients to improve Reinforcement Learning in high demensional movement systems in the presence of uncertainty.},
author = {Peters, J. and Vijayakumar, S. and Schaal, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Peters, Vijayakumar, Schaal - Natural Actor Critic.pdf:pdf},
journal = {Neurocomputing},
number = {7},
title = {{Natural Actor Critic}},
volume = {71},
year = {2008}
}
@article{Lample2016,
abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1609.05521},
author = {Lample, G. and Chaplot, D. S.},
eprint = {1609.05521},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Lample, Chaplot - Playing FPS Games with Deep Reinforcement Learning.pdf:pdf},
number = {2015},
title = {{Playing FPS Games with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1609.05521},
year = {2016}
}
@article{Boujarwah1997,
abstract = {Software testing is an important and critical phase of the application software development life cycle. Testing is a time consuming and costly stage that requires a high degree of ingenuity. In the development stages of safety-critical and dependable computer software such as language compilers and real-time embedded software, testing activities consume about 50{\%} of the project time. In this work we address the area of compiler testing. The aim of compiler testing is to verify that the compiler implementation conforms to its specifications, which is to generate an object code that faithfully corresponds to the language semantic and syntax as specified in the language documentation. A compiler should be carefully verified before its release, since it has to be used by many users. Finding an optimal and complete test suite that can be used in the testing process is often an exhaustive task. Various methods have been proposed for the generation of compiler test cases. Many papers have been published on testing compilers, most of which address classical programming languages. In this paper, we assess and compare various compiler testing techniques with respect to some selected criteria and also propose some new research directions in compiler testing of modem programming languages.},
author = {Boujarwah, A. S. and Saleh, K.},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Boujarwah, Saleh - Compiler Test Case Generation Methods A Survey and Assessment.pdf:pdf},
journal = {Information and Software Technology},
keywords = {TOREAD,compiler,software testing,test case generation,test suite},
mendeley-tags = {TOREAD},
number = {9},
title = {{Compiler Test Case Generation Methods: A Survey and Assessment}},
volume = {39},
year = {1997}
}
@phdthesis{Kyle2015,
author = {Kyle, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kyle - Applications of Information Sharing for Code Generation in Process Virtual Machines.pdf:pdf},
school = {University of Edinburgh},
title = {{Applications of Information Sharing for Code Generation in Process Virtual Machines}},
year = {2015}
}
@inproceedings{Ashari2015a,
annote = {NULL},
author = {Ashari, A. and Boehm, M. and Reinwald, B. and Campbell, K.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ashari et al. - On Optimizing Machine Learning Workloads via Kernel Fusion.pdf:pdf},
isbn = {9781450332057},
keywords = {dense,fused kernel,gpu,machine learning,sparse},
title = {{On Optimizing Machine Learning Workloads via Kernel Fusion}},
year = {2015}
}
@misc{UniversityofEdinburgh2014n,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 6. Instruction Scheduling.pdf:pdf},
title = {{6. Instruction Scheduling}},
year = {2015}
}
@inproceedings{Abbeel2004,
abstract = {First order Markov models have been successfully applied to many prob-lems. Examples include modeling sequential data using Markov chains, and solving control problems posed in the Markov decision processes (MDP) framework. If the Markov model's parameters are estimated from data, the standard maximum likelihood estimates consider the first order (single-step) transitions only. But for many problems the first order con-ditional independence assumptions are not satisfied, as a result of which the higher order transition probabilities can be poorly approximated by the learned model. Motivated by the problem of learning an MDP's pa-rameters for control, we propose an algorithm, one that is not based on maximum likelihood estimation, for learning a first order Markov model that explicitly takes into account higher order interactions during train-ing. Our framework allows the model to capture longer range effects without giving up the benefits of using first order Markov models. Our experiments illustrate the potential advantage of the new objective func-tion over conventional maximum likelihood estimation.},
author = {Abbeel, P. and Ng, A. Y.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Abbeel, Ng - Learning first-order Markov models for control.pdf:pdf},
title = {{Learning first-order Markov models for control}},
year = {2005}
}
@misc{Edinburgh2013,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - University of Edinburgh - COPT 2013 exam.pdf:pdf},
number = {Level 10},
title = {{COPT 2013 exam}},
year = {2013}
}
@inproceedings{Holewinski2012,
annote = {Cited by 72.},
author = {Holewinski, J. and Pouchet, L. and Sadayappan, P.},
booktitle = {SC},
doi = {10.1145/2304576.2304619},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Holewinski, Pouchet, Sadayappan - High-performance Code Generation for Stencil Computations on GPU Architectures.pdf:pdf},
isbn = {978-1-4503-1316-2},
keywords = {gpu,opencl,overlapped tiling,stencil},
title = {{High-performance Code Generation for Stencil Computations on GPU Architectures}},
year = {2012}
}
@book{Edition,
annote = {NULL},
author = {Edition, Revised},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Edition - A PhD is not enough!.pdf:pdf},
isbn = {9780465022229},
title = {{A PhD is not enough!}},
year = {2001}
}
@phdthesis{Hillis1987,
abstract = {Has a chapter on why computer science is no good.$\backslash$par$\backslash$nPatent 4,709,327, Connection Machine, 24 Nov 87$\backslash$n(individuals) {\{}}},
annote = {NULL},
author = {Hillis, W D},
file = {:Users/cec/Google Drive/Mendeley Library/1987 - Hillis - The Connection Machine.pdf:pdf},
isbn = {0-262-58097-7},
title = {{The Connection Machine}},
year = {1987}
}
@inproceedings{Thebault,
annote = {NULL},
author = {Th{\'{e}}bault, Lo{\"{i}}c and Petit, Eric},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Th{\'{e}}bault, Petit - Scalable and Efficient Implementation of 3D Unstructured Meshes Computation A Case Study on Matrix Assembly.pdf:pdf},
isbn = {9781450332057},
keywords = {computational fluid dynamic,divide-and-conquer,domain decomposi-,matrix,task,tion,unstructured mesh,vectorization},
title = {{Scalable and Efficient Implementation of 3D Unstructured Meshes Computation : A Case Study on Matrix Assembly}},
year = {2015}
}
@unpublished{Bundy1989,
abstract = {Getting a Ph.D. or M.Phil is hard work. This document gives advice about various aspects of the task. Section 1 describes the problem - what is a thesis? Section 2 sets out the formal requirements of gaining a thesis. Sections 3 and 4 describe some of the pitfalls and hurdles which students have encountered. Sections 5 and 6 advice about choosing and then executing a research project. Sections 7, 8 and 9 deal with two of the three R's: reading and writing. Section 10 describes the examination process for a research degree, and how to cope with it. Finally section 11 lists journals which accept A.I. material.},
annote = {Essential reading.},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/1989 - Bundy - The Researcher's Bible.pdf:pdf},
title = {{The Researcher's Bible}},
volume = {68},
year = {1989}
}
@article{Dezani-ciancaglini2010,
abstract = {We illustrate the concepts of sessions and session types as they have been developed in the setting of the $\pi$-calculus. Motivated by the goal of obtaining a formalisation closer to existing standards and aiming at their enhancement and strengthening, several extensions of the original core system have been proposed, which we survey together with the embodying of sessions into functional and object-oriented languages, as well as some implementations.},
annote = {This paper surveys the concepts of sessions and session types in the setting of pi-calculus.},
author = {Dezani-ciancaglini, Mariangiola and Liguoro, Ugo De},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Dezani-ciancaglini, Liguoro - Sessions and Session Types An Overview.pdf:pdf},
journal = {Web Services and Formal Methods},
keywords = {process calculi,service oriented computing,type systems},
publisher = {Springer},
title = {{Sessions and Session Types: An Overview}},
year = {2010}
}
@inproceedings{Zhao2009,
abstract = {This paper presents joint research and practice on automated test program generation for an industrial compiler, UniPhier, by Matsushita Electric Industrial Co., Ltd. (MEI) and Institute of Software, Chinese Academy of Sciences (ISCAS) since Sept. 2002. To meet the test requirements of MEI's engineers, we proposed an automated approach to produce test programs for UniPhier, and as a result we developed an integrated tool named JTT. Firstly, we show the script-driven test program generation process in JTT. Secondly, we show how to produce test programs automatically, based on a temporal-logic model of compiler optimizations, to guarantee the execution of optimizing modules under test during compilation. JTT has gained success in testing UniPhier: even after benchmark testing and comprehensive manual testing, JTT still found 6 new serious defects.},
author = {Zhao, C. and Xue, Y. and Tao, Q. and Guo, L. and Wang, Z.},
booktitle = {AST},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Zhao et al. - Automated Test Program Generation for an Industrial Optimizing Compiler.pdf:pdf},
title = {{Automated Test Program Generation for an Industrial Optimizing Compiler}},
year = {2009}
}
@article{Ching2017a,
abstract = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems --patient classification, fundamental biological processes, and treatment of patients --to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.},
author = {Ching, Travers and Himmelstein, Daniel S and Beaulieu-Jones, Brett K and Kalinin, Alexandr A and Do, Brian T and Way, Gregory P and Ferrero, Enrico and Agapow, Paul-Michael and Xie, Wei and Rosen, Gail L and Lengerich, Benjamin J and Israeli, Johnny},
doi = {10.1101/142760},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Ching et al. - Opportunities and obstacles for deep learning in biology and medicine.pdf:pdf},
title = {{Opportunities and obstacles for deep learning in biology and medicine}},
url = {http://dx.doi.org/10.1101/142760},
year = {2017}
}
@inproceedings{Le2013a,
abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations. To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed. Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
annote = {NULL},
author = {Le, V. and Afshari, M. and Su, Z.},
booktitle = {PLDI},
doi = {10.1145/2594291.2594334},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Le, Afshari, Su - Compiler Validation via Equivalence Modulo Inputs.pdf:pdf},
isbn = {9781450327848},
issn = {03621340},
keywords = {automated testing,compiler testing,equivalent program variants,miscompilation},
title = {{Compiler Validation via Equivalence Modulo Inputs}},
year = {2014}
}
@article{Version2016a,
annote = {NULL},
author = {Keir, Paul},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Keir - Porting SLAMBench KFusion to Khronos SYCL.pdf:pdf},
keywords = {c,computer vision,gpgpu,opencl,parallelism},
title = {{Porting SLAMBench KFusion to Khronos SYCL}},
year = {2016}
}
@article{Buck2004,
abstract = {In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.},
annote = {Cited by 1370.},
author = {Buck, I and Foley, T and Horn, D and Sugerman, Jeremy and Fatahalian, Kayvon and Houston, Mike and Hanrahan, Pat},
doi = {10.1145/1015706.1015800},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Buck et al. - Brook for GPUs stream computing on graphics hardware.pdf:pdf},
isbn = {0496960067},
issn = {0730-0301},
journal = {TOG},
keywords = {Brook,Data,GPU Computing,Parallel Computing,Programmable Graphics Hardware,Stream Computing},
number = {3},
title = {{Brook for GPUs: stream computing on graphics hardware}},
url = {http://dl.acm.org/citation.cfm?id=1015800},
volume = {23},
year = {2004}
}
@inproceedings{Allamanis2017b,
abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.},
archivePrefix = {arXiv},
arxivId = {1711.00740},
author = {Allamanis, M. and Brockschmidt, M. and Khademi, M.},
booktitle = {ICLR},
doi = {arXiv:1711.00740v3},
eprint = {1711.00740},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Allamanis, Brockschmidt, Khademi - Learning to Represent Programs with Graphs.pdf:pdf},
issn = {0899-7667},
pmid = {16628114},
title = {{Learning to Represent Programs with Graphs}},
url = {http://arxiv.org/abs/1711.00740},
year = {2017}
}
@article{Skillicorn1998,
abstract = {We survey parallel programming models and languages using six criteria$\backslash$nto assess their suitability for realistic portable parallel programming.$\backslash$nWe argue that an ideal model should by easy to program, should have$\backslash$na software development methodology, should be architecture-independent,$\backslash$nshould be easy to understand, should guarantee performance, and should$\backslash$nprovide accurate information about the cost of programs. These criteria$\backslash$nreflect our belief that developments in parallelism must be driven$\backslash$nby a parallel software industry based on portability and efficiency.$\backslash$nWe consider programming models in six categories, depending on the$\backslash$nlevel of abstraction they provide. Those that are very abstract conceal$\backslash$neven the presence of parallelism at the software level. Such models$\backslash$nmake software easy to build and port, but efficient and predictable$\backslash$nperformance is usually hard to achieve. At the other end of the spectrum,$\backslash$nlow-level models make all of the messy issues of parallel programming$\backslash$nexplicit (how many threads, how to place them, how to express communication,$\backslash$nand how to schedule communication), so that software is hard to build$\backslash$nand not very portable, but is usually efficient. Most recent models$\backslash$nare near the center of this spectrum, exploring the best tradeoffs$\backslash$nbetween expressiveness and performance. A few models have achieved$\backslash$nboth abstractness and efficiency. Both kinds of models raise the$\backslash$npossibility of parallelism as part of the mainstream of computing.},
annote = {NULL},
author = {Skillicorn, David B. and Talia, Domenico},
doi = {10.1145/280277.280278},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Skillicorn, Talia - Models and languages for parallel computation.pdf:pdf},
issn = {03600300},
journal = {CSUR},
number = {2},
title = {{Models and languages for parallel computation}},
volume = {30},
year = {1998}
}
@article{Kwok1999,
abstract = {Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP- complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph- theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.},
annote = {NULL},
author = {Kwok, Y. and Ahmad, I.},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Kwok, Ahmad - Static scheduling algorithms for allocating directed task graphs to multiprocessors.pdf:pdf},
journal = {CSUR},
keywords = {algorithms,design,operating systems,performance,processor architectures,programming techniques,theory},
number = {4},
title = {{Static scheduling algorithms for allocating directed task graphs to multiprocessors}},
volume = {31},
year = {1999}
}
@inproceedings{Allamanis2015,
abstract = {We consider the problem of building probabilistic models that jointly model short natural language utterances and source code snippets. The aim is to bring together recent work on statistical modelling of source code and work on bimodal models of images and natural language. The resulting models are useful for a variety of tasks that involve natural language and source code. We demonstrate their performance on two retrieval tasks: retrieving source code snippets given a natural language query, and retrieving natural language descriptions given a source code query (i.e., source code captioning). The experiments show there to be promise in this direction, and that modelling the structure of source code is helpful towards the retrieval tasks.},
annote = {NULL},
author = {Allamanis, M. and Tarlow, D. and Gordon, A. D. and Wei, Y.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Allamanis et al. - Bimodal Modelling of Source Code and Natural Language.pdf:pdf},
isbn = {9781510810587},
title = {{Bimodal Modelling of Source Code and Natural Language}},
url = {http://jmlr.org/proceedings/papers/v37/allamanis15.html},
volume = {37},
year = {2015}
}
@article{Swersky2014,
abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
archivePrefix = {arXiv},
arxivId = {1406.3896},
author = {Swersky, K. and Snoek, J. and Adams, R. P.},
eprint = {1406.3896},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Swersky, Snoek, Adams - Freeze-Thaw Bayesian Optimization.pdf:pdf},
issn = {10495258},
journal = {arXiv:1406.3896},
title = {{Freeze-Thaw Bayesian Optimization}},
year = {2014}
}
@inproceedings{Kanev2016,
abstract = {With the increasing prevalence of warehouse-scale (WSC) and cloud computing, understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding, this paper presents a detailed microarchitectural analysis of live data-center jobs, measured on more than 20,000 Google machines over a three year period, and comprising thousands of differ-ent applications. We first find that WSC workloads are extremely diverse, breeding the need for architectures that can tolerate appli-cation variability without performance loss. However, some patterns emerge, offering opportunities for co-optimization of hardware and software. For example, we identify com-mon building blocks in the lower levels of the software stack. This " datacenter tax " can comprise nearly 30{\%} of cycles across jobs running in the fleet, which makes its constituents prime candidates for hardware specialization in future server systems-on-chips. We also uncover opportunities for classic microarchitectural optimizations for server processors, espe-cially in the cache hierarchy. Typical workloads place signifi-cant stress on instruction caches and prefer memory latency over bandwidth. They also stall cores often, but compute heav-ily in bursts. These observations motivate several interesting directions for future warehouse-scale computers.},
annote = {Very interesting paper. There is no "killer app" for datacenter workloads. The top 50 hottest binaries cover {\~{}}60{\%} of cycles. However, they identify six "taxes" accounting for 27{\%} of workload: protobuf management, RPCs, memmoves, allocs, hashing, and compression. They also show {\~{}}20{\%} cycles spent in the kernel.},
author = {Kanev, S. and Darago, J. P. and Hazelwood, K. and Ranganathan, P. and Moseley, T. and Wei, G. Y. and Brooks, D.},
booktitle = {ICSA},
doi = {10.1109/MM.2016.38},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Kanev et al. - Profiling a Warehouse-Scale Computer.pdf:pdf},
isbn = {9781450334020},
issn = {02721732},
number = {3},
title = {{Profiling a Warehouse-Scale Computer}},
volume = {36},
year = {2016}
}
@inproceedings{Cummins2016,
abstract = {The physical limitations of microprocessor design have forced the industry towards increasingly heterogeneous designs to extract performance. This trend has not been matched with adequate software tools, leading to a growing disparity between the availability of parallelism and the ability for application developers to exploit it. Algorithmic skeletons simplify parallel programming by providing high-level, reusable patterns of computation. Achieving performant skeleton implementations is a difficult task; skeleton authors must attempt to anticipate and tune for a wide range of architectures and use cases. This results in implementations that target the general case and cannot provide the performance advantages that are gained from tuning low level optimization parameters. Autotuning combined with machine learning offers promising performance benefits in these situations, but the high cost of training and lack of available tools limits the practicality of autotuning for real world programming. We believe that performing autotuning at the level of the skeleton library can overcome these issues. In this work, we present OmniTune - an extensible and distributed framework for dynamic autotuning of optimization parameters at runtime. OmniTune uses a client-server model with a flexible API to support machine learning enabled autotuning. Training data is shared across a network of cooperating systems, using a collective approach to performance tuning. We demonstrate the practicality of OmniTune in a case study using the algorithmic skeleton library SkelCL. By automatically tuning the workgroup size of OpenCL Stencil skeleton kernels, we show that that static tuning across a range of GPUs and programs can achieve only 26{\%} of the optimal performance, while OmniTune achieves 92{\%} of this maximum, equating to an average 5.65x speedup. OmniTune achieves this without introducing a significant runtime overhead, and enables portable, cross-device and cross-program tuning.},
annote = {NULL},
author = {Cummins, C. and Petoumenos, P. and Steuwer, M. and Leather, H.},
booktitle = {HLPGPU},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Cummins et al. - Towards Collaborative Performance Tuning of Algorithmic Skeletons.pdf:pdf},
title = {{Towards Collaborative Performance Tuning of Algorithmic Skeletons}},
year = {2016}
}
@article{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {This looks great.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, A. A. and Rabinowitz, N. C. and Desjardins, G. and Soyer, H. and Kirkpatrick, J. and Kavukcuoglu, K. and Pascanu, R. and Hadsell, R.},
eprint = {1606.04671},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rusu et al. - Progressive Neural Networks.pdf:pdf},
journal = {arXiv:1606.04671},
keywords = {TOPRINT,TOSTUDY},
mendeley-tags = {TOPRINT,TOSTUDY},
title = {{Progressive Neural Networks}},
year = {2016}
}
@article{Probability2005,
annote = {NULL},
author = {Probability, Conditional and Exclusive, Mutually and Events, Jointly Exhaustive},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Probability, Exclusive, Events - 4. Conditional Probability, Total Probability, Bayes's Rule.pdf:pdf},
number = {September},
title = {{4. Conditional Probability, Total Probability, Bayes's Rule}},
year = {2005}
}
@article{Stock2012,
annote = {NULL},
author = {Stock, K. and Pouchet, L. and Sadayappan, P.},
doi = {10.1145/2086696.2086729},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Stock, Pouchet, Sadayappan - Using Machine Learning to Improve Automatic Vectorization Vectorization.pdf:pdf},
journal = {TACO},
title = {{Using Machine Learning to Improve Automatic Vectorization Vectorization}},
year = {2012}
}
@misc{Silver2015k,
abstract = {Lecture of University College London.},
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 7 Policy Gradient.pdf:pdf},
isbn = {9780195306613},
title = {{Lecture 7 : Policy Gradient}},
year = {2015}
}
@article{Chang2001,
abstract = {Increased platform heterogeneity and varying resource availability in distributed systems motivate the design of resource-aware applications, which ensure a desired performance level by continuously adapting their behavior to changing resource characteristics. In this paper, we describe an application-independent adaptation framework that simplifies the design of resource-aware applications. This framework eliminates the need for adaptation decisions to be explicitly programmed into the application by relying on two novel compo- nents: (1) a tunability interface, which exposes adaptation choices in the form of alternate application configurations while encapsulating core application functionality; and (2) a virtual execution environment, which emulates application execution under diverse resource availability enabling off-line collection of information about resulting behavior. Together, these components permit automatic run-time decisions on when to adapt by continuously monitoring resource conditions and application progress, and how to adapt by dynamically choosing an application configuration most appropriate for the prescribed user preference. We evaluate the framework using an interac- tive distributed image visualization application and a parallel image processing application. The framework permits automatic adaptation to changes in execution environment characteristics such as available network bandwidth or data arrival pattern by choosing a different application configuration that satisfies user preferences of output quality and timeliness.},
annote = {This paper describes a framework for tuning distributed applications at runtime by exposing a set of abstraction choices and a virtual execution environment which emulates application execution.},
author = {Chang, F. and Karamcheti, V.},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Chang, Karamcheti - A Framework for Automatic Adaptation of Tunable Distributed Applications.pdf:pdf},
journal = {Cluster Computing},
keywords = {application adaptation,application performance optimization,quality of service (QoS)},
number = {1},
title = {{A Framework for Automatic Adaptation of Tunable Distributed Applications}},
volume = {4},
year = {2001}
}
@inproceedings{Xiang2015,
annote = {NULL},
author = {Xiang, Lingxiang and Scott, Michael L},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Xiang, Scott - Software Partitioning of Hardware Transactions ∗.pdf:pdf},
isbn = {9781450332057},
keywords = {compiler automation,hardware transactional memory,partitioned transac-,tions},
title = {{Software Partitioning of Hardware Transactions ∗}},
year = {2015}
}
@inproceedings{Baishakhi2014a,
abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages.},
annote = {NULL},
author = {Baishakhi, R. and Posnett, D. and Filkov, V. and Devanbu, P.},
booktitle = {FSE},
doi = {10.1145/2635868.2635922},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Baishakhi et al. - A Large Scale Study of Programming Languages and Code Quality in Github.pdf:pdf},
isbn = {9781450330565},
keywords = {bug fix,code quality,empirical research,programming language,regression analysis,software domain,type system},
publisher = {ACM},
title = {{A Large Scale Study of Programming Languages and Code Quality in Github}},
year = {2014}
}
@article{Abadi,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple com-putational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previ-ous " parameter server " designs the management of shared state is built into the system, TensorFlow enables devel-opers to experiment with novel optimizations and train-ing algorithms. TensorFlow supports a variety of appli-cations, with particularly strong support for training and inference on deep neural networks. Several Google ser-vices use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing sys-tems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
annote = {NULL},
author = {Abadi, M. and Barham, P. and Chen, J. and Chen, Z. and Davis, A. and Dean, J. and Devin, M. and Ghemawat, S. and Irving, G. and Isard, M. and Kudlur, M. and Levenberg, J. and Monga, R. and Moore, S. and Murray, D. G. and Steiner, B. and Tucker, P. and Vasudevan, V. and Warden, P. and Wicke, M. and Yu, Y. and Zheng, X.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Abadi et al. - TensorFlow A system for large-scale machine learning.pdf:pdf},
journal = {arXiv:1605.08695},
title = {{TensorFlow: A system for large-scale machine learning}},
year = {2016}
}
@article{Santos2017,
author = {Santos, E. A. and Campbell, J. C. and Hindle, A. and Amaral, N.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Santos et al. - Finding and correcting syntax errors using recurrent neural networks.pdf:pdf},
journal = {PeerJ Preprints},
keywords = {TOSKIM,deep learning,github,javascript,lstm,n-gram,network,neural,program repair,rnn,syntax error,syntax error correction},
mendeley-tags = {TOSKIM},
title = {{Finding and correcting syntax errors using recurrent neural networks}},
year = {2017}
}
@inproceedings{Ritson2016,
annote = {NULL},
author = {Ritson, Carl G. and Owens, Scott},
booktitle = {PPoPP},
doi = {10.1145/2851141.2851150},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ritson, Owens - Benchmarking weak memory models.pdf:pdf},
isbn = {9781450340922},
title = {{Benchmarking weak memory models}},
url = {http://dl.acm.org/citation.cfm?doid=2851141.2851150},
year = {2016}
}
@inproceedings{Mirhoseini2018,
author = {Mirhoseini, A. and Goldie, A. and Pham, H. and Steiner, B. and Le, Q. V. and Dean, J.},
booktitle = {ICLR},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Mirhoseini et al. - A Hierarchical Model for Device Placement.pdf:pdf},
title = {{A Hierarchical Model for Device Placement}},
year = {2018}
}
@article{Carbonneaux2014,
abstract = {Writing parallel programs is not easy, and debugging them is usually a nightmare. Over the last years, several researchers coped with these difficulties by developing a structured approach to parallel programming via template based compiler techniques. The skeleton programming approach uses a set of predefined patterns for parallel computations. The skeletons are higher order functional templates that describe the program underlying parallelism. So, marrying a full-fledged functional language and a carefully crafted skeleton algebra seems to be the way to go to obtain a powerful parallel programming environment. This document describes the Sklml (["sk@lEmEl]) system that embeds an innovative compositional skeleton algebra into the OCaml language. Sklml provides an optimizing compiler and a runtime computing networkmanager. Thanks to its skeleton algebra, Sklml provides two evaluation regimes to programs: a regular sequential evaluation (merely used for prototyping and debugging) and a parallel evaluation obtained via a recompilation of the source program in parallel mode. Sklml is also designed to be a parallel computation driver running worker programs written in heterogeneous external languages (e.g. C, C++, Fortran)},
annote = {NULL},
author = {Carbonneaux, Q and Cl{\'{e}}ment, F and Weis, P},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Carbonneaux, Cl{\'{e}}ment, Weis - Sklml Functional Parallel Programming.pdf:pdf},
title = {{Sklml: Functional Parallel Programming}},
url = {http://sklml.inria.fr/doc/pdf/UserManual.pdf},
year = {2014}
}
@inproceedings{Yang2011b,
abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
annote = {NULL},
author = {Yang, X. and Chen, Y. and Eide, E. and Regehr, J.},
booktitle = {PLDI},
doi = {10.1145/2345156.1993532},
issn = {03621340},
keywords = {automated testi,compiler defect,compiler testing,random program generation,random testing},
title = {{Finding and Understanding Bugs in C Compilers}},
year = {2011}
}
@inproceedings{Prountzos2015a,
annote = {NULL},
author = {Prountzos, Dimitrios and Manevich, Roman},
booktitle = {PLDI},
doi = {10.1145/2737924.2737953},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Prountzos, Manevich - Synthesizing Parallel Graph Programs via Automated Planning.pdf:pdf},
isbn = {9781450334686},
issn = {15232867},
keywords = {Amorphous Data-parallelism,Compiler Optimization,Concurrency,Irregular Programs,Parallelism,Synthesis},
title = {{Synthesizing Parallel Graph Programs via Automated Planning}},
year = {2015}
}
@inproceedings{Magni2014,
abstract = {OpenCL has been designed to achieve functional portability across multi-core devices from different vendors. However, the lack of a single cross-target optimizing compiler severely limits performance portability of OpenCL programs. Pro- grammers need to manually tune applications for each spe- cific device, preventing effective portability. We target a compiler transformation specific for data-parallel languages: thread-coarsening and show it can improve performance across different GPU devices. We then address the problem of se- lecting the best value for the coarsening factor parameter, i.e., deciding how many threads to merge together. We ex- perimentally show that this is a hard problem to solve: good configurations are difficult to find and naive coarsening in fact leads to substantial slowdowns. We propose a solution based on a machine-learning model that predicts the best coarsening factor using kernel-function static features. The model automatically specializes to the different architectures considered. We evaluate our approach on 17 benchmarks on four devices: two Nvidia GPUs and two different generations of AMD GPUs. Using our technique, we achieve speedups between 1.11× and 1.33× on average.},
annote = {NULL},
author = {Magni, A. and Dubach, C. and O'Boyle, M.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Magni, Dubach, O'Boyle - Automatic Optimization of Thread-Coarsening for Graphics Processors.pdf:pdf},
publisher = {ACM},
title = {{Automatic Optimization of Thread-Coarsening for Graphics Processors}},
year = {2014}
}
@techreport{Guo2012,
annote = {A very interesting reading.








"Be proactive in talking with professors to find research topics that are mutually interesting, and no matter what, don't just hole up in isolation."},
author = {Guo, Philip J},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Guo - The Ph.D. Grind.pdf:pdf},
title = {{The Ph.D. Grind}},
year = {2012}
}
@article{Pananilath2014,
annote = {NULL},
author = {Pananilath, I. M.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Pananilath - An Optimizing Code Generator for a Class of Lattice-Boltzmann Computations.pdf:pdf},
number = {July},
title = {{An Optimizing Code Generator for a Class of Lattice-Boltzmann Computations}},
volume = {12},
year = {2014}
}
@article{Hill2007,
abstract = {We apply Amdahl's Law to multicore chips using symmet- ric cores, asymmetric cores, and dynamic techniques that allows cores to work together on sequential execution. To Amdahl's simple software model, we add a simple hardware model based on fixed chip resources. A key result we find is that, even as we enter the multicore era, researchers should still seek methods of speeding sequen- tial execution. Moreover, methods that appear locally ineffi- cient (e.g., tripling sequential performance with a 9x resource cost) can still be globally efficient as they reduce the sequential phase when the rest of the chip's resources are idle.},
annote = {This paper presents a hardware model for Amdahl's law for computing speedups of symmetric, asymmetric, and dynamic multicore chips.

The paper has a number of interesting results and is well worth citing when justifying the need for increased parallelism.},
author = {Hill, Mark D and Marty, Michael R},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Hill, Marty - Amdahl's Law in the Multicore Era.pdf:pdf},
journal = {Computer},
number = {7},
title = {{Amdahl's Law in the Multicore Era}},
volume = {41},
year = {2007}
}
@inproceedings{Calcagno2015,
abstract = {For organisations like Facebook, high quality software is im- portant. However, the pace of change and increasing complexity of mod- ern code makes it difficult to produce error-free software. Available tools are often lacking in helping programmers develop more reliable and se- cure applications. Formal verification is a technique able to detect software errors statically, before a product is actually shipped. Although this aspect makes this technology very appealing in principle, in practice there have been many difficulties that have hindered the application of software verification in industrial environments. In particular, in an organisation like Facebook where the release cycle is fast compared to more traditional industries, the deployment of formal techniques is highly challenging. This paper describes our experience in integrating a verification tool based on static analysis into the software development cycle at Facebook.},
annote = {NULL},
author = {Calcagno, C. and Distefano, D. and Dubreil, J. and Gabi, D. and Luca, M. and Hearn, P. O. and Papakonstantinou, I.},
booktitle = {NASA Formal Methods},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Calcagno et al. - Moving Fast with Software Verification.pdf:pdf},
title = {{Moving Fast with Software Verification}},
year = {2015}
}
@article{Alipour2016,
abstract = {Random testing can be a powerful and scalable method for finding faults in software. However, sophisticated random testers usually test a whole program, not individual components. Writing random testers for individual components of complex programs may require unreasonable effort. In this paper we present a novel method, directed swarm testing, that uses statistics and a variation of random testing to produce random tests that focus on only part of a program, increasing the frequency with which tests cover the targeted code. We demonstrate the effectiveness of this technique using real-world programs and test systems (the YAFFS2 file system, GCC, and Mozilla's SpiderMonkey JavaScript engine), and discuss various strategies for directed swarm testing. The best strategies can improve coverage frequency for targeted code by a factor ranging from 1.1-4.5x on average, and from nearly 3x to nearly 9x in the best case. For YAFFS2, directed swarm testing never decreased coverage, and for GCC and SpiderMonkey coverage increased for over 99{\%} and 73{\%} of targets, respectively, using the best strategies. Directed swarm testing improves detection rates for real SpiderMonkey faults, when the code in the introducing commit is targeted. This lightweight technique is applicable to existing industrial-strength random testers. {\textcopyright} 2016 ACM.},
author = {Alipour, A. and Groce, A. and Gopinath, R. and Christi, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Alipour et al. - Generating Focused Random Tests Using Directed Swarm Testing.pdf:pdf},
journal = {ISSTA},
title = {{Generating Focused Random Tests Using Directed Swarm Testing}},
year = {2016}
}
@inproceedings{Ravishankar2015,
annote = {NULL},
author = {Ravishankar, Mahesh and Ramanujam, J and Rountev, Atanas},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ravishankar, Ramanujam, Rountev - Distributed Memory Code Generation for Mixed Irregular Regular Computations.pdf:pdf},
isbn = {9781450332057},
keywords = {compilation,distributed memory,executor,inspector,irregular computation,polyhedral},
title = {{Distributed Memory Code Generation for Mixed Irregular / Regular Computations}},
year = {2015}
}
@inproceedings{Cammarota2013,
annote = {NULL},
author = {Cammarota, R. and Beni, L. A. and Nicolau, A. and Veidenbaum, A. V.},
booktitle = {APPT},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Cammarota et al. - Optimizing Program Performance via Similarity, Using a Feature-agnostic Approach.pdf:pdf},
keywords = {Cluster Analysis,Empirical Performance Modeling,Feature-agnostic,Program Characterization,Program Optimization},
publisher = {Springer},
title = {{Optimizing Program Performance via Similarity, Using a Feature-agnostic Approach}},
year = {2013}
}
@inproceedings{Auslander1996,
abstract = {Dynamic compilation enables optimizations based on the values of invariant data computed at run-time. Using the values of these run- time constants, a dynamic compiler can eliminate their memory loads, perform constant propagation and folding, remove branches they determine, and fully unroll loops they bound. However, the performance benefits of the more efficient, dynamically-compiled code are offset by the run-time cost of the dynamic compile. Our approach to dynamic compilation strives for both fast dynamic compilation and high-quality dynamically-compiled code: the programmer annotates regions of the programs that should be compiled dynamically; a static, optimizing compiler automatically produces pre-optimized machine-code templates, using a pair of dataflow analyses that identify which variables will be constant at run-time; and a simple, dynamic compiler copies the templates, patching in the computed values of the run-time constants, to produce optimized, executable code. Our work targets general- purpose, imperative programming languages, initially C. Initial experiments applying dynamic compilation to C programs have produced speedups ranging from 1.2 to 1.8.},
address = {New York, New York, USA},
annote = {The paper describes a combination of static and dynamic compilers in order to minimise the overhead of dynamic compilation. It allows programmers to annotate C code so that the static compiler can generate optimised machine code templates which can be specialized at run time by the dynamic compiler. These annotations plan out optimisations that depend on run-time constants.








The process of adding these annotations is entirely manual and dependent on the programmer. For the idea to gain widespread traction, it would be necessary to automate these.},
author = {Auslander, J. and Philipose, M. and Chambers, C. and Eggers, S. J. and Bershad, B. N.},
booktitle = {PLDI},
doi = {10.1145/231379.231409},
file = {:Users/cec/Google Drive/Mendeley Library/1996 - Auslander et al. - Fast, effective dynamic compilation.pdf:pdf},
isbn = {0897917952},
publisher = {ACM Press},
title = {{Fast, effective dynamic compilation}},
url = {http://portal.acm.org/citation.cfm?doid=231379.231409},
year = {1996}
}
@inproceedings{Sharma2015,
abstract = {Previous efforts to formally verify code written for GPUs have focused solely on kernels written within the traditional data-parallel GPU programming model. No previous work has considered the higher performance, but more complex, warp-specialized kernels based on producer-consumer named barriers available on current hardware. In this work we present the first formal operational semantics for named barriers and define what it means for a warp-specialized kernel to be correct. We give algorithms for verifying the correctness of warp-specialized kernels and prove that they are both sound and complete for the most common class of warp-specialized programs. We also present WEFT, a verification tool for checking warp-specialized code. Using WEFT, we discover several non-trivial bugs in production warp-specialized kernels.},
annote = {NULL},
author = {LSharma, R. and Bauer, M. and Aiken, A.},
booktitle = {PLDI},
doi = {10.1145/2737924.2737962},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - LSharma, Bauer, Aiken - Verification of Producer-Consumer Synchronization in GPU Programs.pdf:pdf},
isbn = {9781450334686},
issn = {03621340},
keywords = {GPUs,Verification,barrier recycling,data races,deadlock,named barriers,synchronization,warp specialization},
title = {{Verification of Producer-Consumer Synchronization in GPU Programs}},
url = {http://dl.acm.org/citation.cfm?id=2737924.2737962},
year = {2015}
}
@article{Nasrabadi,
abstract = {This article is aimed at the design and implementation of a file format fuzzer. Files are significant inputs to the most of real-world applications. A substantial difficulty with generating input files as test data is to recon the underlying structure and format of the files. In order to distinguish pure data stored in a file from the meta-data describing the file format, a deep learning method based on a neural language model is proposed in this article. The resultant learned model could be applied as a hybrid test data generator, to generate and fuzz both the textual and none-textual sections of the input file. Moreover, the model could be applied to generate test data to fuzz both the meta-data and the ordinary data stored in the file. Our experiments with two known fuzzing tools, AFL and Learn{\&}Fuzz, demonstrate the relatively high code coverage of our proposed method. The experiments also indicate simple neural language models provide a more accurate learning model, than the complicated encoder-decoder models.},
archivePrefix = {arXiv},
arxivId = {1812.09961v1},
author = {Nasrabadi, M. Z. and Parsa, S. and Kalaee, A.},
eprint = {1812.09961v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Nasrabadi, Parsa, Kalaee - Neural Fuzzing A Neural Approach to Generate Test Data for File Format Fuzzing.pdf:pdf},
journal = {arXiv:1812.09961},
keywords = {Code coverage,Deep learning,Fuzzing,Neural language models,Recurrent neural networks,Test data generation},
title = {{Neural Fuzzing: A Neural Approach to Generate Test Data for File Format Fuzzing}},
url = {https://arxiv.org/pdf/1812.09961.pdf},
year = {2018}
}
@inproceedings{Fursin2007,
abstract = {Current compilers fail to deliver satisfactory levels of performance on modern processors, due to rapidly evolving hardware, fixed and black-box optimization heuristics, simplistic hardware models, inability to fine-tune the ap- plication of transformations, and highly dynamic behavior of the system. This analysis suggests to revisit the structure and interactions of optimizing compil- ers. Building on the empirical knowledge accumulated from previous iterative optimization prototypes, we propose to open the compiler, exposing its control and decision mechanisms to external optimization heuristics. We suggest a sim- ple, practical, and non-intrusive way to modify current compilers, allowing an external tool to access and modify all compiler optimization decisions. To avoid the pitfall of revealing all the compiler intermediate representation and libraries to a point where it would rigidify the whole internals and stiffen further evolution, we choose to control the decision process itself, granting access to the only high-level features needed to effectively take a decision. This restriction is compatible with our fine-tuning and fine-grained interaction, and allows to tune programs for best performance, code size, power consumption; we also believe it allows for joint architecture-compiler design-space exploration.By exposing only the decisions that arise from the opportunities suggested by the program syntax and semantics and only when the associated legality checks are satisfied, we dramatically reduce the transformation search space. We developed an Interactive Compilation Interface (ICI) with different external optimization drivers for the commercial open-source PathScale EKOPath Com- piler (derived from Open64); this interface is being ported to the GCC. This toolset led to strong performance improvements on large applications (rather than just kernels) through the iterative, fine-grain customization of compilation strategies at the loop or instruction-level; it also enabled continuous (dynamic) optimization research.We expect that iterative interactive compilers will replace the current multiplicity of non-portable, rigid transformation frameworks with unnecessary duplications of compiler internals. Furthermore, unifying the inter- face with compiler passes simplifies future compiler developments, where the best optimization strategy is learned automatically and continuously for a given platform, objective function, program or application domain, using statistical or machine learning techniques. It enables life-long, whole-program compilation research, without the overhead of breaking-up the compiler into a set of well- defined compilation components (communicating through persistent intermedi- ate languages), even if such an evolution could be desirable at some point (but much more intrusive). It also opens optimization heuristics to a wide area of iter- ative search, decision and adaptation schemes and allows optimization knowledge reuse among different programs and architectures for collective optimizations.},
annote = {NULL},
author = {Fursin, G. and Cohen, A.},
booktitle = {SMART},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Fursin, Cohen - Building a Practical Iterative Interactive Compiler.pdf:pdf},
title = {{Building a Practical Iterative Interactive Compiler}},
year = {2007}
}
@article{Gebru2017,
abstract = {The United States spends more than {\$}1B each year on the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed half a decade. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may provide a cheaper and faster alternative. Here, we present a method that determines socioeconomic trends from 50 million images of street scenes, gathered in 200 American cities by Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22M automobiles in total (8{\%} of all automobiles in the US), was used to accurately estimate income, race, education, and voting patterns, with single-precinct resolution. (The average US precinct contains approximately 1000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a 15-minute drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next Presidential election (88{\%} chance); otherwise, it is likely to vote Republican (82{\%}). Our results suggest that automated systems for monitoring demographic trends may effectively complement labor-intensive approaches, with the potential to detect trends with fine spatial resolution, in close to real time.},
annote = {A really interesting application of vision. They detect car makes in google street view images, and use that to determine demographics for the area. E.g. if there's more sedans than pickups, the precint (1000 people) is likely to vote Democrat (88{\%}), else Republican (82{\%}).},
archivePrefix = {arXiv},
arxivId = {1702.06683},
author = {Gebru, T. and Krause, J. and Wang, Y. and Chen, D. and Deng, J. and Aiden, E. L. and Fei-Fei, L.},
eprint = {1702.06683},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Gebru et al. - Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US.pdf:pdf},
journal = {arXiv:1702.06683},
title = {{Using Deep Learning and Google Street View to Estimate the Demographic Makeup of the US}},
url = {http://arxiv.org/abs/1702.06683},
year = {2017}
}
@article{Brock2018,
author = {Brock, A. and Donahue, J. and Simonyan, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Brock, Donahue, Simonyan - Large Scale GAN Training For High Fidelity Natural Image Synthesis.pdf:pdf},
journal = {arXiv:1809.11096},
title = {{Large Scale GAN Training For High Fidelity Natural Image Synthesis}},
url = {https://openreview.net/forum?id=B1xsqj09Fm},
year = {2018}
}
@inproceedings{Ogilvie2014a,
abstract = {Building effective optimization heuristics is a challenging task which often takes developers several months if not years to com- plete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data. However, ob- taining this data can take months per platform. This is becoming an ever more critical problem and if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines. In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which significantly reduces this train- ing overhead. Typically in supervised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry. This wastes effort on parts of the space that contribute lit- tle to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples. We demonstrate this technique by automatically constructing a model to determine on which device to execute four parallel programs at differing problem dimensions for a representative Cpu–Gpu based heterogeneous system. Our methodology is remarkably simple and yet effective, making it a strong candidate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state- of-the-art.},
annote = {This paper describes the application of query-by-commitee active learning to reduce the training time of machine learning iterative compilation techniques. Starting with a small set of randomly chosen points within the search space, QBC allows learning algorithms to choose the data from which it learns. QBC uses entropy to assess disagreement between different models. The paper has a weak Related Work section, and does not show experimental results for many benchmarks.},
author = {Ogilvie, W. F. and Petoumenos, P. and Wang, Z. and Leather, H.},
booktitle = {LCPC},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Ogilvie et al. - Fast Automatic Heuristic Construction Using Active Learning.pdf:pdf},
keywords = {machine learning,workload scheduling},
title = {{Fast Automatic Heuristic Construction Using Active Learning}},
year = {2014}
}
@misc{Etessamie,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 23. Shortest Paths and Dijkstra's algorithm.pdf:pdf},
number = {Chapter 10},
title = {{23. Shortest Paths and Dijkstra's algorithm}},
year = {2015}
}
@article{Isola2016,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
annote = {NULL},
author = {Isola, P. and Zhu, J. and Zhou, T. and Efros, A. A.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Isola et al. - Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
journal = {arXiv:1611.07004},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {http://arxiv.org/abs/1611.07004},
year = {2016}
}
@article{Rossum2012f,
annote = {NULL},
author = {Rossum, Guido Van},
doi = {10.1093/aje/kwq356},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Unicode HOWTO.pdf:pdf},
issn = {14766256},
pmid = {20870797},
title = {{Unicode HOWTO}},
year = {2016}
}
@inproceedings{Dastgeer2011,
abstract = {SkePU is a C++ template library that provides a simple and unified interface for specifying data-parallel computa- tions with the help of skeletons on GPUs using CUDA and OpenCL. The interface is also general enough to support other architectures, and SkePU implements both a sequen- tial CPU and a parallel OpenMP backend. It also supports multi-GPU systems. Currently available skeletons in SkePU include map, reduce, mapreduce, map-with-overlap, map- array, and scan. The performance of SkePU generated code is comparable to that of hand-written code, even for more complex applications such as ODE solving. In this paper, we discuss initial results from auto-tuning SkePU using an off-line, machine learning approach where we adapt skeletons to a given platform using training data. The prediction mechanism at execution time uses off-line pre-calculated estimates to construct an execution plan for any desired configuration with minimal overhead. The pre- diction mechanism accurately predicts execution time for repetitive executions and includes a mechanism to predict execution time for user functions of different complexity. The tuning framework covers selection between different back- ends as well as choosing optimal parameter values for the selected backend. We will discuss our approach and initial results obtained for different skeletons (map, mapreduce, reduce).},
annote = {Dastgeer uses training data to to generate a machine-learning model offline, which is used at runtime to determine an execution plan (selecting between sequential, OpenMP, OpenCL, and CUDA back-ends) and parameter values for each. The paper is "first results", and gives no description of the machine learning techniques used, the training process, or the experimental method. Cited by 35.},
author = {Dastgeer, U. and Enmyren, J. and Kessler, C. W.},
booktitle = {IWMSE},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Dastgeer, Enmyren, Kessler - Auto-tuning SkePU a Multi-Backend Skeleton Programming Framework for Multi-GPU Systems.pdf:pdf},
publisher = {ACM},
title = {{Auto-tuning SkePU: a Multi-Backend Skeleton Programming Framework for Multi-GPU Systems}},
year = {2011}
}
@article{Ball1994,
abstract = {This paper describes algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs with respect to the commonly used technique of placing code in each basic block. Program profiling counts the number of times each basic block in a program executes. Instruction tracing records the sequence of basic blocks traversed in a program execution. The algorithms optimize the placement of counting/tracing code with respect to the expected or measured frequency of each block or edge in a program's control-flow graph. We have implemented the algorithms in a profiling/tracing tool, and they substantially reduce the overhead of profiling and tracing.We also define and study the hierarchy of profiling problems. These problems have two dimensions: what is profiled (i.e., vertices (basic blocks) or edges in a control-flow graph) and where the instrumentation code is placed (in blocks or along edges). We compare the optimal solutions to the profiling problems and describe a new profiling problem: basic-block profiling with edge counters. This problem is important because an optimal solution to any other profiling problem (for a given control-flow graph) is never better than an optimal solution to this problem. Unfortunately, finding an optimal placement of edge counters for vertex profiling appears to be a hard problem in general. However, our work shows that edge profiling with edge counters works well in practice because it is simple and efficient and finds optimal counter placements in most cases. Furthermore, it yields more information than a vertex profile. Tracing also benefits from placing instrumentation code along edges rather than on vertices.},
annote = {NULL},
author = {Ball, T. and Larus, J. R.},
doi = {10.1145/183432.183527},
file = {:Users/cec/Google Drive/Mendeley Library/1994 - Ball, Larus - Optimally profiling and tracing programs.pdf:pdf},
isbn = {0897914538},
issn = {01640925},
journal = {TOPLAS},
number = {4},
title = {{Optimally profiling and tracing programs}},
volume = {16},
year = {1994}
}
@inproceedings{Chang,
annote = {NULL},
author = {Chang, Yen-jung},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chang - A Parallel Algorithm for Global States Enumeration in Concurrent Systems.pdf:pdf},
isbn = {9781450332057},
keywords = {global states,parallel algorithm,predicate detection},
title = {{A Parallel Algorithm for Global States Enumeration in Concurrent Systems}},
year = {2015}
}
@inproceedings{Jiao2015,
annote = {NULL},
author = {Jiao, Qing and Lu, Mian and Huynh, Huynh Phung and Mitra, Tulika},
booktitle = {CGO},
doi = {10.1109/CGO.2015.7054182},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Jiao et al. - Improving GPGPU energy-efficiency through concurrent kernel execution and DVFS.pdf:pdf},
isbn = {9781479981618},
publisher = {IEEE},
title = {{Improving GPGPU energy-efficiency through concurrent kernel execution and DVFS}},
year = {2015}
}
@misc{Stirlingb,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 30. Ramsey numbers.pdf:pdf},
number = {Chapter 7},
title = {{30. Ramsey numbers}},
volume = {7},
year = {2015}
}
@article{Bailey1991a,
abstract = {A new set of benchmarks has been developed for the performance evaluation of highly parallel supercomputers. These benchmarks consist of five parallel kernels and three simulated application benchmarks. Together they mimic the computation and data movement characteristics of large scale Computational Fluid Dynamics (CFD) applications. The principal distinguishing feature of these benchmarks is their 'pencil and paper' specification-all details of these benchmarks are specified only algorithmically. In this way many of the difficulties associated with conventional benchmarking approaches on highly parallel systems are avoided.},
annote = {NULL},
author = {Bailey, D. H. and Barszcz, E. and Barton, J. and Browning, D. and Carter, R. and Dagum, L. and Fatoohi, R. and Fineberg, S. and Frederickson, P. and Lasinski, T. and Schreiber, R. and Simon, H. and Venkatakrishnan, V. and Weeratunga, S.},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Bailey et al. - The NAS Parallel Benchmarks.pdf:pdf},
journal = {IJHPCA},
number = {3},
title = {{The NAS Parallel Benchmarks}},
volume = {5},
year = {1991}
}
@article{Arcuri2012,
abstract = {A substantial amount of work has shed light on whether random$\backslash$ntesting is actually a useful testing technique. Despite its simplicity,$\backslash$nseveral successful real-world applications have been reported in the$\backslash$nliterature. Although it is not going to solve all possible testing$\backslash$nproblems, random testing appears to be an essential tool in the hands of$\backslash$nsoftware testers. In this paper, we review and analyze the debate about$\backslash$nrandom testing. Its benefits and drawbacks are discussed. Novel results$\backslash$naddressing general questions about random testing are also presented,$\backslash$nsuch as how long does random testing need, on average, to achieve$\backslash$ntesting targets (e.g., coverage), how does it scale, and how likely is$\backslash$nit to yield similar results if we rerun it on the same testing problem$\backslash$n(predictability). Due to its simplicity that makes the mathematical$\backslash$nanalysis of random testing tractable, we provide precise and rigorous$\backslash$nanswers to these questions. Results show that there are practical$\backslash$nsituations in which random testing is a viable option. Our theorems are$\backslash$nbacked up by simulations and we show how they can be applied to most$\backslash$ntypes of software and testing criteria. In light of these results, we$\backslash$nthen assess the validity of empirical analyzes reported in the$\backslash$nliterature and derive guidelines for both practitioners and scientists.},
annote = {NULL},
author = {Arcuri, Andrea and Iqbal, Muhammad Zohaib and Briand, Lionel},
doi = {10.1109/TSE.2011.121},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Arcuri, Iqbal, Briand - Random testing Theoretical results and practical implications.pdf:pdf},
isbn = {978-1-4503-1454-1},
issn = {00985589},
journal = {TSE},
keywords = {Coupon collector,Schur function,adaptive random testing,partition testing,predictability,random testing,theory},
number = {2},
title = {{Random testing: Theoretical results and practical implications}},
volume = {38},
year = {2012}
}
@inproceedings{Fagan,
annote = {NULL},
author = {Fagan, Michael and Mellor-crummey, John},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Fagan, Mellor-crummey - High Performance Locks for Multi-level NUMA Systems.pdf:pdf},
isbn = {9781450332057},
keywords = {analyt-,hierarchical locks,ical modeling,lock fairness,lock throughput,mcs,numa,shared-memory numa system,spin locks},
title = {{High Performance Locks for Multi-level NUMA Systems}},
year = {2015}
}
@article{Wigner1960,
abstract = {No Abstract.},
author = {Wigner, E.},
file = {:Users/cec/Google Drive/Mendeley Library/1960 - Wigner - The Unreasonable Effectiveness of Mathematics in the Natural Sciences.pdf:pdf},
isbn = {053403201X},
issn = {1097-0312},
journal = {Communications on Pure and Applied Mathematics},
keywords = {TOPRINT,TOSTUDY},
mendeley-tags = {TOPRINT,TOSTUDY},
number = {1},
title = {{The Unreasonable Effectiveness of Mathematics in the Natural Sciences}},
volume = {13},
year = {1960}
}
@article{Ioannidis2018,
abstract = {The era of data deluge has sparked the interest in graph-based learning methods in a number of disciplines such as sociology, biology, neuroscience, or engineering. In this paper, we introduce a graph recurrent neural network (GRNN) for scalable semi-supervised learning from multi-relational data. Key aspects of the novel GRNN architecture are the use of multi-relational graphs, the dynamic adaptation to the different relations via learnable weights, and the consideration of graph-based regularizers to promote smoothness and alleviate over-parametrization. Our ultimate goal is to design a powerful learning architecture able to: discover complex and highly non-linear data associations, combine (and select) multiple types of relations, and scale gracefully with respect to the size of the graph. Numerical tests with real data sets corroborate the design goals and illustrate the performance gains relative to competing alternatives.},
archivePrefix = {arXiv},
arxivId = {1811.02061},
author = {Ioannidis, Vassilis N. and Marques, Antonio G. and Giannakis, Georgios B.},
doi = {10.1109/CIT/IUCC/DASC/PICOM.2015.235},
eprint = {1811.02061},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ioannidis, Marques, Giannakis - A Recurrent Graph Neural Network for Multi-Relational Data.pdf:pdf},
isbn = {0092861512},
issn = {2168-4790},
title = {{A Recurrent Graph Neural Network for Multi-Relational Data}},
url = {http://arxiv.org/abs/1811.02061},
year = {2018}
}
@inproceedings{Jamshidi2014,
annote = {NULL},
author = {Jamshidi, D. A. and Samadi, M. and Mahlke, S.},
booktitle = {PACT},
doi = {10.1145/2628071.2628072},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Jamshidi, Samadi, Mahlke - D2MA Accelerating Coarse-Grained Data Transfer for GPUs.pdf:pdf},
isbn = {9781450328098},
issn = {1089795X},
keywords = {dma,dynamic management,gpus,shared memory,software-managed caches,throughput processing},
publisher = {ACM},
title = {{D2MA: Accelerating Coarse-Grained Data Transfer for GPUs}},
year = {2014}
}
@misc{UniversityofEdinburgh2014b,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 3. Dataflow analysis.pdf:pdf},
title = {{3. Dataflow analysis}},
year = {2015}
}
@inproceedings{Kim2012,
abstract = {Automatic parallelization for clusters is a promising alternative to time-consuming, error-prone manual parallelization. However, au- tomatic parallelization is frequently limited by the imprecision of static analysis. Moreover, due to the inherent fragility of static anal- ysis, small changes to the source code can significantly undermine performance. By replacing static analysiswith speculation and pro- filing, automatic parallelization becomes more robust and applica- ble. Ana{\"{i}}ve automatic speculative parallelization does not scale for distributed memory clusters, due to the high bandwidth required to validate speculation. This work is the first automatic speculative DOALL (Spec-DOALL) parallelization system for clusters. We have implemented a prototype automatic parallelization system, called Cluster Spec-DOALL, which consists of a Spec-DOALL parallelizing compiler and a speculative runtime for clusters. Since the compiler optimizes communication patterns, and the runtime is optimized for the cases in which speculation succeeds, Cluster Spec-DOALL minimizes the communication and validation over- heads of the speculative runtime. Across 8 benchmarks, Cluster Spec-DOALL achieves a geomean speedup of 43.8× on a 120- core cluster, whereas DOALL without speculation achieves only 4.5× speedup. This demonstrates that speculation makes scalable fully-automatic parallelization for clusters possible.},
annote = {Cited by 25.},
author = {Kim, H. and Johnson, N. P. and Lee, J. W. and Mahlke, S. A. and August, D. I.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kim et al. - Automatic Speculative DOALL for Clusters.pdf:pdf},
number = {Cgo},
publisher = {IEEE},
title = {{Automatic Speculative DOALL for Clusters}},
year = {2012}
}
@inproceedings{Keryell2014,
annote = {NULL},
author = {Keryell, Ronan},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Keryell - Modern C, OpenCL SYCL {\&} OpenCL CL2.hpp.pdf:pdf},
title = {{Modern C++, OpenCL SYCL {\&} OpenCL CL2.hpp}},
year = {2014}
}
@inproceedings{Vassiliadis,
annote = {NULL},
author = {Vassiliadis, Vassilis and Parasyris, Konstantinos and Chalios, Charalambos and Antonopoulos, Christos D and Bellas, Nikolaos and Nikolopoulos, Dimitrios S},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Vassiliadis et al. - A Programming Model and Runtime System for Significance-Aware Energy-Efficient Computing.pdf:pdf},
isbn = {9781450332057},
keywords = {approximate computing,energy saving,model,programming,runtime system},
title = {{A Programming Model and Runtime System for Significance-Aware Energy-Efficient Computing}},
year = {2015}
}
@inproceedings{Danelutto2006,
abstract = {Structured parallel programming promises to raise the level of abstraction perceived by programmers when implementing parallel applications. In the meanwhile, however, it restricts the freedom of pro- grammers to implement arbitrary parallelism exploitation patterns. In this work we discuss a data flow implementation methodology for skele- ton based structured parallel programming environments that easily in- tegrates arbitrary, user-defined parallelism exploitation patterns while preserving most of the benefits typical of structured parallel program- ming models.},
annote = {NULL},
author = {Danelutto, M and Dazzi, P},
booktitle = {ICCS},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Danelutto, Dazzi - Joint Structured Unstructured Parallelism Exploitation in muskel.pdf:pdf},
publisher = {Springer},
title = {{Joint Structured / Unstructured Parallelism Exploitation in muskel}},
year = {2006}
}
@inproceedings{Fursin2007b,
abstract = {Iterative optimization has become a popular technique to obtain im- provements over the default settings in a compiler for performance-critical appli- cations, such as embedded applications. An implicit assumption, however, is that the best configuration found for any arbitrary data set will work well with other data sets that a program uses. In this article, we evaluate that assumption based on 20 data sets per bench- mark of the MiBench suite. We find that, though a majority of programs exhibit stable performance across data sets, the variability can significantly increase with many optimizations. However, for the best optimization configurations, we find that this variability is in fact small. Furthermore, we showthat it is possible to find a compromise optimization configuration across data sets which is often within 5{\%} of the best possible configuration for most data sets, and that the iterative process can converge in less than 20 iterations (for a population of 200 optimiza- tion configurations). All these conclusions have significant and positive implica- tions for the practical utilization of iterative optimization.},
annote = {MiDataSets is a dataset suite for MiBench benchmarks.},
author = {Fursin, G. and Cavazos, J. and O'Boyle, M. and Temam, O.},
booktitle = {HiPEAC},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Fursin et al. - MiDataSets Creating the Conditions for a More Realistic Evaluation of Iterative Optimization.pdf:pdf},
publisher = {Springer},
title = {{MiDataSets: Creating the Conditions for a More Realistic Evaluation of Iterative Optimization}},
year = {2007}
}
@inproceedings{Yang2011,
abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
annote = {NULL},
author = {Yang, X. and Chen, Y. and Eide, E. and Regehr, J.},
booktitle = {PLDI},
doi = {10.1145/2345156.1993532},
issn = {03621340},
keywords = {automated testi,compiler defect,compiler testing,random program generation,random testing},
title = {{Finding and Understanding Bugs in C Compilers}},
year = {2011}
}
@inproceedings{Memon2013,
abstract = {Software and hardware co-design and optimization of HPC systems has become intolerably complex, ad-hoc, time consuming and error prone due to enormous number of available design and optimization choices, complex interactions between all software and hardware components, and multiple strict requirements placed on performance, power consumption, size, reliability and cost. We present our novel long-term holistic and practical solution to this problem based on customizable, plugin-based, schema-free, heterogeneous, open-source Collective Mind repository and infrastructure with unified web interfaces and on- line advise system. This collaborative framework distributes analysis and multi- objective off-line and on-line auto-tuning of computer systems among many par- ticipants while utilizing any available smart phone, tablet, laptop, cluster or data center, and continuously observing, classifying and modeling their realistic behav- ior. Any unexpected behavior is analyzed using shared data mining and predictive modeling plugins or exposed to the community at cTuning.org for collaborative explanation, top-down complexity reduction, incremental problem decomposition and detection of correlating program, architecture or run-time properties (features). Gradually increasing optimization knowledge helps to continuously improve op- timization heuristics of any compiler, predict optimizations for new programs or suggest efficient run-time (online) tuning and adaptation strategies depending on end-user requirements. We decided to share all our past research artifacts includ- ing hundreds of codelets, numerical applications, data sets, models, universal ex- perimental analysis and auto-tuning pipelines, self-tuning machine learning based meta compiler, and unified statistical analysis and machine learning plugins in a public repository to initiate systematic, reproducible and collaborative R{\&}D with a new publication model where experiments and techniques are validated, ranked and improved by the community.},
annote = {This paper describes Collective Mind, which is a set of plugins and modules to allow developers of auto-tuning software to share their results.},
author = {Memon, A. W. and Fursin, G.},
booktitle = {PARCO},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Memon, Fursin - Crowdtuning systematizing auto-tuning using predictive modeling and crowdsourcing.pdf:pdf},
keywords = {Collective Mind,agile research and development,big data processing and compaction,c-mind.org,cTuning.org,compiler-agnostic tuning,complexity reduction,crowdsourcing auto-tuning and co- design,crowdtuning,customizable plugin- based infrastructure,data mining,decremental (differential) analysis,incremental problem decomposition,machine learning,metadata,on-line advice system,on-line tuning and learning,predictive modeling,public repository of knowledge,software and hardware co-design and co-optimizatio,systematic and reproducible research and experimen,systematic behavior modeling,top- down optimization,tuning dimension reduction,validation by community},
title = {{Crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing}},
year = {2013}
}
@techreport{Wing2002,
annote = {NULL},
author = {Wing, Jeannette M},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Wing - FAQ on $\pi$-Calculus.pdf:pdf},
title = {{FAQ on $\pi$-Calculus}},
year = {2002}
}
@inproceedings{Rupp2010,
abstract = {The vast computing resources in graphics processing units (GPUs) have become very attractive for general purpose sci- entific computing over the past years. Moreover, central processing units (CPUs) consist of an increasing number of individual cores. Most applications today still make use of a single core only, because standard data types and algorithms in wide-spread procedural languages such as C++ make use of a single core only. A customized adaption of existing algorithms to parallel architecture requires a considerable amount of effort both from algorithmic and programming point of view. Taking this additional amount of work hours required for an adaption to GPUs starting from scratch into account, the use of GPUs may not pay off on the overall. The Vienna Computing Library (ViennaCL), which is pre- sented in this work, aims at providing standard data types for linear algebra operations on GPUs and multi-core CPUs. It is based on OpenCL, which provides unified access to both GPUs and multi-core CPUs. The ViennaCL API fol- lowing existing programming and interface conventions es- tablished with uBLAS, which is part of the peer-reviewed Boost library. Thus, the open source library can be easily integrated into existing C++ implementations and therefore reduces the necessary code changes in existing software to a minimum. In addition, algorithms provided with ViennaCL can directly be used with uBLAS types due to the common interface. The algorithmic focus of ViennaCL is on iterative solvers, which are often used for the solution of large systems of lin- ear equations typically encountered in the discretization of partial differential equations using e.g. finite element meth- ods. Benchmark results given in this work show that the performance gain of ViennaCL over uBLAS is on both GPUs and multi-core CPUs up up to an order of magnitude. For small amounts of data, the use of ViennaCL may not pay off due to an OpenCLmanagement overhead associated with the launch of compute kernels.},
annote = {NULL},
author = {Rupp, Karl and Rudolf, Florian and Weinbub, J},
booktitle = {GPUScA},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Rupp, Rudolf, Weinbub - ViennaCL-a high level linear algebra library for GPUs and multi-core CPUs.pdf:pdf},
title = {{ViennaCL-a high level linear algebra library for GPUs and multi-core CPUs}},
url = {http://www.iue.tuwien.ac.at/pdf/ib{\_}2010/Rupp{\_}GPUScA.pdf},
year = {2010}
}
@inproceedings{Benoit2004,
abstract = {We show in this paper how to evaluate the performance of skeleton-based high level parallel programs. Since many applications fol- low some commonly used algorithmic skeletons, we identify such skele- tons and model them with process algebra in order to get relevant in- formation about the performance of the application, and be able to take some “good” scheduling decisions. This concept is illustrated through the case study of the Pipeline skeleton, and a tool which generates auto- matically a set of models and solves them is presented. Some numerical results are provided, proving the efficiency of this approach.},
annote = {NULL},
author = {Benoit, A. and Cole, M. and Gilmore, S. and Hillston, J.},
booktitle = {ICCS},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Benoit et al. - Evaluating the performance of skeleton-based high level parallel programs.pdf:pdf},
title = {{Evaluating the performance of skeleton-based high level parallel programs}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-24688-6{\_}40},
year = {2004}
}
@misc{CanergieMellonUniversity2005a,
annote = {NULL},
author = {{Canergie Mellon University}},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Canergie Mellon University - 1. Interacting with your faculty investigator.pdf:pdf},
title = {{1. Interacting with your faculty investigator}},
year = {2005}
}
@inproceedings{Falch2015,
abstract = {Heterogeneous computing, which combines devices with different architectures, is rising in popularity, and promises increased performance combined with reduced energy consumption. OpenCL has been proposed as a standard for programing such systems, and offers functional portability. It does, however, suffer from poor performance portability, code tuned for one device must be re-tuned to achieve good performance on another device. In this paper, we use machine learning-based auto-tuning to address this problem. Benchmarks are run on a random subset of the entire tuning parameter configuration space, and the results are used to build an artificial neural network based model. The model can then be used to find interesting parts of the parameter space for further search. We evaluate our method with different benchmarks, on several devices, including an Intel i7 3770 CPU, an Nvidia K40 GPU and an AMD Radeon HD 7970 GPU. Our model achieves a mean relative error as low as 6.1{\%}, and is able to find configurations as little as 1.3{\%} worse than the global minimum.},
annote = {Only three benchmarks, each hand-parameterised. Parameters},
author = {Falch, T. L. and Elster, A. C.},
booktitle = {IPDPSW},
doi = {10.1002/cpe},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Falch, Elster - Machine Learning Based Auto-tuning for Enhanced OpenCL Performance Portability.pdf:pdf},
isbn = {2007015102},
issn = {15320626},
keywords = {OpenCL,artificial,artificial neural networks,auto-tuning,auto-tuning involves automatically mea-,auto-tuning may be used,heterogeneous computing,in its,machine learning,neural networks,opencl,simplest form,to overcome this issue},
pmid = {23335858},
publisher = {IEEE},
title = {{Machine Learning Based Auto-tuning for Enhanced OpenCL Performance Portability}},
year = {2015}
}
@article{Henderson2017,
abstract = {We catalog and describe Google's key software engineering practices.},
annote = {Good summary of Google's practises. READ BEFORE A GOOGLE INTERVIEW.},
author = {Henderson, F.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Henderson - Software Engineering at Google.pdf:pdf},
journal = {arXiv:1702.01715},
title = {{Software Engineering at Google}},
year = {2017}
}
@article{Moreno2012,
abstract = {We propose to use knowledge about a parallel application's structure that was acquired with the use of a skeleton based development strategy to dynamically improve its performance. Parallel/distributed programming provides the possibility of solving highly demanding computational problems. However, this type of application requires support tools in all phases of the development cycle because the implementation is extremely difficult, especially for non-expert programmers. This work shows a new strategy for dynamically improving the performance of pipeline applications. We call this approach Dynamic Pipeline Mapping (DPM), and the key idea is to have free computational resources by gathering the pipeline's fastest stages and then using these resources to replicate the slowest stages. We present two versions of this strategy, both with complexity O(N log (N)) on the number of pipe stages, and we compare them to an optimal mapping algorithm and to the Binary Search Closest (BSC) algorithm [1]. Our results show that the DPM leads to significant performance improvements, increasing the application throughput up to 40{\%} on average. ?? 2011 Elsevier B.V. All rights reserved.},
annote = {Load balancing for pipeling parallel skeletons. Cited by 6.},
author = {Moreno, A. and Cesar, E. and Guevara, A. and Sorribes, J. and Margalef, T.},
doi = {10.1016/j.parco.2011.11.001},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Moreno et al. - Load balancing in homogeneous pipeline based applications.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Load balancing,Performance,Pipeline},
number = {3},
publisher = {Elsevier B.V.},
title = {{Load balancing in homogeneous pipeline based applications}},
url = {http://dx.doi.org/10.1016/j.parco.2011.11.001},
volume = {38},
year = {2012}
}
@inproceedings{Chen,
annote = {NULL},
author = {Chen, Y. and Huang, Y. and Eeckhout, L. and Fursin, G. and Peng, L. and Temam, O. and Wu, C.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Chen et al. - Evaluating Iterative Optimization Across 1000 Data Sets.pdf:pdf},
isbn = {9781450300193},
keywords = {Benchmarking,Compiler optimization,Iterative optimization},
title = {{Evaluating Iterative Optimization Across 1000 Data Sets}},
year = {2010}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. {\textcopyright} 2015 Macmillan Publishers Limited. All rights reserved.},
annote = {NULL},
author = {LeCun, Y. and Bengio, Y. and Hinton, G.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - LeCun, Bengio, Hinton - Deep learning.pdf:pdf},
journal = {Nature},
number = {7553},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Aimone2017,
author = {Aimone, J. B.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Aimone - Exponential scaling of neural algorithms - a future beyond Moore's Law.pdf:pdf},
journal = {arXiv:1705.02042},
title = {{Exponential scaling of neural algorithms - a future beyond Moore's Law?}},
year = {2017}
}
@article{Krizhevsky2014,
abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
author = {Krizhevsky, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Krizhevsky - One Weird Trick for Parallelizing Convolutional Neural Networks.pdf:pdf},
journal = {arXiv:1404.5997},
title = {{One Weird Trick for Parallelizing Convolutional Neural Networks}},
year = {2014}
}
@inproceedings{Solomonik2010,
abstract = {Sorting is a commonly used process with a wide breadth of applications in the high performance computing field. Early research in parallel processing has provided us with comprehensive analysis and theory for parallel sorting algorithms. However, modern super- computers have advanced rapidly in size and changed significantly in architecture, forcing new adaptations to these algorithms. To fully utilize the potential of highly parallel machines, tens of thousands of processors are used. Efficiently scaling parallel sorting on machines of this magnitude is inhibited by the communication-intensive problem of migrating large amounts of data between processors. The challenge is to design a highly scalable sorting algorithm that uses minimal communication, max- imizes overlap between computation and communication, and uses memory efficiently. This paper presents a scal- able extension of the Histogram Sorting method, making fundamental modifications to the original algorithm in order to minimize message contention and exploit overlap. We implement Histogram Sort, Sample Sort, and Radix Sort in CHARM++ and compare their performance. The choice of algorithm as well as the importance of the optimizations is validated by performance tests on two predominant modern supercomputer architectures: XT4 at ORNL (Jaguar) and Blue Gene/P at ANL (Intrepid).},
annote = {NULL},
author = {Solomonik, E. and Kal, L. V.},
booktitle = {IPDPS},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Solomonik, Kal - Highly Scalable Parallel Sorting.pdf:pdf},
title = {{Highly Scalable Parallel Sorting}},
year = {2010}
}
@misc{Anderson,
archivePrefix = {arXiv},
arxivId = {1804.04637v2},
eprint = {1804.04637v2},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Unknown - Notes on Machine Learning Models.pdf:pdf},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Notes on Machine Learning Models}},
year = {2018}
}
@article{Darmon1995,
annote = {NULL},
author = {Darmon, Henri and Ha, Canada and Diamond, Fred and Taylor, Richard},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Darmon et al. - Fermat's Last Theorem.pdf:pdf},
journal = {Current Developments in Mathematics},
title = {{Fermat's Last Theorem}},
volume = {1},
year = {1995}
}
@inproceedings{Bala2000,
abstract = {We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of –O optimized SpecInt95 benchmark binaries created by the HP product C compiler is improved to a level comparable to their –O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system.},
address = {New York, NY, USA},
annote = {The paper presents a Dynamo, a dynamic optimiser which focuses on runtime attributes. Dynamo is a transparent optimiser which optimizes hot paths on an input stream of native instructions, supporting both statically and JIT compiled streams.
















This seems like a paper worth scrutinizing. The idea is solid, and it is well cited.},
author = {Bala, V. and Duesterwald, E. and Banerjia, S.},
booktitle = {PLDI},
doi = {10.1145/349299.349303},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Bala, Duesterwald, Banerjia - Dynamo A Transparent Dynamic Optimization System.pdf:pdf},
isbn = {1581131992},
publisher = {ACM},
title = {{Dynamo: A Transparent Dynamic Optimization System}},
year = {2000}
}
@inproceedings{Schkufza2013,
abstract = {We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.},
annote = {NULL},
author = {Schkufza, E. and Sharma, R. and Aiken, A.},
booktitle = {ASPLOS},
doi = {10.1145/2499368.2451150},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Schkufza, Sharma, Aiken - Stochastic superoptimization.pdf:pdf},
isbn = {9781450318709},
issn = {03621340},
keywords = {64-bit,binary,carlo,ecx,edx,markov chain monte,mcmc,r8,rdi,rsi,smt,stochastic search,superoptimization,x86,x86-64},
publisher = {ACM},
title = {{Stochastic superoptimization}},
year = {2013}
}
@inproceedings{Bergstrom2014,
annote = {NULL},
author = {Bergstrom, L. and Fluet, M. and Reppy, J.},
booktitle = {ICFP},
doi = {10.1145/2628136.2628153},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bergstrom, Fluet, Reppy - Practical and Effective Higher-Order Optimizations.pdf:pdf},
isbn = {9781450328739},
issn = {0362-1340},
keywords = {control-flow analysis,inlining,optimization},
title = {{Practical and Effective Higher-Order Optimizations}},
year = {2014}
}
@article{Singh2017,
abstract = {The ability to automatically discover a program consistent with a given user intent (specification) is the holy grail of Computer Science. While significant progress has been made on the so-called problem of Program Synthesis, a number of challenges remain; particularly for the case of synthesizing richer and larger programs. This is in large part due to the difficulty of search over the space of programs. In this paper, we argue that the above-mentioned challenge can be tackled by learning synthesizers automatically from a large amount of training data. We present a first step in this direction by describing our novel synthesis approach based on two neural architectures for tackling the two key challenges of Learning to understand partial input-output specifications and Learning to search programs. The first neural architecture called the Spec Encoder computes a continuous representation of the specification, whereas the second neural architecture called the Program Generator incrementally constructs programs in a hypothesis space that is conditioned by the specification vector. The key idea of the approach is to train these architectures using a large set of (spec,P) pairs, where P denotes a program sampled from the DSL L and spec denotes the corresponding specification satisfied by P. We demonstrate the effectiveness of our approach on two preliminary instantiations. The first instantiation, called Neural FlashFill, corresponds to the domain of string manipulation programs similar to that of FlashFill. The second domain considers string transformation programs consisting of composition of API functions. We show that a neural system is able to perform quite well in learning a large majority of programs from few input-output examples. We believe this new approach will not only dramatically expand the applicability and effectiveness of Program Synthesis, but also would lead to the coming together of the Program Synthesis and Machine Learning research disciplines.},
author = {Singh, R. and Kohli, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Singh, Kohli - AP Artificial Programming.pdf:pdf},
journal = {SNAPL},
keywords = {neural flashfill,neural program synthesis,neural programming},
title = {{AP: Artificial Programming}},
year = {2017}
}
@article{Serot1999,
annote = {NULL},
author = {S{\'{e}}rot, J and Ginhac, D and D{\'{e}}rutin, JP},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - S{\'{e}}rot, Ginhac, D{\'{e}}rutin - SKiPPER a skeleton-based parallel programming environment for real-time image processing applications.pdf:pdf},
journal = {Parallel Computing Technologies},
keywords = {caml,fast proto-,image processing,parallelism,skeleton,typing,vehicle tracking},
number = {1998},
title = {{SKiPPER: a skeleton-based parallel programming environment for real-time image processing applications}},
volume = {1662},
year = {1999}
}
@article{Lee1987,
abstract = {Data flow is a natural paradigm for describing DSP applications for concurrent implementation on parallel hardware. Data flow programs for signal processing are directed graphs where each node represents a function and each arc represents a signal path. Synchronous data flow (SDF) is a special case of data flow (either atomic or large grain) in which the number of data samples produced or consumed by each node on each invocation is specified a priori. Nodes can be scheduled statically (at compile time) onto single or parallel programmable processors so the run-time overhead usually associated with data flow evaporates. Multiple sample rates within the same system are easily and naturally handled. Conditions for correctness of SDF graph are explained and scheduling algorithms are described for homogeneous parallel processors sharing memory. A preliminary SDF software system for automatically generating assembly language code for DSP microcomputers is described. Two new efficiency techniques are introduced, static buffering and an extension to SDF to efficiently implement conditionals.},
annote = {NULL},
author = {Lee, Edward A. and Messerschmitt, David G.},
doi = {10.1109/PROC.1987.13876},
file = {:Users/cec/Google Drive/Mendeley Library/1987 - Lee, Messerschmitt - Synchronous data flow.pdf:pdf},
isbn = {0005044804},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {9},
title = {{Synchronous data flow}},
volume = {75},
year = {1987}
}
@article{Phansalkar2007,
abstract = {The recently released SPEC CPU2006 benchmark suite is expected to be used by computer designers and computer architecture researchers for pre-silicon early design analysis. Partial use of benchmark suites by researchers, due to simulation time constraints, compiler difficulties, or library or system call issues is likely to happen; but a random subset can lead to misleading results. This paper analyzes the SPEC CPU2006 benchmarks using performance counter based experimentation from several state of the art systems, and uses statistical techniques such as principal component analysis and clustering to draw inferences on the similarity of the benchmarks and the redundancy in the suite and arrive at meaningful subsets. The SPEC CPU2006 benchmark suite contains several programs from areas such as artificial intelligence and includes none from the electronic design automation (EDA) application area. Hence there is a concern on the application balance in the suite. An analysis from the perspective of fundamental program characteristics shows that the included programs offer characteristics broader than the EDA programs' space. A subset of 6 integer programs and 8 floating point programs can yield most of the information from the entire suite.},
annote = {NULL},
author = {Phansalkar, Aashish and Joshi, Ajay and John, Lizy K.},
doi = {10.1145/1273440.1250713},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Phansalkar, Joshi, John - Analysis of redundancy and application balance in the SPEC CPU2006 benchmark suite.pdf:pdf},
isbn = {9781595937063},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {benchmark subset,clustering,spec cpu2006},
number = {2},
title = {{Analysis of redundancy and application balance in the SPEC CPU2006 benchmark suite}},
volume = {35},
year = {2007}
}
@misc{Arapinis2014d,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 10. The principle of (ordinary) induction.pdf:pdf},
title = {{10. The principle of (ordinary) induction}},
year = {2015}
}
@article{Kuchen2012,
abstract = {In the past years, multi-core processors and clusters of multi-core processors have emerged to be promising ap- proaches to meet the growing demand for computing performance. They deliver scalable performance, certainly at the costs of tedious and complex parallel programming. Due to a lack of high-level abstractions, developers of par- allel applications have to deal with low-level details such as coordinating threads or synchronizing processes. Thus, parallel programming still remains a difficult and error-prone task. In order to shield the programmer from these low- level details, algorithmic skeletons have been proposed. They encapsulate typical parallel programming patterns and have emerged to be an efficient and scalable approach to simplifying the development of parallel applications. In this paper, we present a Java binding of our skeleton library Muesli. We point out strengths and weaknesses of Java with respect to parallel and distributed computing. A matrix multiplication benchmark demonstrates that the Java Generics deliver poor performance, thus the Java implementation is unable to compete with the C++ implementation in terms of performance.},
annote = {The paper describes a partial re-implementation of C++ Skeleton library in Java. They give an overview of some of the challenges faced with porting this code accross, and implement currying in Java using partial application. They benchmark their Java implementation using a matrix multiplication algorithm against the C++ implementation. The results are much slower than the C++ implementation, which they attribute partially to the overhead of using Java generics.




This paper is poor. The resounding question after reading it are "why bother? What's the point in this work? What have they done which hasn't been done before?". Besides this, the structure is unusual: they put the Related Work section before describing their own work, and provide no insight into the problems with this existing work. The benchmark results are highly suspect, with no experimental method given. The speedups they claim are suspiciously high, implying that the benchmark program is memory bound, not CPU bound (in which case, they could achieve the same speedups by simply re-writing their sequential code to more effectively utilise the memory).},
author = {Kuchen, H. and Ernsting, S.},
doi = {10.1016/j.procs.2012.04.200},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kuchen, Ernsting - Data Parallel Skeletons in Java.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Java for HPC,algorithmic skeletons,distributed programming,message passing,parallel programming,programming environments},
month = {jan},
publisher = {Elsevier Masson SAS},
title = {{Data Parallel Skeletons in Java}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050912003213},
volume = {9},
year = {2012}
}
@book{Mckenny2013,
annote = {NULL},
author = {McKenney, Paul E},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - McKenney - Is Parallel Programming Hard, And, If So, What Can You Do About It.pdf:pdf},
publisher = {Linux Technology Center, IBM Beaverton},
title = {{Is Parallel Programming Hard, And, If So, What Can You Do About It?}},
year = {2011}
}
@misc{Allen1984,
annote = {Cited by 246.},
author = {Allen, J. R. and Kennedy, K.},
booktitle = {Supercomputers: Design and Applications},
file = {:Users/cec/Google Drive/Mendeley Library/1984 - Allen, Kennedy - PFC A Program to Convert Fortran to Parallel Form.pdf:pdf},
title = {{PFC: A Program to Convert Fortran to Parallel Form}},
year = {1984}
}
@inproceedings{Mikolov2013a,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
annote = {NULL},
author = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Mikolov et al. - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Problem,
author = {Heule, M. J. H. and Kullmann, O.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Heule, Kullmann - The Science of Brute Force.pdf:pdf},
journal = {Communications of the ACM},
number = {8},
title = {{The Science of Brute Force}},
volume = {60},
year = {2017}
}
@inproceedings{Seo2011,
abstract = {Heterogeneous parallel computing platforms, which are composed of different processors (e.g., CPUs, GPUs, FPGAs, and DSPs), are widening their user base in all computing domains. With this trend, parallel programming models need to achieve portability across different processors as well as high performance with reasonable programming effort. OpenCL (Open Computing Language) is an open standard and emerging parallel programming model to write parallel applications for such heterogeneous platforms. In this paper, we characterize the performance of an OpenCL implementation of the NAS Parallel Benchmark suite (NPB) on a heterogeneous parallel platform that consists of general-purpose CPUs and a GPU. We believe that understanding the performance characteristics of conventional workloads, such as the NPB, with an emerging programming model (i.e., OpenCL) is important for developers and researchers to adopt the programming model. We also compare the performance of the NPB in OpenCL to that of the OpenMP version. We describe the process of implementing the NPB in OpenCL and optimizations applied in our implementation. Experimental results and analysis show that the OpenCL version has different characteristics from the OpenMP version on multicore CPUs and exhibits different performance characteristics depending on different OpenCL compute devices. The results also indicate that the application needs to be rewritten or re-optimized for better performance on a different compute device although OpenCL provides source-code portability.},
annote = {NULL},
author = {Seo, S. and Jo, G. and Lee, J.},
booktitle = {IISWC},
doi = {10.1109/IISWC.2011.6114174},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Seo, Jo, Lee - Performance Characterization of the NAS Parallel Benchmarks in OpenCL.pdf:pdf},
isbn = {9781457720642},
publisher = {IEEE},
title = {{Performance Characterization of the NAS Parallel Benchmarks in OpenCL}},
year = {2011}
}
@article{Meng2011a,
abstract = {Iterative stencil loops (ISLs) are used in many applications and tiling is a well-known technique to localize their computation. When ISLs are tiled across a parallel architecture, there are usually halo regions that need to be updated and exchanged among different processing elements (PEs). In addition, synchronization is often used to signal the completion of halo exchanges. Both communication and synchronization may incur significant overhead on parallel architectures with shared memory. This is especially true in the case of graphics processors (GPUs), which do not preserve the state of the per-core L1 storage across global synchronizations. To reduce these overheads, ghost zones can be created to replicate stencil operations, reducing communication and synchronization costs at the expense of redundantly computing some values on multiple PEs. However, the selection of the optimal ghost zone size depends on the characteristics of both the architecture and the application, and it has only been studied for message-passing systems in a grid environment. To automate this process on shared memory systems, we establish a performance model using NVIDIA's Tesla architecture as a case study and propose a framework that uses the performance model to automatically select the ghost zone size that performs best and generate appropriate code. The modeling is validated by four diverse ISL applications, for which the predicted ghost zone configurations are able to achieve a speedup no less than 98{\%} of the optimal speedup.},
annote = {Cited by 23.},
author = {Meng, Jiayuan and Skadron, Kevin},
doi = {10.1007/s10766-010-0142-5},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Meng, Skadron - A performance study for iterative stencil loops on GPUs with ghost zone optimizations.pdf:pdf},
isbn = {9781605584980},
issn = {08857458},
journal = {IJPP},
keywords = {GPU,Ghost zone,Halo,Iterative stencil loops,Performance model,Tiling},
number = {1},
publisher = {Springer},
title = {{A performance study for iterative stencil loops on GPUs with ghost zone optimizations}},
volume = {39},
year = {2011}
}
@techreport{Woods1973,
annote = {Pointless and funny.},
author = {Woods, D. R. and Lyon, J. M.},
file = {:Users/cec/Google Drive/Mendeley Library/1973 - Woods, Lyon - The INTERCAL Programming Language Reference Manual.pdf:pdf},
title = {{The INTERCAL Programming Language Reference Manual}},
year = {1973}
}
@inproceedings{Yotov2003,
abstract = {Empirical program optimizers estimate the values of key optimi- zation parameters by generating different program versions and running them on the actual hardware to determine which values give the best performance. In contrast, conventional compilers use models of programs and machines to choose these parameters. It is widely believed that model-driven optimization does not com- pete with empirical optimization, but few quantitative compari- sons have been done to date. To make such a comparison, we replaced the empirical optimization engine in ATLAS (a system for generating a dense numerical linear algebra library called the BLAS) with a model-driven optimization engine that used de- tailed models to estimate values for optimization parameters, and then measured the relative performance of the two systems on three different hardware platforms. Our experiments show that model-driven optimization can be surprisingly effective, and can generate code whose performance is comparable to that of code generated by empirical optimizers for the BLAS.},
address = {New York, New York, USA},
annote = {This paper argues that model-driven optimisation can perform nearly as well as empirical program optimizers for certain scenarios.








This paper is certainly worth picking through, as it makes the rare claim that models can compare well against empirical evidence.},
author = {Yotov, K. and Wu, P. and Li, X. and Ren, G. and Cibulskis, M. and DeJong, G. and Garzaran, M. and Padua, D. and Pingali, K. and Stodghill, P.},
booktitle = {PLDI},
doi = {10.1145/781139.781140},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Yotov et al. - A Comparison of Empirical and Model-driven Optimization.pdf:pdf},
isbn = {1581136625},
keywords = {BLAS,Blocking,Code generation,Compilers,Empirical optimization,Memory hierarchy,Model-driven optimization,Program transformation,Tiling,Unrolling},
publisher = {ACM Press},
title = {{A Comparison of Empirical and Model-driven Optimization}},
url = {http://portal.acm.org/citation.cfm?doid=781131.781140},
year = {2003}
}
@article{Parisotto2016,
abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
author = {Parisotto, E. and Mohamed, A. and Singh, R. and Li, L. and Zhou, D. and Kohli, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Parisotto et al. - Neuro-Symbolic Program Synthesis.pdf:pdf},
journal = {arXiv:1611.01855},
title = {{Neuro-Symbolic Program Synthesis}},
year = {2016}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, N. and Hinton, G. and Krizhevsky, A. and Sutskever, I. and Salakhutdinov, R.},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {JMLR},
keywords = {TOSKIM,deep learning,model combination,neural networks,regularization},
mendeley-tags = {TOSKIM},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@incollection{English2010,
abstract = {Most people who bother with the matter at all would admit that the English language is in a bad way, but it is generally assumed that we cannot by conscious action do anything about it. Our civilization is decadent and our language — so the argument runs — must inevitably share in the general collapse. It follows that any struggle against the abuse of language is a sentimental archaism, like preferring candles to electric light or hansom cabs to aeroplanes. Underneath this lies the half-conscious belief that language is a natural growth and not an instrument which we shape for our own purposes.},
author = {English, Modern},
booktitle = {Politics and the English Language and Other Essays},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - English - Politics and the English Language By George Orwell.pdf:pdf},
title = {{Politics and the English Language By George Orwell}},
year = {2010}
}
@inproceedings{Jablin2012a,
annote = {NULL},
author = {Jablin, T. B. and Jablin, J. A. and Prabhu, P. and Liu, F. and August, D. I.},
booktitle = {CGO},
doi = {10.1145/2259016.2259038},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Jablin et al. - Dynamically Managed Data for CPU-GPU Architectures.pdf:pdf},
isbn = {978-1-4503-1206-6},
publisher = {IEEE},
title = {{Dynamically Managed Data for CPU-GPU Architectures}},
url = {http://doi.acm.org/10.1145/2259016.2259038},
year = {2012}
}
@misc{UniversityofEdinburgh2015b,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 9. Program Transformations.pdf:pdf},
title = {{9. Program Transformations}},
year = {2015}
}
@inproceedings{Buckmann2015,
annote = {NULL},
author = {Buckmann, M.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Buckmann - Learning From Small Samples An Analysis of Simple Decision Heuristics.pdf:pdf},
title = {{Learning From Small Samples: An Analysis of Simple Decision Heuristics}},
year = {2015}
}
@inproceedings{Pantridge2017,
author = {Pantridge, E. and Spector, L.},
booktitle = {GECCO},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Pantridge, Spector - PyshGP PushGP in Python.pdf:pdf},
title = {{PyshGP: PushGP in Python}},
year = {2017}
}
@article{Sarkar,
author = {Sarkar, V. and Fleming, S. and Bacon, D. and Field, J. and Rogers, I. and Smith, R.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Sarkar et al. - Software Modernization via Code Mining.pdf:pdf},
journal = {Google Fellowship Proposal},
title = {{Software Modernization via Code Mining}}
}
@article{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, music generation, and video game playing all require that a model generate sequences of outputs. In other domains, such as time series prediction, video analysis, and music information retrieval, a model must learn from sequences of inputs. Significantly more interactive tasks, such as natural language translation, engaging in dialogue, and robotic control, often demand both. Recurrent neural networks (RNNs) are a powerful family of connectionist models that capture time dynamics via cycles in the graph. Unlike feedforward neural networks, recurrent networks can process examples one at a time, retaining a state, or memory, that reflects an arbitrarily long context window. While these networks have long been difficult to train and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled large-scale learning with recurrent nets. Over the past few years, systems based on state of the art long short-term memory (LSTM) and bidirectional recurrent neural network (BRNN) architectures have demonstrated record-setting performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this review of the literature we synthesize the body of research that over the past three decades has yielded and reduced to practice these powerful models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a mostly self-contained explication of state of the art systems, together with a historical perspective and ample references to the primary research.},
annote = {NULL},
author = {Lipton, Z. C. and Berkowitz, J. and Elkan, C.},
doi = {10.1145/2647868.2654889},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lipton, Berkowitz, Elkan - A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf:pdf},
isbn = {9781450330633},
issn = {9781450330633},
journal = {arXiv:1506.00019},
pmid = {18267787},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
year = {2015}
}
@inproceedings{Lee,
abstract = {Recent work has explored using higher level lan- guages to improve programmer productivity on GPUs. These languages often utilize high level computation patterns (e.g., Map and Reduce) that encode parallel semantics to enable automatic compilation to GPU kernels. However, the problem of efficiently mapping patterns to GPU hardware becomes significantly more difficult when the patterns are nested, which is common in non- trivial applications. To address this issue, we present a general analysis frame- work for automatically and efficiently mapping nested patterns onto GPUs. The analysis maps nested patterns onto a logical multidimensional domain and parameterizes the block size and degree of parallelism in each dimension. We then add GPU- specific hard and soft constraints to prune the space of possible mappings and select the best mapping.We also perform multiple compiler optimizations that are guided by the mapping to avoid dynamic memory allocations and automatically utilize shared memory within GPU kernels. We compare the performance of our automatically selected mappings to hand-optimized imple- mentations on multiple benchmarks and show that the average performance gap on 7 out of 8 benchmarks is 24{\%}. Furthermore, our mapping strategy outperforms simple 1D mappings and existing 2D mappings by up to 28.6x and 9.6x respectively.},
annote = {The authors present an automatic mapping strategy for nested parallel skeletons on the GPU. They port Rodinia benchmarks to their own IR, which then maps them to CUDA.},
author = {Lee, H. and Brown, K. J. and Sujeeth, A. K. and Rompf, T. and Olukotun, K.},
booktitle = {MICRO},
doi = {10.1109/MICRO.2014.23},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Lee et al. - Locality-Aware Mapping of Nested Parallel Patterns on GPUs.pdf:pdf},
publisher = {ACM},
title = {{Locality-Aware Mapping of Nested Parallel Patterns on GPUs}},
year = {2014}
}
@inproceedings{Nugteren2012,
abstract = {Recent advances in multi-core and many-core processors re- quires programmers to exploit an increasing amount of par- allelism from their applications. Data parallel languages such as CUDA and OpenCL make it possible to take ad- vantage of such processors, but still require a large amount of effort from programmers. A number of parallelizing source-to-source compilers have recently been developed to ease programming of multi-core and many-core processors. This work presents and evalu- ates a number of such tools, focused in particular on C-to- CUDA transformations targeting GPUs. We compare these tools both qualitatively and quantitatively to each other and identify their strengths and weaknesses. In this paper, we address the weaknesses by presenting a new classification of algorithms. This classification is used in a new source-to-source compiler, which is based on the algo- rithmic skeletons technique. The compiler generates target code based on skeletons of parallel structures, which can be seen as parameterisable library implementations for a set of algorithm classes. We furthermore demonstrate that the presented compiler requires little modifications to the original sequential source code, generates readable code for further fine-tuning, and delivers superior performance compared to other tools for a set of 8 image processing kernels.},
annote = {NULL},
author = {Nugteren, C. and Corporaal, H.},
booktitle = {GPGPU},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Nugteren, Corporaal - Introducing 'Bones' a parallelizing source-to-source compiler based on algorithmic skeletons.pdf:pdf},
keywords = {Graphics Processing Units,Parallel Programming,Source-to-Source Compilation,algorithmic skeletons},
publisher = {ACM},
title = {{Introducing 'Bones': a parallelizing source-to-source compiler based on algorithmic skeletons}},
url = {http://dl.acm.org/citation.cfm?id=2159431},
year = {2012}
}
@inproceedings{Examples,
abstract = {The widespread success of deep learning in a variety of domains is being hailed as a new revolution in artificial intelligence. It has taken 20 years to go from defeating Kasparov at Chess to Lee Sedol at Go. But what have the real advances been across this time? The fundamental change has been in terms of data availability and compute availability. The underlying technology has not changed much in the last 20 years. So what does that mean for areas like medicine and health? Significant challenges remain, improving the data efficiency of these algorithms and retaining the balance between individual privacy and predictive power of the models. In this talk we will review these challenges and propose some ways forward. Bio: Neil Lawrence is a Professor of Machine Learning and Computational Biology at the University of Sheffield. His main research interest is machine learning through probabilistic models. He focuses on both the algorithmic side of these models and their application. He has a particular focus on applications in personalized health and applications in the developing world. He is well known for his work with Gaussian processes, and has proposed Gaussian process variants of many of the succesful deep learning architectures. He is highly active in the machine learning community, most recently Program Chairing the NIPS conference in 2014 and General Chairing (alongside Corinna Cortes) in 2015.},
address = {London, UK},
annote = {NULL},
author = {Lawrence, N.},
booktitle = {Deep Learning Summit},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Lawrence - The Data Delusion.pdf:pdf},
title = {{The Data Delusion}},
year = {2016}
}
@book{Buduma2015,
annote = {NULL},
author = {Buduma, N.},
publisher = {O'Reilly},
title = {{Fundamentals of Deep Learning}},
year = {2015}
}
@article{Xian2017,
abstract = {Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.},
author = {Xian, Y. and Schiele, B. and Akata, Z.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Xian, Schiele, Akata - Zero-Shot Learning - The Good, the Bad and the Ugly.pdf:pdf},
journal = {arXiv:1703.04394},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Zero-Shot Learning - The Good, the Bad and the Ugly}},
year = {2017}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
annote = {See notes: https://github.com/karpathy/paper-notes/blob/master/matching{\_}networks.md},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, O. and Blundell, C. and Lillicrap, T. and Kavukcuoglu, K. and Wierstra, D.},
eprint = {1606.04080},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Vinyals et al. - Matching Networks for One Shot Learning.pdf:pdf},
title = {{Matching Networks for One Shot Learning}},
url = {http://arxiv.org/abs/1606.04080},
year = {2016}
}
@article{Zhou,
abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly com-petitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to differ-ent data from different domains, excellent perfor-mance can be achieved by almost same settings of hyper-parameters. The training process of gcFor-est is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facili-ties, and the efficiency advantage may be more ap-parent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for the-oretical analysis than deep neural networks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1702.08835},
author = {Zhou, Z. and Feng, J.},
eprint = {1702.08835},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Zhou, Feng - Deep Forest Towards An Alternative to Deep Neural Networks.pdf:pdf},
title = {{Deep Forest: Towards An Alternative to Deep Neural Networks}}
}
@article{Li2018,
abstract = {Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.},
archivePrefix = {arXiv},
arxivId = {1803.03324},
author = {Li, Y. and Vinyals, O. and Dyer, C. and Pascanu, R. and Battaglia, P.},
doi = {10.1146/annurev-statistics-010814-020120},
eprint = {1803.03324},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Li et al. - Learning Deep Generative Models of Graphs.pdf:pdf},
issn = {2326-8298},
journal = {arXiv:1803.03324},
title = {{Learning Deep Generative Models of Graphs}},
year = {2018}
}
@inproceedings{Yi2007,
abstract = {The excessive complexity of both machine architectures and applications have made it difficult for compilers to stat- ically model and predict application behavior. This observa- tion motivates the recent interest in performance tuning using empirical techniques. We present a new embedded scripting language, POET (Parameterized Optimization for Empirical Tuning), for parameterizing complex code transformations so that they can be empirically tuned. The POET language aims to significantly improve the generality, flexibility, and efficiency of existing empirical tuning systems. We have used the language to parameterize and to empirically tune three loop optimizations—interchange, blocking, and unrolling— for two linear algebra kernels. We show experimentally that the time required to tune these optimizations using POET, which does not require any program analysis, is significantly shorter than that when using a full compiler- based source-code optimizer which performs sophisticated program analysis and optimizations.},
annote = {POET is a scripting language which describes program transformations. It is designed to reduce the amount of time required to collect empirical data about the performance of different program transformations by reducing the code generation time.




As always with these kinds of technologies, their widespread adoption will be limited by the programmer's laziness. No-one wants to write programs using some pig-ugly language just to speed up iterative compilation.},
author = {Yi, Q and Seymour, K and You, H and Vuduc, R and Quinlan, D},
booktitle = {IPDPS},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Yi et al. - POET Parameterized Optimization for Empirical Tuning.pdf:pdf},
publisher = {Ieee},
title = {{POET: Parameterized Optimization for Empirical Tuning}},
year = {2007}
}
@misc{Guillou2014,
annote = {A nice succint checlist of tips for posters.},
author = {Guillou, Liane and Llewellyn, Clare},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Guillou, Llewellyn - Poster Printing and Presentation Tips.pdf:pdf},
number = {April},
title = {{Poster Printing and Presentation Tips}},
year = {2014}
}
@article{Khan2013,
abstract = {This article presents a novel compiler framework for CUDA code generation. The compiler structure is designed to support autotuning, whichemploys empirical techniques to evaluate a set of alternative mappings of computation kernels and select the mapping that obtains the best performance. This article introduces a Transformation Strategy Generator, a meta-optimizer that generates a set of transformation recipes,which are descriptions of the mapping of the sequential code to parallel CUDA code. These recipes comprise a search space of possible implementations. This system achieves performance comparable and sometimes better than manually tuned libraries and exceeds the performance of a state-of-the-art GPU compiler.},
annote = {NULL},
author = {Khan, Malik and Basu, Protonu and Rudy, Gabe and Hall, Mary and Chen, Chun and Chame, Jacqueline},
doi = {10.1145/2400682.2400690},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Khan et al. - A script-based autotuning compiler system to generate high-performance CUDA code.pdf:pdf},
issn = {15443566},
journal = {TACO},
keywords = {Algorithms,Experimentation,Languages,Performance},
number = {4},
title = {{A script-based autotuning compiler system to generate high-performance CUDA code}},
url = {http://dl.acm.org/citation.cfm?doid=2400682.2400690},
volume = {9},
year = {2013}
}
@incollection{Hoffman2014,
abstract = {In 1999 the National Library of Medicine engaged a small company called Kitware to develop a better way to configure, build, and deploy complex software across many different platforms. This work was part of the Insight Segmentation and Registration Toolkit, or ITK . Kitware, the engineering 1 lead on the project, was tasked with developing a build system that the ITK researchers and developers could use. The system had to be easy to use, and allow for the most productive use of the researchers' programming time. Out of this directive emerged CMake as a replacement for the aging autoconf/libtool approach to building software. It was designed to address the weaknesses of existing tools while maintaining their strengths. In addition to a build system, over the years CMake has evolved into a family of development tools: CMake, CTest, CPack, and CDash. CMake is the build tool responsible for building software. CTest is a test driver tool, used to run regression tests. CPack is a packaging tool used to create platform- specific installers for software built with CMake. CDash is a web application for displaying testing results and performing continuous integration testing.},
annote = {NULL},
author = {Hoffman, Bill and Martin, Kenneth},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Hoffman, Martin - CMake.pdf:pdf},
title = {{CMake}},
year = {2014}
}
@techreport{EPSRC2014,
abstract = {Arrangements and procedures for research grants and research fellowships.},
annote = {NULL},
author = {EPSRC},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - EPSRC - Funding Guide.pdf:pdf},
institution = {EPSRC},
number = {January},
title = {{Funding Guide}},
year = {2014}
}
@article{Ciresan2011,
annote = {NULL},
author = {Ciresan, D. C. and Meier, U. and Masci, J. and Gambardella, L. M. and Schmidhuber, J.},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Ciresan et al. - Flexible, High Performance Convolutional Neural Networks for Image Classification.pdf:pdf},
journal = {IJCAI},
title = {{Flexible, High Performance Convolutional Neural Networks for Image Classification}},
year = {2011}
}
@techreport{Gonnet2013,
annote = {Recent (unpublished as of writing) work has extended QucikSched for dynamic parallelism on GPUs, so this paper may be worth a visit in a while.},
author = {Gonnet, Pedro},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Gonnet - QuickSched Task-based parallelism with dependencies and conflicts.pdf:pdf},
institution = {Technical Report ECS-TR 2013/06, School of Engineering and Computing Sciences, Durham University, South Road, DH1 3LE Durham, United Kingdom},
keywords = {task-based parallelism},
title = {{QuickSched: Task-based parallelism with dependencies and conflicts}},
year = {2013}
}
@article{Melorose2015,
annote = {NULL},
author = {Glasser, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Glasser - Copyrights in Computer-Generated Works Whom, if Anyone, do we Reward.pdf:pdf},
journal = {Duke L. {\&} Tech. Rev.},
title = {{Copyrights in Computer-Generated Works: Whom, if Anyone, do we Reward?}},
year = {2001}
}
@inproceedings{Tarakji,
annote = {NULL},
author = {Tarakji, Ayman and B{\"{o}}rger, Lukas and Leupers, Rainer},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Tarakji, B{\"{o}}rger, Leupers - A Comparative Investigation of Device-Specific Mechanisms for Exploiting HPC Accelerators.pdf:pdf},
isbn = {9781450334075},
keywords = {GCN,GPGPU,Kepler,MIC,OpenCL},
title = {{A Comparative Investigation of Device-Specific Mechanisms for Exploiting HPC Accelerators}},
year = {2015}
}
@inproceedings{Flautner2001,
abstract = {The emphasis on processors that are both low power and high performance has resulted in the incorporation of dynamic voltage scaling into processor designs. This fea- ture allows one to make fine granularity trade-offs between power use and performance, provided there is a mechanism in the OS to control that trade-off. In this paper, we describe a novel software approach to automatically controlling dynamic voltage scaling in order to optimize energy use. Our mechanism is implemented in the Linux kernel and requires no modification of user programs. Unlike previous automated approaches, our method works equally well with irregular and multiprogrammed workloads. Moreover, it has the ability to ensure that the quality of interactive per- formance is within user specified parameters. Our experi- ments show that as a result of our algorithm, processor energy savings of as much as 75{\%} can be achieved with only a minimal impact on the user experience.},
address = {New York, New York, USA},
annote = {The paper presents an approach to balancing power conumption against perceived system performance using frequency scaling. It focuses on interactive programs, and ensuring that "episodes" (user interactions) occur within a given perception threshold (50ms).




An interesting concept, but questionably executed.},
author = {Flautner, K. and Reinhardt, S. and Mudge, T.},
booktitle = {MobiCom},
doi = {10.1145/381677.381702},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Flautner, Reinhardt, Mudge - Automatic performance setting for dynamic voltage scaling.pdf:pdf},
isbn = {1581134223},
publisher = {ACM Press},
title = {{Automatic performance setting for dynamic voltage scaling}},
url = {http://portal.acm.org/citation.cfm?doid=381677.381702},
year = {2001}
}
@inproceedings{Enmyren2010,
abstract = {We present SkePU, a C++ template library which provides a simple and unified interface for specifying data-parallel computations with the help of skeletons on GPUs using CUDA and OpenCL. The interface is also general enough to support other architectures, and SkePU implements both a sequential CPU and a parallel OpenMP backend. It also supports multi-GPU systems. Copying data between the host and the GPU device memory can be a performance bottleneck. A key technique in SkePU is the implementation of lazy memory copying in the container type used to represent skeleton operands, which allows to avoid unnecessary memory transfers. We evaluate SkePU with small benchmarks and a larger appli- cation, a Runge-Kutta ODE solver. The results show that a skeleton approach to GPU programming is viable, especially when the com- putation burden is large compared to memory I/O (the lazy memory copying can help to achieve this). It also shows that utilizing several GPUs have a potential for performance gains. We see that SkePU offers good performance with a more complex and realistic task such as ODE solving, with up to 10 times faster run times when using SkePU with a GPU backend compared to a sequential solver running on a fast CPU.},
annote = {SkePU is a data-parallel skeleton library with support for seq CPU, OpenMP, CUDA and OpenCL execution. It is a C++ template library which uses lazy copying to minimize the amount of data transferred between heterogeneous devices. Cited by 106.},
author = {Enmyren, J and Kessler, CW},
booktitle = {HLPP},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Enmyren, Kessler - SkePU a multi-backend skeleton programming library for multi-GPU systems.pdf:pdf},
keywords = {CUDA,Data Parallelism,GPU,OpenCL,Skeleton Programming},
publisher = {ACM},
title = {{SkePU: a multi-backend skeleton programming library for multi-GPU systems}},
year = {2010}
}
@article{Castillo,
abstract = {Style transfer is an important task in which the style of a source image is mapped onto that of a target image. The method is useful for synthesizing derivative works of a partic-ular artist or specific painting. This work considers targeted style transfer, in which the style of a template image is used to alter only part of a target image. For example, an artist may wish to alter the style of only one particular object in a target image without altering the object's general morphology or surroundings. This is useful, for example, in augmented re-ality applications (such as the recently released Pok{\'{e}}mon go), where one wants to alter the appearance of a single real-world object in an image frame to make it appear as a cartoon. Most notably, the rendering of real-world objects into cartoon char-acters has been used in a number of films and television show, such as the upcoming series Son of Zorn. We present a method for targeted style transfer that simultaneously segments and stylizes single objects selected by the user. The method uses a Markov random field model to smooth and anti-alias outlier pixels near object boundaries, so that stylized objects natu-rally blend into their surroundings.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.02357},
author = {Castillo, C. and De, S. and Han, X. and Singh, B. and Yadav, A. K. and Goldstein, T.},
eprint = {1701.02357},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Castillo et al. - Son of Zorn's Lemma Targeted Style Transfer Using Instance-Aware Semantic Segmentation.pdf:pdf},
keywords = {Convolution neural network,Image filtering,Index Terms— Style transfer,Instance-aware semantic segmentation,Markov random fields},
title = {{Son of Zorn's Lemma: Targeted Style Transfer Using Instance-Aware Semantic Segmentation}}
}
@inproceedings{Boehm2008,
abstract = {Currently multi-threaded C or C++ programs combine a single- threaded programming language with a separate threads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well- defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort: We (mostly) insist on sequential consistency for race-free pro- grams, in spite of implementation issues that came to light after the Java work. We give no semantics to programs with data races. There are no benign C++ data races. We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock . This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, to- gether with some practical, but often under-appreciated implemen- tation constraints, drives us towards the above decisions.},
annote = {NULL},
author = {Boehm, H. and Adve, S. V.},
booktitle = {PLDI},
doi = {10.1145/1379022.1375591},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Boehm, Adve - Foundations of the C concurrency memory model.pdf:pdf},
isbn = {9781595938602},
issn = {03621340},
keywords = {Memory consistency,memory model,sequential consi},
title = {{Foundations of the C++ concurrency memory model}},
year = {2008}
}
@inproceedings{Grant1997,
annote = {NULL},
author = {Grant, B. and Mock, M. and Philipose, M. and Chambers, C. and Eggers, S. J.},
booktitle = {PEPM},
doi = {10.1145/258994.259016},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Grant et al. - Annotation-directed run-time specialization in C.pdf:pdf},
isbn = {0897919173},
issn = {03621340},
title = {{Annotation-directed run-time specialization in C}},
year = {1997}
}
@misc{UniversityofEdinburgh,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 3. Computational Complexity.pdf:pdf},
title = {{3. Computational Complexity}},
year = {2015}
}
@article{Zhou2018,
archivePrefix = {arXiv},
arxivId = {1812.08434},
author = {Zhou, J. and Cui, G. and Zhang, Z. and Yang, C. and Liu, Z. and Sun, M.},
doi = {arXiv:1812.08434v1},
eprint = {1812.08434},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Zhou et al. - Graph Neural Networks A Review of Methods and Applications.pdf:pdf},
journal = {arXiv:1812.08434},
title = {{Graph Neural Networks: A Review of Methods and Applications}},
url = {https://arxiv.org/abs/1812.08434},
year = {2018}
}
@phdthesis{Ansel2009,
abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking.},
annote = {NULL},
author = {Ansel, Jason},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Ansel - PetaBricks a language and compiler for algorithmic choice.pdf:pdf},
school = {MIT},
title = {{PetaBricks: a language and compiler for algorithmic choice}},
year = {2009}
}
@inproceedings{Kasim2008,
abstract = {The development of microprocessors design has been shifting to multi-core architectures. Therefore, it is expected that parallelism will play a significant role in future generations of applications. Throughout the years, there has been a myriad number of parallel programming models proposed. In choosing a parallel programming model, not only the performance aspect is important, but also qualitative the aspect of how well parallelism is abstracted to developers. A model with a well abstraction of parallelism leads to a higher application-development productivity. In this paper, we propose seven criteria to qualitatively evaluate parallel programming models. Our focus is on how parallelism is abstracted and presented to application developers. As a case study, we use these criteria to investigate six well-known parallel programming models in the HPC community.},
annote = {NULL},
author = {Kasim, Henry and March, Verdi and Zhang, Rita and See, Simon},
booktitle = {NPC},
doi = {10.1007/978-3-540-88140-7_24},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Kasim et al. - Survey on Parallel Programming Model.pdf:pdf},
isbn = {978-3-540-88139-1},
issn = {0302-9743},
keywords = {cuda,distributed memory,fortress,mpi,openmp,pthreads,shared memory,upc},
publisher = {Springer},
title = {{Survey on Parallel Programming Model}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-88140-7{\_}24 http://dx.doi.org/10.1007/978-3-540-88140-7{\_}24},
volume = {5245},
year = {2008}
}
@article{Tran2017,
abstract = {We propose Edward, a new Turing-complete probabilistic programming language which builds on two compositional representations—random variables and infer- ence. We show how to integrate our language into existing computational graph frameworks such as TensorFlow; this provides significant speedups over existing probabilistic systems. We also show how Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point es- timation, to variational inference, to MCMC. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as computationally efficient and flexible as traditional deep learning. For flexibil- ity, we show how to reuse the modeling representation within inference to design variational auto-encoders and generative adversarial networks. For efficiency, we showthat our implementation of Hamiltonian Monte Carlo is 35x faster than hand- optimized software such as Stan.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.03757},
author = {Tran, D. and Saurous, R. A. and Hoffman, M. D. and Brevdo, E. and Murphy, K. and Blei, D. M.},
eprint = {1701.03757},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Tran et al. - Deep Probabilistic Programming.pdf:pdf},
journal = {arXiv:1701.03757},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Deep Probabilistic Programming}},
year = {2017}
}
@article{Legg,
archivePrefix = {arXiv},
arxivId = {arXiv:1109.5951v2},
author = {Legg, S. and Veness, J.},
eprint = {arXiv:1109.5951v2},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Legg, Veness - Universal Intelligence Measure.pdf:pdf},
journal = {arXiv:1109.5951},
title = {{Universal Intelligence Measure}},
year = {2011}
}
@inproceedings{West,
annote = {NULL},
author = {West, Scott and Nanz, Sebastian and Meyer, Bertrand},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - West, Nanz, Meyer - Efficient and Reasonable Object-Oriented Concurrency.pdf:pdf},
isbn = {9781450332057},
keywords = {concurrency,object-oriented,optimiza-,performance},
title = {{Efficient and Reasonable Object-Oriented Concurrency}},
year = {2015}
}
@inproceedings{Midtgaard2017,
author = {Midtgaard, J. and Justesen, M. N. and Kasting, P. and Nielson, F. and Nielson, H. R.},
booktitle = {ICFP},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Midtgaard et al. - Effect-Driven QuickChecking of Compilers.pdf:pdf},
keywords = {QuickCheck,compiler testing,type and effect system},
title = {{Effect-Driven QuickChecking of Compilers}},
year = {2017}
}
@inproceedings{Zhang2017,
annote = {Received a perfect 10.0 score at ICLR.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03530v1},
author = {Zhang, C. and Bengio, S. and Hardt, M. and Recht, B. and Vinyals, O.},
booktitle = {ICLR},
eprint = {arXiv:1611.03530v1},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Zhang et al. - Deep Learning Requires Requires Rethinking Generalization.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Deep Learning Requires Requires Rethinking Generalization}},
year = {2017}
}
@misc{Silver2015l,
author = {Silver, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 8 Integrating Learning and Planning.pdf:pdf},
title = {{Lecture 8 : Integrating Learning and Planning}},
year = {2015}
}
@misc{UniversityofEdinburgh2014k,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 5. Instruction Selection.pdf:pdf},
title = {{5. Instruction Selection}},
year = {2015}
}
@inproceedings{Ogilvie2014,
abstract = {Building effective optimization heuristics is a challenging task which often takes developers several months if not years to complete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data, however, obtaining this data can take months per platform. This is becoming an ever more criti- cal problem as the pace of change in architecture increases. Indeed, if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines. In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which signif- icantly reduces this training overhead. Typically in super- vised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry, but this wastes effort on parts of the space that con- tribute little to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples and thus reduces the training overhead. We demonstrate this technique by automatically creat- ing a model to determine on which device to execute four parallel programs at differing problem dimensions for a rep- resentative Cpu–Gpu based system. Our methodology is remarkably simple and yet effective, making it a strong can- didate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state-of-the-art.},
annote = {A massively cut down version of [1].




[1] W. F. Ogilvie, P. Petoumenos, Z. Wang, and H. Leather, “Fast Automatic Heuristic Construction Using Active Learning.”},
author = {Ogilvie, W. F. and Petoumenos, P. and Wang, Z. and Leather, H.},
booktitle = {PACT},
doi = {10.1145/2628071.2628128},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Ogilvie et al. - Active learning accelerated automatic heuristic construction for parallel program mapping.pdf:pdf},
isbn = {9781450328098},
keywords = {Active Learning,Compilers,Machine Learning},
publisher = {ACM},
title = {{Active learning accelerated automatic heuristic construction for parallel program mapping}},
year = {2014}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
annote = {NULL},
author = {Sutskever, I. and Vinyals, O. and Le, Q. V.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Sutskever, Vinyals, Le - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}
@inproceedings{Wang2017b,
abstract = {We present a new approach to example-guided program synthesis based on counterexample-guided abstraction refinement. Our method uses the abstract semantics of the underlying DSL to find a program {\$}P{\$} whose abstract behavior satisfies the examples. However, since program {\$}P{\$} may be spurious with respect to the concrete semantics, our approach iteratively refines the abstraction until we either find a program that satisfies the examples or prove that no such DSL program exists. Because many programs have the same input-output behavior in terms of their abstract semantics, this synthesis methodology significantly reduces the search space compared to existing techniques that use purely concrete semantics. While synthesis using abstraction refinement (SYNGAR) could be implemented in different settings, we propose a refinement-based synthesis algorithm that uses abstract finite tree automata (AFTA). Our technique uses a coarse initial program abstraction to construct an initial AFTA, which is iteratively refined by constructing a proof of incorrectness of any spurious program. In addition to ruling out the spurious program accepted by the previous AFTA, proofs of incorrectness are also useful for ruling out many other spurious programs. We implement these ideas in a framework called $\backslash$tool. We have used the BLAZE framework to build synthesizers for string and matrix transformations, and we compare BLAZE with existing techniques. Our results for the string domain show that BLAZE compares favorably with FlashFill, a domain-specific synthesizer that is now deployed in Microsoft PowerShell. In the context of matrix manipulations, we compare BLAZE against Prose, a state-of-the-art general-purpose VSA-based synthesizer, and show that BLAZE results in a 90x speed-up over Prose.},
author = {Wang, X. and Dillig, I. and Singh, R.},
booktitle = {POPL},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Wang, Dillig, Singh - Program Synthesis using Abstraction Refinement.pdf:pdf},
title = {{Program Synthesis using Abstraction Refinement}},
year = {2017}
}
@inproceedings{Arslan,
annote = {NULL},
author = {Arslan, Mehmet Ali},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Arslan - Programming Support for Reconfigurable Custom Vector Architectures.pdf:pdf},
isbn = {9781450334044},
title = {{Programming Support for Reconfigurable Custom Vector Architectures}},
year = {2015}
}
@article{Mullin2002,
annote = {NULL},
author = {Mullin, Lenore R and Bond, Robert},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Mullin, Bond - Monolithic Compiler Experiments Using C Expression Templates.pdf:pdf},
title = {{Monolithic Compiler Experiments Using C ++ Expression Templates}},
year = {2002}
}
@article{Yong2014,
annote = {NULL},
author = {Yong, Nathan and Chong, Seng},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Yong, Chong - Scalable Verification Techniques for Data-Parallel Programs.pdf:pdf},
number = {September},
title = {{Scalable Verification Techniques for Data-Parallel Programs}},
year = {2014}
}
@inproceedings{Wu2016,
abstract = {Graphics Processing Units have emerged as powerful accelerators for massively parallel, numerically intensive workloads. The two dominant software models for these devices are NVIDIA's CUDA and the cross-platform OpenCL standard. Until now, there has not been a fully open-source compiler targeting the CUDA environment, hampering general compiler and architecture research and making deployment difficult in datacenter or supercomputer environments. In this paper, we present gpucc, an LLVM-based, fully open-source, CUDA compatible compiler for high performance computing. It performs various general and CUDA-specific optimizations to generate high performance code. The Clang-based frontend supports modern language features such as those in C++11 and C++14. Compile time is 8{\%} faster than NVIDIA's toolchain (nvcc) and it reduces compile time by up to 2.4x for pathological compilations ({\textgreater}100 secs), which tend to dominate build times in parallel build environments. Compared to nvcc, gpucc's runtime performance is on par for several open-source benchmarks, such as Rodinia (0.8{\%} faster), SHOC (0.5{\%} slower), or Tensor (3.7{\%} faster). It outperforms nvcc on internal large-scale end-to-end benchmarks by up to 51.0{\%}, with a geometric mean of 22.9{\%}.},
annote = {NULL},
author = {Wu, Jingyue and Hundt, Robert and Belevich, Artem and Bendersky, Eli and Heffernan, Mark and Leary, Chris and Pienaar, Jacques and Roune, Bjarke and Springer, Rob and Weng, Xuetian},
booktitle = {CGO},
doi = {10.1145/2854038.2854041},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Wu et al. - gpucc An Open-Source GPGPU Compiler.pdf:pdf},
isbn = {9781450337786},
keywords = {GPU,compiler,optimization},
publisher = {IEEE},
title = {{gpucc: An Open-Source GPGPU Compiler}},
url = {http://dl.acm.org/citation.cfm?id=2854038.2854041},
year = {2016}
}
@article{Bolukbasi2016,
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
annote = {NULL},
author = {Bolukbasi, T. and Chang, K. and Zou, J. and Saligrama, V. and Kalai, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Bolukbasi et al. - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings.pdf:pdf},
journal = {arXiv:1607.06520},
title = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
year = {2016}
}
@inproceedings{Amer2015,
annote = {NULL},
author = {Amer, A. and Lu, H.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Amer, Lu - MPI Threads Runtime Contention and Remedies.pdf:pdf},
isbn = {9781450332057},
keywords = {critical section,mpi,runtime contention,threads},
title = {{MPI + Threads : Runtime Contention and Remedies}},
year = {2015}
}
@article{Danelutto2012,
abstract = {We discuss the implementation of a minimalist parallel library in OCaml. The library provides parallel map and fold (reduce) higher order functions and targets standard cache coherent shared memory multi-cores. Our Parmap.parmap and Parmap.parfold functions may be used to seamlessly replace OCaml List map and fold standard functions preserving their full functional semantics while achieving nearly optimal speedup on standard multi-core architectures. We discuss the design of the Parmap module, the main implementation features and we present some experimental results assessing the efficiency of the Parmap parallel functions. Overall, Parmap rep- resents a perfect incarnation of the “propagate the concept with minimal disruption” principle introduced in Cole's algorithmic skeleton manifesto.},
annote = {NULL},
author = {Danelutto, M. and {Di Cosmo}, R.},
doi = {10.1016/j.procs.2012.04.202},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Danelutto, Di Cosmo - A “Minimal Disruption” Skeleton Experiment Seamless Map {\&} Reduce Embedding in OCaml.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {algorithmic skeletons,map,reduce,structured parallel programming},
month = {jan},
publisher = {Elsevier Masson SAS},
title = {{A “Minimal Disruption” Skeleton Experiment: Seamless Map {\&} Reduce Embedding in OCaml}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050912003237},
volume = {9},
year = {2012}
}
@incollection{Ramey2014,
annote = {NULL},
author = {Ramey, Chet},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Ramey - The Bourne-Again Shell.pdf:pdf},
title = {{The Bourne-Again Shell}},
year = {2014}
}
@article{Du2012,
abstract = {In this work, we evaluate OpenCL as a programming tool for developing performance- portable applications for GPGPU. While the Khronos group developed OpenCL with pro- gramming portability in mind, performance is not necessarily portable. OpenCL has required performance-impacting initializations that do not exist in other languages such as CUDA. Understanding these implications allows us to provide a single library with decent performance on a variety of platforms. We choose triangular solver (TRSM) and matrix multiplication (GEMM) as representative level 3 BLAS routines to implement in OpenCL. We profile TRSM to get the time distribution of the OpenCL runtime system. We then provide tuned GEMM kernels for both the NVIDIA Tesla C2050 and ATI Radeon 5870, the latest GPUs offered by both companies. We explore the benefits of using the tex- ture cache, the performance ramifications of copying data into images, discrepancies in the OpenCL and CUDA compilers' optimizations, and other issues that affect the performance. Experimental results show that nearly 50{\%} of peak performance can be obtained in GEMM on both GPUs in OpenCL. We also show that the performance of these kernels is not highly portable. Finally, we propose the use of auto-tuning to better explore these kernels' param- eter space using search harness.},
annote = {This paper profiles the performance portability of OpenCL accross different GPU architectures, in order to evaluate its usefulness for a numerical linear algebra library. The paper has a nice introduction on the history of GPGPU. It's worth citing for it's shoutout to autotuning in the conclusion.},
author = {Du, P. and Weber, R. and Luszczek, P. and Tomov, S. and Peterson, G. and Dongarra, J.},
doi = {10.1016/j.parco.2011.10.002},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Du et al. - From CUDA to OpenCL Towards a performance-portable solution for multi-platform GPU programming.pdf:pdf},
issn = {0167-8191},
journal = {Parallel Computing},
keywords = {auto-tuning,hardware accelerators,portability},
number = {8},
publisher = {Elsevier B.V.},
title = {{From CUDA to OpenCL: Towards a performance-portable solution for multi-platform GPU programming}},
url = {http://dx.doi.org/10.1016/j.parco.2011.10.002},
volume = {38},
year = {2012}
}
@article{Kamil2006,
abstract = {Abstract Stencil -based kernels constitute the core of many scientific applications on block- structured grids. Unfortunately, these codes achieve a low fraction of peak performance, due primarily to the disparity between processor and main memory speeds. We examine ...},
annote = {Examining optimisations of stencil PDE solvers for cache-based memory systems and heterogeneous multi-core designs. Optimisations included cache olbivious algorithsma dnd time skewed optimisations. Cited by 117.},
author = {Kamil, S. and Datta, K. and Williams, S. and Oliker, L. and Shalf, J. and Yelick, K.},
doi = {10.1145/1178597.1178605},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Kamil et al. - Implicit and explicit optimizations for stencil computations.pdf:pdf},
isbn = {1595935789},
journal = {MSPC},
title = {{Implicit and explicit optimizations for stencil computations}},
url = {http://portal.acm.org/citation.cfm?doid=1178597.1178605},
year = {2006}
}
@article{Cvpr2017,
author = {Luan, F. and Paris, S. and Shechtman, E. and Bala, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Luan et al. - Deep Photo Style Transfer.pdf:pdf},
journal = {arXiv:1703.07511},
title = {{Deep Photo Style Transfer}},
year = {2017}
}
@inproceedings{Ausavarungnirun2015,
abstract = {In a GPU, all threads within a warp execute the same instruction in lockstep. For a memory instruc- tion, this can lead to memory divergence: the memory requests for some threads are serviced early, while the remaining requests incur long latencies. This divergence stalls the warp, as it cannot execute the next instruction until all requests from the current instruction complete. In this work, we make three new observations. First, GPGPU warps exhibit heterogeneous memory diver- gence behavior at the shared cache: some warps have most of their requests hit in the cache (high cache utility), while other warps see most of their request miss (low cache utility). Second, a warp retains the same divergence behavior for long periods of execution. Third, due to high memory level parallelism, requests going to the shared cache can incur queuing delays as large as hundreds of cycles, exacerbating the effects of memory divergence. We propose a set of techniques, collectively called Memory Divergence Correction (MeDiC), that reduce the negative performance impact of memory divergence and cache queuing. MeDiC uses warp divergence characteri- zation to guide three components: (1) a cache bypassing mechanism that exploits the latency tolerance of low cache utility warps to both alleviate queuing delay and increase the hit rate for high cache utility warps, (2) a cache insertion policy that prevents data from high cache utility warps from being prematurely evicted, and (3) a memory controller that prioritizes the few requests received from high cache utility warps to minimize stall time. We compare MeDiC to four cache management techniques, and find that it delivers an average speedup of 21.8{\%}, and 20.1{\%} higher energy efficiency, over a state-of-the-art GPU cache management mechanism across 15 different GPGPU applications. 1.},
annote = {NULL},
author = {Ausavarungnirun, R. and Ghose, S. and Kayiran, O. and Loh, G. H. and Das, C. R. and Kandemir, M. T. and Mutlu, O.},
booktitle = {PACT},
doi = {10.1109/PACT.2015.38},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ausavarungnirun et al. - Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance.pdf:pdf},
isbn = {978-1-4673-9524-3},
issn = {1089795X},
publisher = {ACM},
title = {{Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance}},
year = {2015}
}
@article{Wiles1995,
abstract = {モジュラー楕円曲線とフェルマーの最終定理},
annote = {NULL},
author = {Wiles, A.},
doi = {10.2307/2118559},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Wiles - Modular Elliptic Curves and Fermat's Last Theorem.pdf:pdf},
isbn = {0003486X},
issn = {0003486X},
journal = {Annals of Mathematics},
number = {3},
pmid = {3512303736425612710},
title = {{Modular Elliptic Curves and Fermat's Last Theorem}},
url = {http://www.jstor.org/stable/10.2307/2118559{\%}5Cnhttp://www.jstor.org/stable/2118559?origin=crossref},
volume = {141},
year = {1995}
}
@article{Ba2014,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1312.6184},
author = {Ba, L. J. and Caruana, R.},
doi = {10.1038/nature14539},
eprint = {1312.6184},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Ba, Caruana - Do Deep Nets Really Need to be Deep.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {arXiv:1312.6184},
pmid = {26017442},
title = {{Do Deep Nets Really Need to be Deep?}},
year = {2014}
}
@inproceedings{Chen2015,
annote = {NULL},
author = {Chen, Haibo},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chen - NUMA-Aware Graph-Structured Analytics.pdf:pdf},
isbn = {9781450332057},
keywords = {graph-structured analytics,non-uniform memory},
title = {{NUMA-Aware Graph-Structured Analytics}},
year = {2015}
}
@article{Bourgoin2014,
abstract = {General purpose (GP)GPU programming demands to couple highly paral- lel computing units with classic CPUs to obtain a high performance. Heterogenous systems lead to complex designs combining multiple paradigms and programming languages to manage each hardware architecture. In this paper, we present tools to harnessGPGPU programming through the high-levelOCaml programming language. We describe theSPOClibrary that allowsto handleGPGPUsubprograms (kernels) and data transfers between devices.We then present howSPOC expressesGPGPU kernel: through interoperability with common low-level extensions (from Cuda and OpenCL frameworks) but also via an embedded DSL for OCaml. Using simple benchmarks as well as a real world HPC software, we show that SPOC can offer a high perfor- mance while efficiently easing development. To allow better abstractions over tasks and data,weintroduce some parallel skeletons built uponSPOCaswell as composition constructs over those skeletons.},
annote = {NULL},
author = {Bourgoin, Mathias and Chailloux, Emmanuel and Lamotte, Jean Luc},
doi = {10.1007/s10766-013-0261-y},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bourgoin, Chailloux, Lamotte - Efficient abstractions for GPGPU programming.pdf:pdf},
issn = {08857458},
journal = {IJPP},
keywords = {DSL,GPGPU,OCaml,Parallel abstractions,Parallel skeletons},
number = {4},
publisher = {Springer},
title = {{Efficient abstractions for GPGPU programming}},
volume = {42},
year = {2014}
}
@misc{Guermonprez2012,
annote = {NULL},
author = {Guermonprez, Paul},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Guermonprez - Parallel Programming Course Threading Building Blocks (TBB).pdf:pdf},
title = {{Parallel Programming Course Threading Building Blocks (TBB)}},
year = {2012}
}
@article{Cohen1995,
annote = {Cited by 21.},
author = {Cohen, R. F. and Tamassia, R.},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Cohen, Tamassia - Dynamic Expression Trees.pdf:pdf},
journal = {Algorithmica},
number = {3},
title = {{Dynamic Expression Trees}},
volume = {13},
year = {1995}
}
@article{Gottbrath1999,
abstract = {We show that, in the context of Moore's Law, overall productivity can be increased for large enough computations by `slacking' or waiting for some period of time before purchasing a computer and beginning the calculation.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {astro-ph/9912202},
author = {Gottbrath, C and Bailin, J and Meakin, C and Thompson, T and Charfman, J. J.},
eprint = {9912202},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Gottbrath et al. - The Effects of Moore's Law and Slacking on Large Computations.pdf:pdf},
primaryClass = {astro-ph},
title = {{The Effects of Moore's Law and Slacking on Large Computations}},
url = {http://arxiv.org/abs/astro-ph/9912202},
year = {1999}
}
@article{Xue1997,
annote = {Cited by 29.},
author = {Xue, Jingling},
doi = {10.1016/S0167-8191(96)00063-4},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Xue - Unimodular transformations of non-perfectly nested loops.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Data dependence,Feautrier's PIP,Fourier-Motzkin elimination,Imperfect loop nest,Non-Basic-to-Basic-Loop transformation,Unimodular Transformation},
number = {12},
publisher = {Elsevier},
title = {{Unimodular transformations of non-perfectly nested loops}},
volume = {22},
year = {1997}
}
@inproceedings{Morrison,
annote = {NULL},
author = {Morrison, Adam},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Morrison - Predicate RCU An RCU for Scalable Concurrent Updates.pdf:pdf},
isbn = {9781450332057},
keywords = {concurrent data structures,rcu,synchronization},
title = {{Predicate RCU : An RCU for Scalable Concurrent Updates}},
year = {2015}
}
@inproceedings{Heule2016,
address = {Santa Barbara, CA},
annote = {NULL},
author = {Heule, S. and Schkufza, E. and Sharma, R. and Aiken, A.},
booktitle = {PLDI},
doi = {10.1145/2908080.2908121},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Heule et al. - Stratified synthesis automatically learning the x86-64 instruction set.pdf:pdf},
isbn = {9781450342612},
keywords = {ISA specification,program synthesis,x86-64},
title = {{Stratified synthesis: automatically learning the x86-64 instruction set}},
year = {2016}
}
@inproceedings{Godefroid2008a,
abstract = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53{\%} to 81{\%} while using three times fewer tests.},
author = {Godefroid, P and Kiezun, A and Levin, M. Y.},
booktitle = {PLDI},
doi = {10.1145/1379022.1375607},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Godefroid, Kiezun, Levin - Grammar-based whitebox fuzzing.pdf:pdf},
isbn = {9781595938602},
issn = {03621340},
keywords = {all or part of,automatic test generation,grammars,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,program verification,provided that copies are,software testing,this work for},
title = {{Grammar-based whitebox fuzzing}},
year = {2008}
}
@article{Espeholt2018,
abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
archivePrefix = {arXiv},
arxivId = {1802.01561},
author = {Espeholt, L. and Soyer, H. and Munos, R. and Simonyan, K. and Mnih, V. and Ward, T. and Doron, Y. and Firoiu, V. and Harley, T. and Dunning, I. and Legg, S. and Kavukcuoglu, K.},
doi = {10.1021/ic401666m},
eprint = {1802.01561},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Espeholt et al. - IMPALA Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.pdf:pdf},
isbn = {3034413300},
journal = {arXiv:1802.01561},
title = {{IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures}},
url = {http://arxiv.org/abs/1802.01561},
year = {2018}
}
@inproceedings{Auler2014,
abstract = {JavaScript has long outpaced its original target applications, being used not only for coding complex web clients, but also web servers, game development and even desktop applications. The most appealing advantage of moving applications to JavaScript is its capability to run the same code in a large number of different devices. It is not surpris- ing that many compilers target JavaScript as an intermediate language. However, writing optimizations and analyses passes for a compiler that emits JavaScript is challenging: a long time spent in optimizing the code in a certain way can be excellent for some browsers, but a futile effort for others. For example, we show that applying JavaScript code optimiza- tions in a tablet with Windows 8 and Internet Explorer 11 increased performance by, on average, 5 times, while running in a desktop with Windows 7 and Firefox decreased performance by 20{\%}. Such a scenario demands a radical new solution for the traditional compiler optimiza- tion flow. This paper proposes collecting web clients performance data to build a crowdsourced compiler flag suggestion system in the cloud that helps the compiler perform the appropriate optimizations for each client platform. Since this information comes from crowdsourcing rather than manual investigations, fruitless or harmful optimizations are automati- cally discarded. Our approach is based on live measurements done while clients use the application on real platforms, proposing a new paradigm on how optimizations are tested.},
annote = {This paper presents a study of a distributed system for recommending JS compiler flags based on the program and architecture. It collects data from the web.
















It's pretty interesting and can be seen as MILEPOST-lite, for JITs. The distributed nature is totally natural given that it's embedded in web applications.},
author = {Auler, R. and Borin, E. and de Halleux, P. and Moskal, M. and Tillmann, N.},
booktitle = {CC},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Auler et al. - Addressing JavaScript JIT engines performance quirks A crowdsourced adaptive compiler.pdf:pdf},
keywords = {Adaptive compilation,JavaScript engines,just-in-time compilation},
publisher = {Springer},
title = {{Addressing JavaScript JIT engines performance quirks: A crowdsourced adaptive compiler}},
year = {2014}
}
@misc{UniversityofEdinburgh2015a,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 8. Depenedence Analysis.pdf:pdf},
title = {{8. Depenedence Analysis}},
year = {2015}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, G. and Vinyals, O. and Dean, J.},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hinton, Vinyals, Dean - Distilling the Knowledge in a Neural Network.pdf:pdf},
issn = {0022-2488},
journal = {arXiv:1503.02531},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@inproceedings{Henkel2018,
abstract = {With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied. In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93{\%} top-1 accuracy on a benchmark consisting of over 19,000 API-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions.},
archivePrefix = {arXiv},
arxivId = {1803.06686},
author = {Henkel, J. and Lahiri, S. K. and Liblit, B. and Reps, T.},
booktitle = {FSE},
doi = {10.1145/3236024.3236085},
eprint = {1803.06686},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Henkel et al. - Code Vectors Understanding Programs Through Embedded Abstracted Symbolic Traces.pdf:pdf},
isbn = {9781450355735},
keywords = {analogical reasoning,program understanding,word embeddings},
title = {{Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces}},
year = {2018}
}
@article{Expression2011,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Regular Expression.pdf:pdf},
isbn = {9780878933914},
title = {{Regular Expression}},
year = {2016}
}
@techreport{Petoumenos,
author = {Petoumenos, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Petoumenos - Royal Academy of Engineering Research Fellowship.pdf:pdf},
title = {{Royal Academy of Engineering Research Fellowship}},
year = {2017}
}
@book{Santner2013,
annote = {Cited by 1806.},
author = {Santner, Thomas J and Williams, Brian J and Notz, William I},
publisher = {Springer Science {\&} Business Media},
title = {{The Design and Analysis of Computer Experiments}},
year = {2013}
}
@book{Lieder,
annote = {Preview as of Feb 2017. Due for release June 2017. See: '@One Note/Reference/B/Book Notes: Learning TensorFlow'},
author = {Lieder, I. and Resheff, Y. S. and Hope, T.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Lieder, Resheff, Hope - Learning TensorFlow.pdf:pdf},
title = {{Learning TensorFlow}}
}
@article{Rossum2014c,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - An introduction to the ipaddress module.pdf:pdf},
title = {{An introduction to the ipaddress module}},
year = {2016}
}
@misc{Cook1971,
annote = {NULL},
author = {Cook, S. A.},
booktitle = {STOC},
doi = {10.1145/800157.805047},
file = {:Users/cec/Google Drive/Mendeley Library/1971 - Cook - The Complexity of Theorem-proving Procedures.pdf:pdf},
title = {{The Complexity of Theorem-proving Procedures}},
url = {http://doi.acm.org/10.1145/800157.805047},
year = {1971}
}
@misc{Arapinis2014a,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 03. Predicate logic.pdf:pdf},
title = {{03. Predicate logic}},
year = {2015}
}
@article{McGlothlin2010,
abstract = {Intergroup attitudes were assessed in 7 and 10 years old European American and African American children from ethnically heterogeneous schools and in 7 and 10 years old European American children from ethnically homogeneous schools in order to test hypotheses about racial biases and judgments regarding cross-race peer interactions (N¼302). Using an Ambiguous Situations Task, the findings revealed that European American children attending homogeneous schools displayed racial bias in their interpretations of ambiguous situations as well as in their evaluations of cross-race friendship. Bias was not found, however, in the interpretations and evaluations of European American or African American children from heterogeneous schools. This study is the first to empirically demonstrate significant and direct relationships between intergroup contact in the school environment and children's intergroup biases as well as judgments about the potential for cross-race friendships.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1311.3475},
author = {McGlothlin, Heidi and Killen, Melanie},
doi = {10.1002/ejsp},
eprint = {1311.3475},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - McGlothlin, Killen - How are habits formed Modelling habit formation in the real world.pdf:pdf},
isbn = {0022-1031},
issn = {00224537},
journal = {EJSP},
number = {June 2009},
pmid = {18179320},
title = {{How are habits formed: Modelling habit formation in the real world}},
volume = {40},
year = {2010}
}
@article{Sidwell2012,
annote = {NULL},
author = {Sidwell, N. and Prus, V. and Alves, P. and Loosemore, S. and Blandy, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Sidwell et al. - Non-stop Multi-threaded debugging in GDB.pdf:pdf},
keywords = {Debugging,GNU debugger,Mentor Embedded,Mentor Graphics,Sourcery tools,breakpoint GDB,breakpoint insertion,breakpoint removal,event loop,mult-threading,run control threading},
title = {{Non-stop Multi-threaded debugging in GDB}},
url = {http://communities.mentor.com/mgcx/docs/DOC-3037},
year = {2012}
}
@inproceedings{Sharifi,
annote = {NULL},
author = {Sharifi, H. and Aaziz, O. and Cook, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Sharifi, Aaziz, Cook - Monitoring HPC Applications in the Production Environment.pdf:pdf},
isbn = {9781450334051},
keywords = {application monitoring,hpc,run-time analysis},
title = {{Monitoring HPC Applications in the Production Environment}},
year = {2015}
}
@article{Tillet2012,
abstract = {An automatic OpenCL compute kernel generator framework for linear algebra operations is presented. It allows for specifying matrix and vector operations in high-level C++ code, while the low-level details of OpenCL compute kernel generation and handling are dealt with in the background. Our approach releases users from considerable additional effort required for learning the details of programming graphics processing units (GPUs), and we demonstrate that higher performance than for a fixed, predefined set of OpenCL compute kernels is obtained due to the minimization of launch overhead. The generator is made available in the Vienna Computing Library (ViennaCL) and is demonstrated here with the stabilized bi-conjugate gradient algorithm, for which performance gains up to 40 percent are observed.},
annote = {NULL},
author = {Tillet, P. and Rupp, K. and Selberherr, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Tillet, Rupp, Selberherr - An Automatic OpenCL Compute Kernel Generator for Basic Linear Algebra Operations.pdf:pdf},
isbn = {9781618397881},
issn = {07359276},
journal = {Simulation Series},
keywords = {Automatic Code Generation,High Performance Computing,Linear Algebra,OpenCL,ViennaCL},
number = {6},
title = {{An Automatic OpenCL Compute Kernel Generator for Basic Linear Algebra Operations}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84876465331{\&}partnerID=tZOtx3y1},
volume = {44},
year = {2012}
}
@article{Silver2016,
annote = {NULL},
author = {Silver, D. and Huang, A. and Maddison, C. J. and Guez, A. and Sifre, L. and van den Driessche, G. and Schrittwieser, J. and Antonoglou, I. and Panneershelvam, V. and Lanctot, M. and Dieleman, S. and Grewe, D. and Nham, J. and Kalchbrenner, N. and Sutskever, I. and Lillicrap, T. and Leach, M. and Kavukcuoglu, K. and Graepel, T. and Hassabis, D.},
doi = {10.1038/nature16961},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Silver et al. - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
number = {7587},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@inproceedings{Langer,
annote = {NULL},
author = {Langer, Akhil and Totoni, Ehsan and Palekar, Udatta S and Kal, Laxmikant V},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Langer et al. - Energy-efficient Computing for HPC Workloads on Heterogeneous Manycore Chips.pdf:pdf},
isbn = {9781450334044},
keywords = {energy,heterogeneity,integer programming,low,multicore chips,near threshold voltage computing,optimization,power,process,quadratic integer,variation,voltage computing},
title = {{Energy-efficient Computing for HPC Workloads on Heterogeneous Manycore Chips}},
year = {2015}
}
@inproceedings{Malloy2001,
author = {Malloy, Brian A},
booktitle = {ICIS},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Malloy - An Interpretation of Purdom's Algorithm for Automatic Generation of Test Cases.pdf:pdf},
keywords = {black-box testing,context-free grammar,implementa-,ing,parsing,re-engineering,structural-based testing,tion-based testing,white-box test-},
title = {{An Interpretation of Purdom's Algorithm for Automatic Generation of Test Cases}},
year = {2001}
}
@inproceedings{Aldinucci2010,
abstract = {Shared memory multiprocessors have returned to popularity thanks to rapid spreading of commodity multi-core architectures. However, little attention has been paid to supporting effective streaming applications on these architectures. In this paper we describe FastFlow, a low-level programming framework based on lock-free queues explic- itly designed to support high-level languages for stream- ing applications. We compare FastFlow with state-of-the- art programming frameworks such as Cilk, OpenMP, and Intel TBB. We experimentally demonstrate that FastFlow is always more efficient than them on a given real world application: the speedup of FastFlow over other solutions may be substantial for fine grain tasks, for example +35{\%} over OpenMP, +226{\%} over Cilk, +96{\%} over TBB for the alignment of protein P01111 against UniProt DB using the Smith-Waterman algorithm.},
annote = {NULL},
author = {Aldinucci, M. and Meneghin, M. and Torquati, M.},
booktitle = {PDP},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Aldinucci, Meneghin, Torquati - Efficient Smith-Waterman on multi-core with FastFlow.pdf:pdf},
publisher = {IEEE},
title = {{Efficient Smith-Waterman on multi-core with FastFlow}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5452443},
year = {2010}
}
@inproceedings{Blackburn2006,
abstract = {Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.},
annote = {NULL},
author = {Blackburn, Stephen M and Garner, Robin and Hoffmann, Chris and Khan, Asjad M and Mckinley, Kathryn S and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J Eliot B and Phansalkar, Aashish and Stefanovi, Darko},
booktitle = {OOPSLA},
doi = {10.1145/1167515.1167488},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Blackburn et al. - The DaCapo Benchmarks Java Benchmarking Development and Analysis.pdf:pdf},
isbn = {1595933484},
issn = {03621340},
keywords = {benchmark,dacapo,java,methodology,spec},
publisher = {ACM},
title = {{The DaCapo Benchmarks: Java Benchmarking Development and Analysis}},
year = {2006}
}
@misc{NVIDIA2016,
annote = {NULL},
author = {NVIDIA},
title = {{CUDA-GDB}},
url = {https://developer.nvidia.com/cuda-gdb},
year = {2016}
}
@article{Dean2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the under- lying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make effi- cient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
annote = {NULL},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Dean, Ghemawat - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
journal = {Communications of the ACM},
number = {1},
title = {{MapReduce: Simplified Data Processing on Large Clusters}},
volume = {51},
year = {2008}
}
@inproceedings{Ding2015,
annote = {NULL},
author = {Ding, Y. and Ansel, J. and Veeramachaneni, K. and Shen, X. and O'Reilly, U. and Amarasinghe, S.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ding et al. - Autotuning Algorithmic Choice for Input Sensitivity.pdf:pdf},
isbn = {9781450334686},
keywords = {algorithmic optimization,autotuning,in-,input sensitivity,petabricks,put adaptive,two-level input learning},
publisher = {ACM},
title = {{Autotuning Algorithmic Choice for Input Sensitivity}},
year = {2015}
}
@inproceedings{Cavazos2006,
abstract = {Developing an optimizing compiler for a newly proposed architecture is extremely difficult when there is only a simulator of the machine available. Designing such a compiler requires running many experiments in order to understand how different optimizations interact. Given that simulators are orders of magnitude slower than real processors, such experiments are highly restricted. This paper develops a technique to automatically build a performance model for predicting the impact of program transformations on any architecture, based on a limited number of automatically selected runs. As a result, the time for evaluating the impact of any compiler optimization in early design stages can be drastically reduced such that all selected potential compiler optimizations can be evaluated. This is achieved by first evaluating a small set of sample compiler optimizations on a prior set of benchmarks in order to train a model, followed by a very small number of evaluations, or probes, of the target program. We show that by training on less than 0.7{\%} of all possible transformations (640 samples collected from 10 benchmarks out of 880000 possible samples, 88000 per training benchmark) and probing the new program on only 4 transformations, we can predict the performance of all program transformations with an error of just 7.3{\%} on average. As each prediction takes almost no time to generate, this scheme provides an accurate method of evaluating compiler performance, which is several orders of magnitude faster than current approaches.},
author = {Cavazos, J. and Dubach, C. and Agakov, F. and Bonilla, E. and O'Boyle, M. and Fursin, G. and Temam, O.},
booktitle = {CASES},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Cavazos et al. - Automatic Performance Model Construction for the Fast Software Exploration of New Hardware Designs.pdf:pdf},
keywords = {learning,statistics {\&} optimisation},
title = {{Automatic Performance Model Construction for the Fast Software Exploration of New Hardware Designs}},
year = {2006}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
annote = {NULL},
author = {Zeiler, M. D. and Fergus, R.},
booktitle = {ECCV},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Zeiler, Fergus - Visualizing and Understanding Convolutional Networks.pdf:pdf},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@inproceedings{Yu2018,
abstract = {Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments. This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations. We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.},
author = {Yu, Y. and Abadi, M. and Barham, P. and Brevdo, E. and Burrows, M. and Davis, A. and Dean, J. and Ghemawat, S. and Harley, T. and Hawkins, P. and Isard, M. and Kudlur, M. and Monga, R. and Murray, D. and Zheng, X.},
booktitle = {EuroSys},
title = {{Dynamic Control Flow in Large-Scale Machine Learning}},
year = {2018}
}
@phdthesis{Fursin2004,
annote = {NULL},
author = {Fursin, G.},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Fursin - Iterative Compilation and Performance Prediction for Numerical Applications.pdf:pdf},
publisher = {University of Edinburgh. College of Science and Engineering. School of Informatics.},
school = {University of Edinburgh},
title = {{Iterative Compilation and Performance Prediction for Numerical Applications}},
year = {2004}
}
@inproceedings{Aubrey-Jones2014,
abstract = {Distributed memory architectures such as Linux clusters have be- come increasingly common but remain difficult to program. We target this problem and present a novel technique to automatically generate data distri- bution plans, and subsequently MPI implementations in C++, from programs written in a functional core language. The main novelty of our approach is that we support distributed arrays, maps, and lists in the same framework, rather than just arrays.We do this by formalizing our distributed data layouts as types, which are then used both to search (via type inference) for optimal data distribution plans and to generate the MPI implementations. We introduce the core language and explain our formalization of distributed data layouts. We describe how we search for data distribution plans using an adaptation of the Damas-Milner type inference algorithm, and how we generate MPI implementations in C++ from such plans.},
annote = {NULL},
author = {Aubrey-Jones, T and Fischer, B},
booktitle = {HLPP},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Aubrey-Jones, Fischer - Synthesizing MPI Implementations from Functional Data-Parallel Programs.pdf:pdf},
title = {{Synthesizing MPI Implementations from Functional Data-Parallel Programs}},
url = {http://www.cs.sun.ac.za/{~}bfischer/pdfs/hlpp-14.pdf},
year = {2014}
}
@article{Bolz2003,
annote = {Cited by 895.},
author = {Bolz, J. and Farmer, I. and Grinspun, E. and Schroder, P.},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Bolz et al. - Sparse matrix solvers on the GPU conjugate gradients and multigrid.pdf:pdf},
journal = {TOG},
keywords = {Conjugate Gradient,Fluid Simulation,GPU Computing,Mesh Smoothing,Multigrid,Navier-Stokes,Numerical Simulation},
number = {3},
title = {{Sparse matrix solvers on the GPU: conjugate gradients and multigrid}},
volume = {22},
year = {2003}
}
@article{Long2018,
abstract = {In recent years, there is a surge on machine learning applications in industry. Many of them are based on popular AI frameworks like Tensorflow, Torch, Caffe, or MxNet, etc, and are enpowered by accelerator platforms such as GPUs. One important challenge of running Tensorflow computations on GPUs is the fine granularity problem, namely, FLOPS of individual ops are far from enough to fully exploit the computing power of underlying accelerators. The XLA framework provides a solid foundation to explore this problem further. In this paper, we propose FusionStitching, a novel, comprehensive Op fusion and code generation system to stitch computations into large GPU kernels. Experimental results on four public models and two of our large inhouse applications show another 55{\%} (geometric mean) reduction of GPU kernel launches, compared to the XLA fusion baseline. This increases the E2E performance of both of our latency critical inhouse applications up to 20{\%}.},
archivePrefix = {arXiv},
arxivId = {1811.05213},
author = {Long, G. and Yang, J. and Zhu, K. and Lin, W.},
eprint = {1811.05213},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Long et al. - FusionStitching Deep Fusion and Code Generation for Tensorflow Computations on GPUs.pdf:pdf},
journal = {arXiv:1811.05213},
title = {{FusionStitching: Deep Fusion and Code Generation for Tensorflow Computations on GPUs}},
url = {http://arxiv.org/abs/1811.05213},
year = {2018}
}
@article{Zima1988,
abstract = {This paper describes the design of an interactive system for the semi-automatic transformation of FORTRAN 77 programs into parallel programs for the SUPERNUM machine. The system is characterized by a powerful analysis component, a catalog of MIMD and SIMD parallelization transformations, and a flexible dialog facility. It contains specific knowledge about the parallelization of an important class of numerical algorithms.},
annote = {NULL},
author = {Zima, Hans P and Bast, Heinz-J and Gerndt, Michael},
doi = {10.1016/0167-8191(88)90002-6},
file = {:Users/cec/Google Drive/Mendeley Library/1988 - Zima, Bast, Gerndt - SUPERB A tool for semi-automatic MIMDSIMD parallelization.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {analysis of algorithms,muitiprocessors,program transformations},
title = {{SUPERB: A tool for semi-automatic MIMD/SIMD parallelization}},
volume = {6},
year = {1988}
}
@article{Tartara2013,
abstract = {Optimizing programs to exploit the underlying hardware architecture is an important task. Much research has been done on enabling compilers to find the best set of code optimizations that can build the fastest and less resource-hungry executable for a given program. A common approach is iterative compilation, sometimes enriched by machine learning techniques. This provides good results, but requires extremely long compilation times and an initial training phase lasting even for days or weeks. We present long-term learning, a new algorithm that allows the compiler user to improve the performance of compiled programs with reduced compilation times with respect to iterative compilation, and without an initial training phase. Our algorithm does not just build good programs: it acquires knowledge every time a program is compiled and it uses such knowledge to learn compiler heuristics, without the need for an expert to manually define them. The heuristics are evolved during every compilation, by evaluating their effect on the generated programs.We present implementations of long-term learning on top of two different compilers, and experimental data gathered on multiple hardware configurations showing its effectiveness.},
annote = {Well written abstract. This paper presents an iterative compilation technique, built on GCC, which requires no initial training phase. Instead, it continuously refines its heuristics after every compilation, with an emphasis on online, long-term learning. Genetic algorithms are used to mutate compilation configurations, which consist of static features. The experimental results are somewhat suspicious. Why do they report maximum speedup as well as average speedup? Why do they spend so long comparing performance to -O0 instead of -O3?},
author = {Tartara, M. and {Crespi Reghizzi}, S.},
doi = {10.1145/2400682.2400705},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Tartara, Crespi Reghizzi - Continuous learning of compiler heuristics.pdf:pdf},
issn = {15443566},
journal = {TACO},
keywords = {Iterative compilation,PetaBricks,machine learning},
month = {jan},
number = {4},
title = {{Continuous learning of compiler heuristics}},
volume = {9},
year = {2013}
}
@book{Lozano-Perez2012,
annote = {NULL},
author = {Lozano-Perez, T. and Cox, I. J. and Wilfong, G. T.},
publisher = {Springer},
title = {{Autonomous Robot Vehicles}},
year = {2012}
}
@inproceedings{Terragni2016,
abstract = {Popular Q{\&}A sites like StackOverflow have collected nu-merous code snippets. However, many of them do not have complete type information, making them uncompilable and inapplicable to various software engineering tasks. This paper analyzes this problem, and proposes a technique CSnippEx to automatically convert code snippets into compilable Java source code files by resolving external dependencies, gener-ating import declarations, and fixing syntactic errors. We implemented CSnippEx as a plug-in for Eclipse and evalu-ated it with 242,175 StackOverflow posts that contain code snippets. CSnippEx successfully synthesized compilable Java files for 40,410 of them. It was also able to effectively recover import declarations for each post with a precision of 91.04{\%} in a couple of seconds.},
author = {Terragni, Valerio and Liu, Yepang and Cheung, Shing-Chi},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Terragni, Liu, Cheung - CSNIPPEX automated synthesis of compilable code snippets from Q{\&}A sites.pdf:pdf},
keywords = {crowd-generated snippets,developer social networks,pro-},
title = {{CSNIPPEX: automated synthesis of compilable code snippets from Q{\&}A sites}},
year = {2016}
}
@inproceedings{Sundermeyer2012,
abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a se- quence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of re- current models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we ana- lyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 {\%} relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.},
annote = {Cited by 87.},
author = {Sundermeyer, M. and Schl, R. and Ney, H.},
booktitle = {Interspeech},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Sundermeyer, Schl, Ney - LSTM Neural Networks for Language Modeling.pdf:pdf},
isbn = {9781622767595},
keywords = {LSTM neural networks,language modeling,recurrent neural networks},
title = {{LSTM Neural Networks for Language Modeling}},
year = {2012}
}
@misc{Chinneck1999,
abstract = {This note describes how to organize the written thesis which is the central element of your graduate degree. To know how to organize the thesis document, you first have to understand what graduate-level research is all about, so that is covered too. In other words, this note should be helpful when you are just getting started in your graduate program, as well as later when you start to write your thesis.},
annote = {NULL},
author = {Chinneck, Prof John W},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Chinneck - How to Organize your Thesis.pdf:pdf},
title = {{How to Organize your Thesis}},
year = {1999}
}
@inproceedings{Wang2017c,
abstract = {Programs that take highly-structured files as inputs normally process inputs in stages: syntax parsing, semantic check-ing, and application execution. Deep bugs are often hidden in the application execution stage, and it is non-trivial to automatically generate test inputs to trigger them. Mutation-based fuzzing gen-erates test inputs by modifying well-formed seed inputs randomly or heuristically. Most inputs are rejected at the early syntax pars-ing stage. Differently, generation-based fuzzing generates inputs from a specification (e.g., grammar). They can quickly carry the fuzzing beyond the syntax parsing stage. However, most inputs fail to pass the semantic checking (e.g., violating semantic rules), which restricts their capability of discovering deep bugs. In this paper, we propose a novel data-driven seed generation approach, named Skyfire, which leverages the knowledge in the vast amount of existing samples to generate well-distributed seed inputs for fuzzing programs that process highly-structured inputs. Skyfire takes as inputs a corpus and a grammar, and consists of two steps. The first step of Skyfire learns a probabilistic context-sensitive grammar (PCSG) to specify both syntax features and semantic rules, and then the second step leverages the learned PCSG to generate seed inputs. We fed the collected samples and the inputs generated by Skyfire as seeds of AFL to fuzz several open-source XSLT and XML engines (i.e., Sablotron, libxslt, and libxml2). The results have demonstrated that Skyfire can generate well-distributed inputs and thus significantly improve the code coverage (i.e., 20{\%} for line coverage and 15{\%} for function coverage on average) and the bug-finding capability of fuzzers. We also used the inputs generated by Skyfire to fuzz the closed-source JavaScript and rendering engine of Internet Explorer 11. Altogether, we discovered 19 new memory corruption bugs (among which there are 16 new vulnerabilities) and 32 denial-of-service bugs.},
author = {Wang, J. and Chen, B. and Wei, L. and Liu, Y.},
booktitle = {S{\&}P},
doi = {10.1109/SP.2017.23},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Wang et al. - Skyfire Data-Driven Seed Generation for Fuzzing.pdf:pdf},
isbn = {9781509055326},
issn = {10816011},
title = {{Skyfire: Data-Driven Seed Generation for Fuzzing}},
year = {2017}
}
@inproceedings{Tristan2015a,
abstract = {We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler — and unlike an uncollapsed Gibbs sampler — it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler — and unlike a collapsed Gibbs sampler — it is embarrassingly parallel, and can use approximate counters.},
annote = {NULL},
author = {Tristan, J. and Tassarotti, J. and Steele, G.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Tristan, Tassarotti, Steele - Efficient Training of LDA on a GPU by Mean-for-Mode Estimation.pdf:pdf},
title = {{Efficient Training of LDA on a GPU by Mean-for-Mode Estimation}},
url = {http://jmlr.org/proceedings/papers/v37/tristan15.html},
volume = {37},
year = {2015}
}
@article{Menon,
author = {Menon, A. K. and Tamuz, O. and Gulwani, S. and Lampson, B. and Kalai, A. T.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Menon et al. - Textual Features for Programming by Example.pdf:pdf},
journal = {arXiv:1209.3811},
title = {{Textual Features for Programming by Example}},
year = {2012}
}
@inproceedings{Gerstmann2009,
annote = {NULL},
author = {Gerstmann, Derek},
booktitle = {Siggraph Asia},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Gerstmann - Advanced OpenCL Event Model Usage.pdf:pdf},
title = {{Advanced OpenCL Event Model Usage}},
year = {2009}
}
@article{Nicolescu2001,
abstract = {The paper presents a data and task parallel low-level image processing environment for distributed memory systems. Image processing operators are parallelized by data decomposition using algorithmic skeletons. Image processing applications are parallelized by task decomposition, based on the image application task graph. In this way, an image processing application can be parallelized both by data and task decomposition, and thus better speed-ups can be obtained. We validate our method on the multi-baseline stereo vision application. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
annote = {NULL},
author = {Nicolescu, C. and Jonker, P.},
doi = {10.1109/ICPPW.2001.951848},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - Nicolescu, Jonker - A data and task parallel image processing environment for distributed memory systems.pdf:pdf},
isbn = {0769512607},
issn = {15302016},
journal = {ICPP},
keywords = {Biomedical imaging,Computer architecture,Image processing,Parallel architectures,Parallel processing,Pattern recognition,Physics,Skeleton,Stereo vision,Weather forecasting},
title = {{A data and task parallel image processing environment for distributed memory systems}},
year = {2001}
}
@inproceedings{Vanderbruggen2014,
annote = {NULL},
author = {Vanderbruggen, T. and Cavazos, J.},
booktitle = {IWOCL},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Vanderbruggen, Cavazos - Generating OpenCL C Kernels from OpenACC.pdf:pdf},
keywords = {OpenACC,OpenCL,execution model,performance,portability},
title = {{Generating OpenCL C Kernels from OpenACC}},
year = {2014}
}
@article{Mpeis2015,
abstract = {The abundance of poorly optimized mobile applications coupled with their increasing centrality in our digital lives make a framework for mobile app optimization an imperative. While tuning strategies for desktop and server applications have a long history, it is difficult to adapt them for use on mobile phones. Reference inputs which trigger behavior similar to a mobile application's typical are hard to construct. For many classes of applications the very concept of typical behavior is nonexistent, each user interacting with the application in very different ways. In contexts like this, optimization strategies need to evaluate their effectiveness against real user input, but doing so online runs the risk of user dissatisfaction when suboptimal optimizations are evaluated. In this paper we present an iterative compiler which employs a novel capture and replay technique in order to collect real user input and use it later to evaluate different transformations offline. The proposed mechanism identifies and stores only the set of memory pages needed to replay the most heavily used functions of the application. At idle periods, this minimal state is combined with different binaries of the application, each one build with different optimizations enabled. Replaying the targeted functions allows us to evaluate the effectiveness of each set of optimizations for the actual way the user interacts with the application. For the BEEBS benchmark suite, our approach was able to improve performance by up to 57{\%}, while keeping the slowdown experienced by the user on average at 0.8{\%}. By focusing only on heavily used functions, we are able to conserve storage space by between two and three orders of magnitude compared to typical capture and replay implementations.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1511.02603},
author = {Mpeis, P. and Petoumenos, P. and Leather, H.},
eprint = {1511.02603},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Mpeis, Petoumenos, Leather - Iterative compilation on mobile devices.pdf:pdf},
journal = {arXiv:1511.02603},
title = {{Iterative compilation on mobile devices}},
url = {http://arxiv.org/abs/1511.02603},
year = {2016}
}
@misc{Stirlinga,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 26. Conditional probabability {\&} Bayes' theorem.pdf:pdf},
number = {Chapter 7},
title = {{26. Conditional probabability {\&} Bayes' theorem}},
volume = {7},
year = {2015}
}
@inproceedings{Alistarh2014,
annote = {NULL},
author = {Alistarh, D. and Kopinsky, J. and Li, J. and Shavit, N.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Alistarh et al. - The SprayList A Scalable Relaxed Priority Queue.pdf:pdf},
isbn = {9781450332057},
keywords = {concurrent data structures,parallel algorithms},
title = {{The SprayList: A Scalable Relaxed Priority Queue}},
url = {http://research.microsoft.com/pubs/209108/SprayList.pdf},
year = {2015}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies di-rectly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learn-ing Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
annote = {NULL},
author = {Mnih, V. and Kavukcuoglu, K. and Silver, D. and Graves, A. and Antonoglou, I. and Wierstra, D. and Riedmiller, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
journal = {arXiv:1312.5602},
title = {{Playing Atari with Deep Reinforcement Learning}},
year = {2013}
}
@inproceedings{Mcpherson,
annote = {NULL},
author = {Mcpherson, Andrew J and Cintra, Marcelo},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Mcpherson, Cintra - Fence Placement for Legacy Data-Race-Free Programs via Synchronization Read Detection.pdf:pdf},
isbn = {9781450332057},
keywords = {fence placement,relaxed memory models},
title = {{Fence Placement for Legacy Data-Race-Free Programs via Synchronization Read Detection}},
year = {2015}
}
@article{Rossum2015,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Python Frequently Asked Questions.pdf:pdf},
title = {{Python Frequently Asked Questions}},
year = {2016}
}
@article{Vinas2015,
abstract = {The use of heterogeneous devices is becoming increasingly widespread. Their main drawback is their low programmability due to the large amount of details that must be handled. Another important problem is the reduced code portability, as most of the tools to program them are vendor or device-specific. The exception to this observation is OpenCL, which largely suffers from the reduced programmability problem mentioned, particularly in the host side. The Heterogeneous Programming Library (HPL) is a recent proposal to improve this situation, as it couples portability with good programmability. While the HPL kernels must be written in a language embedded in C++, users may prefer to use OpenCL kernels for several reasons such as their growing availability or a faster development from existing codes. In this paper we extend HPL to support the execution of native OpenCL kernels and we evaluate the resulting solution in terms of performance and programmability, achieving very good results.},
annote = {Cited by 0.},
author = {Vi{\~{n}}as, M. and Fraguela, B. B. and Bozkus, Z. and Andrade, D.},
doi = {10.1016/j.procs.2015.05.208},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Vi{\~{n}}as et al. - Improving OpenCL Programmability with the Heterogeneous Programming Library.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {heterogeneity,libraries,opencl,portability,programmability},
publisher = {Elsevier},
title = {{Improving OpenCL Programmability with the Heterogeneous Programming Library}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050915010169},
volume = {51},
year = {2015}
}
@misc{UniversityofEdinburgh2013,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - University of Edinburgh - IAML 2013 Exam.pdf:pdf},
number = {May},
title = {{IAML 2013 Exam}},
year = {2013}
}
@misc{Raina2007,
abstract = {We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.},
annote = {NULL},
author = {Raina, R. and Battle, A. and Lee, H. and Packer, B. and Ng, A. Y.},
booktitle = {ICML},
doi = {10.1145/1273496.1273592},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Raina et al. - Self-taught Learning Transfer Learning from Unlabeled Data.pdf:pdf},
isbn = {9781595937933},
issn = {1595937935},
title = {{Self-taught Learning: Transfer Learning from Unlabeled Data}},
year = {2007}
}
@misc{Silver2014,
abstract = {tutorial on all bandits approaches},
author = {Silver, D.},
booktitle = {Courses},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Silver - Lecture 9 Exploration and Exploitation.pdf:pdf},
title = {{Lecture 9 : Exploration and Exploitation}},
year = {2014}
}
@article{Klockner2014a,
abstract = {Today's highly heterogeneous computing landscape places a burden on programmers wanting to achieve high performance on a reasonably broad cross-section of machines. To do so, computations need to be expressed in many different but mathematically equivalent ways, with, in the worst case, one variant per target machine. Loo.py, a programming system embedded in Python, meets this challenge by defining a data model for array-style computations and a library of transformations that operate on this model. Offering transformations such as loop tiling, vectorization, storage management, unrolling, instruction-level parallelism, change of data layout, and many more, it provides a convenient way to capture, parametrize, and re-unify the growth among code variants. Optional, deep integration with numpy and PyOpenCL provides a convenient computing environment where the transition from prototype to high-performance implementation can occur in a gradual, machine-assisted form.},
archivePrefix = {arXiv},
arxivId = {1405.7470},
author = {Kl{\"{o}}ckner, A.},
doi = {10.1145/2627373.2627387},
eprint = {1405.7470},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Kl{\"{o}}ckner - Loo.py transformation-based code generation for GPUs and CPUs.pdf:pdf},
isbn = {9781450329378},
journal = {ARRAY},
keywords = {code generation,consisting of a tree,data layout,embedded language,gpu,gramming language python,high-level language,high-performance,of polyhedra,out in a language,the computa-,the user first specifies,tion,tion to be carried,vectoriza-},
title = {{Loo.py: transformation-based code generation for GPUs and CPUs}},
url = {http://arxiv.org/abs/1405.7470{\%}5Cnhttp://dx.doi.org/10.1145/2627373.2627387{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2627373.2627387},
year = {2014}
}
@article{Hamrick2018,
abstract = {While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a "relational inductive bias": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object- and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more naive approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.},
archivePrefix = {arXiv},
arxivId = {1806.01203},
author = {Hamrick, J. B. and Allen, K. R. and Bapst, V. and Zhu, T. and McKee, K. R. and Tenenbaum, J. B. and Battaglia, P. W.},
doi = {10.1016/j.ecolind.2008.03.002},
eprint = {1806.01203},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Hamrick et al. - Relational Inductive Bias for Physical Construction in Humans and Machines.pdf:pdf},
isbn = {1470-160X},
issn = {1470160X},
journal = {arXiv:1806.01203},
keywords = {deep learning,object-based reasoning,physical construction,reinforcement learning,relational reasoning},
title = {{Relational Inductive Bias for Physical Construction in Humans and Machines}},
url = {http://arxiv.org/abs/1806.01203},
year = {2018}
}
@inproceedings{Loncaric2016,
annote = {NULL},
author = {Loncaric, C. and Emina, T. and Ernst, M. D.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Loncaric, Emina, Ernst - Fast Synthesis of Fast Collections.pdf:pdf},
keywords = {data structure synthesis},
title = {{Fast Synthesis of Fast Collections}},
year = {2016}
}
@article{Silva2018,
author = {Silva, T. W. B. and Morais, D. C. and Andrade, H. G. R. and Lima, A. M. N. and Melcher, E. U. K. and Brito, A. V.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Silva et al. - Environment for integration of distributed heterogeneous computing systems.pdf:pdf},
journal = {JISA},
keywords = {Distributed computing,Heterogeneous computing,Midd,distributed computing,heterogeneous computing,middleware},
number = {1},
publisher = {Journal of Internet Services and Applications},
title = {{Environment for integration of distributed heterogeneous computing systems}},
volume = {9},
year = {2018}
}
@inproceedings{Tan2014,
abstract = {Providing high level tools for parallel programming while sustaining a high level of performance has been a challenge that techniques like Domain Specific Embedded Languages try to solve. In previous works, we investi- gated the design of such a DSEL – NT2 – providing a Matlab -like syntax for parallel numerical computations inside a C++ library. In this paper, we show how NT2 has been redesigned for shared memory systems in an extensible and portable way. The new NT2design relies on a tiered Parallel Skeleton system built using asynchronous task management and automatic compile-time task- ification of user level code. We describe how this system can operate various shared memory runtimes and evaluate the design by using several benchmarks implementing linear algebra algorithms.},
annote = {NULL},
author = {Tan, Antoine Tran and Falcou, Joel and Etiemble, Daniel and Kaiser, Hartmut},
booktitle = {HLPP},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Tan et al. - Automatic Task-based Code Generation for High Performance Domain Specific Embedded Language.pdf:pdf},
keywords = {C++,asynchronous programming,generative programming,parallel skeletons},
title = {{Automatic Task-based Code Generation for High Performance Domain Specific Embedded Language}},
url = {http://stellar.cct.lsu.edu/pubs/dsl{\_}paper.pdf},
year = {2014}
}
@article{Misale2016,
annote = {NULL},
author = {Misale, C. and Drocco, M. and Aldinucci, M. and Tremblay, G.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Misale et al. - A Survey of Big Data Frameworks on a Layered Dataflow Model.pdf:pdf},
title = {{A Survey of Big Data Frameworks on a Layered Dataflow Model}},
year = {2016}
}
@inproceedings{Tsay2014,
abstract = {Open source software is commonly portrayed as a meritocracy, where decisions are based solely on their technical merit. However, literature on open source suggests a complex social structure underlying the meritocracy. Social work environments such as GitHub make the relationships between users and between users and work artifacts transparent. This transparency enables developers to better use information such as technical value and social connections when making work decisions. We present a study on open source software contribution in GitHub that focuses on the task of evaluating pull requests, which are one of the primary methods for contributing code in GitHub. We analyzed the association of various technical and social measures with the likelihood of contribution acceptance. We found that project managers made use of information signaling both good technical contribution practices for a pull request and the strength of the social connection between the submitter and project manager when evaluating pull requests. Pull requests with many comments were much less likely to be accepted, moderated by the submitter's prior interaction in the project. Well-established projects were more conservative in accepting pull requests. These findings provide evidence that developers use both technical and social information when evaluating potential contributions to open source software projects. Categories},
annote = {NULL},
author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James},
booktitle = {ICSE},
doi = {10.1145/2568225.2568315},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Tsay, Dabbish, Herbsleb - Influence of social and technical factors for evaluating contribution in GitHub.pdf:pdf},
isbn = {9781450327565},
issn = {15345351},
keywords = {contribution,github,group-social,open source,signaling,social computing,social media,theory,transparency},
title = {{Influence of social and technical factors for evaluating contribution in GitHub}},
year = {2014}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, B. and Le, Q. V.},
eprint = {1611.01578},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zoph, Le - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
issn = {1938-7228},
journal = {arXiv:1611.01578},
title = {{Neural Architecture Search with Reinforcement Learning}},
year = {2016}
}
@inproceedings{Bielik2017,
annote = {NULL},
author = {Bielik, P. and Raychev, V. and Vechev, M.},
booktitle = {ICLR},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Bielik, Raychev, Vechev - Program Synthesis for Character Level Language Modeling.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Program Synthesis for Character Level Language Modeling}},
year = {2017}
}
@incollection{Ng,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 1-14).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 1-14)}},
year = {2018}
}
@inproceedings{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive chal-lenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
annote = {NULL},
author = {Hermann, K. M. and Kocisky, T. and Grefenstette, E.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hermann, Kocisky, Grefenstette - Teaching Machines to Read and Comprehend.pdf:pdf},
title = {{Teaching Machines to Read and Comprehend}},
year = {2015}
}
@inproceedings{Dubach2009,
abstract = {Building an optimising compiler is a difficult and time consum- ing task which must be repeated for each generation of a micro- processor. As the underlying microarchitecture changes from one generation to the next, the compiler must be retuned to optimise specifically for that new system. It may take several releases of the compiler to effectively exploit a processor's performance potential, by which time a new generation has appeared and the process starts again. We address this challenge by developing a portable optimising compiler. Our approach employs machine learning to automati- cally learn the best optimisations to apply for any new program on a new microarchitectural configuration. It achieves this by learn- ing a model off-line which maps a microarchitecture description plus the hardware counters from a single run of the program to the best compiler optimisation passes. Our compiler gains 67{\%} of the maximumspeedup obtainable by an iterative compiler search using 1000 evaluations. We obtain, on average, a 1.16x speedup over the highest default optimisation level across an entire microarchitec- ture configuration space, achieving a 4.3x speedup in the best case. We demonstrate the robustness of this technique by applying it to an extended microarchitectural space where we achieve comparable performance.},
annote = {This paper presents an iterative offline compiler which uses 1000 evaluations of a program to build a machine learning model that creates portable optimisations.},
author = {Dubach, C. and Jones, T. M. and Bonilla, E. V. and Fursin, G. and O'Boyle, M.},
booktitle = {MICRO},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Dubach et al. - Portable Compiler Optimisation Across Embedded Programs and Microarchitectures using Machine Learning.pdf:pdf},
keywords = {Design,Experimentation,Performance},
publisher = {ACM},
title = {{Portable Compiler Optimisation Across Embedded Programs and Microarchitectures using Machine Learning}},
year = {2009}
}
@book{Mitchell2006,
abstract = {Over the past 50 years the study of Machine Learning has grown from the efforts of a handful of computer en- gineers exploring whether computers could learn to play games, and a field of Statistics that largely ignored computational considerations, to a broad discipline that has produced fundamental statistical-computational theories of learning processes, has designed learning algorithms that are routinely used in commercial sys- tems for speech recognition, computer vision, and a variety of other tasks, and has spun off an industry in data mining to discover hidden regularities in the growing volumes of online data. This document provides a brief and personal view of the discipline that has emerged as Machine Learning, the fundamental questions it addresses, its relationship to other sciences and society, and where it might be headed.},
annote = {NULL},
author = {Mitchell, T. M.},
booktitle = {Machine Learning},
doi = {10.1080/026404199365326},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Mitchell - The Discipline of Machine Learning.pdf:pdf},
isbn = {0070428077},
issn = {0264-0414},
keywords = {machine learning},
pmid = {10622353},
publisher = {Carnegie Mellon University, School of Computer Science, Machine Learning Department},
title = {{The Discipline of Machine Learning}},
url = {http://www-cgi.cs.cmu.edu/{~}tom/pubs/MachineLearningTR.pdf},
year = {2006}
}
@inproceedings{Agakov,
abstract = {Iterative compiler optimization has been shown to out- perform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper de- velops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an indepen- dent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C6713 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magni- tude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.},
annote = {Introducing ML to iterative compilation. Cited by 316.},
author = {Agakov, F. and Bonilla, E. and Cavazos, J. and Franke, B. and Fursin, G. and O'Boyle, M. and Thomson, J. and Toussaint, M. and Williams, C. K. I.},
booktitle = {CGO},
doi = {10.1109/CGO.2006.37},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Agakov et al. - Using Machine Learning to Focus Iterative Optimization.pdf:pdf},
isbn = {0-7695-2499-0},
publisher = {IEEE},
title = {{Using Machine Learning to Focus Iterative Optimization}},
year = {2006}
}
@inproceedings{Muralidharan2016,
annote = {NULL},
author = {Muralidharan, S. and Roy, A. and Hall, M. and Garland, M. and Rai, P.},
booktitle = {ASPLOS},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Muralidharan et al. - Architecture-Adaptive Code Variant Tuning.pdf:pdf},
keywords = {autotuning,cross-architectural tuning,device feature selection,input-adaptive,multi-task learning},
publisher = {ACM},
title = {{Architecture-Adaptive Code Variant Tuning}},
year = {2016}
}
@misc{UniversityofEdinburgh2014h,
annote = {NULL},
author = {{University of Edinburgh}},
booktitle = {Parallel Architectures},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - 2. Parallel Architectures.pdf:pdf},
title = {{2. Parallel Architectures}},
year = {2014}
}
@inproceedings{Lashgar2013,
abstract = {There are a number of design decisions that impact a GPU's performance. Among such decisions deciding the right warp size can deeply influence the rest of the design. Small warps reduce the performance penalty associated with branch divergence at the expense of a reduction in memory coalescing. Large warps enhance memory coalescing significantly but also increase branch divergence. This leaves designers with two choices: use small warps and invest in finding new solutions to enhance coalescing or use large warps and address branch divergence employing effective control-flow solutions. In this work our goal is to investigate the answer to this question. We analyze warp size impact on memory coalescing and branch divergence. We use our findings to study two machines: a GPU using small warps but equipped with excellent memory coalescing (SW+) and a GPU using large warps but employing an MIMD engine immune from control-flow costs (LW+). Our evaluations show that building coalescing-enhanced small warp GPUs is a better approach compared to pursuing a control- flow},
annote = {NULL},
author = {Lashgar, Ahmad and Baniasadi, Amirali and Khonsari, Ahmad},
booktitle = {GPGPU},
doi = {10.1145/2458523.2458538},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Lashgar, Baniasadi, Khonsari - Warp size impact in GPUs large or small.pdf:pdf},
isbn = {9781450320177},
keywords = {Branch divergence,GPU architecture,Memory divergence,SIMD efficiency,Warp size},
title = {{Warp size impact in GPUs: large or small?}},
url = {http://dl.acm.org/citation.cfm?id=2458523.2458538{\&}coll=DL{\&}dl=ACM{\&}CFID=262835316{\&}CFTOKEN=66795774},
year = {2013}
}
@inproceedings{Toshev2014,
abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images.},
annote = {NULL},
author = {Toshev, A. and Szegedy, C.},
booktitle = {CVPR},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Toshev, Szegedy - DeepPose Human Pose Estimation via Deep Neural Networks.pdf:pdf},
publisher = {IEEE},
title = {{DeepPose: Human Pose Estimation via Deep Neural Networks}},
year = {2014}
}
@article{Samanta2018,
abstract = {Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with graphs due to their unique characteristics--their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of local structural and functional properties in the generated graphs. Experiments reveal that our model is able to learn and mimic the generative process of several well-known random graph models and can be used to discover new molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also identify molecules that maximize certain desirable properties more effectively than alternatives.},
author = {Samanta, B. and De, A. and Ganguly, N. and Gomez-Rodriguez, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Samanta et al. - Designing Random Graph Models Using Variational Autoencoders With Applications to Chemical Design.pdf:pdf},
journal = {arXiv:1802.05283},
title = {{Designing Random Graph Models Using Variational Autoencoders With Applications to Chemical Design}},
year = {2018}
}
@inproceedings{Baek2010,
abstract = {Energy-efficient computing is important in several systems rang- ing from embedded devices to large scale data centers. Several ap- plication domains offer the opportunity to tradeoff quality of ser- vice/solution (QoS) for improvements in performance and reduc- tion in energy consumption. Programmers sometimes take advan- tage of such opportunities, albeit in an ad-hoc manner and often without providing any QoS guarantees. We propose a system called Green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical QoS guarantees. Green enables programmers to approximate expensive functions and loops and operates in two phases. In the calibration phase, it builds a model of the QoS loss produced by the approximation. This model is used in the opera- tional phase to make approximation decisions based on the QoS constraints specified by the programmer. The operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and QoS model to provide strong statistical QoS guarantees. To evaluate the effectiveness of Green, we implemented our sys- tem and language extensions using the Phoenix compiler frame- work. Our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an in-production, real-world web search engine, indicate that Green can produce significant improvements in performance and energy consumption with small and controlled QoS degradation.},
annote = {Green is a framework for trading QoS for energy efficiency, by approximating expensive functions and loops. It consists of two phases: the first build a model of QoS loss by approximated, the operational phase uses this model to make approximation decisions.},
author = {Baek, Woongki and Chilimbi, Trishul M.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Baek, Chilimbi - Green A Framework for Supporting Energy-Conscious Programming using Controlled Approximation.pdf:pdf},
keywords = {Controlled Approximation,Energy-Conscious Programming},
title = {{Green: A Framework for Supporting Energy-Conscious Programming using Controlled Approximation}},
year = {2010}
}
@misc{IntelCorporation2016,
annote = {NULL},
author = {{Intel Corporation}},
title = {{OpenCL Debugger for Linux OS}},
url = {https://software.intel.com/en-us/node/530816},
year = {2016}
}
@inproceedings{Aldinucci2008,
abstract = {Autonomic management can be used to improve the QoS provided by parallel/distributed applications. We discuss behavioural skeletons introduced in earlier work: rather than relying on programmer ability to design “from scratch” efficient autonomic policies, we encapsulate gen- eral autonomic controller features into algorithmic skele- tons. Then we leave to the programmer the duty of speci- fying the parameters needed to specialise the skeletons to the needs of the particular application at hand. This re- sults in the programmer having the ability to fast prototype and tune distributed/parallel applications with non-trivial autonomic management capabilities. We discuss how be- havioural skeletons have been implemented in the frame- work ofGCM(theGridComponentModel developedwithin theCoreGRIDNoE and currently being implemented within the GridCOMP STREP project). We present results evalu- ating the overhead introduced by autonomic management activities as well as the overall behaviour of the skeletons. We also present results achieved with a long running ap- plication subject to autonomic management and dynami- cally adapting to changing features of the target architec- ture. Overall the results demonstrate both the feasibility of implementing autonomic control via behavioural skeletons and the effectiveness of our sample behavioural skeletons in managing the “functional replication” pattern(s).},
annote = {NULL},
author = {Aldinucci, M. and Campa, S. and Danelutto, M. and Vanneschi, M. and Kilpatrick, P. and Dazzi, P. and Laforenza, D. and Tonellotto, N.},
booktitle = {PDP},
doi = {10.1109/PDP.2008.46},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Aldinucci et al. - Behavioural Skeletons in GCM Autonomic Management of Grid Components.pdf:pdf},
isbn = {978-0-7695-3089-5},
month = {feb},
publisher = {Ieee},
title = {{Behavioural Skeletons in GCM: Autonomic Management of Grid Components}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4457104},
year = {2008}
}
@article{Google2013,
annote = {NULL},
author = {Google, The and Interview, Technical},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Google, Interview - The Google Technical Interview How to Get Your Dream Job.pdf:pdf},
title = {{The Google Technical Interview How to Get Your Dream Job}},
year = {2013}
}
@inproceedings{Triantafyllis2003,
abstract = {To meet the performance demands of modern architectures, compilers incorporate an ever- increasing number of aggressive code transformations. Since most of these transformations are not universally beneficial, compilers traditionally control their application through predictive heuris- tics, which attempt to judge an optimization's effect on final code quality a priori. However, com- plex target architectures and unpredictable optimization interactions severely limit the accuracy of these judgments, leading to performance degradation because of poor optimization decisions. This performance loss can be avoided through the iterative compilation approach, which ad- vocates exploring many optimization options and selecting the best one a posteriori. However, existing iterative compilation systems suffer from excessive compile times and narrow application domains. By overcoming these limitations, Optimization-Space Exploration (OSE) becomes the first iterative compilation technique suitable for general-purpose production compilers. OSE nar- rows down the space of optimization options explored through limited use of heuristics. Acompiler tuning phase further limits the exploration space. At compile time, OSE prunes the remaining opti- mization configurations in the search space by exploiting feedback from earlier configurations tried. Finally, rather than measuring actual runtimes, OSE compares optimization outcomes through static performance estimation, further enhancing compilation speed. An OSE-enhanced version of Intel's reference compiler for the Itanium architecture yields a performance improvement of more than 20{\%} for some SPEC benchmarks.},
annote = {Cited by 221.},
author = {Triantafyllis, S. and August, D. I.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Triantafyllis, August - Compiler Optimization-Space Exploration.pdf:pdf},
publisher = {IEEE},
title = {{Compiler Optimization-Space Exploration}},
year = {2003}
}
@inproceedings{Palomba2017,
abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use archi- tecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the secu- rity viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
author = {Palomba, F. and Panichella, A. and Zaidman, A. and Oliveto, R. and {De Lucia}, A.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Palomba et al. - Automatic Test Case Generation What if Test Code Quality Matters.pdf:pdf},
keywords = {Multimodal learning analytics,Multimodal teaching analytics,STEM education,Sensors,Smart classroom,Smart school},
title = {{Automatic Test Case Generation: What if Test Code Quality Matters?}},
volume = {1828},
year = {2016}
}
@inproceedings{Chabbi,
annote = {NULL},
author = {Chabbi, Milind and Mellor-crummey, John},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chabbi, Mellor-crummey - Barrier Elision for Production Parallel Programs.pdf:pdf},
isbn = {9781450332057},
keywords = {barrier elision,dynamic analysis,dynamic optimization,hpc,nwchem,pgas,synchronization},
title = {{Barrier Elision for Production Parallel Programs}},
year = {2015}
}
@phdthesis{Garvey2015b,
annote = {NULL},
author = {Garvey, J. D.},
doi = {10.1109/ICPP.2015.39},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Garvey - Automatic Performance Tuning of Stencil Computations on GPUs.pdf:pdf},
isbn = {978-1-4673-7587-0},
issn = {01903918},
school = {University of Toronto},
title = {{Automatic Performance Tuning of Stencil Computations on GPUs}},
year = {2015}
}
@incollection{Yearning-draftc,
abstract = {Users of your cat pictures app have uploaded 10,000 images, which you have manually labeled as containing cats or not. You also have a larger set of 200,000 images that you downloaded off the internet. How should you define train/dev/test sets? Since the 10,000 user images closely reflect the actual probability distribution of data you want to do well on, you might use that for your dev and test sets. If you are training a data-hungry deep learning algorithm, you might give it the additional 200,000 internet images for training. Thus, your training and dev/test sets come from different probability distributions. How does this affect your work? Instead of partitioning our data into train/dev/test sets, we could take all 210,000 images we have, and randomly shuffle them into train/dev/test sets. In this case, all the data comes from the same distribution. But I recommend against this method, because about 205,000/210,000 ≈ 97.6{\%} of your dev/test data would come from internet images, which does not reflect the actual distribution you want to do well on. Remember our recommendation on choosing dev/test sets: Choose dev and test sets to reflect data you expect to get in the future and want to do well on.},
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 36-39).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 36-39)}},
year = {2018}
}
@inproceedings{Lozi,
annote = {NULL},
author = {Lozi, Jean-pierre and Funston, Justin and Gaud, Fabien and Qu, Vivien and Fedorova, Alexandra},
booktitle = {EuroSys},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Lozi et al. - The Linux Scheduler a Decade of Wasted Cores.pdf:pdf},
isbn = {9781450342407},
title = {{The Linux Scheduler: a Decade of Wasted Cores}},
year = {2016}
}
@inproceedings{Moscovici2017,
abstract = {We propose a design for a fine-grained lock-based skiplist optimized for Graphics Processing Units (GPUs). While GPUs are often used to accelerate streaming parallel com-putations, it remains a significant challenge to efficiently offload concurrent computations with more complicated data-irregular access and fine-grained synchronization. Nat-ural building blocks for such computations would be con-current data structures, such as skiplists, which are widely used in general purpose computations. Our design utilizes array-based nodes which are accessed and updated by warp-cooperative functions, thus taking advantage of the fact that GPUs are most efficient when memory accesses are coa-lesced and execution divergence is minimized. The proposed design has been implemented, and measurements demon-strate improved performance of up to 2.6x over skiplist de-signs for the GPU existing today.},
author = {Moscovici, N. and Cohen, N. and Petrank, E.},
booktitle = {PACT},
doi = {10.1145/3018743.3019032},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Moscovici, Cohen, Petrank - A GPU-Friendly Skiplist Algorithm.pdf:pdf},
isbn = {9781450344937},
keywords = {Data Structures,GPU,SIMD,Skip List},
title = {{A GPU-Friendly Skiplist Algorithm}},
year = {2017}
}
@article{Lee2007,
annote = {NULL},
author = {Lee, B. C. and Brooks, D. M. and de Supinski, B. R. and Schulz, M. and Singh, K. and McKee, S. A.},
doi = {10.1145/1229428.1229479},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Lee et al. - Methods of inference and learning for performance modeling of parallel applications.pdf:pdf},
isbn = {9781595936028},
journal = {PPoPP},
keywords = {neural networks,numerical methods,performance prediction,regression,statistics},
title = {{Methods of inference and learning for performance modeling of parallel applications}},
url = {http://portal.acm.org/citation.cfm?doid=1229428.1229479},
year = {2007}
}
@article{Bohm2001,
abstract = {During the last decade, multimedia databases have become increasingly important in many application areas such as medicine, CAD, geography, or molecular biology. An important research issue in the field of multimedia databases is the content based retrieval of similar multimedia objects such as images, text, and videos. However, in contrast to searching data in a relational database, a content based retrieval requires the search of similar objects as a basic functionality of the database system. Most of the approaches addressing similarity search use a so-called feature transformation which transforms important properties of the multimedia objects into high-dimension- al points (feature vectors). Thus, the similarity search is transformed into a search of points in the feature space which are close to a given query point in the high-dimen- sional feature space. Query Processing in high-dimensional spaces has therefore been a very active research area over the last few years. A number of new index structures and algorithms have been proposed. It has been shown that the new index structures considerably improve the performance in querying large multimedia databases. Based on recent tutorials [BK 98, BK 00], in this survey we provide an overview of the current state-of-the-art in querying multimedia databases, describing the index struc- tures and algorithms for an efficient query processing in high-dimensional spaces. We identify the problems of processing queries in high-dimensional space, and we provide an overview of the proposed approaches to overcome these problems.},
annote = {A survey of state-of-the-art in searching high-dimensional spaces used to represent content using feature vectors.


Cited by 899.},
author = {B{\"{o}}hm, Christian and Berchtold, Stefan and Keim, Daniel a.},
doi = {10.1145/502807.502809},
file = {:Users/cec/Google Drive/Mendeley Library/2001 - B{\"{o}}hm, Berchtold, Keim - Searching in high-dimensional spaces Index structures for improving the performance of multimedia databa.pdf:pdf},
isbn = {9781450307178},
issn = {03600300},
journal = {CSUR},
title = {{Searching in high-dimensional spaces: Index structures for improving the performance of multimedia databases}},
volume = {33},
year = {2001}
}
@article{Manzanera1999,
annote = {NULL},
author = {Manzanera, A and Bernard, TM},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Manzanera, Bernard - Ultra-fast skeleton based on an isotropic fully parallel algorithm.pdf:pdf},
journal = {Lecture Notes in Computer Science},
title = {{Ultra-fast skeleton based on an isotropic fully parallel algorithm}},
url = {http://link.springer.com/chapter/10.1007/3-540-49126-0{\_}24},
year = {1999}
}
@inproceedings{Karrenberg2011,
abstract = {Data-parallel programming languages are an important component in today's parallel computing landscape. Among those are domain-specific languages like shading languages in graphics (HLSL, GLSL, RenderMan, etc.) and “general-purpose” languages like CUDA or OpenCL. Current implementations of those languages on CPUs solely rely on multi-threading to implement parallelism and ignore the additional intra-core parallelism provided by the SIMD instruction set of those processors (like Intel's SSE and the upcoming AVX or Larrabee instruction sets). In this paper, we discuss several aspects of implementing dataparallel languages on machines with SIMD instruction sets. Our main contribution is a language- and platform-independent code transformation that performs whole-function vectorization on low-level intermediate code given by a control flow graph in SSA form. We evaluate our technique in two scenarios: First, incorporated in a compiler for a domain-specific language used in realtime ray tracing. Second, in a stand-alone OpenCL driver. We observe average speedup factors of 3.9 for the ray tracer and factors between 0.6 and 5.2 for different OpenCL kernels.},
author = {Karrenberg, R. and Hack, S.},
booktitle = {CGO},
doi = {10.1109/CGO.2011.5764682},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Karrenberg, Hack - Whole-function vectorization.pdf:pdf},
isbn = {9781612843551},
issn = {0138-9130},
title = {{Whole-function vectorization}},
year = {2011}
}
@techreport{Hoffmann2010,
abstract = {We present PowerDial, a system for dynamically adapting application behavior to execute successfully in the face of load and power fluctuations. PowerDial trans- forms static configuration parameters into dynamic knobs that the PowerDial control system can manipulate to dy- namically trade off the accuracy of the computation in return for reductions in the computational resources that the application requires to produce its results. These re- ductions translate into power savings. Our experimental results show that PowerDial can en- able our benchmark applications to execute responsively in the face of power caps (imposed, for example, in re- sponse to cooling system failures) that would otherwise significantly impair the delivered performance. They also show that PowerDial can reduce the number of ma- chines required to meet peak load, in our experiments enabling up to a 75{\%} reduction in direct power and cap- ital costs.},
annote = {PowerDial is a system for adapting application behaviour to fluctuationg power and load at runtime. PowerDial uses dynamic configuration parameters to allow programs to respond to power caps, by trading accuracy of computation for a reduction in required resources.},
author = {Hoffmann, H. and Sidiroglou, S. and Misailovic, S. and Agarwal, A. and Rinard, M. and Carbin, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Hoffmann et al. - Dynamic Knobs for Power-Aware Computing.pdf:pdf},
institution = {MIT},
title = {{Dynamic Knobs for Power-Aware Computing}},
year = {2010}
}
@inproceedings{Oquab2014,
abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large-scale visual recognition challenge (ILSVRC2012). The suc-cess of CNNs is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification meth-ods. Learning CNNs, however, amounts to estimating mil-lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi-ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep-resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
annote = {NULL},
author = {Oquab, M. and Bottou, L. and Laptev, I. and Sivic, J.},
booktitle = {CVPR},
doi = {10.1109/CVPR.2014.222},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Oquab et al. - Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
publisher = {IEEE},
title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
year = {2014}
}
@inproceedings{Shi2015,
annote = {NULL},
author = {Shi, X. and Liang, J. and Di, S. and He, B. and Jin, H. and Lu, L. and Wang, Z. and Luo, X. and Zhong, J.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Shi et al. - Optimization of Asynchronous Graph Processing on GPU with Hybrid Coloring Model.pdf:pdf},
isbn = {9781450332057},
keywords = {asynchronous computing,gpgpu,graph processing},
title = {{Optimization of Asynchronous Graph Processing on GPU with Hybrid Coloring Model}},
year = {2015}
}
@article{Ryoo2015,
annote = {NULL},
author = {Ryoo, Jee Ho and Quirem, Saddam J. and Lebeane, Michael and Panda, Reena and Song, Shuang and John, Lizy K.},
doi = {10.1109/ICPP.2015.41},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ryoo et al. - GPGPU Benchmark Suites How Well Do They Sample the Performance Spectrum.pdf:pdf},
isbn = {978-1-4673-7587-0},
issn = {01903918},
journal = {ICPP},
title = {{GPGPU Benchmark Suites: How Well Do They Sample the Performance Spectrum?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7349587},
year = {2015}
}
@article{Groce2016,
author = {Groce, A. and Alipour, M. A. and Zhang, C. and Chen, Y. and Regehr, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Groce et al. - Cause Reduction Delta Debugging, even without Bugs.pdf:pdf},
journal = {STVR},
number = {1},
title = {{Cause Reduction: Delta Debugging, even without Bugs}},
volume = {26},
year = {2016}
}
@misc{Silver2015n,
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 10 Classic Games.pdf:pdf},
title = {{Lecture 10 : Classic Games}},
year = {2015}
}
@inproceedings{Wang2010,
abstract = {Stream based languages are a popular approach to express- ing parallelism in modern applications. The efficient map- ping of streaming parallelism to multi-core processors is, however, highly dependent on the program and underlying architecture. We address this by developing a portable and automatic compiler-based approach to partitioning stream- ing programs using machine learning. Our technique pre- dicts the ideal partition structure for a given streaming ap- plication using prior knowledge learned off-line. Using the predictor we rapidly search the program space (without ex- ecuting any code) to generate and select a good partition. We applied this technique to standard StreamIt applications and compared against existing approaches. On a 4-core plat- form, our approach achieves 60{\%} of the best performance found by iteratively compiling and executing over 3000 dif- ferent partitions per program. We obtain, on average, a 1.90x speedup over the already tuned partitioning scheme of the StreamIt compiler. When compared against a state- of-the-art analytical, model-based approach, we achieve, on average, a 1.77x performance improvement. By porting our approach to a 8-core platform, we are able to obtain 1.8x im- provement over the StreamIt default scheme, demonstrating the portability of our approach.},
annote = {The paper describes an approach to paritioning StreamIt by using a feature set to represent a partion scheme, and using kNN machine learning to build a model to predict speedup using the feature set. For a new program, 100k possible partion layouts are generated. These layouts are clustered and the {\~{}}3000 most distinct are evaluated to see which is closest to the ideal partition layout from the training set.




A generally solid paper but with questionable results.},
author = {Wang, Z. and O'Boyle, M.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Wang, O'Boyle - Partitioning Streaming Parallelism for Multi-cores A Machine Learning Based Approach.pdf:pdf},
isbn = {9781450301787},
keywords = {Compiler Optimization,Machine Learning,Partitioning Streaming Parallelism},
publisher = {ACM},
title = {{Partitioning Streaming Parallelism for Multi-cores: A Machine Learning Based Approach}},
year = {2010}
}
@article{Hu2015,
annote = {Not published yet.},
author = {Hu, Y. and Koppelman, D. M. and Brandt, S. R.},
doi = {10.1109/ICPP.2015.39},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hu, Koppelman, Brandt - Model-Driven Auto-Tuning of Stencil Computations on GPUs.pdf:pdf},
keywords = {GPGPU,auto-tuning,machine learning,stencil},
title = {{Model-Driven Auto-Tuning of Stencil Computations on GPUs}},
year = {2015}
}
@inproceedings{Kaleem2014,
annote = {NULL},
author = {Kaleem, Rashid and Barik, Rajkishore and Shpeisman, Tatiana and Lewis, Brian T. and Hu, Chunling and Pingali, Keshav},
booktitle = {PACT},
doi = {10.1145/2628071.2628088},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Kaleem et al. - Adaptive heterogeneous scheduling for integrated GPUs.pdf:pdf},
isbn = {9781450328098},
issn = {1089795X},
keywords = {heterogeneous computing,integrated gpus,irregular applications,load balancing,scheduling},
publisher = {ACM},
title = {{Adaptive heterogeneous scheduling for integrated GPUs}},
url = {http://dl.acm.org/citation.cfm?id=2628071.2628088},
year = {2014}
}
@inproceedings{Dabbish,
abstract = {Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation.},
annote = {NULL},
author = {Dabbish, Laura and Stuart, Colleen and Tsay, Jason and Herbsleb, Jim},
booktitle = {CSCW},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Dabbish et al. - Social Coding in GitHub Transparency and Collaboration in an Open Software Repository.pdf:pdf},
keywords = {Author Keywords Transparency,Design,HCI),awareness,collaboration,coordination,open source software development},
publisher = {ACM},
title = {{Social Coding in GitHub: Transparency and Collaboration in an Open Software Repository}},
year = {2012}
}
@inproceedings{Muralidharan,
annote = {NULL},
author = {Muralidharan, Saurav and Garland, Michael and Catanzaro, Bryan and Sidelnik, Albert and Hall, Mary},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Muralidharan et al. - A Collection-Oriented Programming Model for Performance Portability.pdf:pdf},
isbn = {9781450332057},
keywords = {nested-data-parallelism,performance-portability},
title = {{A Collection-Oriented Programming Model for Performance Portability}},
year = {2015}
}
@article{VanDerWalt2011a,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1102.1523},
author = {{Van Der Walt}, St??fan and Colbert, S. Chris and Varoquaux, Ga??l},
doi = {10.1109/MCSE.2011.37},
eprint = {1102.1523},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Van Der Walt, Colbert, Varoquaux - The NumPy array A structure for efficient numerical computation.pdf:pdf},
isbn = {1521-9615 VO - 13},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {NumPy,Python,numerical computations,programming libraries,scientific programming},
number = {2},
pmid = {1000224770},
title = {{The NumPy array: A structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@article{Holler2012,
abstract = {Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.},
author = {Holler, C. and Herzig, K. and Zeller, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Holler, Herzig, Zeller - Fuzzing with Code Fragments.pdf:pdf},
journal = {Usenix},
keywords = {fuzz testing,grammar,security,security testing},
title = {{Fuzzing with Code Fragments}},
year = {2012}
}
@unpublished{Reis2013,
annote = {NULL},
author = {Reis, Gabriel Dos},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Reis - Object Lifetime, Low-level Programming, and memcpy.pdf:pdf},
title = {{Object Lifetime, Low-level Programming, and memcpy}},
year = {2013}
}
@inproceedings{Srivastava2015,
abstract = {We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations ("percepts") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.},
annote = {NULL},
author = {Srivastava, N. and Mansimov, E. and Salakhutdinov, R.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Srivastava, Mansimov, Salakhutdinov - Unsupervised Learning of Video Representations using LSTMs.pdf:pdf},
title = {{Unsupervised Learning of Video Representations using LSTMs}},
year = {2015}
}
@inproceedings{Xie2013,
annote = {NULL},
author = {Xie, C. and Chen, R. and Guan, H. and Zang, B. and Chen, H.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Xie et al. - SYNC or ASYNC Time to Fuse for Distributed Graph-parallel Computation Institute of Parallel and Distributed Systems.pdf:pdf},
isbn = {9781450332057},
keywords = {computa-,distributed graph-parallel computation},
title = {{SYNC or ASYNC : Time to Fuse for Distributed Graph-parallel Computation Institute of Parallel and Distributed Systems SYNC or ASYNC : Time to Fuse for Distributed Graph-parallel Computation}},
year = {2015}
}
@misc{Edinburgh2011,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - COPT 2014 exam.pdf:pdf},
number = {May},
title = {{COPT 2014 exam}},
year = {2014}
}
@article{Abramson1989,
abstract = {Configuring N mutually nonattacking queens on an B-by-N chessboard is a classical problem that was first posed over a century ago. Over the past few decades, this problem has become important to computer scientists by serving as the standard example of a globally constrained problem which is solvable using backtracking search methods. A related problem, placing the N queens on a toroidal board, has been discussed in detail by Poyla dn Chandra. Their work focused on characterizing the solvable cases and finging solutions which arrange the queens in a regular pattern. This paper describes a new divide-and-conquer algorithm that solves both problems and investigates the relationship between theem. The connectoin between the soliutions of the two problems illustrates an important, but frequently overlooked, method of algorithm design: detailed combinatorial analysis of an overconstrained variation can reveal solutions to the corresponding original problem. The solution is an example of solving a globally constrained problem using the divide-and-conquer technique, rather than the usual backtracking algorithm. The former is much faster in both sequential and parallel environments.},
annote = {NULL},
author = {Abramson, B. and Yung, M.},
doi = {10.1016/0743-7315(89)90011-7},
file = {:Users/cec/Google Drive/Mendeley Library/1989 - Abramson, Yung - Divide and conquer under global constraints A solution to the N-queens problem.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
month = {jun},
number = {3},
title = {{Divide and conquer under global constraints: A solution to the N-queens problem}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0743731589900117},
volume = {6},
year = {1989}
}
@inproceedings{Collobert2011,
abstract = {Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be in- terfaced to third-party software thanks to Lua's light interface.},
annote = {NULL},
author = {Collobert, R. and Kavukcuoglu, K. and Farabet, C.},
booktitle = {BigLearn},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Collobert, Kavukcuoglu, Farabet - Torch7 A Matlab-like Environment for Machine Learning.pdf:pdf},
title = {{Torch7: A Matlab-like Environment for Machine Learning}},
year = {2011}
}
@inproceedings{Berkeley2009,
abstract = {Understanding the most efficient design and utilization of emerging multicore systems is one of the most challenging questions faced by the mainstream and scientific computing industries in several decades. Our work explores multicore stencil (nearest-neighbor) computations -- a class of algorithms at the heart of many structured grid codes, including PDE solvers. We develop a number of effective optimization strategies, and build an auto-tuning environment that searches over our optimizations and their parameters to minimize runtime, while maximizing performance portability. To evaluate the effectiveness of these strategies we explore the broadest set of multicore architectures in the current HPC literature, including the Intel Clovertown, AMD Barcelona, Sun Victoria Falls, IBM QS22 PowerXCell 8i, and NVIDIA GTX280. Overall, our auto- tuning optimization methodology results in the fastest multicore stencil performance to date. Finally, we present several key insights into the architectural trade-offs of emerging multicore designs and their implications on scientific algorithm development.},
annote = {Datta presents an application-specific auto-tuner for stencils. 3D stencil problems are decomposed into core blocks, sufficiently small to avoid last level cache capacity misses. These are then decomposed to thread blocks, designed to exploit common locality threads may have within a shared cache or loca lmemory. Thread blocks are then decomposed to register blocks, designed to take advantage of data level parallelism provided by available registers. Data allocation is optimised on NUMA systems. The performance evaluation considers speedups of various optimisations with and without consideration for host/device transfer overhead. Cited by 445.},
author = {Berkeley, L. and Datta, K. and Murphy, M. and Volkov, V. and Williams, S. and Carter, J.},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Berkeley et al. - Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures.pdf:pdf},
title = {{Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures}},
year = {2008}
}
@phdthesis{Dubach2009a,
abstract = {Designing newmicroprocessors is a time consuming task. Architects rely on slowsimulators to evaluate performance and a significant proportion of the design space has to be explored before an implementation is chosen. This process becomes more time consuming when compiler optimisations are also considered. Once the architecture is selected, a new compiler must be developed and tuned. What is needed are techniques that can speedup this whole process and develop a new optimising compiler automatically. This thesis proposes the use ofmachine-learning techniques to address architecture/compiler co-design. First, two performance models are developed and are used to efficiently search the design space of amicroarchitecture. Thesemodels accurately predict performance metrics such as cycles or energy, or a tradeoff of the two. The first model uses just 32 simulations to model the entire design space of new applications, an order of magnitude fewer than state-of-the-art techniques. The second model addresses offline training costs and predicts the average be- haviour of a complete benchmark suite. Compared to state-of-the-art, it needs five times fewer training simulations when applied to the SPEC CPU 2000 andMiBench benchmark suites. Next, the impact of compiler optimisations on the design process is considered. This has the potential to change the shape of the design space and improve performance significantly. A new model is proposed that predicts the performance obtainable by an optimising compiler for any design point, without having to build the compiler. Compared to the state-of-the-art, this model achieves a significantly lower error rate. Finally, a new machine-learning optimising compiler is presented that predicts the best compiler optimisation setting for any new program on any new microarchitecture. It achieves an average speedup of 1.14x over the default best gcc optimisation level. This represents 61{\%} of the maximum speedup available, using just one profile run of the application.},
annote = {NULL},
author = {Dubach, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Dubach - Using Machine-Learning to Efficiently Explore the Architecture Compiler Co-Design Space.pdf:pdf},
keywords = {Thesis},
mendeley-tags = {Thesis},
school = {Univeristy of Edinburgh},
title = {{Using Machine-Learning to Efficiently Explore the Architecture / Compiler Co-Design Space}},
year = {2009}
}
@inproceedings{Yang2011c,
abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
annote = {NULL},
author = {Yang, X. and Chen, Y. and Eide, E. and Regehr, J.},
booktitle = {PLDI},
doi = {10.1145/2345156.1993532},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Yang et al. - Finding and Understanding Bugs in C Compilers.pdf:pdf},
issn = {03621340},
keywords = {automated testi,compiler defect,compiler testing,random program generation,random testing},
title = {{Finding and Understanding Bugs in C Compilers}},
year = {2011}
}
@inproceedings{Ionescu2016,
annote = {NULL},
author = {Ionescu, Alex},
booktitle = {Blackhat},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ionescu - The Linux Kernel Hidden Inside Windows 10.pdf:pdf},
title = {{The Linux Kernel Hidden Inside Windows 10}},
year = {2016}
}
@inproceedings{Aldinucci2013,
abstract = {FastFlow is a structured parallel programming framework targeting shared memory multi-core architectures. In this paper we intro- duce a FastFlow extension aimed at supporting also a network of multi- core workstations. The extension supports the execution of FastFlow programs by coordinating–in a structured way–the fine grain parallel ac- tivities running on a single workstation. We discuss the design and the implementation of this extension presenting preliminary experimental results validating it on state-of-the-art networked multi-core nodes.},
annote = {This paper describes an attempt to extend the multi-core FastFlow skeleton library to distributed environments.},
author = {Aldinucci, M. and Campa, S. and Danelutto, M. and Kilpatrick, P. and Torquati, M.},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Aldinucci et al. - Targeting Distributed Systems in FastFlow.pdf:pdf},
keywords = {fine grain,multi-core,structured parallel programming},
publisher = {Springer},
title = {{Targeting Distributed Systems in FastFlow}},
year = {2013}
}
@techreport{Striegnitz2000,
abstract = {Many authors have proposed the use of algorithmic skeletons as a high level, machine independent means of developing parallel appli- cations. Since now their implementation and use was restricted to either functional-, or some sophisticated imperative languages. In this paper we will discuss how far C++ supports the integration of algorithmic skele- tons and identify currying as the only missing feature.We will show how this gap can be closed, by integrating currying into C++ through code that is compliant with the ANSI/ISO standard, thus, by using the lan- guage itself instead of extending it. We will prove that our method does not yield any runtime penalties if a highly optimizing C++ compiler is used and, therefore, is competitive with existing sophisticated languages.},
annote = {Striegnitz implements currying and functional functors in C++ using templates. This is to support partial application, which is the one of the language features identified as being essential for implementing algorithmic skeletons:




- Polymorphic types
- Higher order functions
- Partial application




The approach to implementing currying seems sound, and the experimental results are promising, although lacking in description. The paper contains no related work or literature review, so it is difficult to place the contribution in context.},
author = {Striegnitz, J},
file = {:Users/cec/Google Drive/Mendeley Library/2000 - Striegnitz - Making C Ready for Algorithmic Skeletons.pdf:pdf},
number = {September},
title = {{Making C ++ Ready for Algorithmic Skeletons}},
url = {http://www.fz-juelich.de/zam/FACT Abstract.},
volume = {2000},
year = {2000}
}
@inproceedings{Thies2002,
abstract = {We characterize high-performance streaming applications as a new and distinct domain of programs that is becoming increasingly im- portant. The StreamIt language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreamIt compiler aims to im- prove the performance of streaming applications via stream-specific anal- yses and optimizations. In this paper, we motivate, describe and justify the language features of StreamIt, which include: a structured model of streams, a messaging system for control, a re-initialization mechanism, and a natural textual syntax.},
annote = {NULL},
author = {Thies, W. and Karczmarek, M. and Amarasinghe, S.},
booktitle = {CC},
doi = {10.1007/3-540-45937-5},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Thies, Karczmarek, Amarasinghe - StreamIt A language for streaming applications.pdf:pdf},
isbn = {3540433694},
issn = {0302-9743},
title = {{StreamIt: A language for streaming applications}},
url = {http://www.springerlink.com/index/LC5B77HWR8J2UBHK.pdf{\%}5Cnhttp://groups.csail.mit.edu/commit/papers/02/streamit-cc.pdf},
volume = {LNCS 2304},
year = {2002}
}
@book{Beej2010,
annote = {NULL},
author = {Hall, B.},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Hall - Beej's Guide to Unix IPC.pdf:pdf},
title = {{Beej's Guide to Unix IPC}},
year = {2010}
}
@inproceedings{Kaleem,
annote = {NULL},
author = {Kaleem, Rashid},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kaleem - Stochastic Gradient Descent on GPUs Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334075},
keywords = {edge-coloring,gpgpu,stochastic gradient descent},
title = {{Stochastic Gradient Descent on GPUs Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Bash2015,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dijkstra, E. W.},
booktitle = {ACM Turing Lecture},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/1972 - Dijkstra - The Humble Programmer.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{The Humble Programmer}},
year = {1972}
}
@inproceedings{Fauzia2015,
abstract = {Effective parallel programming for GPUs requires careful attention to several factors, including ensuring coalesced access of data from global memory. There is a need for tools that can provide feedback to users about statements in a GPU kernel where non-coalesced data access occurs, and assistance in fixing the problem. In this paper, we address both these needs. We develop a two-stage framework where dynamic analysis is first used to detect and characterize uncoalesced accesses in arbitrary PTX programs. Transformations to optimize global memory access by introducing coalesced access are then implemented, using feedback from the dynamic analysis or using a model-driven approach. Experimental results demonstrate the use of the tools on a number of benchmarks from the Rodinia and Polybench suites.},
annote = {NULL},
author = {Fauzia, Naznin and Pouchet, Louis-Noel and Sadayappan, P.},
booktitle = {CGO},
doi = {10.1109/CGO.2015.7054183},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Fauzia, Pouchet, Sadayappan - Characterizing and enhancing global memory data coalescing on GPUs.pdf:pdf},
isbn = {978-1-4799-8161-8},
keywords = {GPU,Geometry,Graphics processing units,Instruction sets,Instruments,Kernel,Optimization,PTX,Performance analysis,Polybench suites,Rodinia suites,arbitrary PTX programs,coalesced access,coalescing,dynamic analysis,global memory data coalescing,graphics processing units,locality,model-driven approach,parallel programming,polyhedral compilation,program transformation,storage management,system monitoring,uncoalesced access characterization,uncoalesced access detection},
publisher = {IEEE},
title = {{Characterizing and enhancing global memory data coalescing on GPUs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7054183},
year = {2015}
}
@article{Liu2016,
abstract = {Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.},
author = {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Liu, Qiu, Huang - Recurrent Neural Network for Text Classification with Multi-Task Learning.pdf:pdf},
journal = {Proceedings of the 25th International Joint Conference on Artificial Intelligence IJCAI-16},
keywords = {Natural Language Processing,TOREAD},
mendeley-tags = {TOREAD},
pages = {to appear},
title = {{Recurrent Neural Network for Text Classification with Multi-Task Learning}},
year = {2016}
}
@article{Jaderberg2017,
abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present $\backslash$emph{\{}Population Based Training (PBT){\}}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
archivePrefix = {arXiv},
arxivId = {1711.09846},
author = {Jaderberg, M. and Dalibard, V. and Osindero, S. and Czarnecki, W. M. and Donahue, J. and Razavi, A. and Vinyals, O. and Green, T. and Dunning, I. and Simonyan, K. and Fernando, C. and Kavukcuoglu, K.},
doi = {10.1016/j.ins.2014.10.042},
eprint = {1711.09846},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Jaderberg et al. - Population Based Training of Neural Networks.pdf:pdf},
isbn = {1089-778X VO - PP},
issn = {00200255},
journal = {arXiv:1711.09846},
pmid = {24860047},
title = {{Population Based Training of Neural Networks}},
url = {http://arxiv.org/abs/1711.09846},
year = {2017}
}
@inproceedings{Moss1997,
abstract = {Program execution speed on modern computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the proces- sor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, ob- taining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empirical results show that just a few features are ad- equate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly opti- mally with respect to the features used.},
annote = {NULL},
author = {Moss, Eliot and Utgoff, Paul and Cavazos, John and Brodley, Carla and Scheeff, David},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Moss et al. - Learning to Schedule Straight-Line Code.pdf:pdf},
title = {{Learning to Schedule Straight-Line Code}},
year = {1997}
}
@article{Du2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.02054v1},
author = {Du, S. S. and Xiyu, Z. and Poczos, B. and Singh, A.},
eprint = {arXiv:1810.02054v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Du et al. - Gradient Descent Provably Optimizes Over-parameterized Neural Networks.pdf:pdf},
journal = {arXiv:1810.02054},
title = {{Gradient Descent Provably Optimizes Over-parameterized Neural Networks}},
year = {2018}
}
@article{Blume1994,
annote = {Cited by 106.},
author = {Blume, William and Eigenmann, Rudolf and Hoeflinger, Jay and Padua, David and Petersen, Paul},
file = {:Users/cec/Google Drive/Mendeley Library/1994 - Blume et al. - Automatic Detection of Parallelism.pdf:pdf},
journal = {IEEE Concurrency},
number = {3},
publisher = {IEEE Computer Society},
title = {{Automatic Detection of Parallelism}},
volume = {2},
year = {1994}
}
@article{Fursin2014a,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1406.4020},
author = {Fursin, G. and Dubach, C.},
doi = {10.1145/2618137.2618142},
eprint = {1406.4020},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Fursin, Dubach - Community-driven reviewing and validation of publications.pdf:pdf},
isbn = {9781450329514},
journal = {arXiv:1406.4020},
title = {{Community-driven reviewing and validation of publications}},
year = {2014}
}
@article{Rossum2012b,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Descriptor HowTo Guide.pdf:pdf},
title = {{Descriptor HowTo Guide}},
year = {2016}
}
@misc{UniversityofEdinburgh2014i,
annote = {NULL},
author = {{University of Edinburgh}},
doi = {10.1145/365719.366426},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 4. Compiling techniques.pdf:pdf},
issn = {00010782},
title = {{4. Compiling techniques}},
volume = {9},
year = {2015}
}
@inproceedings{Luk2009,
abstract = {Heterogeneous multiprocessors are increasingly important in the multi-core era due to their potential for high performance and en- ergy efficiency. In order for software to fully realize this potential, the step that maps computations to processing elements must be as automated as possible. However, the state-of-the-art approach is to rely on the programmer to specify this mapping manually and statically. This approach is not only labor intensive but also not adaptable to changes in runtime environments like problem sizes and hardware/software configurations. In this study, we propose adaptive mapping, a fully automatic technique to map computa- tions to processing elements on a CPU+GPU machine. We have implemented it in our experimental heterogeneous programming system called Qilin. Our results show that, by judiciously distribut- ing works over the CPU and GPU, automatic adaptive mapping achieves a 25{\%} reduction in execution time and a 20{\%} reduction in energy consumption than static mappings on average for a set of important computation benchmarks. We also demonstrate that our technique is able to adapt to changes in the input problem size and system configuration.},
annote = {This paper proposes an automatic mapping of CPUs and GPUs using adaptive mapping. The process involves dividing input problems into smaller subproblems, and distributing them between the CPU and GPU. The ratio of problems distributed to each device is determined by estimating performance by comparing against a database of execution times.




The paper makes the unjustified assumption that bus bandwidth is not a mjaor factor when comparing CPU and GPU exection times.},
author = {Luk, Chi-Keung and Hong, Sunpyo and Kim, Hyesoon},
booktitle = {MICRO},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Luk, Hong, Kim - Qilin Exploiting Parallelism on Heterogeneous Multiprocessors with Adaptive Mapping Categories and Subject Descr.pdf:pdf},
isbn = {9781605587981},
keywords = {GPU,Multicore,adaptive,dynamic compilation,heterogeneous,mapping},
publisher = {ACM},
title = {{Qilin: Exploiting Parallelism on Heterogeneous Multiprocessors with Adaptive Mapping Categories and Subject Descriptors}},
year = {2009}
}
@article{Iqbal2017,
abstract = {We address the problem of designing artificial agents capable of reproducing human behavior in a competitive game involving dynamic control. Given data consisting of multiple realizations of inputs generated by pairs of interacting players, we model each agent's actions as governed by a time-varying latent goal state coupled to a control model. These goals, in turn, are described as stochastic processes evolving according to player-specific value functions depending on the current state of the game. We model these value functions using generative adversarial networks (GANs) and show that our GAN-based approach succeeds in producing sample gameplay that captures the rich dynamics of human agents. The latent goal dynamics inferred and generated by our model has applications to fields like neuroscience and animal behavior, where the underlying value functions themselves are of theoretical interest.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1702.07319},
author = {Iqbal, S. and Pearson, J.},
eprint = {1702.07319},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Iqbal, Pearson - Learning to Draw Dynamic Agent Goals with Generative Adversarial Networks.pdf:pdf},
journal = {arXiv:1702.07319},
title = {{Learning to Draw Dynamic Agent Goals with Generative Adversarial Networks}},
year = {2017}
}
@phdthesis{Trigkas2014,
annote = {SYCL is a Khronos standardised C++ framework for OpenCL programming. Cited by 0.},
author = {Trigkas, Angelos},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Trigkas - Investigation of the OpenCL SYCL Programming Model.pdf:pdf},
school = {University of Edinburgh},
title = {{Investigation of the OpenCL SYCL Programming Model}},
year = {2014}
}
@article{Estebanez2014,
annote = {NULL},
author = {Estebanez, Alvaro and Llanos, Diego R and Gonzalez-Escribano, Arturo},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Estebanez, Llanos, Gonzalez-Escribano - New Data Structures to Handle Speculative Parallelization at Runtime.pdf:pdf},
journal = {IJPP},
keywords = {memory,speculative parallelism,thread-level speculation},
title = {{New Data Structures to Handle Speculative Parallelization at Runtime}},
year = {2014}
}
@inproceedings{Bird2009,
abstract = {We are now witnessing the rapid growth of decentralized source code management (DSCM) systems, in which every developer has her own repository. DSCMs facilitate a style of collaboration in which work output can flow sideways (and privately) between collaborators, rather than always up and down (and publicly) via a central repository. Decentralization comes with both the promise of new data and the peril of its misinterpretation. We focus on git, a very popular DSCM used in high-profile projects. Decentralization, and other features of git, such as automatically recorded contributor attribution, lead to richer content histories, giving rise to new questions such as ldquoHow do contributions flow between developers to the official project repository?rdquo However, there are pitfalls. Commits may be reordered, deleted, or edited as they move between repositories. The semantics of terms common to SCMs and DSCMs sometimes differ markedly, potentially creating confusion. For example, a commit is immediately visible to all developers in centralized SCMs, but not in DSCMs. Our goal is to help researchers interested in DSCMs avoid these and other perils when mining and analyzing git data.},
annote = {NULL},
author = {Kalliamvakou, E. and Singer, L. and Gousios, G. and German, D. M. and Blincoe, K. and Damian, D.},
booktitle = {MSR},
doi = {10.1109/MSR.2009.5069475},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Kalliamvakou et al. - The Promises and Perils of Mining GitHub.pdf:pdf},
isbn = {9781424434930},
issn = {15737616},
keywords = {code reviews,git,github,mining software repositories},
title = {{The Promises and Perils of Mining GitHub}},
year = {2009}
}
@inproceedings{Tobias,
annote = {NULL},
author = {Tobias, M},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Tobias - Patty A Pattern-based Parallelization Tool for the Multicore Age.pdf:pdf},
isbn = {9781450334044},
title = {{Patty : A Pattern-based Parallelization Tool for the Multicore Age}},
year = {2015}
}
@article{Radford2016,
abstract = {We explore the properties of byte-level recur-rent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised man-ner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be pos-itive or negative generates samples with the cor-responding positive or negative sentiment.},
annote = {One of the best websites for a paper I've seen: https://blog.openai.com/unsupervised-sentiment-neuron/},
author = {Radford, A. and Jozefowicz, R. and Sutskever, I.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Radford, Jozefowicz, Sutskever - Learning to Generate Reviews and Discovering Sentiment.pdf:pdf},
journal = {arXiv:1704.01444},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Learning to Generate Reviews and Discovering Sentiment}},
year = {2017}
}
@article{Wu2019,
author = {Wu, Z. and Pan, S. and Chen, F. and Long, G. and Zhang, C. and Yu, P. S.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Wu et al. - A Comprehensive Survey on Graph Neural Networks.pdf:pdf},
journal = {arXiv:1901.00596},
title = {{A Comprehensive Survey on Graph Neural Networks}},
year = {2019}
}
@inproceedings{Khairy,
annote = {NULL},
author = {Khairy, Mahmoud},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Khairy - Efficient Utilization of GPGPU Cache Hierarchy Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334075},
keywords = {avoiding,cache bypassing,cache management,conflict-,gpgpu,warp throttling},
title = {{Efficient Utilization of GPGPU Cache Hierarchy Categories and Subject Descriptors}},
year = {2015}
}
@inproceedings{Steuwer2016,
annote = {NULL},
author = {Steuwer, M. and Remmelg, T. and Dubach, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Steuwer, Remmelg, Dubach - Matrix Multiplication Beyond Auto-Tuning Rewrite-based GPU Code Generation.pdf:pdf},
isbn = {9781450344821},
title = {{Matrix Multiplication Beyond Auto-Tuning: Rewrite-based GPU Code Generation}},
year = {2016}
}
@article{Id2014,
annote = {NULL},
author = {Peng, X. B. and Berseth, G. and Panne, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Peng, Berseth, Panne - Terrain-Adaptive Locomotion Skills using Deep Reinforcement Learning.pdf:pdf},
isbn = {9781450342797},
keywords = {animation,computing methodologies,concepts,physical,physics-based characters,reinforcement learning},
number = {4},
title = {{Terrain-Adaptive Locomotion Skills using Deep Reinforcement Learning}},
volume = {35},
year = {2014}
}
@article{Tillmann2014,
abstract = {Autotuning is an established technique for adjusting perfor- mance-critical parameters of applications to their specific run-time envi- ronment. In this paper, we investigate the potential of online autotuning for general purpose computation on GPUs. Our application-independent autotuner AtuneRT optimizes GPU-specific parameters such as block size and loop-unrolling degree. We also discuss the peculiarities of auto- tuning on GPUs. We demonstrate tuning potential using CUDA and by instrumenting the parallel algorithms library Thrust. We evaluate our online autotuning approach with various GPUs and sample applications.},
annote = {Cited by 8.},
author = {Tillmann, M. and Karcher, T. and Dachsbacher, C. and Tichy, W. F.},
doi = {10.3233/978-1-61499-381-0-626},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Tillmann et al. - Application-independent autotuning for GPUs.pdf:pdf},
isbn = {9781614993803},
issn = {09275452},
journal = {Advances in Parallel Computing},
keywords = {Autotuning,CUDA,GPGPU,GPU,Thrust},
number = {1},
title = {{Application-independent autotuning for GPUs}},
volume = {25},
year = {2014}
}
@misc{Driscoll2014,
abstract = {Primary research involves collecting data about a given subject directly from the real world. This section includes information on what primary research is, how to get started, ethics involved with primary research and different types of research you can do. It includes details about interviews, surveys, observations, and analysis.},
annote = {Primary research is any type of research that you go out and collect yourself. This is opposed to secondary sources, such as journals and books.


Not especially revelant to my studies, due to the heavy focus on obtaining data through interviews, surveys, and the like.},
author = {Driscoll, D. L. and Brizee, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Driscoll, Brizee - What is Primary Research and How do I get Started.pdf:pdf},
title = {{What is Primary Research and How do I get Started?}},
year = {2014}
}
@inproceedings{Allamanis2014a,
abstract = {Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project{\&}{\#}8217;s coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94 {\%} accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted.},
annote = {NULL},
author = {Allamanis, M. and Barr, E. T. and Bird, C. and Sutton, C.},
booktitle = {FSE},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Allamanis et al. - Learning Natural Coding Conventions.pdf:pdf},
keywords = {Coding conventions,naturalness of software},
publisher = {ACM},
title = {{Learning Natural Coding Conventions}},
year = {2014}
}
@inproceedings{Wentzlaff2014,
abstract = {Cloud computers and multicore processors are two emerging classes of computational hardware that have the potential to provide un- precedented compute capacity to the average user. In order for the user to effectively harness all of this computational power, operat- ing systems (OSes) for these new hardware platforms are needed. Existing multicore operating systems do not scale to large num- bers of cores, and do not support clouds. Consequently, current day cloud systems push much complexity onto the user, requiring the user to manage individual Virtual Machines (VMs) and deal with many system-level concerns. In this work we describe the mechanisms and implementation of a factored operating system named fos. fos is a single system image operating system across both multicore and Infrastructure as a Service (IaaS) cloud sys- tems. fos tackles OS scalability challenges by factoring the OS into its component system services. Each system service is further factored into a collection of Internet-inspired servers which com- municate via messaging. Although designed in a manner similar to distributed Internet services, OSservices instead provide traditional kernel services such as file systems, scheduling, memory manage- ment, and access to hardware. fos also implements new classes of OS services like fault tolerance and demand elasticity. In this work, we describe our working fos implementation, and provide early performance measurements of fos for both intra-machine and inter-machine operations.},
annote = {NULL},
author = {Wentzlaff, D. and {Gruenwald III}, C. and Beckmann, N. and Modzelewski, K. and Belay, A. and Youseff, L. and Miller, J. and Agarwal, A.},
booktitle = {SoCC},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wentzlaff et al. - An Operating System for Multicore and Clouds Mechanisms and Implementation.pdf:pdf},
isbn = {9781450300360},
keywords = {Cloud Computing,Multicores,Scalable Operating Systems,Single System Image},
publisher = {ACM},
title = {{An Operating System for Multicore and Clouds: Mechanisms and Implementation}},
year = {2014}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Bengio - Practical recommendations for gradient-based training of deep architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
volume = {7700 LECTU},
year = {2012}
}
@article{Winn2011a,
annote = {NULL},
author = {Winn, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Winn - Probabilistic Programming.pdf:pdf},
isbn = {9781450328654},
number = {September},
title = {{Probabilistic Programming}},
volume = {10},
year = {2011}
}
@inproceedings{David2016a,
address = {Santa Barbara, CA},
annote = {NULL},
author = {David, Yaniv and Yahav, Eran},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - David, Yahav - Statistical Similarity of Binaries.pdf:pdf},
isbn = {9781450342612},
keywords = {partial equivalence,static binary analysis,statistical similarity,verification-aided similarity},
number = {212},
title = {{Statistical Similarity of Binaries}},
volume = {1},
year = {2016}
}
@misc{Arapinis2014f,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 08. Sequences, sums, cardinality.pdf:pdf},
title = {{08. Sequences, sums, cardinality}},
year = {2015}
}
@misc{Leyton2010,
abstract = {This paper argues that algorithmic skeletons are a suitable programming model for multi-core architectures. The high-level abstractions offered by algorithmic skeletons provide a simple way for non-parallel programmers to address parallel programming. Previous algorithmic skeleton frameworks and libraries have addressed distributed computing environments such as Clusters and Grids. This paper proposes a parallel skeleton library, Skandium; and concludes, after an experi- mental evaluation, that algorithmic skeletons are an effective methodology to program multi-core architectures.},
annote = {Skandium is an object orientated skeleton library for multi-core architectures, written in Java. Skeletons and muscle functions are modelled using classes and inheritance. The coordination logic of skeletons is implicit, but the parallelism is not. That is, programmers are required to adequately guard access to shared memory within mutable objects, which means that the efficiency of the skeletons can be severely limited if the user has a poor parallelism design.},
author = {Leyton, Mario and Piquer, Jose M JM},
booktitle = {PDP},
doi = {10.1109/PDP.2010.26},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Leyton, Piquer - Skandium Multi-core Programming with Algorithmic Skeletons.pdf:pdf},
isbn = {978-1-4244-5672-7},
keywords = {algorithmic skeletons,multi-core,parallel programming,patterns},
month = {feb},
publisher = {Ieee},
title = {{Skandium: Multi-core Programming with Algorithmic Skeletons}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5452456 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5452456},
year = {2010}
}
@article{Bernabe2013,
abstract = {The detection of (moving or static) targets in remotely sensed hyperspectral images often requires real-time responses for swift decisions that depend upon high computing performance of algorithm analysis. The automatic target detection and classification algorithm (ATDCA) has been widely used for this purpose. In this letter, we develop several optimizations for accelerating the computational performance of ATDCA. The first one focuses on the use of the Gram–Schmidt orthogonalization method instead of the orthogonal projection process adopted by the classic algorithm. The second one is focused on the development of a new implementation of the algorithm on commodity graphics processing units (GPUs). The proposed GPU implementation properly exploits the GPU architecture at low level, including shared memory, and provides coalesced accesses to memory that lead to very significant speedup factors, thus taking full advantage of the computational power of GPUs. The GPU implementation is specifically tailored to hyperspectral imagery and the special characteristics of this kind of data, achieving real-time performance of ATDCA for the first time in the literature. The proposed optimizations are evaluated not only in terms of target detection accuracy but also in terms of computational performance using two different GPU architectures by NVIDIA: Tesla C1060 and GeForce GTX 580, taking advantage of the performance of operations in single-precision floating point. Experiments are conducted using hyperspectral data sets collected by three different hyperspectral imaging instruments. These results reveal considerable acceleration factors while retaining the same target detection accuracy for the algorithm.},
annote = {NULL},
author = {Bernabe, S. and Lopez, S. and Plaza, A. and Sarmiento, R.},
doi = {10.1109/LGRS.2012.2198790},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Bernabe et al. - GPU Implementation of an Automatic Target Detection and Classification Algorithm for Hyperspectral Image Analysi.pdf:pdf},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Automatic target detection and classification algo,Gram-Schmidt (GS) orthogonalization,commodity graphics processing units (GPUs),hyperspectral imaging},
number = {2},
title = {{GPU Implementation of an Automatic Target Detection and Classification Algorithm for Hyperspectral Image Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6218752},
volume = {10},
year = {2013}
}
@misc{CanergieMellonUniversity2005b,
annote = {NULL},
author = {{Canergie Mellon University}},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Canergie Mellon University - 4. Factor Analysis.pdf:pdf},
number = {February},
title = {{4. Factor Analysis}},
year = {2010}
}
@inproceedings{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
annote = {NULL},
author = {Yosinski, J. and Clune, J. and Bengio, Y. and Lipson, H.},
booktitle = {NIPS},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Yosinski et al. - How Transferable are Features in Deep Neural Networks.pdf:pdf},
issn = {10495258},
title = {{How Transferable are Features in Deep Neural Networks?}},
year = {2014}
}
@misc{Purugganan,
abstract = {Reading a scientific article is a complex task. The worst way to approach this task is to treat it like the reading of a textbook—reading from title to literature cited, digesting every word along the way without any reflection or criticism. Rather, you should begin by skimming the article to identify its structure and features. As you read, look for the author's main points. Generate questions before, during, and after reading. Draw inferences based on your own experiences and knowledge. And to really improve understanding and recall, take notes as you read. This handout discusses each of these strategies in more detail.},
annote = {NULL},
author = {Purugganan, Mary and Hewitt, Jan},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Purugganan, Hewitt - How to Read a Scientific Article.pdf:pdf},
title = {{How to Read a Scientific Article}}
}
@inproceedings{Raychev2015,
abstract = {We present a new approach for predicting program properties from massive codebases (aka "Big Code"). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs. The key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic graphical models such as conditional random fields (CRFs) in order to perform joint prediction of program properties. As an example of our approach, we built a scalable prediction engine called JSNice for solving two kinds of problems in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNice predicts correct names for 63{\%} of name identifiers and its type annotation predictions are correct in 81{\%} of the cases. In the first week since its release, JSNice was used by more than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community. By formulating the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context, our work opens up new possibilities for attacking a wide range of difficult problems in the context of "Big Code" including invariant generation, decompilation, synthesis and others.},
annote = {NULL},
author = {Raychev, V. and Vechev, M. and Krause, A.},
booktitle = {POPL},
doi = {10.1145/2676726.2677009},
file = {:Users/cec/Google Drive/Mendeley Library//2015 - Raychev, Vechev, Krause - Predicting Program Properties from Big Code.pdf:pdf},
isbn = {9781450333009},
issn = {15232867},
keywords = {big code,closure compiler,conditional random fields,javascript,names,program properties,structured prediction,types},
title = {{Predicting Program Properties from "Big Code"}},
year = {2015}
}
@inproceedings{Barik2017,
author = {Barik, T. and Smith, J. and Lubick, K. and Holmes, E. and Feng, J. and Murphy-Hill, E. and Parnin, C.},
booktitle = {ICSE},
doi = {10.1109/ICSE.2017.59},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Barik et al. - Do Developers Read Compiler Error Messages.pdf:pdf},
keywords = {-compiler errors,eye tracking,ficulty of reading error,integrated develop-,ment environments,messages is comparable to,programmer comprehension,reading,the,visual},
publisher = {IEEE},
title = {{Do Developers Read Compiler Error Messages?}},
year = {2017}
}
@article{Bahdanau2015,
abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1508.04395},
author = {Bahdanau, D. and Chorowski, J. and Serdyuk, D. and Brakel, P. and Bengio, U.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1508.04395},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Bahdanau et al. - End-to-End Attention-based Large Vocabulary Speech Recognition.pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
journal = {arXiv:1508.04395},
pmid = {15991970},
title = {{End-to-End Attention-based Large Vocabulary Speech Recognition}},
year = {2015}
}
@article{Segler2018,
abstract = {Chemical reaction databases that are automatically filled from the literature have made the planning of chemical syntheses, whereby target molecules are broken down into smaller and smaller building blocks, vastly easier over the past few decades. However, humans must still search these databases manually to find the best way to make a molecule. This involves many steps and choices. Some degree of automation has been achieved by encoding 'rules' of synthesis into computer programs, but this is time consuming owing to the numerous rules and subtleties involved. Here, Mark Waller and colleagues apply deep neural networks to plan chemical syntheses. They trained an algorithm on essentially every reaction published before 2015 so that it could learn the 'rules' itself and then predict synthetic routes to various small molecules not included in the training set. In blind testing, trained chemists could not distinguish between the solutions found by the algorithm and those taken from the literature.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Segler, M. H. S. and Preuss, M. and Waller, M. P.},
doi = {10.1038/nature25978},
eprint = {NIHMS150003},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Segler, Preuss, Waller - Planning Chemical Syntheses with Deep Neural Networks and Symbolic AI.pdf:pdf},
isbn = {0008-5472 (Print)$\backslash$r0008-5472 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7698},
pages = {604--610},
pmid = {29595767},
publisher = {Nature Publishing Group},
title = {{Planning Chemical Syntheses with Deep Neural Networks and Symbolic AI}},
volume = {555},
year = {2018}
}
@book{Stroustrup2013,
abstract = {C++ is a general purpose programming language designed to make programming more enjoyable for the serious programmer. Except for minor details, C++ is a superset of the C programming lan- guage. In addition to the facilities provided by C, C++ provides flexible and efficient facilities for defining new types. A programmer can partition an application into manageable pieces by defining new types that closely match the concepts of the application. This technique for program construc- tion is often called data abstraction. Objects of some user-defined types contain type information. Such objects can be used conveniently and safely in contexts in which their type cannot be deter- mined at compile time. Programs using objects of such types are often called object based. When used},
annote = {The definitive teatrise on C++ programming, updated for the C++11 standard.},
author = {Stroustrup, Bjarne},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Stroustrup - The C Programming Language.pdf:pdf},
isbn = {9780321563842},
title = {{The C ++ Programming Language}},
year = {2013}
}
@article{Knijnenburg2003,
abstract = {Loop tiling and unrolling are two important program$\backslash$ntransformations to exploit locality and expose instruction level$\backslash$nparallelism, respectively. In this paper, we address the problem of how$\backslash$nto select tile sizes and unroll factors simultaneously. We approach this$\backslash$nproblem in an architecturally adaptive manner by means of iterative$\backslash$ncompilation, where we generate many versions of a program and decide$\backslash$nupon the best by actually executing them and measuring their execution$\backslash$ntime. We evaluate several iterative strategies. We compare the levels of$\backslash$noptimization obtained by iterative compilation to several well-known$\backslash$nstatic techniques and show that we outperform each of them on a range of$\backslash$nbenchmarks across a variety of architectures. Finally, we show how to$\backslash$nquantitatively trade-off the number of profiles needed and the level of$\backslash$noptimization that can be reached},
annote = {NULL},
author = {Knijnenburg, P. M. W. and Kisuki, T. and O'Boyle, M.},
doi = {10.1023/A:1020989410030},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Knijnenburg, Kisuki, O'Boyle - Combined selection of tile sizes and unroll factors using iterative compilation.pdf:pdf},
isbn = {0-7695-0622-4},
issn = {09208542},
journal = {Journal of Supercomputing},
keywords = {Adaptive compilation,Instruction level parallelism,Locality optimization,Program optimization,Program transformation},
number = {1},
title = {{Combined selection of tile sizes and unroll factors using iterative compilation}},
volume = {24},
year = {2003}
}
@article{Chen2016a,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, X. and Duan, Y. and Houthooft, R. and Schulman, J. and Sutskever, I. and Abbeel, P.},
eprint = {1606.03657},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Chen et al. - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
journal = {arXiv:1606.03657},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@article{Erren2007,
annote = {NULL},
author = {Erren, Thomas C and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030102},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Erren, Bourne - Ten simple rules for a good poster presentation.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Computational Biology},
keywords = {Algorithms,Audiovisual Aids,Biomedical Research,Communication,Congresses as Topic,Exhibits as Topic,Information Dissemination,Information Dissemination: methods,Professional Competence},
month = {may},
number = {5},
pmid = {17530921},
title = {{Ten simple rules for a good poster presentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1876493{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2007}
}
@inproceedings{Castro2014,
annote = {NULL},
author = {Castro, P De Oliveira and Kashnikov, Y},
booktitle = {CGO},
doi = {10.1145/2581122.2544144},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Castro, Kashnikov - Fine-grained Benchmark Subsetting for System Selection.pdf:pdf},
isbn = {9781450326704},
number = {February},
publisher = {IEEE},
title = {{Fine-grained Benchmark Subsetting for System Selection}},
url = {http://dl.acm.org/citation.cfm?id=2544144},
year = {2014}
}
@inproceedings{Jan2016,
abstract = {XML is extensively used in web services for integration and data exchange. Its popularity and wide adoption make it an attractive target for attackers and a number of XML-based attack types have been reported recently. This raises the need for cost-effective, automated testing of web services to detect XML-related vulnerabilities, which is the focus of this paper. We discuss a taxonomy of the types of XML injection attacks and use it to derive four different ways to mutate XML messages, turning them into attacks (tests) automatically. Further, we consider domain constraints and attack grammars, and use a constraint solver to generate XML messages that are both malicious and valid, thus making it more difficult for any protection mechanism to recognise them. As a result, such messages have a better chance to detect vulnerabilities. Our evaluation on an industrial case study has shown that a large proportion (78.86{\{}{\{}{\}}{\{}{\%}{\}}{\{}{\}}{\}}) of the attacks generated using our approach could circumvent the first layer of security protection, an XML gateway (firewall), a result that is much better than what a state-of-the-art tool based on fuzz testing could achieve.},
author = {Jan, S. and Nguyen, C. D. and Briand, L. C.},
booktitle = {ISSTA},
doi = {10.1145/2931037.2931042},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Jan, Nguyen, Briand - Automated and effective testing of web services for XML injection attacks.pdf:pdf},
isbn = {9781450343909},
keywords = {XML injection,constraint solving,security testing},
title = {{Automated and effective testing of web services for XML injection attacks}},
url = {http://doi.acm.org/10.1145/2931037.2931042{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2931037.2931042},
year = {2016}
}
@inproceedings{Joshi2002,
abstract = {This paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very high-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler gener- ates adequately efficient code, and where current engineering practice is to use hand-coded machine language. The pa- per describes the design of the superoptimizer, and presents some encouraging preliminary results.},
annote = {Denali is a superoptimizer which generates near-optimal code for hot paths and small compute kernels. It first translates a low level machine code into GMA, then uses a matching algorithm to build an E-graph of all of a set of logical axioms which match parts of the graph, before using a SAT solver to disprove the conjecture that a program cannot be written in N instructions. Cited by 73.},
author = {Joshi, R. and Nelson, G. and Randall, K.},
booktitle = {PLDI},
doi = {10.1145/543552.512566},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Joshi, Nelson, Randall - Denali a goal-directed superoptimizer.pdf:pdf},
isbn = {1-58113-463-0},
issn = {03621340},
keywords = {optimizing compiler,superoptimizer},
publisher = {ACM},
title = {{Denali: a goal-directed superoptimizer}},
year = {2002}
}
@misc{Marlow2010,
abstract = {We present a complete redesign of evaluation strategies, a key ab- straction for specifying pure, deterministic parallelism in Haskell. Our new formulation preserves the compositionality and modular- ity benefits of the original, while providing significant new ben- efits. First, we introduce an evaluation-order monad to provide clearer, more generic, and more efficient specification of parallel evaluation. Secondly, the new formulation resolves a subtle space management issue with the original strategies, allowing parallelism (sparks) to be preserved while reclaiming heap associated with su- perfluous parallelism. Related to this, the newformulation provides far better support for speculative parallelism as the garbage collec- tor now prunes unneeded speculation. Finally, the new formula- tion provides improved compositionality: we can directly express parallelism embedded within lazy data structures, producing more compositional strategies, and our basic strategies are parametric in the coordination combinator, facilitating a richer set of parallelism combinators. We give measurements over a range of benchmarks demonstrating that the runtime overheads of the new formulation relative to the original are low, and the new strategies even yield slightly better speedups on average than the original strategies.},
annote = {NULL},
author = {Marlow, S. and Maier, P. and Loidl, H. W.},
booktitle = {Haskell},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Marlow, Maier, Loidl - Seq no more better strategies for parallel Haskell.pdf:pdf},
keywords = {Parallel Functional Programming,Strategies},
number = {11},
title = {{Seq no more: better strategies for parallel Haskell}},
url = {http://dl.acm.org/citation.cfm?id=1863535},
volume = {45},
year = {2010}
}
@article{Rahimi2013,
abstract = {This brief proposes a novel technique to alleviate the cost of timing error recovery, building upon the lockstep execution of single-instruction-multiple-data (SIMD) architectures. To support spatial memoization at the instruction level, we propose a single-strong-lane-multiple-weak-lane (SSMW) architecture. Spatial memoization exploits the value locality inside parallel programs, memoizes the result of an error-free execution of an instruction on the SS lane, and concurrently reuses the result to spatially correct errant instructions across MW lanes. Experiment results on Taiwan Semiconductor Manufacturing Company 45-nm technology confirm that this technique avoids the recovery for 62{\%} of the errant instructions on average, for both error-tolerant and error-intolerant general-purpose applications.},
annote = {NULL},
author = {Rahimi, Abbas and Benini, Luca and Gupta, Rajesh K.},
doi = {10.1109/TCSII.2013.2281934},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Rahimi, Benini, Gupta - Spatial memoization Concurrent instruction reuse to correct timing errors in SIMD architectures.pdf:pdf},
isbn = {1549-7747},
issn = {15497747},
journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
keywords = {Instruction reuse,memoization,recovery,resilience,timing error correction},
number = {12},
title = {{Spatial memoization: Concurrent instruction reuse to correct timing errors in SIMD architectures}},
volume = {60},
year = {2013}
}
@article{Harrow2012,
abstract = {Quantum computing is not merely a recipe for new computing devices, but a new way of looking at the world.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0001},
author = {Harrow, Aram},
doi = {10.1145/2090276.2090288},
eprint = {arXiv:1501.0001},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Harrow - Why now is the right time to study quantum computing.pdf:pdf},
issn = {15284972},
journal = {XRDS: Crossroads, The ACM Magazine for Students},
number = {3},
title = {{Why now is the right time to study quantum computing}},
volume = {18},
year = {2012}
}
@inproceedings{Barik2016,
annote = {NULL},
author = {Barik, Rajkishore and Lewis, Brian T},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Barik, Lewis - A Black-box Approach to Energy-Aware Scheduling on Integrated CPU-GPU Systems.pdf:pdf},
isbn = {9781450337786},
keywords = {energy efficiency,heterogeneous cpu-gpu,power characterization,scheduling},
publisher = {IEEE},
title = {{A Black-box Approach to Energy-Aware Scheduling on Integrated CPU-GPU Systems}},
year = {2016}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural$\backslash$rnetwork with a small central layer to reconstruct high-dimensional input vectors. Gradient descent$\backslash$r$\backslash$ncan be used for fine-tuning the weights in such ‘‘autoencoder'' networks, but this works well only if$\backslash$r$\backslash$nthe initial weights are close to a good solution. We describe an effective way of initializing the$\backslash$r$\backslash$nweights that allows deep autoencoder networks to learn low-dimensional codes that work much$\backslash$r$\backslash$nbetter than principal components analysis as a tool to reduce the dimensionality of data.$\backslash$r$\backslash$n},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, G. E. and Salakhutdinov, R.},
doi = {10.1126/science.1127647},
eprint = {20},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Hinton, Salakhutdinov - Reducing the Dimensionality of Data with Neural Networks.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {1095-9203},
journal = {Science},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
number = {5786},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
volume = {313},
year = {2006}
}
@inproceedings{Bitirgen2008,
annote = {Using Artificial Neural Networks to predict performance effects of resource allocation on CMPs. It looks like their setup was a little limiting (only quad-workloads on 4-core machines). Does it scale?

Cited by 151 (2014).},
author = {Bitirgen, R. and Ipek, E. and Martinez, J. F.},
booktitle = {MICRO},
doi = {10.1109/MICRO.2008.4771801},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Bitirgen, Ipek, Martinez - Coordinated Management of Multiple Interacting Resources in Chip Multiprocessors A Machine Learning Ap.pdf:pdf},
isbn = {978-1-4244-2836-6},
keywords = {Efficient sharing of system resources is critical,allowing us to adapt our allocation decisions as a,and learns a predictive model of system performanc,but this is possible only if accompanied by effici,coordinated management of multiple interacting res,it becomes possible to make reliable comparisons a,our approach makes it possible to anticipate the s,our resource management scheme monitors the execut,resources in a coordinated fashion to enforce high},
month = {nov},
publisher = {ACM},
title = {{Coordinated Management of Multiple Interacting Resources in Chip Multiprocessors: A Machine Learning Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4771801},
year = {2008}
}
@article{Katajainen1999,
abstract = {It is shown that an array of n elements can be sorted using O(1) extra space, O(n log n / log log n) element moves, and n log2 n + O(n log log n) comparisons. This is the first in-place sorting algorithm requiring O(n log n) moves in the worst case while guaranteeing O(n log n) comparisons but, due to the constant factors involved, the algorithm is predominantly of theoretical interest.},
annote = {NULL},
author = {Katajainen, J. and Pasanen, T. A.},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Katajainen, Pasanen - In-place sorting with fewer moves.pdf:pdf},
journal = {Information Processing Letters},
keywords = {in-place algorithms,mergesort,merging,multiway merge,sorting},
title = {{In-place sorting with fewer moves}},
volume = {70},
year = {1999}
}
@misc{UniversityofEdinburgh2014v,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 4.pdf:pdf},
title = {{IAML 4}},
year = {2014}
}
@inproceedings{Baroni2014,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
annote = {NULL},
author = {Baroni, M. and Dinu, G. and Kruszewski, G.},
booktitle = {ACL},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Baroni, Dinu, Kruszewski - Don't Count, Predict! A Systematic Comparison of Context-Counting vs . Context-Predicting Semantic Vec.pdf:pdf},
title = {{Don't Count, Predict! A Systematic Comparison of Context-Counting vs . Context-Predicting Semantic Vectors}},
year = {2014}
}
@inproceedings{Zhang,
annote = {NULL},
author = {Zhang, Minjia and Cao, Man and Bond, Michael D},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Zhang, Cao, Bond - Low-Overhead Software Transactional Memory with Progress Guarantees and Strong Semantics ∗.pdf:pdf},
isbn = {9781450332057},
keywords = {biased reader,concurrency control,managed languages,software transactional memory,strong atomicity,writer locks},
title = {{Low-Overhead Software Transactional Memory with Progress Guarantees and Strong Semantics ∗}},
year = {2015}
}
@inproceedings{Regondi,
annote = {NULL},
author = {Regondi, P. and Albanese, C.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Regondi, Albanese - BLAS Extensions for Algebraic Pricing Methods.pdf:pdf},
isbn = {9781450334051},
keywords = {GPU,PDE pricing methods,cuBLAS extension},
title = {{BLAS Extensions for Algebraic Pricing Methods}},
year = {2015}
}
@inproceedings{Li2015,
abstract = {Although graphics processing units (GPUs) rely on threadlevel parallelism to hide long off-chip memory access latency, judicious utilization of on-chip memory resources, including register files, shared memory, and data caches, is critical to application performance. However, explicitly managing GPU on-chip memory resources is a non-trivial task for application developers. More importantly, as onchip memory resources vary among different GPU generations, performance portability has become a daunting challenge. In this paper, we tackle this problem with compilerdriven automatic data placement. We focus on programs that have already been reasonably optimized either manually by programmers or automatically by compiler tools. Our proposed compiler algorithms refine these programs by revising data placement across different types of GPU on-chip resources to achieve both performance enhancement and performance portability. Among 12 benchmarks in our study, our proposed compiler algorithm improves the performance by 1.76x on average on Nvidia GTX480, and by 1.61x on average on GTX680.},
annote = {NULL},
author = {Li, Chao and Yang, Yi and Lin, Zhen and Zhou, Huiyang},
booktitle = {CGO},
doi = {10.1109/CGO.2015.7054184},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Li et al. - Automatic data placement into GPU on-chip memory resources.pdf:pdf},
isbn = {9781479981618},
publisher = {IEEE},
title = {{Automatic data placement into GPU on-chip memory resources}},
year = {2015}
}
@inproceedings{Ryoo2008a,
abstract = {GPUs have recently attracted the attention of many application developers as commodity data-parallel coprocessors. The newest generations of GPU architecture provide easier programmability and increased generality while maintaining the tremendous mem- ory bandwidth and computational power of traditional GPUs. This opportunity should redirect efforts inGPGPUresearch fromad hoc porting of applications to establishing principles and strategies that allow efficient mapping of computation to graphics hardware. In this work we discuss the GeForce 8800 GTX processor's organiza- tion, features, and generalized optimization strategies. Key to per- formance on this platform is using massive multithreading to uti- lize the large number of cores and hide global memory latency. To achieve this, developers face the challenge of striking the right balance between each thread's resource usage and the number of si- multaneously active threads. The resources to manage include the number of registers and the amount of on-chip memory used per thread, number of threads per multiprocessor, and global memory bandwidth. We also obtain increased performance by reordering accesses to off-chip memory to combine requests to the same or contiguous memory locations and apply classical optimizations to reduce the number of executed operations. We apply these strate- gies across a variety of applications and domains and achieve be- tween a 10.5X to 457X speedup in kernel codes and between 1.16X to 431X total application speedup.},
annote = {This is a really nice write-up of the challenges and approaches to optimising GPU programs. CUDA programming requires the developer to explicitly manage data layout in DRAM, caching, thread communication and other resources. Performance of such programs depends heavily on fully utilizing zero-overhead thread scheduling, memory bandwidth, thread grouping, shared control flow, and intra-block thread communication. The paper gives an example of optimising matrix multiplication by utilising shared memory through tiling, and loop unrolling. Cited by 772.},
author = {Ryoo, S. and Rodrigues, C. I. and Baghsorkhi, S. S. and Stone, S. S. and Kirk, D. B. and Hwu, W. W.},
booktitle = {PPoPP},
doi = {10.1145/1345206.1345220},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Ryoo et al. - Optimization principles and application performance evaluation of a multithreaded GPU using CUDA.pdf:pdf},
isbn = {9781595937957},
issn = {00778923},
keywords = {GPU computing,parallel computing},
title = {{Optimization principles and application performance evaluation of a multithreaded GPU using CUDA}},
year = {2008}
}
@article{Park2013,
abstract = {Significant advances in compiler optimization have been made in recent years, enabling many transformations such as tiling, fusion, parallelization and vectorization on imperfectly nested loops. Nevertheless, the problem of finding the best combination of loop transformations remains a major challenge. Polyhedral models for compiler optimization have demonstrated strong potential for enhancing program performance, in particular for compute-intensive applications. But existing static cost models to optimize polyhedral transformations have significant limitations, and iterative compilation has become a very promising alternative to these models to find the most effective transformations. But since the number of polyhedral optimization alternatives can be enormous, it is often impractical to iterate over a significant fraction of the entire space of polyhedrally transformed variants. Recent research has focused on iterating over this search space either with manually-constructed heuristics or with automatic but very expensive search algorithms (e.g., genetic algorithms) that can eventually find good points in the polyhedral space. In this paper, we propose the use of machine learning to address the problem of selecting the best polyhedral optimizations. We show that these models can quickly find high-performance program variants in the polyhedral space, without resorting to extensive empirical search. We introduce models that take as input a characterization of a program based on its dynamic behavior, and predict the performance of aggressive high-level polyhedral transformations that includes tiling, parallelization and vectorization. We allow for a minimal empirical search on the target machine, discovering on average 83{\%} of the search-space-optimal combinations in at most 5 runs. Our end-to-end framework is validated using numerous benchmarks on two multi-core platforms.},
annote = {NULL},
author = {Park, E. and Cavazos, J. and Pouchet, L. N. and Bastoul, C. and Cohen, A. and Sadayappan, P.},
doi = {10.1007/s10766-013-0241-1},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Park et al. - Predictive Modeling in a Polyhedral Optimization Space.pdf:pdf},
isbn = {9781612843551},
issn = {08857458},
journal = {IJPP},
keywords = {Iterative compilation,Loop transformation,Machine learning,Performance counters,Polyhedral optimization},
number = {5},
title = {{Predictive Modeling in a Polyhedral Optimization Space}},
volume = {41},
year = {2013}
}
@misc{UniversityofEdinburghe,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 5. Computational Complexity.pdf:pdf},
title = {{5. Computational Complexity}},
year = {2015}
}
@inproceedings{Rojas2017,
author = {Rojas, J. M. and White, T. D. and Clegg, B. S and Fraser, G.},
booktitle = {ICSE},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Rojas et al. - Code Defenders Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game.pdf:pdf},
title = {{Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game}},
year = {2017}
}
@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
doi = {10.1109/WACV.2017.108},
eprint = {1706.01427},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Santoro et al. - A simple neural network module for relational reasoning.pdf:pdf},
isbn = {9781509048229},
issn = {21607516},
pages = {1--16},
pmid = {189384},
title = {{A simple neural network module for relational reasoning}},
url = {http://arxiv.org/abs/1706.01427},
year = {2017}
}
@inproceedings{Dybdal2016,
annote = {NULL},
author = {Dybdal, M. and Elsman, M. and Svensson, B. J. and Sheeran, M.},
booktitle = {International Workshop on Functional High-Performance Computing},
doi = {10.1145/2975991.2975996},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Dybdal et al. - Low-level Functional GPU Programming for Parallel Algorithms.pdf:pdf},
isbn = {978-1-4503-4433-3},
keywords = {GPU programming,Type systems,array-programming,data-parallel languages,hierarchical machine models,iteration schemes,pull-arrays,push-arrays},
title = {{Low-level Functional GPU Programming for Parallel Algorithms}},
url = {http://doi.acm.org/10.1145/2975991.2975996},
year = {2016}
}
@article{Bottinger2018,
abstract = {Fuzzing is the process of finding security vulnerabilities in input-processing code by repeatedly testing the code with modified inputs. In this paper, we formalize fuzzing as a reinforcement learning problem using the concept of Markov decision processes. This in turn allows us to apply state-of-the-art deep Q-learning algorithms that optimize rewards, which we define from runtime properties of the program under test. By observing the rewards caused by mutating with a specific set of actions performed on an initial program input, the fuzzing agent learns a policy that can next generate new higher-reward inputs. We have implemented this new approach, and preliminary empirical evidence shows that reinforcement fuzzing can outperform baseline random fuzzing.},
author = {B{\"{o}}ttinger, K. and Godefroid, P. and Singh, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - B{\"{o}}ttinger, Godefroid, Singh - Deep Reinforcement Fuzzing.pdf:pdf},
journal = {arXiv:1801.04589},
title = {{Deep Reinforcement Fuzzing}},
year = {2018}
}
@inproceedings{Palsberg1994,
abstract = {Binding-time analysis is important in partial evaluators. Its task is to determine which parts of a program can be evaluated if some of the expected input is known. Two approaches to do this are abstract interpretation and type inference. We compare two specific such analyses to see which one determines most program ports to be eliminable. The first is a an abstract interpretation approach based on closure analysis and the second is the type inference approach of Gomard and Jones (1991). Both apply to the pure {\&}lambda;-calculus. We prove that the abstract interpretation approach is more powerful than that of Gomard and Jones: the former determines the same and possibly more program parts to be eliminable as the latter},
annote = {NULL},
author = {Palsberg, J. and Schwartzbach, M.I.},
booktitle = {ICCL},
doi = {10.1109/ICCL.1994.288372},
file = {:Users/cec/Google Drive/Mendeley Library/1994 - Palsberg, Schwartzbach - Binding-time analysis abstract interpretation versus type inference.pdf:pdf},
isbn = {0-8186-5640-X},
issn = {10748970},
title = {{Binding-time analysis: abstract interpretation versus type inference}},
year = {1994}
}
@article{Stefan2010,
abstract = {Keeping a visible record of your rejected applications can help others to deal with setbacks, says Melanie Stefan.},
annote = {NULL},
author = {Stefan, Melanie},
doi = {10.1038/nj7322-467a},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Stefan - A CV of failures.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {7322},
title = {{A CV of failures}},
url = {papers3://publication/uuid/BDFF12AA-2B33-4BEF-B7F1-22ACF2D8DE2F{\%}5Cnhttp://www.nature.com/doifinder/10.1038/nj7322-467a},
volume = {468},
year = {2010}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, G. E. and Srivastava, N. and Krizhevsky, A. and Sutskever, I. and Salakhutdinov, R. R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Hinton et al. - Improving Neural Networks by Preventing Co-adaptation of Feature Detectors.pdf:pdf},
isbn = {9781467394673},
issn = {9781467394673},
journal = {arXiv:1207.0580},
pmid = {1000104337},
title = {{Improving Neural Networks by Preventing Co-adaptation of Feature Detectors}},
year = {2012}
}
@incollection{Ng2018,
abstract = {Suppose your training, dev and test sets all come from the same distribution. Then you should always try to get more training data, since that can only improve performance, right? Even though having more data can't hurt, unfortunately it doesn't always help as much as you might hope. It could be a waste of time to work on getting more data. So, how do you decide when to add data, and when not to bother? There are two major sources of error in machine learning: bias and variance. Understanding them will help you decide whether adding data, as well as other tactics to improve performance, are a good use of time. Suppose you hope to build a cat recognizer that has 5{\%} error. Right now, your training set has an error rate of 15{\%}, and your dev set has an error rate of 16{\%}. In this case, adding training data probably won't help much. You should focus on other changes. Indeed, adding more examples to your training set only makes it harder for your algorithm to do well on the training set. (We explain why in a later chapter.) If your error rate on the training set is 15{\%} (or 85{\%} accuracy), but your target is 5{\%} error (95{\%} accuracy), then the first problem to solve is to improve your algorithm​ '​ s performance on your training set. Your dev/test set performance is usually worse than your training set performance. So if you are getting 85{\%} accuracy on the examples your algorithm has seen, there's no way you're getting 95{\%} accuracy on examples your algorithm hasn't even seen. Suppose as above that your algorithm has 16{\%} error (84{\%} accuracy) on the dev set. We break the 16{\%} error into two components:},
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning (Chapters 20-22).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 20-22)}},
url = {https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/37024aa7-8550-44d9-aa5e-e80aa82ec436/Ng{\_}MLY03.pdf},
year = {2018}
}
@article{Moravcik2017,
abstract = {Artificial intelligence has seen a number of breakthroughs in recent years, with games often serving as significant milestones. A common feature of games with these successes is that they involve information symmetry among the players, where all players have identical information. This property of perfect information, though, is far more common in games than in real-world problems. Poker is the quintessential game of imperfect information, and it has been a longstanding challenge problem in artificial intelligence. In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition about arbitrary poker situations that is automatically learned from self-play games using deep learning. In a study involving dozens of participants and 44,000 hands of poker, DeepStack becomes the first computer program to beat professional poker players in heads-up no-limit Texas hold'em. Furthermore, we show this approach dramatically reduces worst-case exploitability compared to the abstraction paradigm that has been favored for over a decade.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.01724},
author = {Morav{\v{c}}{\'{i}}k, M. and Schmid, M. and Burch, N. and Lis{\'{y}}, V. and Morrill, D. and Bard, N. and Davis, T. and Waugh, K. and Johanson, M. and Bowling, M.},
eprint = {1701.01724},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Morav{\v{c}}{\'{i}}k et al. - DeepStack Expert-Level Artificial Intelligence in No-Limit Poker.pdf:pdf},
journal = {Science},
title = {{DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker}},
year = {2017}
}
@inproceedings{Piao,
annote = {NULL},
author = {Piao, Xianglan and Kim, Channoh and Kim, Jincheon and Lee, Jae W},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Piao et al. - JAWS A JavaScript Framework for Adaptive CPU-GPU Work Sharing.pdf:pdf},
isbn = {9781450332057},
keywords = {data parallelism,gpu,heterogeneity,javascript,multi-core,scheduler,sharing,web browser,work},
title = {{JAWS : A JavaScript Framework for Adaptive CPU-GPU Work Sharing}},
year = {2015}
}
@incollection{Hunter2014,
abstract = {matplotlib is a Python-based plotting library with full support for 2D and limited support for 3D graphics, widely used in the Python scientific computing community. The library targets a broad range of use cases. It can embed graphics in the user interface toolkit of your choice, and currently supports interactive graphics on all major desktop operating systems using the GTK+, Qt, Tk, FLTK, wxWidgets and Cocoa toolkits. It can be called interactively from the interactive Python shell to produce graphics with simple, procedural commands, much like Mathematica, IDL or MATLAB. matplotlib can also be embedded in a headless webserver to provide hardcopy in both raster- based formats like Portable Network Graphics (PNG) and vector formats like PostScript, Portable Document Format (PDF) and Scalable Vector Graphics (SVG) that look great on paper.},
annote = {A nice write-up of the architecture of matplotlib, with some insightful background about the authours.},
author = {Hunter, J. and Droettboom, M.},
booktitle = {The Architecture of Open Source Applications},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Hunter, Droettboom - matplotlib.pdf:pdf},
number = {Volume 2},
title = {matplotlib},
volume = {2},
year = {2014}
}
@article{Hume1991,
abstract = {Since the Boyer-Moore algorithm was described in 1977, it has been the standard benchmark for the practical string search literature. Yet this yardstick compares badly with current practice. We describe two algorithms that perform 47{\%} fewer comparisons and are about 4.5 times faster across a wide range of architectures and compilers. These new variants are members of a family of algorithms based on the skip loop structure of the preferred, but often neglected, fast form of Boyer-Moore. We present a taxonomy for this family, and describe a toolkit of components that can be used to design an algorithm most appropriate for a given set of requirements.},
annote = {NULL},
author = {Hume, A. and Sunday, D.},
doi = {10.1002/spe.4380211105},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Hume, Sunday - Fast string searching.pdf:pdf},
issn = {00380644},
journal = {Software: Practice and Experience},
keywords = {boyer moore,string searching pattern matching},
number = {11},
title = {{Fast string searching}},
url = {http://doi.wiley.com/10.1002/spe.4380211105},
volume = {21},
year = {1991}
}
@inproceedings{Grewe2013,
abstract = {General purpose GPU based systems are highly attractive as they give potentially massive performance at little cost. Re- alizing such potential is challenging due to the complexity of programming. This paper presents a compiler based ap- proach to automatically generate optimized OpenCL code from data-parallel OpenMP programs for GPUs. Such an approach brings together the benefits of a clear high lev- el language (OpenMP) and an emerging standard (OpenCL) for heterogeneous multi-cores. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architec- tures and uses predictive modeling to automatically deter- mine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on the multi-core host. We applied our approach to the entire NAS parallel benchmark suite and evaluated it on two distinct GPU based systems: Core i7/NVIDIA GeForce GTX 580 and Core i7/AMD Radeon 7970. We achieved average (up to) speedups of 4.51x and 4.20x (143x and 67x) respectively over a sequential base- line. This is, on average, a factor 1.63 and 1.56 times faster than a hand-coded, GPU-specific OpenCL implementation developed by independent expert programmers.},
annote = {From Duplicate 2 (Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous Systems - Grewe, Dominik; Wang, Zheng; O'Boyle, Michael F P Mfp)

Compiler from OpenMP -{\textgreater} OpenCL. 8 NAS benchmarks, 5 dataset sizes for each. Cited by 37.

From Duplicate 3 (Portable mapping of data parallel programs to OpenCL for heterogeneous systems - Wang, Zheng; Grewe, Dominik; O'Boyle, Michael F.P.; Wang, Zheng; O'Boyle, Michael F.P.)

From Duplicate 1 (Portable mapping of data parallel programs to OpenCL for heterogeneous systems - Grewe, Dominik; Wang, Zheng; O'Boyle, Michael F.P.)

Compiler from OpenMP -{\textgreater} OpenCL. 8 NAS benchmarks, 5 dataset sizes for each. Cited by 37.},
author = {Grewe, D. and Wang, Z. and O'Boyle, M.},
booktitle = {CGO},
doi = {10.1109/CGO.2013.6494993},
file = {:Users/cec/Google Drive/Mendeley Library//2013 - Grewe, Wang, O'Boyle - Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous Systems.pdf:pdf;:Users/cec/Google Drive/Mendeley Library/2013 - Grewe, Wang, O'Boyle - Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous Systems(2).pdf:pdf},
isbn = {9781467355254},
issn = {15443566},
keywords = {gpu,machine-learning mapping,opencl},
publisher = {IEEE},
title = {{Portable Mapping of Data Parallel Programs to OpenCL for Heterogeneous Systems}},
year = {2013}
}
@inproceedings{Cavazos2008,
abstract = {The industry is now in agreement that the future of architecture design lies in multiple cores. As a consequence, all computer systems today, from embedded devices to petascale computing systems, are being developed using multicore processors. Although researchers in industry and academia are exploring many different multicore hardware design choices, most agree that developing portable software that achieves high performance on multicore processors is a major unsolved problem. We now see a plethora of architectural features, with little consensus on how the computation, memory, and communication structures in multicore systems will be organized. The wide disparity in hardware systems available has made it nearly impossible to write code that is portable in functionality while still taking advantage of the performance potential of each system. In this paper, we propose exploring the viability of developing intelligent compilers, focusing on key components that will allow application portability while still achieving high performance.},
annote = {Cited by 8.},
author = {Cavazos, J.},
booktitle = {ICCC},
doi = {10.1109/CLUSTR.2008.4663796},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Cavazos - Intelligent compilers.pdf:pdf},
isbn = {9781424426409},
issn = {15525244},
title = {{Intelligent compilers}},
year = {2008}
}
@misc{UniversityofEdinburghb,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 4. Computational Complexity.pdf:pdf},
title = {{4. Computational Complexity}},
year = {2015}
}
@inproceedings{Ansel2009a,
abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse- grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algo- rithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
annote = {This paper presents PetaBricks, an implicitly parallel programming language that supports algorithmic choice at the language level. PetaBricks consists of a compiler with C++ code generator, an autotuner, and a runtime library.








The concept is novel and well thought out. However, it is questionable as to whether the performance improvements offered by PetaBricks are sufficiently impressive to encourage widespread adoption.},
author = {Ansel, A. and Chan, C. and Wong, Y. L. and Olszewski, M. and Zhao, Q. and Edelman, A. and Amarasinghe, S.},
booktitle = {PLDI},
doi = {10.1145/1542476.1542481},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Ansel et al. - PetaBricks A Language and Compiler for Algorithmic Choice.pdf:pdf},
isbn = {9781605583921},
keywords = {adaptive,algorithmic choice,autotuning,compiler,implicitly parallel,language},
publisher = {ACM},
title = {{PetaBricks: A Language and Compiler for Algorithmic Choice}},
year = {2009}
}
@inproceedings{Barnes2008,
abstract = {Many applied scientific domains are increasingly relying on large- scale parallel computation. Consequently, many large clusters now have thousands of processors. However, the ideal number of pro- cessors to use for these scientific applications varies with both the input variables and the machine under consideration, and predict- ing this processor count is rarely straightforward. Accurate pre- diction mechanisms would provide many benefits, including im- proving cluster efficiency and identifying system configuration or hardware issues that impede performance. We explore novel regression-based approaches to predict parallel program scalability. We use several program executions on a small subset of the processors to predict execution time on larger num- bers of processors. We compare three different regression-based techniques: one based on execution time only; another that uses per-processor information only; and a third one based on the global critical path. These techniques provide accurate scaling predic- tions, with median prediction errors between 6.2{\%} and 17.3{\%} for seven applications.},
annote = {NULL},
author = {Barnes, B. J. and Rountree, B. and Lowenthal, D. K. and Livermore, L. and Schulz, M.},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Barnes et al. - A Regression-Based Approach to Scalability Prediction.pdf:pdf},
isbn = {9781605581583},
keywords = {MPI,Prediction,Regression,Scalability,modeling},
publisher = {ACM},
title = {{A Regression-Based Approach to Scalability Prediction}},
year = {2008}
}
@article{Basically1995,
annote = {NULL},
author = {Basically, C},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Basically - min, max, and more.pdf:pdf},
journal = {C++ Report},
number = {January},
title = {min, max, and more},
year = {1995}
}
@inproceedings{Hu,
annote = {NULL},
author = {Hu, Raymond and Martins, Francisco},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hu, Martins - Dynamic Deadlock Verification for General Barrier Synchronisation.pdf:pdf},
isbn = {9781450332057},
keywords = {barrier synchronisation,deadlock avoidance,deadlock detection,java,phasers,x10},
title = {{Dynamic Deadlock Verification for General Barrier Synchronisation}},
year = {2015}
}
@inproceedings{Lattner2004,
abstract = {This paper describes LLVM (Low Level Virtual Machine), a compiler framework designed to support transparent, life- long program analysis and transformation for arbitrary pro- grams, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in Static Single Assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capa- bilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We de- scribe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative exam- ples of the benefits LLVM provides for several challenging compiler problems.},
annote = {A very compelling paper with a well justified goal and solid experimental results.},
author = {Lattner, C. and Adve, V.},
booktitle = {CGO},
doi = {10.1109/CGO.2004.1281665},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Lattner, Adve - LLVM A compilation framework for lifelong program analysis {\&} transformation.pdf:pdf},
isbn = {0-7695-2102-9},
publisher = {IEEE},
title = {{LLVM: A compilation framework for lifelong program analysis {\&} transformation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1281665},
year = {2004}
}
@misc{Zalewski,
author = {Zalewski, M.},
title = {{American Fuzzy Lop}}
}
@misc{Bryan2007,
author = {Bryan, T.},
title = {{Random C program generator}},
year = {2007}
}
@inproceedings{Vollmer2015,
annote = {NULL},
author = {Vollmer, Michael},
booktitle = {FHPC},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Vollmer - Meta-Programming and Auto-Tuning in the Search for High Performance GPU Code.pdf:pdf},
isbn = {9781450338073},
keywords = {auto-tuning,gpus,meta-programming,parallelism},
title = {{Meta-Programming and Auto-Tuning in the Search for High Performance GPU Code}},
year = {2015}
}
@article{Eigenmann1998,
abstract = {This paper presents the results of the Cedar Hand-Parallelization
Experiment conducted from 1989 through 1992, within the Center for
Supercomputing Research and Development (CSRD) at the University of
Illinois. In this experiment, we manually transformed the Perfect
Benchmarks(R) into parallel program versions. In doing so, we used
techniques that may be automated in an optimizing compiler. We then ran
these programs on the Cedar multiprocessor (built at CSRD during the
1980s) and measured the speed improvement due to each technique. The
results presented here extend the findings previously reported. The
techniques credited most for the performance gains include array
privatization, parallelization of reduction operations, and the
substitution of generalized induction variables. All these techniques
can be considered extensions of transformations that were available in
vectorizers and commercial restructuring compilers of the late 1980s. We
applied these transformations by hand to the given programs, in a
mechanical manner, similar to that of a parallelizing compiler. Because
of our success with these transformations, we believed that it would be
possible to implement many of these techniques in a new parallelizing
compiler. Such a compiler has been completed in the meantime and we show
preliminary results},
annote = {Cited by 118.},
author = {Eigenmann, R. and Hoeflinger, J. and Padua, D.},
doi = {10.1109/71.655238},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Eigenmann, Hoeflinger, Padua - On the automatic parallelization of the Perfect Benchmarks(R).pdf:pdf},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
title = {{On the automatic parallelization of the Perfect Benchmarks(R)}},
volume = {9},
year = {1998}
}
@article{Learned-miller2016,
annote = {NULL},
author = {Learned-Miller, E. and Huang, G. and Roychowdhury, A. and Li, H. and Hua, G.},
doi = {10.1007/978-3-319-25958-1_8},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Learned-Miller et al. - Labeled Faces in the Wild A Survey.pdf:pdf},
isbn = {9783319259581},
title = {{Labeled Faces in the Wild: A Survey}},
year = {2016}
}
@unpublished{Jones2011,
annote = {Tutorial slides.},
author = {Jones, Simon Peyton and Bundy, Alan and Oberlander, Jon},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Jones, Bundy, Oberlander - Working on your IRP Argument, identifying claims.pdf:pdf},
number = {February},
title = {{Working on your IRP: Argument, identifying claims}},
year = {2011}
}
@article{Huang,
abstract = {The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In the context of high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Unfortunately, choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. As a result, existing solutions rely on a variety of sub-optimal heuristics. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. To this end, we implement a framework that takes any group of programs and finds a sequence of passes that optimize the performance of these programs. Without loss of generality, we instantiate this framework in the context of an LLVM compiler and target multiple High-Level Synthesis programs. We compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16{\%} improvement in circuit performance over the-O3 compiler flag.},
archivePrefix = {arXiv},
arxivId = {1901.04615v1},
author = {Huang, Q. and Haj-Ali, A. and Moses, W. and Xiang, J. and Stoica, I. and Asanovic, K. and Wawrzynek, J.},
eprint = {1901.04615v1},
file = {:Users/cec/Google Drive/Mendeley Library/2019 - Huang et al. - AutoPhase Compiler Phase-Ordering for High-Level Synthesis with Deep Reinforcement Learning.pdf:pdf},
journal = {arXiv:1901.04615},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{AutoPhase: Compiler Phase-Ordering for High-Level Synthesis with Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1901.04615.pdf},
year = {2019}
}
@article{Bailey1991,
abstract = {The recent success of vector computers such as the Cray-1 and array processors such as those manufactured by Floating Point Systems has increased interest in making vector operations available to the FORTRAN programmer. The FORTRAN standards committee is currently considering a successor to FORTRAN 77, usually called FORTRAN 8x, that will permit the programmer to explicitly specify vector and array operations. Although FORTRAN 8x will make it convenient to specify explicit vector operations in new programs, it does little for existing code. In order to benefit from the power of vector hardware, existing programs will need to be rewritten in some language (presumably FORTRAN 8x) that permits the explicit specification of vector operations. One way to avoid a massive manual recoding effort is to provide a translator that discovers the parallelism implicit in a FORTRAN program and automatically rewrites that program in FORTRAN 8x. Such a translation from FORTRAN to FORTRAN 8x is not straightforward because FORTRAN DO loops are not always semantically equivalent to the corresponding FORTRAN 8x parallel operation. The semantic difference between these two constructs is precisely captured by the concept of dependence. A translation from FORTRAN to FORTRAN 8x preserves the semantics of the original program if it preserves the dependences in that program. The theoretical background is developed here for employing data dependence to convert FOR- TRAN programs to parallel form. Dependence is defined and characterized in terms of the conditions that give rise to it; accurate tests to determine dependence are presented; and transformations that use dependence to uncover additional parallelism are discussed.},
annote = {Cited by 858.},
author = {Bailey, D. H.},
doi = {10.1145/29873.29875},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Bailey - Automatic Translation of Fortran Programs to Multiprecision.pdf:pdf},
isbn = {1-58113-359-6},
issn = {0164-0925},
keywords = {FORTRAN,detection of parallelism,language translators,vector computing},
number = {4},
title = {{Automatic Translation of Fortran Programs to Multiprecision}},
url = {http://hdl.handle.net/2060/19970012091},
volume = {9},
year = {1991}
}
@inproceedings{Chung2002,
abstract = {In this paper we present the Active Harmony automated runtime tuning system. We describe the interface used by programs to make applications tunable. We present the Library Specification Layer which helps program library developers expose multiple variations of the same API using different algorithms. The Library Specification Language helps to select the most appropriate program library to tune the overall performance. We also present the optimization algorithm that we used to adjust parameters in the application and the libraries. Finally, we present results that show how the system is able to tune several real applications. The automated tuning system is able to tune the application parameters to within a few percent of the best value after evaluating only 11 configurations out of over 1,700 possible combinations.},
annote = {Active Harmony is a runtime tuning system which allows programmers to expose multiple variations of the same API using different algorithms, and automatically searches through them.},
author = {Chung, I. and Hollingsworth, J. K. and Server, H.},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Chung, Hollingsworth, Server - Active Harmony Towards Automated Performance Tuning.pdf:pdf},
publisher = {IEEE Computer Society},
title = {{Active Harmony: Towards Automated Performance Tuning}},
year = {2002}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, S. and Finn, C. and Darrell, T. and Abbeel, P.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Levine et al. - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {JMLR},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@inproceedings{DeMesmay2009,
author = {{De Mesmay}, F. and Rimmel, A. and Voronenko, Y. and Puschel, M.},
booktitle = {ICML},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - De Mesmay et al. - Bandit-Based Optimization on Graphs with Application to Library Performance Tuning.pdf:pdf},
title = {{Bandit-Based Optimization on Graphs with Application to Library Performance Tuning}},
year = {2009}
}
@inproceedings{Yan,
annote = {NULL},
author = {Yan, Yonghong and Lin, Pei-hung and Liao, Chunhua and Supinski, Bronis R De and Quinlan, Daniel J},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Yan et al. - Supporting Multiple Accelerators in High-Level Programming Models Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334044},
keywords = {Accelerators,Data Distribution,Directives,OpenMP},
title = {{Supporting Multiple Accelerators in High-Level Programming Models Categories and Subject Descriptors}},
year = {2015}
}
@article{Graves,
annote = {NULL},
author = {Graves, A. and Schmidhuber, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Graves, Schmidhuber - Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures.pdf:pdf},
journal = {Neural Networks},
number = {5},
title = {{Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures}},
volume = {5},
year = {2005}
}
@misc{Silver2015p,
author = {Silver, D.},
booktitle = {UCL,Computer Science Department, Reinforcement Learning Lectures},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Silver - Lecture 1 Introduction to Reinforcement Learning Outline.pdf:pdf},
isbn = {9780195306613},
title = {{Lecture 1 : Introduction to Reinforcement Learning Outline}},
year = {2015}
}
@techreport{Leather,
abstract = {The computing industry faces an enormous challenge. Power constraints have forced computer architects to introduce heterogeneous multicore systems of bewildering diversity, hoping that software techniques will be found to make use of them. Software developers, however, are struggling to cope with this dramatic increase in complexity, the existing tools and languages are simply inadequate to the task. Expressing parallelism is hard, optimising for it on a heterogeneous system with subtle inter-component interactions is harder still, and even if successfully achieved the task must likely be started from scratch when moving to a new platform. These issues are particularly problematic for industries involved in data-intensive applications which covers many areas of technology from smartphones to supercomputers and the communications systems that connect them. The data-intensive applications software industry accounts for tens of billions of euros world-wide. If we are unable to solve these problems, then we will be unable to make use of these new, emerging architectures, and for the first time in decades progress in the software industries will stagnate. What is needed is way to allow programmers to describe what parallelism is possible with minimal effort and for a commercial grade toolchain to manage all other aspects of what and how to parallelise and optimise their programs. This project offers precisely that. We will create an extensible system of parallel skeletons in C++ and associated data structures which precisely capture the programmer's algorithmic intention, with code uncluttered by artefacts of parallelism and parallel optimisation. We will fully integrate these skeletons, creating a portable compiler toolchain over heterogeneous platforms, and we will build automatic optimisers that are able to improve the performance and energy efficiency of programs both statically, using predictive modelling, and dynamically, using runtime-adaption. The toolchain will be complete, offering debuggers and profilers that are able to take advantage of the compiler's newknowledge about the code patterns in the programs. We will cou- ple these optimisers with novel performance modelling techniques, and new source level micro-benchmarking. The consortium is made up of world leading academics in the areas of parallelisation, auto-tuning, distributed, and heterogeneous systems, as well as cutting edge European companies working at the forefront of compilers, systems, vision, and artificial intelligence, ensuring that it is perfectly placed to meet the goals of the project and deliver significant innovation and impact, both scientifically and commercially. When EXOSKELETON is successful, the toolchain it creates will show how to harness the potential of hetero- geneous multi-cores. We will demonstrate our techniques on real world applications in the domains of artificial intelligence, graphics, computer vision, and HPC, provided by our industrial partners. We will validate the im- proved programmability of using our parallel abstractions by comparing to the amount of code needed for hand parallelisation of real world applications, for which we target a 90{\%} reduction. We will show that our automatic optimisations are able to increase performance and energy efficiency for those applications by 50{\%} compared to unoptimised code, and are not more than 5{\%} away from hand optimisation. We will prove the portability of our system by showing that our applications remain optimised for different architectures, without the developer needing to alter a single line of code. Part},
annote = {Grant proposal which didn't get the funding. Apparently because of a lack of related work. Dope abstract, really hits a home run with the "crisis, solution, happiness" themes.},
author = {Leather, H.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Leather - ExoSkeleton Easy-to-use Reusable Components for Accelerating Software using Heterogeneous Multi-core Processors.pdf:pdf},
title = {{ExoSkeleton: Easy-to-use Reusable Components for Accelerating Software using Heterogeneous Multi-core Processors}}
}
@inproceedings{Wang2009,
abstract = {The efficient mapping of program parallelism to multi-core proces- sors is highly dependent on the underlying architecture. This pa- per proposes a portable and automatic compiler-based approach to mapping such parallelism using machine learning. It develops two predictors: a data sensitive and a data insensitive predictor to select the best mapping for parallel programs. They predict the number of threads and the scheduling policy for any given program using a model learnt off-line. By using low-cost profiling runs, they pre- dict the mapping for a new unseen program across multiple input data sets. We evaluate our approach by selecting parallelism map- ping configurations for OpenMP programs on two representative but different multi-core platforms (the Intel Xeon and the Cell pro- cessors). Performance of our technique is stable across programs and architectures. On average, it delivers above 96{\%} performance of the maximum available on both platforms. It achieve, on aver- age, a 37{\%} (up to 17.5 times) performance improvement over the OpenMP runtime default scheme on the Cell platform. Compared to two recent prediction models, our predictors achieve better per- formance with a significant lower profiling cost.},
annote = {This paper proposes using machine learning to map parallel MPI programs to hardware, by using profiling runs to build a feature set.




A somewhat solid paper with reasonable results. However, as has been demonstrated in [1], this technique does not provide optimal results for different bencharks. The paper is well written, with clear methodology.




[1] A. Collins, C. Fensch, H. Leather, and M. Cole, “MaSiF: Machine learning guided auto-tuning of parallel skeletons,” 20th Annu. Int. Conf. High Perform. Comput., pp. 186–195, Dec. 2013.},
author = {Wang, Z. and O'Boyle, M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Wang, O'Boyle - Mapping Parallelism to Multi-cores A Machine Learning Based Approach.pdf:pdf},
isbn = {9781605583976},
keywords = {Artificial neural networks,Compiler optimization,Machine learning,Performance modeling,Support vector machine},
number = {15},
publisher = {ACM},
title = {{Mapping Parallelism to Multi-cores: A Machine Learning Based Approach}},
year = {2009}
}
@misc{Api2011,
annote = {NULL},
author = {{Khronos OpenCL Group Inc}},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Khronos OpenCL Group Inc - OpenCL 1.2 Reference Card.pdf:pdf},
title = {{OpenCL 1.2 Reference Card}},
year = {2011}
}
@misc{Etessamid,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 29. Markov's and Chebyshev's Inequalities, {\&} the birthday problem.pdf:pdf},
number = {Chapter 7},
title = {{29. Markov's and Chebyshev's Inequalities, {\&} the birthday problem}},
volume = {7},
year = {2015}
}
@article{Vasilescu2015,
abstract = {Understanding one's work environment is important for one's success, especially when working in teams. In virtual collaborative environments this amounts to being aware of the technical and social attributes of one's team members. Focusing on Open Source Software teams, naturally very diverse both socially and technically, we report the results of a user survey that tries to resolve how teamwork and individual attributes are perceived by developers collaborating on GITHUB, and how those perceptions influence their work. Our findings can be used as complementary data to quantitative studies of developers' behavior on GITHUB.},
annote = {NULL},
author = {Vasilescu, B. and Filkov, V. and Serebrenik, A.},
doi = {10.1109/CHASE.2015.14},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Vasilescu, Filkov, Serebrenik - Perceptions of Diversity on GitHub A User Survey.pdf:pdf},
isbn = {9781467370318},
journal = {Chase},
title = {{Perceptions of Diversity on GitHub: A User Survey}},
year = {2015}
}
@article{Bernstein1997,
abstract = {Recently a great deal of attention has been focused on quantum computation follow- ing a sequence of results [Bernstein and Vazirani, in Proc. 25th Annual ACM Symposium Theory Comput., 1993, pp. 11–20, SIAM J. Comput., 26 (1997), pp. 1411–1473], [Simon, in Proc. 35th Annual IEEE Symposium Foundations Comput. Sci., 1994, pp. 116–123, SIAM J. Comput., 26 (1997), pp. 1474–1483], [Shor, in Proc. 35th Annual IEEE Symposium Foundations Comput. Sci., 1994, pp. 124–134] suggesting that quantum computers are more powerful than classical probabilistic computers. Following Shor's result that factoring and the extraction of discrete logarithms are both solvable in quantum polynomial time, it is natural to ask whether all of NP can be efficiently solved in quantum polynomial time. In this paper, we address this question by proving that relative to an oracle chosen uniformly at random with probability 1 the class NP cannot be solved on a quantum Turing machine (QTM) in time o(2n/2). We also show that relative to a permutation oracle chosen uniformly at random with probability 1 the class NP∩ co-NP cannot be solved on a QTM in time o(2n/3). The former bound is tight since recent work of Grover [in Proc. 28th Annual ACM Sympo- sium Theory Comput., 1996] shows how to accept the class NP relative to any oracle on a quantum computer in time O(2n/2).},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {quant-ph/9701001},
author = {Bernstein, Ethan and Bennett, Charles H and Brassard, Gilles and Vazirani, Umesh V},
doi = {10.1137/S0097539796300933},
eprint = {9701001},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Bernstein et al. - Strengths and weaknesses of quantum computing.pdf:pdf},
isbn = {0097-5397},
issn = {0097-5397},
journal = {SICOMP},
keywords = {oracle quantum turing machines,quantum polynomial,quantum turing machines},
number = {5},
primaryClass = {quant-ph},
title = {{Strengths and weaknesses of quantum computing}},
url = {http://pm1.bu.edu/{~}tt/qcl/pdf/bennettc19940807050c.pdf{\%}5Cnpapers3://publication/uuid/93A42224-16A4-4950-B6EF-05BE4C12A683},
volume = {26},
year = {1997}
}
@inproceedings{Awatramani2015,
annote = {NULL},
author = {Awatramani, M. and Zhu, X. and Zambreno, J. and Rover, D.},
booktitle = {PACT},
doi = {10.1109/PACT.2015.31},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Awatramani et al. - Phase Aware Warp Scheduling Mitigating Effects of Phase Behavior in GPGPU Applications.pdf:pdf},
isbn = {978-1-4673-9524-3},
publisher = {ACM},
title = {{Phase Aware Warp Scheduling: Mitigating Effects of Phase Behavior in GPGPU Applications}},
year = {2015}
}
@article{Kingma2013a,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, D. P. and Welling, M.},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Kingma, Welling - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@misc{Arapinis2014c,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 04. Proof techniques.pdf:pdf},
title = {{04. Proof techniques}},
year = {2015}
}
@techreport{Cummins2014a,
abstract = {The isoelectric point or pI of a protein corresponds to the solution pH at which the net surface charge is zero. Since the earliest days of solution biochemistry, the pI has been recorded and reported, and literature reports of pI abound. The protein isoelectric point database (PIP-DB) has collected and collated this legacy data to provide an in- creasingly comprehensive database for comparison and benchmarking purposes. As part of a collaboration between Aston University's Computer Science and Life and Health Sci- ences departments, a web application has been developed to warehouse this database and provide public access to this important information. A schema and file format (YAPS) has been designed to house protein isoelectric point datasets, with appropriate tooling to convert between PIP-DB and the YAPS format. A search engine and domain specific language has been designed which enables searching of PIP-DB by representing compound queries using tree structures in LISP. Support for protein sequence searching has been implemented using the NCBI BLAST+ search tools. A human-centred approach to designing web applications has been adopted, with a heavy focus on interaction design and usability testing. The results of usability testing show numerous advantages in the interface design, such as a widget which dynamically indicates the number of results to be returned by a search engine query. A public API for allowing programmatic communication with the search engine has been designed. The website makes extensive use of mobile code, implementing a thin presentation layer wrapper around the API. The use of mobile code in websites is discussed and a comparison is made with server-side rendering of HTML. A unique emphasis has been placed on development of infrastructure, with several new tools being written for reuse in other projects. Among these is the novel application of Markov text generators for creating test payloads from confidential datasets, and a project management program (pipbot) which automates version control and build configurations. A parallelised build system has been developed which provides homogeneous devel- opment and deployment configurations. Tests show how the use of shell-level parallelism reduces build times by a factor of 5. An implementation of checksum based cache invali- dation and on-demand build systems using the Linux's inotify subsystem is described.},
annote = {NULL},
author = {Cummins, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Cummins - Protein Isoelectric Point Database.pdf:pdf},
number = {May},
title = {{Protein Isoelectric Point Database}},
year = {2014}
}
@inproceedings{Pearce,
annote = {NULL},
author = {Pearce, O. and Gamblin, T. and Supinski, B. R. D. and Schulz, M. and Amato, N. M.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Pearce et al. - Decoupled Load Balancing.pdf:pdf},
isbn = {9781450332057},
keywords = {load balance,parallel algorithm,performance},
title = {{Decoupled Load Balancing}},
year = {2015}
}
@article{Campa2014,
annote = {NULL},
author = {Campa, S and Danelutto, M},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Campa, Danelutto - Parallel patterns for heterogeneous CPUGPU architectures Structured parallelism from cluster to cloud.pdf:pdf},
journal = {Future Generation Computer Systems},
title = {{Parallel patterns for heterogeneous CPU/GPU architectures: Structured parallelism from cluster to cloud}},
url = {http://www.sciencedirect.com/science/article/pii/S0167739X14000041},
volume = {37},
year = {2014}
}
@inproceedings{Balog2017,
annote = {Learning Inductive Program Synthesis (LIPS): Use a DSL with attribute vectors describing programs. Machine learning model predicts required attributes to allow search of DSL for matching attributes.},
author = {Balog, M. and Gaunt, A. L. and Brockschmidt, M. and Nowozin, S. and Tarlow, D.},
booktitle = {ICLR},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Balog et al. - DeepCoder Learning to Write Programs.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{DeepCoder: Learning to Write Programs}},
year = {2017}
}
@inproceedings{Cummins2017b,
author = {Cummins, C. and Petoumenos, P. and Wang, Z. and Leather, H.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Cummins et al. - End-to-end Deep Learning of Optimization Heuristics.pdf:pdf},
publisher = {IEEE},
title = {{End-to-end Deep Learning of Optimization Heuristics}},
year = {2017}
}
@inproceedings{Zhou2016,
annote = {NULL},
author = {Zhou, Hao and Xue, Jingling},
booktitle = {CGO},
doi = {10.1145/2854038.2854054},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zhou, Xue - Exploiting Mixed SIMD Parallelism by Reducing Data Reorganization Overhead.pdf:pdf},
isbn = {9781450337786},
keywords = {data reorganization optimization,lelism,loop vectorization,mixed simd paral-,slp},
publisher = {IEEE},
title = {{Exploiting Mixed SIMD Parallelism by Reducing Data Reorganization Overhead}},
year = {2016}
}
@article{Legg2007,
abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {0712.3329},
author = {Legg, S. and Hutter, M.},
doi = {10.1007/s11023-007-9079-x},
eprint = {0712.3329},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Legg, Hutter - Universal intelligence A definition of machine intelligence.pdf:pdf},
isbn = {0924-6495},
issn = {09246495},
journal = {arXiv:0712.3329},
keywords = {AIXI,Complexity theory,Intel- ligence definitions,Intelligence,Intelligence tests,Theoretical foundations,Turing test},
title = {{Universal intelligence: A definition of machine intelligence}},
year = {2007}
}
@article{Freedom2012,
annote = {NULL},
author = {Turing, A. M.},
doi = {10.1093/mind/LI.202.200},
file = {:Users/cec/Google Drive/Mendeley Library/1950 - Turing - Computing machinery and intelligence.pdf:pdf},
isbn = {2723415155},
issn = {00264423},
journal = {Mind},
number = {236},
title = {{Computing machinery and intelligence}},
volume = {59},
year = {1950}
}
@article{Downey,
annote = {NULL},
author = {Downey, Allen B.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Downey - Think Stats Exploratory Data Analysis in Python.pdf:pdf},
title = {{Think Stats Exploratory Data Analysis in Python}}
}
@inproceedings{Li,
annote = {NULL},
author = {Li, Xiangyu and Grossman, Max and Kaeli, David},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Li, Grossman, Kaeli - Mahout on Heterogeneous Clusters using HadoopCL.pdf:pdf},
isbn = {9781450334051},
keywords = {apu,gpgpu,machine learning,mapreduce},
title = {{Mahout on Heterogeneous Clusters using HadoopCL}},
year = {2015}
}
@article{McKeeman1998,
abstract = {Differential testing, a form of random testing, is a component of a mature testing technology for large software systems. It complements regression testing based on commercial test suites and locally developed tests During prod- uct development and deployment. Differential que testing requires two or more comparable systems be available to the tester. These sys- tems are presented with an exhaustive series of mechanically generated test cases. If (w might say when) the results differ or one of the systems loops indefinitely or crashes, the tester has a candidate for a bug-exposing test. Implementing differential testing is an interest- ing technical problem. Getting it into use is an even more interesting social challenge. This paper is derived from experience in differential testing of compilers and run-time systems at DIGITAL over the last few years and recently at Compaq. A working prototype for testing C compilers is available on the web.},
annote = {NULL},
author = {McKeeman, W. M.},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - McKeeman - Differential Testing for Software.pdf:pdf},
issn = {0898-901X},
journal = {DTJ},
number = {1},
title = {{Differential Testing for Software}},
volume = {10},
year = {1998}
}
@inproceedings{Chan2009,
abstract = {Algorithmic choice is essential in any problem domain to re- alizing optimal computational performance. Multigrid is a prime example: not only is it possible to make choices at the highest grid resolution, but a program can switch techniques as the problem is recursively attacked on coarser grid levels to take advantage of algorithms with different scaling behav- iors. Additionally, users with different convergence criteria must experiment with parameters to yield a tuned algorithm that meets their accuracy requirements. Even after a tuned algorithm has been found, users often have to start all over when migrating from one machine to another. We present an algorithm and autotuning methodology that address these issues in a near-optimal and efficient man- ner. The freedom of independently tuning both the algo- rithmand the number of iterations at each recursion level re- sults in an exponential search space of tuned algorithms that have different accuracies and performances. To search this space efficiently, our autotuner utilizes a novel dynamic pro- gramming method to build efficient tuned algorithms from the bottom up. The results are customized multigrid al- gorithms that invest targeted computational power to yield the accuracy required by the user. The techniques we describe allow the user to automati- cally generate tuned multigrid cycles of different shapes tar- geted to the user's specific combination of problem, hard- ware, and accuracy requirements. These cycle shapes dic- tate the order in which grid coarsening and grid refinement are interleaved with both iterative methods, such as Jacobi or Successive Over-Relaxation, as well as direct methods, which tend to have superior performance for small problem sizes. The need to make choices between all of these meth- ods brings the issue of variable accuracy to the forefront. Not only must the autotuning framework compare different possible multigrid cycle shapes against each other, but it also needs the ability to compare tuned cycles against both direct and (non-multigrid) iterative methods. We address this problem by using an accuracy metric for measuring the effectiveness of tuned cycle shapes and making comparisons over all algorithmic types based on this common yardstick. In our results, we find that the flexibility to trade perfor- mance versus accuracy at all levels of recursive computation enables us to achieve excellent performance on a variety of platforms compared to algorithmically static implementa- tions of multigrid. Our implementation uses PetaBricks, an implicitly paral- lel programming language where algorithmic choices are ex- posed in the language. The PetaBricks compiler uses these choices to analyze, autotune, and verify the PetaBricks pro- gram. These language features, most notably the autotuner, were key in enabling our implementation to be clear, correct, and fast.},
annote = {This paper describes a dynamic autotuning technique for Multigrid using PetaBricks.},
author = {Chan, C. and Ansel, H. and Wong, Y. L. and Amarasinghe, S. and Edelman, A.},
booktitle = {SC},
doi = {10.1145/1654059.1654065},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Chan et al. - Autotuning multigrid with PetaBricks.pdf:pdf},
isbn = {9781605587448},
title = {{Autotuning multigrid with PetaBricks}},
year = {2009}
}
@article{Bokhari1988a,
annote = {NULL},
author = {Bokhari, Shahid H.},
doi = {10.1109/12.75137},
file = {:Users/cec/Google Drive/Mendeley Library/1988 - Bokhari - Partitioning Problems in Parallel, Pipelined, and Distributed Computing.pdf:pdf},
issn = {00189340},
journal = {TC},
keywords = {Assignments,bottleneck paths,distributed computing,doubly weighted graphs,host-satellite systems,parallel processing,partitioning,pipelining,shortest paths,sum-bottleneck algorithm},
number = {1},
title = {{Partitioning Problems in Parallel, Pipelined, and Distributed Computing}},
volume = {37},
year = {1988}
}
@inproceedings{Chen2015c,
annote = {NULL},
author = {Chen, X and Xiao, H and Wardi, Y and Yalamanchili, S},
booktitle = {HiPC},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Chen et al. - Throughput Regulation in Shared Memory Multicore Processors.pdf:pdf},
keywords = {3D,multicore,multicore processor,throughput regulation,variable gain controller},
title = {{Throughput Regulation in Shared Memory Multicore Processors}},
year = {2015}
}
@misc{Zhang2006,
abstract = {Fixing runtime bugs in long running programs using trace based analyses such as dynamic slicing was believed to be prohibitively expensive. In this paper, we present a novel execution fast forwarding technique that makes this feasible. While a naive solution is to divide the entire execution by checkpoints, and then apply dynamic slicing enabled by tracing to one checkpoint interval at a time, it is still too costly even with state-of-the-art tracing techniques. Our technique is derived from two key observations. The first one is that long running programs are usually driven by events, which has been taken advantage of by checkpointing/replaying techniques to deterministically replay an execution from the event log. The second observation is that all the events are not relevant to replaying a particular part of the execution, in which the programmer suspects an error happened. We develop a slicing-like technique that can be used to prune irrelevant events from the event log. Driven by the reduced log, the replayed execution is now traced for fault location. This replayed execution has the effect of fast forwarding, i.e the amount of executed instructions is significantly reduced without losing the accuracy of reproducing a failure. Our evaluation shows that skipping irrelevant events can reduce the space requirement for dynamic slicing by factors ranging from 72 to 44490. We also describe how checkpointing and tracing enabled dynamic slicing are combined, which we believe is the first attempt to integrate these two techniques. Finally, the dynamic slices of a set of reported bugs for long running programs are studied to show the effectiveness of dynamic slicing.},
annote = {An approach to reducing the time and space overhead for replaying failures in long running executions. Problems: They only look at heap buffer overflow bugs that are caused by incorrect strlen() behaviour for charactersets - are they cherry-picking their tests? The mutt application is long-running because it's compute intensive, it's long running because it spools idly waiting for user input. There's only two pieces of related work. Pros: I learned that character set handling is a source of bus. Cited by 52.},
author = {Zhang, X. and Tallam, S. and Gupta, R.},
booktitle = {FSE},
doi = {10.1145/1181775.1181786},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Zhang, Tallam, Gupta - Dynamic Slicing Long Running Programs Through Execution Fast Forwarding.pdf:pdf},
isbn = {1-59593-468-5},
keywords = {Debugging,checkpointing,data slicing,debugging,event logging,replay},
publisher = {ACM},
title = {{Dynamic Slicing Long Running Programs Through Execution Fast Forwarding}},
url = {http://doi.acm.org/10.1145/1181775.1181786},
year = {2006}
}
@inproceedings{Shazeer2017c,
abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. In this work, we present a new kind of layer, the Sparsely-Gated Mixture-of-Experts (MoE), which can be used to effectively increase model ca-pacity with only a modest increase in computation. This layer consists of up to tens of thousands of feed-forward sub-networks (experts) containing a total of up to tens of billions of parameters. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the task of language modeling, where model capacity is critical for ab-sorbing the vast quantities of world knowledge available in the training corpora. We present new language model architectures where an MoE layer is inserted between stacked LSTMs, resulting in models with orders of magnitude more pa-rameters than would otherwise be feasible. On language modeling and machine translation benchmarks, we achieve comparable or better results than state-of-the-art at lower computational cost, including test perplexity of 29.9 on the 1 Billion Word Language Modeling Benchmark and BLEU scores of 40.56 and 26.03 on the WMT'14 En to Fr and En to De datasets respectively.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.07938v1},
author = {Shazeer, N. and Mirhoseini, A. and Maziarz, K. and Davis, A. and Le, Q. and Hinton, G. and Dean, J.},
booktitle = {ICLR},
eprint = {arXiv:1511.07938v1},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Shazeer et al. - Outrageously Large Neural Networks the Sparsely-Gated Mixture-of-Experts Layer.pdf:pdf},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Outrageously Large Neural Networks: the Sparsely-Gated Mixture-of-Experts Layer}},
year = {2017}
}
@article{Shahriari2016,
author = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and de Freitas, N.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Shahriari et al. - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf:pdf},
journal = {Proceedings of the IEEE},
number = {1},
title = {{Taking the Human Out of the Loop: A Review of Bayesian Optimization}},
volume = {104},
year = {2016}
}
@misc{Etessamia,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 19. Graphs.pdf:pdf},
number = {Chapter 6},
title = {{19. Graphs}},
year = {2015}
}
@article{Gaunt2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1608.04428v1},
author = {Gaunt, A. L. and Kushman, N. and Brockschmidt, M. and Kohli, P. and Tarlow, D. and Singh, R. and Taylor, J.},
eprint = {arXiv:1608.04428v1},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Gaunt et al. - TerpreT A Probabilistic Programming Language for Program Induction.pdf:pdf},
journal = {arXiv:1608.04428},
title = {{TerpreT: A Probabilistic Programming Language for Program Induction}},
year = {2016}
}
@article{Kuck1977,
abstract = {Objective: To examine the reproducibility of 2 chronometric tests: time-dependent motor imagery (TDMI) screening test and temporal Congruence test. Design: Test-retest 10 to 14 days apart. Setting: Laboratory of a University-affiliated center for research in rehabilitation. Participants: Twenty persons post cerebrovascular accident (CVA) and 46 healthy persons (controls). Intervention: The reproducibility of the TDMI screening test, wherein the number of stepping movements (performed in sitting) imagined over 15, 25, and 45 seconds is recorded, and of the temporal congruence test wherein the duration of physically executed (E) and imagined (1) stepping movements is recorded, was evaluated. Main Outcome Measures: The test-retest reliability of the number of imagined movements (TDMI screening test), movement duration and I/E time ratios (temporal congruence test), and intrasession reliability of the temporal congruence test were assessed by using intraclass correlation coefficients (ICCs). Results: For the TDMI screening test, the ICCs ranged from .88 to .93 (CVA, n=20) and from .87 to .92 (controls, n=9). For the temporal congruence test, when the total duration of 2 series of 5 stepping movements was averaged, ICCs ranged from .76 to .97 (CVA, n=20) and from .77 to .93, (controls, n=46), whereas for 1 series the ICCs ranged from .71 to .95 and from .63 to .95 in the CVA and control groups, respectively. The ICCs for intrasession reliability for the CVA (n=20) and control (n=46) groups, respectively, ranged from .90 to .98 and .95 to .97. Conclusions: The present findings support the reproducibility of both tests in both groups. Mental chronometry can be used reliably for the screening of patients capable of motor imagery or for measuring temporal congruence between real and imagined movements poststroke.},
annote = {NULL},
author = {Kuck, D. J.},
doi = {10.1145/356683.356686},
file = {:Users/cec/Google Drive/Mendeley Library/1977 - Kuck - A Survey of Parallel Machine Organization and Programming.pdf:pdf},
issn = {03600300},
journal = {CSUR},
number = {1},
title = {{A Survey of Parallel Machine Organization and Programming}},
volume = {9},
year = {1977}
}
@article{Stratton2012,
abstract = {The Parboil benchmarks are a set of throughput computing applications useful for studying the performance of throughput computing architecture and compilers. The name comes from the culinary term for a partial cooking process, which represents our belief that useful throughput computing benchmarks must be “cooked”, or preselected to implement a scalable algorithm with fine-grained parallel tasks. But useful benchmarks for this field cannot be “fully cooked”, because the architectures and programming models and supporting tools are evolving rapidly enough that static benchmark codes will lose relevance very quickly. We have collected benchmarks from throughput computing application researchers in many different scientific and commercial fields including image processing, biomolecular simulation, fluid dynamics, and astronomy. Each benchmark includes several implementations. Some implementations we provide as readable base implementations from which new optimization efforts can begin, and others as examples of the current state-of-the-art targeting specific CPU and GPU architectures. As we continue to optimize these benchmarks for new and existing architectures ourselves, we will also gladly accept new implementations and benchmark contributions from developers to recognize those at the frontier of performance optimization on each architecture. Finally, by including versions of varying levels of optimization of the same fundamental algorithm, the bench- marks present opportunities to demonstrate tools and architectures that help programmers get the most out of their parallel hardware. Less optimized versions are presented as challenges to the compiler and architecture research communities: to develop the technology that automatically raises the performance of simpler implementations to the performance level of sophisticated programmer-optimized implementations, or demonstrate any other performance or programmability improvements. We hope that these benchmarks will facilitate effective demonstrations of such technology.},
annote = {Parboil is a benchmark suite collected from image processing, biomolecular simulation, fluid dynamics, and astronomy. Each benchmark consists of a generic "base" implementation, and current state of the art implementations targetting specific devices, e.g. CPU, OpenCL.},
author = {Stratton, J. A. and Rodrigues, C. and Sung, I. and Obeid, N. and Chang, L. and Anssari, N. and Liu, G. D. and Hwu, W. W.},
doi = {IMPACT-12-01},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Stratton et al. - Parboil A Revised Benchmark Suite for Scientific and Commercial Throughput Computing.pdf:pdf},
journal = {Center for Reliable and High-Performance Computing},
keywords = {benchmark suite,gpgpu,gpgpu benchmarks,gpgpu clusters,performance modeling},
title = {{Parboil: A Revised Benchmark Suite for Scientific and Commercial Throughput Computing}},
year = {2012}
}
@article{Xin2016,
abstract = {Human identification plays an important role in human-computer interaction. There have been numerous methods proposed for human identification (e.g., face recognition, gait recognition, fingerprint identification, etc.). While these methods could be very useful under different conditions, they also suffer from certain shortcomings (e.g., user privacy, sensing coverage range). In this paper, we propose a novel approach for human identification, which leverages WIFI signals to enable non-intrusive human identification in domestic environments. It is based on the observation that each person has specific influence patterns to the surrounding WIFI signal while moving indoors, regarding their body shape characteristics and motion patterns. The influence can be captured by the Channel State Information (CSI) time series of WIFI. Specifically, a combination of Principal Component Analysis (PCA), Discrete Wavelet Transform (DWT) and Dynamic Time Warping (DTW) techniques is used for CSI waveform-based human identification. We implemented the system in a 6m*5m smart home environment and recruited 9 users for data collection and evaluation. Experimental results indicate that the identification accuracy is about 88.9{\%} to 94.5{\%} when the candidate user set changes from 6 to 2, showing that the proposed human identification method is effective in domestic environments.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1608.03430},
author = {Xin, T. and Guo, B. and Wang, Z. and Li, M. and Yu, Z.},
eprint = {1608.03430},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Xin et al. - FreeSense Indoor Human Identification with WiFi Signals.pdf:pdf},
keywords = {Human identification,WIFI sensing,channel state,feature extraction,information,smart home},
title = {{FreeSense: Indoor Human Identification with WiFi Signals}},
url = {http://arxiv.org/abs/1608.03430},
year = {2016}
}
@inproceedings{Ansel2013,
annote = {NULL},
author = {Ansel, J. and Kamil, S. and Veeramachaneni, K. and Reilly, U. O. and Amarasinghe, S.},
booktitle = {PACT},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Ansel et al. - OpenTuner An Extensible Framework for Program Autotuning.pdf:pdf},
publisher = {ACM},
title = {{OpenTuner: An Extensible Framework for Program Autotuning}},
year = {2013}
}
@techreport{Guo2013,
abstract = {Throughout a five-month period in 2012–2013, I applied for tenure-track assistant professor positions in computer science departments throughout the United States. This document contains a mix of my personal reflections and advice on the job search process. Although it's most useful for job applicants, professors on hiring committees might also like to read the perspective of a recent (very naive!) applicant. Lots of professors have written guides on the faculty job search process, so I will try to avoid repeating their advice. In particular, I've benefited most from articles by computer scientists such as Michael Ernst [2] (my undergrad and master's thesis advisor), Matt Might [4] and John Regehr [5, 6] (my academic blogging role models), Zachary Ives [3], Jeannette Wing [10], and professors-turned-Googlers Ellen Spertus [7] and Matt Welsh [9]. UC Berkeley's career center also has a useful guide [8]. Finally, Tao Xie maintains, to my knowledge, the most comprehensive advice collection [11] for aspiring and current professors. I hope that this document complements all of these excellent resources. Two standard disclaimers: At the time of writing, I've never served on a faculty hiring committee, so I have no idea how effective my advice actually is. Also, the more similar you are to me in terms of academic background, the more relevant this document will be to your circumstances.},
annote = {NULL},
author = {Guo, Philip J},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Guo - Reflections on my tenure-track assistant professor job search.pdf:pdf},
number = {November 2012},
title = {{Reflections on my tenure-track assistant professor job search}},
year = {2013}
}
@article{Franz2007a,
abstract = {Researchers often calculate ratios of measured quantities. Specifying confidence limits for ratios is difficult and the appropriate methods are often unknown. Appropriate methods are described (Fieller, Taylor, special bootstrap methods). For the Fieller method a simple geometrical interpretation is given. Monte Carlo simulations show when these methods are appropriate and that the most frequently used methods (index method and zero-variance method) can lead to large liberal deviations from the desired confidence level. It is discussed when we can use standard regression or measurement error models and when we have to resort to specific models for heteroscedastic data. Finally, an old warning is repeated that we should be aware of the problems of spurious correlations if we use ratios.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {0710.2024},
author = {Franz, Volker H.},
eprint = {0710.2024},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Franz - Ratios A short guide to confidence limits and proper use.pdf:pdf},
keywords = {Fieller's theorem,Indices,deflated variables,spurious correlations},
month = {oct},
number = {0},
title = {{Ratios: A short guide to confidence limits and proper use}},
url = {http://arxiv.org/abs/0710.2024},
volume = {49},
year = {2007}
}
@inproceedings{Raychev2014,
abstract = {We address the problem of synthesizing code completions for pro-grams using APIs. Given a program with holes, we synthesize com-pletions for holes with the most likely sequences of method calls. Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments. Experiments show that our approach is fast and effective. Virtu-ally all computed completions typecheck, and the desired comple-tion appears in the top 3 results in 90{\%} of the cases.},
annote = {NULL},
author = {Raychev, V. and Vechev, M. and Yahav, E.},
booktitle = {PLDI},
doi = {10.1145/2594291.2594321},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Raychev, Vechev, Yahav - Code Completion with Statistical Language Models.pdf:pdf},
keywords = {Automatic Programming,Program synthesis,Software/Program Verification,Statistical methods},
title = {{Code Completion with Statistical Language Models}},
year = {2014}
}
@article{Kruger1999,
annote = {NULL},
author = {Kruger, Justin and Dunning, David},
doi = {10.1037//0022-3514.77.6.1121},
file = {:Users/cec/Google Drive/Mendeley Library/1999 - Kruger, Dunning - Unskilled and unaware of it How difficulties in recognizing one's own incompetence lead to inflated self-assess.pdf:pdf},
issn = {0022-3514},
journal = {Journal of Personality and Social Psychology},
number = {6},
title = {{Unskilled and unaware of it: How difficulties in recognizing one's own incompetence lead to inflated self-assessments.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.77.6.1121},
volume = {77},
year = {1999}
}
@article{Kossatchev2005,
abstract = {Compilers are used for creating executable modules for programs written in high-level languages; therefore, the presence of errors in a compiler is a serious danger for the quality of the software developed with the use of this compiler. As in the case of any other software, testing is one of the most important methods of quality control and error detection in compilers. The survey is devoted to methods for generating, running, and checking the quality of compiler test suites, which are based on formal specifications of the programming language syntax and semantics.},
author = {Kossatchev, A. S. and Posypkin, M. A.},
doi = {10.1007/s11086-005-0002-z},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Kossatchev, Posypkin - Survey of Compiler Testing Methods.pdf:pdf},
issn = {03617688},
journal = {Programming and Computer Software},
number = {1},
title = {{Survey of Compiler Testing Methods}},
volume = {31},
year = {2005}
}
@article{Jin2018,
abstract = {Recently a variety of methods have been developed to encode graphs into low-dimensional vectors that can be easily exploited by machine learning algorithms. The majority of these methods start by embedding the graph nodes into a low-dimensional vector space, followed by using some scheme to aggregate the node embeddings. In this work, we develop a new approach to learn graph-level representations, which includes a combination of unsupervised and supervised learning components. We start by learning a set of node representations in an unsupervised fashion. Graph nodes are mapped into node sequences sampled from random walk approaches approximated by the Gumbel-Softmax distribution. Recurrent neural network (RNN) units are modified to accommodate both the node representations as well as their neighborhood information. Experiments on standard graph classification benchmarks demonstrate that our proposed approach achieves superior or comparable performance relative to the state-of-the-art algorithms in terms of convergence speed and classification accuracy. We further illustrate the effectiveness of the different components used by our approach.},
archivePrefix = {arXiv},
arxivId = {1805.07683},
author = {Jin, Yu and JaJa, Joseph F.},
doi = {arXiv:1805.07683v4},
eprint = {1805.07683},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Jin, JaJa - Learning Graph-Level Representations with Recurrent Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Learning Graph-Level Representations with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1805.07683},
year = {2018}
}
@inproceedings{Long2015,
annote = {NULL},
author = {Long, Bo},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Long - Large Scale Machine Learning for Response Prediction ( Invited Talk ).pdf:pdf},
isbn = {9781450334051},
title = {{Large Scale Machine Learning for Response Prediction ( Invited Talk )}},
year = {2015}
}
@misc{Foster2002,
annote = {NULL},
author = {Foster, Mary Ellen},
file = {:Users/cec/Google Drive/Mendeley Library/2002 - Foster - Using LATEX to prepare an Informatics thesis.pdf:pdf},
title = {{Using LATEX to prepare an Informatics thesis}},
year = {2002}
}
@article{McGregor2014,
annote = {Cited by 38.},
author = {McGregor, Andrew},
doi = {10.1145/2627692.2627694},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - McGregor - Graph Stream Algorithms A Survey.pdf:pdf},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {1},
title = {{Graph Stream Algorithms: A Survey}},
url = {http://dl.acm.org/citation.cfm?doid=2627692.2627694},
volume = {43},
year = {2014}
}
@article{Leather2015,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 11. Parallelisation.pdf:pdf},
title = {{11. Parallelisation}},
year = {2015}
}
@misc{Bundy2014,
abstract = {This short guide is intended to assist Informatics researchers to write scientific papers, whether these are for conferences, journals, dissertations or some other purpose. Although it is not itself a scientific paper, it is based on an hypothesis: The key to successful paper writing is an explicit statement of both a scientific hypothesis and the evidence to support (or refute) it. We will show how this key idea underpins the overall structure of the paper and determines the story it tells.},
annote = {A descriptive guide to writing an experimental informatics paper. Bundy suggests that the key to a successful paper is an explicity statement of both a scientific hypothesis and the evidence to support or refute it. It then provides a guidline for the structure of a default experimental informatics paper.


A very useful checklist that should be referred back to when reviewing the drafts of my own work.},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bundy - How to Write an Informatics Paper.pdf:pdf},
title = {{How to Write an Informatics Paper}},
year = {2014}
}
@article{Wu2017,
abstract = {In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model [Mnih et al. (2016)]) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information [Lample {\&} Chaplot (2016)]. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZ- Doom AI Competition 2016 by a large margin, 35{\%} higher score than the second place.},
annote = {NULL},
author = {Wu, Y. and Tian, Y.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Wu, Tian - Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning.pdf:pdf},
journal = {Iclr},
title = {{Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning}},
url = {https://openreview.net/forum?id=Hk3mPK5gg},
year = {2017}
}
@inproceedings{Lam2016,
abstract = {Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.},
annote = {English is poor.},
author = {Lam, A. N. and Nguyen, A. T. and Nguyen, H. A. and Nguyen, T. N.},
booktitle = {ASE},
doi = {10.1109/ASE.2015.73},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lam et al. - Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports.pdf:pdf},
isbn = {9781509000241},
keywords = {Bug Localization,Bug Reports,Deep Learning,Deep Neural Network,Information Retrieval},
title = {{Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports}},
year = {2015}
}
@article{Henning2006,
annote = {NULL},
author = {Henning, John L},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Henning - SPEC CPU2006 benchmark descriptions.pdf:pdf},
journal = {ACM SIGARCH Computer Architecture News},
number = {4},
title = {{SPEC CPU2006 benchmark descriptions}},
volume = {34},
year = {2006}
}
@article{Jaderberg2016,
abstract = {In this work we present an end-to-end system for text spotting -- localising and recognising text in natural scene images -- and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.1842v1},
author = {Jaderberg, M. and Simonyan, K. and Vedaldi, A. and Zisserman, A.},
doi = {10.1007/s11263-015-0823-z},
eprint = {arXiv:1412.1842v1},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Jaderberg et al. - Reading Text in the Wild with Convolutional Neural Networks.pdf:pdf},
issn = {15731405},
journal = {IJCV},
keywords = {Convolutional neural networks,Deep learning,Synthetic data,Text detection,Text recognition,Text retrieval,Text spotting},
number = {1},
publisher = {Springer US},
title = {{Reading Text in the Wild with Convolutional Neural Networks}},
volume = {116},
year = {2016}
}
@inproceedings{Annesi2014,
annote = {NULL},
author = {Annesi, P. and Croce, D. and Basili, R.},
booktitle = {CIKM},
doi = {10.1145/2661829.2661955},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Annesi, Croce, Basili - Semantic Compositionality in Tree Kernels.pdf:pdf},
isbn = {9781450325981},
title = {{Semantic Compositionality in Tree Kernels}},
year = {2014}
}
@inproceedings{Razavian2014,
abstract = {Recent results indicate that the generic descriptors ex- tracted from the convolutional neural networks are very powerful [ 10 , 29 , 48 ]. This paper adds to the mount- ing evidence that this is indeed the case. We report on a series of experiments conducted for different recogni- tion tasks using the publicly available code and model of the OverFeat network which was trained to perform ob- ject classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image represen- tation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the orig- inal task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L 2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representa- tions are further modified using simple augmentation tech- niques e.g. jittering. The results strongly suggest that fea- tures obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, A. S. and Azizpour, H. and Sullivan, J. and Carlsson, S.},
booktitle = {CVPRW},
doi = {10.1109/CVPRW.2014.131},
eprint = {1403.6382},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Razavian et al. - CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf:pdf},
isbn = {9781479943098},
issn = {21607516},
pmid = {87882338},
publisher = {IEEE},
title = {{CNN Features off-the-shelf: an Astounding Baseline for Recognition}},
year = {2014}
}
@article{Haidl2016,
annote = {NULL},
author = {Haidl, Michael and Steuwer, Michel and Humernbrum, Tim and Gorlatch, Sergei},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Haidl et al. - Multi-Stage Programming for GPUs in Modern C using PACXX.pdf:pdf},
keywords = {GPUs,Modern C++,Multi-Stage Programming,Runtime Code Generation,Runtime Optimization},
title = {{Multi-Stage Programming for GPUs in Modern C ++ using PACXX}},
year = {2016}
}
@article{MaximilianNickelLorenzoRosasco2016a,
abstract = {Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.04935},
author = {Nickel, M. and Rosaco, L. and Poggio, T.},
eprint = {arXiv:1510.04935},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Nickel, Rosaco, Poggio - Holographic Embeddings of Knowledge Graphs.pdf:pdf},
journal = {arXiv:1510.04935v2},
title = {{Holographic Embeddings of Knowledge Graphs}},
url = {http://arxiv.org/abs/1510.04935},
year = {2016}
}
@article{Shalev-Shwartz2017,
abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
author = {Shalev-Shwartz, S. and Shamir, O. and Shammah, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Shalev-Shwartz, Shamir, Shammah - Failures of Deep Learning.pdf:pdf},
journal = {arXiv:1703.07950},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Failures of Deep Learning}},
year = {2017}
}
@article{Munkhdalai20161020,
abstract = {Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2{\%} to 2.6{\%} accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1610.06454},
author = {Munkhdalai, T. and Yu, H.},
eprint = {1610.06454},
file = {:Users/cec/Google Drive/Mendeley Library/20161020 - Munkhdalai, Yu - Reasoning with Memory Augmented Neural Networks for Language Comprehension.pdf:pdf},
journal = {arXiv},
title = {{Reasoning with Memory Augmented Neural Networks for Language Comprehension}},
year = {20161020}
}
@article{Kalchbrenner2016,
abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
author = {Kalchbrenner, N. and Espeholt, L. and Simonyan, K. and Oord, A. and Graves, A. and Kavukcuoglu, K.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Kalchbrenner et al. - Neural Machine Translation in Linear Time.pdf:pdf},
journal = {arXiv:1610.10099},
title = {{Neural Machine Translation in Linear Time}},
year = {2016}
}
@article{Knuth1984,
abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
annote = {NULL},
author = {Knuth, D. E.},
file = {:Users/cec/Google Drive/Mendeley Library/1984 - Knuth - Literate programming.pdf:pdf},
journal = {Computer},
number = {2},
title = {{Literate programming}},
volume = {27},
year = {1984}
}
@inproceedings{Chakravarty2011,
abstract = {Current GPUs are massively parallel multicore processors opti- mised for workloads with a large degree of SIMD parallelism. Good performance requires highly idiomatic programs, whose de- velopment is work intensive and requires expert knowledge. To raise the level of abstraction, we propose a domain-specific high-level language of array computations that captures appropri- ate idioms in the form of collective array operations. We embed this purely functional array language in Haskell with an online code generator for NVIDIA's CUDA GPGPU programming envi- ronment. We regard the embedded language's collective array op- erations as algorithmic skeletons; our code generator instantiates CUDA implementations of those skeletons to execute embedded array programs. This paper outlines our embedding in Haskell, details the design and implementation of the dynamic code generator, and reports on initial benchmark results. These results suggest that we can com- pete with moderately optimised native CUDA code, while enabling much simpler source programs.},
annote = {NULL},
author = {Chakravarty, Manuel MT and Keller, Gabriele and Lee, Sean and McDonell, Trevor L and Grover, Vinod},
booktitle = {DAMP},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Chakravarty et al. - Accelerating Haskell array codes with multicore GPUs.pdf:pdf},
keywords = {Arrays,Data parallelism,Dynamic compilation,GPGPU,Haskell,Skeletons},
publisher = {ACM},
title = {{Accelerating Haskell array codes with multicore GPUs}},
url = {http://dl.acm.org/citation.cfm?id=1926358},
year = {2011}
}
@article{Chen2017a,
author = {Chen, Junjie and Bai, Yanwei and Hao, Dan and Zhang, Lingming and Zhang, Lu and Xie, Bing},
doi = {10.1109/ICST.2017.45},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Chen et al. - How Do Assertions Impact Coverage-Based Test-Suite Reduction.pdf:pdf},
isbn = {9781509060313},
journal = {Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017},
keywords = {Assertions,Empirical Study,Test-suite Reduction},
pages = {418--423},
title = {{How Do Assertions Impact Coverage-Based Test-Suite Reduction?}},
year = {2017}
}
@misc{Voss,
abstract = {Hyperthreaded (HT) and simultaneous multithreaded (SMT) processors are now available in commodity worksta- tions and servers. This technology is designed to increase throughput by executing multiple concurrent threads on a single physical processor. These multiple threads share the processor's functional units and on-chip memory hierarchy in an attempt to make better use of idle resources. Most OpenMP applications have been written assuming an Sym- metric Multiprocessor (SMP), not an SMT, model. Threads executing on the same physical processor have interactions on data locality and resource sharing that do not occur on traditional SMPs. This work focuses on tuning the behavior of OpenMP applications executing on SMPs with SMT pro- cessors. We propose two adaptive loop schedulers that de- termine effective hierarchical schedulers for individual par- allel loops. We compare the performance of our two pro- posed schedulers against several standard schedulers and the per-region adaptive scheduler proposed by Zhang et al. using the SPEC and NAS OpenMP benchmark suites. We show that both of our proposed schedulers outperform all other schedulers on average, and increase speedup on av- erage by over 25{\%} when all thread contexts are used.},
annote = {NULL},
author = {Zhang, Yun and Voss, Michael and {Rogers Sr}, ES},
booktitle = {IPDPS},
doi = {10.1109/IPDPS.2005.386},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Zhang, Voss, Rogers Sr - Runtime empirical selection of loop schedulers on hyperthreaded SMPs.pdf:pdf},
isbn = {0-7695-2312-9},
publisher = {Ieee},
title = {{Runtime empirical selection of loop schedulers on hyperthreaded SMPs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1419864},
year = {2005}
}
@article{Dyer2016,
abstract = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.},
annote = {Interesting.},
author = {Dyer, C. and Kuncoro, A. and Ballesteros, M. and Smith, N. A.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Dyer et al. - Recurrent Neural Network Grammars.pdf:pdf},
isbn = {078036404X},
journal = {arXiv:1602.07776v4},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Recurrent Neural Network Grammars}},
year = {2016}
}
@inproceedings{Zhang2016c,
abstract = {Mutation testing is a powerful methodology for evaluating test suite quality. In mutation testing, a large number of mutants are generated and executed against the test suite to check the ratio of killed mutants. Therefore, mutation testing is widely believed to be a computationally expensive technique. To alleviate the efficiency concern of mutation testing, in this paper, we propose predictive mutation test-ing (PMT), the first approach to predicting mutation testing results without mutant execution. In particular, the pro-posed approach constructs a classification model based on a series of features related to mutants and tests, and uses the classification model to predict whether a mutant is killed or survived without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (i.e., cross-version and cross-project). The experimental re-sults demonstrate that PMT improves the efficiency of mu-tation testing by up to 151.4X while incurring only a small accuracy loss when predicting mutant execution results, in-dicating a good tradeoff between efficiency and effectiveness of mutation testing.},
author = {Zhang, J. and Wang, Z. and Zhang, L. and Hao, D. and Zang, L. and Cheng, S. and Zhang, L.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Zhang et al. - Predictive mutation testing.pdf:pdf},
keywords = {machine learning,mutation testing,software testing},
title = {{Predictive mutation testing}},
year = {2016}
}
@article{Baldassi2016,
abstract = {In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare---but extremely dense and accessible---regions of configurations in the network weight space. We define a novel measure, which we call the $\backslash$emph{\{}robust ensemble{\}} (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.06444},
author = {Baldassi, C. and Borgs, C. and Chayes, J. and Ingrosso, A. and Lucibello, C. and Saglietti, L. and Zecchina, R.},
eprint = {1605.06444},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Baldassi et al. - Unreasonable Effectiveness of Learning Neural Nets Accessible States and Robust Ensembles.pdf:pdf},
journal = {arXiv:1605.06444},
keywords = {()},
title = {{Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles}},
url = {http://arxiv.org/abs/1605.06444},
year = {2016}
}
@misc{UniversityofEdinburghd,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 1. Computational Complexity.pdf:pdf},
title = {{1. Computational Complexity}},
year = {2015}
}
@article{Hodosh2015,
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated. {\textcopyright} 2013 AI Access Foundation. All rights reserved.},
annote = {Cited by 124.},
author = {Hodosh, M. and Young, P. and Hockenmaier, J.},
doi = {10.1613/jair.3994},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Hodosh, Young, Hockenmaier - Framing image description as a ranking task Data, models and evaluation metrics.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI},
title = {{Framing image description as a ranking task: Data, models and evaluation metrics}},
volume = {2015-Janua},
year = {2015}
}
@article{Rossum2014d,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Python Setup and Usage.pdf:pdf},
title = {{Python Setup and Usage}},
year = {2016}
}
@inproceedings{Stuart2011,
abstract = {We present GPMR, our stand-alone MapReduce library that leverages the power of GPU clusters for large-scale computing. To better utilize the GPU, we modify MapReduce by combining large amounts of map and reduce items into chunks and using partial reductions and accumulation. We use persistent map and reduce tasks and stress aspects of GPMR with a set of standard MapReduce benchmarks. We run these benchmarks on a GPU cluster and achieve desirable speedup and efficiency for all benchmarks. We compare our implementation to the current-best GPU-MapReduce library (runs only on a solo GPU) and a highly-optimized multi-core MapReduce to show the power of GPMR. We demonstrate how typical MapReduce tasks are easily modified to fit into GPMR and leverage a GPU cluster. We highlight how total and relative amounts of communication affect GPMR. We conclude with an exposition on the types of MapReduce tasks well-suited to GPMR, and why some tasks need more modifications than others to work well with GPMR.},
annote = {NULL},
author = {Stuart, Jeff A. and Owens, John D.},
booktitle = {IPDPS},
doi = {10.1109/IPDPS.2011.102},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Stuart, Owens - Multi-GPU MapReduce on GPU clusters.pdf:pdf},
isbn = {9780769543857},
issn = {1530-2075},
title = {{Multi-GPU MapReduce on GPU clusters}},
year = {2011}
}
@inproceedings{Vachharajani,
abstract = {The recent trend in the processor industry of packing multiple pro- cessor cores in a chip has increased the importance of automatic techniques for extracting thread level parallelism. A promising ap- proach for extracting thread level parallelism in general purpose applications is to apply memory alias or value speculation to break dependences amongst threads and executes them concurrently. In this work, we present a speculative parallelization technique called Speculative Parallel Iteration Chunk execution (Spice) which relies on a novel software-only value prediction mechanism. Our value prediction technique predicts the loop live-ins of only a few iterations of a given loop, enabling speculative threads to start from those iterations. It also increases the probability of successful spec- ulation by only predicting that the values will be used as live-ins in some future iterations of the loop. These twin properties enable our value prediction scheme to have high prediction accuracies while exposing significant coarse-grained thread-level parallelism. Spice has been implemented as an automatic transformation in a research compiler. The technique results in up to 157{\%} speedup (101{\%} on average) with 4 threads.},
annote = {Cited by 24.},
author = {Vachharajani, N. and August, D. I.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Vachharajani, August - Spice Speculative Parallel Iteration Chunk Execution.pdf:pdf},
isbn = {9781595939784},
keywords = {allelization,and tools group,automatic paralleization,currently with the performance,ibm austin,multicore architectures,speculative par-,thread level parallelism,value speculation},
publisher = {IEEE},
title = {{Spice: Speculative Parallel Iteration Chunk Execution}},
year = {2008}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, D. P. and Ba, J. L.},
eprint = {1412.6980},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kingma, Ba - Adam a Method for Stochastic Optimization.pdf:pdf},
journal = {ICLR},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@inproceedings{Tesauro2005,
abstract = {This paper considers a novel application domain for rein- forcement learning: that of “autonomic computing,” i.e. self- managing computing systems. RL is applied to an online re- source allocation task in a distributed multi-application com- puting environment with independent time-varying load in each application. The task is to allocate servers in real time so as to maximize the sum of performance-based expected utility in each application. This task may be treated as a com- posite MDP, and to exploit the problem structure, a simple lo- calized RL approach is proposed, with better scalability than previous approaches. The RL approach is tested in a realistic prototype data center comprising real servers, real HTTP re- quests, and realistic time-varying demand. This domain poses a number of major challenges associated with live training in a real system, including: the need for rapid training, explo- ration that avoids excessive penalties, and handling complex, potentially non-Markovian system effects. The early results are encouraging: in overnight training, RL performs as well as or slightly better than heavily researched model-based ap- proaches derived from queuing theory.},
annote = {NULL},
author = {Tesauro, G.},
booktitle = {AAAI},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Tesauro - Online Resource Allocation Using Decompositional Reinforcement Learning.pdf:pdf},
title = {{Online Resource Allocation Using Decompositional Reinforcement Learning}},
year = {2005}
}
@article{Paper2012,
abstract = {Traditional network architectures are ill-suited to meet the requirements of today's enterprises, carriers, and end users. Thanks to a broad industry effort spearheaded by the Open Networking Foundation (ONF), Software- Defined Networking (SDN) is transforming networking architecture. In the SDN architecture, the control and data planes are decoupled, network intelligence and state are logically centralized, and the underlying network infrastructure is abstracted from the applications. As a result, enterprises and carriers gain unprecedented programmability, automation, and network control, enabling them to build highly scalable, flexible networks that readily adapt to changing business needs. The ONF is a non-profit industry consortium that is leading the advancement of SDN and standardizing critical elements of the SDN architecture such as the OpenFlow™ protocol, which structures communication between the control and data planes of supported network devices. OpenFlow is the first standard interface designed specifically for SDN, providing high-performance, granular traffic control across multiple vendors' network devices.},
annote = {A high-level technical overview of SDN, offering little insight into},
author = {Paper, O N F White},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Paper - Software-Defined Networking The New Norm for Networks.pdf:pdf},
journal = {ONF White Paper},
title = {{Software-Defined Networking: The New Norm for Networks}},
year = {2012}
}
@phdthesis{CaetanoDeOliveiraRicga2017a,
author = {{Caetano De Oliveira Ricga}, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Caetano De Oliveira Ricga - Online Iterative Compilation Guided by Work-based Profiling(2).pdf:pdf},
title = {{Online Iterative Compilation Guided by Work-based Profiling}},
year = {2017}
}
@article{Leather2014,
abstract = {Recent work has shown that machine learning can automate and in some cases outperform handcrafted compiler optimisations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This article develops a novel mechanism to automatically find those features that most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modelling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCCs hard-coded heuristic achieves only 3{\%} of the maximum performance available, whereas a state-of-the-art machine learning approach with hand-coded features obtains 59{\%}. Our feature generation technique is able to achieve 76{\%} of the maximum available speedup, outperforming existing approaches.},
annote = {They developed a simple grammar for describing features which are automatically extracted from program IR, then used genetic programming to search the space of these features, evaluating each against a machine learning algorithm and reporting the one which provided the best performance. Cited by 96.},
author = {Leather, H. and Bonilla, E. and O'Boyle, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Leather, Bonilla, O'Boyle - Automatic Feature Generation for Machine Learning Based Optimizing Compilation.pdf:pdf},
journal = {TACO},
number = {1},
publisher = {IEEE},
title = {{Automatic Feature Generation for Machine Learning Based Optimizing Compilation}},
volume = {11},
year = {2014}
}
@inproceedings{Sanchez-Gonzalez2018,
abstract = {Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.},
archivePrefix = {arXiv},
arxivId = {1806.01242},
author = {Sanchez-Gonzalez, A. and Heess, N. and Springenberg, J. T. and Merel, J. and Riedmiller, M. and Hadsell, R. and Battaglia, P.},
booktitle = {ICML},
doi = {10.2174/157340310791658785},
eprint = {1806.01242},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Sanchez-Gonzalez et al. - Graph networks as learnable physics engines for inference and control.pdf:pdf},
isbn = {1806.01242v1},
issn = {00407453},
pmid = {21804773},
title = {{Graph networks as learnable physics engines for inference and control}},
url = {http://arxiv.org/abs/1806.01242},
year = {2018}
}
@inproceedings{Wen2015,
abstract = {Heterogeneous systems consisting of multiple CPUs and GPUs are increasingly attractive as platforms for high performance computing. Such platforms are usually programmed using OpenCL which provides program portability by allowing the same program to execute on different types of device. As such systems become more mainstream, they will move from application dedicated devices to platforms that need to support multiple concurrent user applications. Here there is a need to determine when and where to map different applications so as to best utilize the available heterogeneous hardware resources. In this paper, we present an efficient OpenCL task scheduling scheme which schedules multiple kernels from multiple programs on CPU/GPU heterogeneous platforms. It does this by determining at runtime which kernels are likely to best utilize a device. We show that speedup is a good scheduling priority function and develop a novel model that predicts a kernel's speedup based on its static code structure. Our scheduler uses this prediction and runtime input data size to prioritize and schedule tasks. This technique is applied to a large set of concurrent OpenCL kernels. We evaluated our approach for system throughput and average turn-around time against competitive techniques on two different platforms: a Core i7/Nvidia GTX590 and a Core i7/AMD Tahiti 7970 platforms. For system throughput, we achieve, on average, a 1.21x and 1.25x improvement over the best competitors on the NVIDIA and AMD platforms respectively. Our approach reduces the turnaround time, on average, by at least 1.5x and 1.2x on the NVIDIA and AMD platforms respectively, when compared to alternative approaches.},
annote = {NULL},
author = {Wen, Y. and Wang, Z. and O'Boyle, M.},
booktitle = {HiPC},
doi = {10.1109/HiPC.2014.7116910},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Wen, Wang, O'Boyle - Smart Multi-Task Scheduling for OpenCL Programs on CPUGPU Heterogeneous Platforms.pdf:pdf},
isbn = {9781479959761},
keywords = {GPU,Open CL,machine learning,task scheduling},
publisher = {IEEE},
title = {{Smart Multi-Task Scheduling for OpenCL Programs on CPU/GPU Heterogeneous Platforms}},
year = {2014}
}
@misc{UniversityofEdinburgh2014m,
annote = {NULL},
author = {{University of Edinburgh}},
booktitle = {Parallel Architectures},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - 1. Parallel Architectures.pdf:pdf},
title = {{1. Parallel Architectures}},
year = {2014}
}
@article{Rossum2014a,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - Argument Clinic How-To.pdf:pdf},
title = {{Argument Clinic How-To}},
year = {2016}
}
@misc{UniversityofEdinburgh2014p,
annote = {NULL},
author = {{University of Edinburgh}},
booktitle = {Parallel Architectures},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - 4. Parallel Architectures.pdf:pdf},
title = {{4. Parallel Architectures}},
volume = {8},
year = {2014}
}
@misc{Swanson2004,
annote = {NULL},
doi = {10.1007/978-90-481-8954-0},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Unknown - Weka Primer.pdf:pdf},
isbn = {9789048189540},
issn = {0162-1459},
title = {{Weka Primer}},
year = {2004}
}
@misc{Wellein2009,
abstract = {We present a pipelined wavefront parallelization approach for stencil-based computations. Within a fixed spatial domain successive wavefronts are executed by threads scheduled to a multicore processor chip with a shared outer level cache. By re-using data from cache in the successive wavefronts this multicore-aware parallelization strategy employs temporal blocking in a simple and efficient way. We use the Jacobi algorithm in three dimensions as a prototype or stencil-based computations and prove the efficiency of our approach on the latest generations of Intel's times86 quad- and hexa-core processors.},
annote = {NULL},
author = {Wellein, G. and Hager, G. and Zeiser, T. and Wittmann, M. and Fehske, H.},
booktitle = {COMPSAC},
doi = {10.1109/COMPSAC.2009.82},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Wellein et al. - Efficient temporal blocking for stencil computations by multicore-aware wavefront parallelization.pdf:pdf},
isbn = {9780769537269},
issn = {07303157},
keywords = {Multicore,Stencil computations,Temporal blocking,Wavefront parallelization},
title = {{Efficient temporal blocking for stencil computations by multicore-aware wavefront parallelization}},
year = {2009}
}
@article{Farabet2013,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Farabet, C. and Couprie, C. and Najman, L. and LeCun, Y.},
doi = {10.1109/TPAMI.2012.231},
eprint = {1011.1669},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Farabet et al. - Learning Hierarchical Features for Scence Labeling.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {TPAMI},
pmid = {23787344},
publisher = {IEEE},
title = {{Learning Hierarchical Features for Scence Labeling}},
year = {2013}
}
@inproceedings{NairDeepMind2009,
abstract = {We present a novel reinforcement learning (RL) approach to learning a fast and highly scalable solver for a two-stage stochastic integer program in the large-scale data setting. Mixed integer programming solvers do not scale to large datasets for this problem class. Additionally, they solve each instance independently, without any knowledge transfer across instances. We address these limitations with a learnable local search solver that jointly learns two policies, one to generate an initial solution and another to iteratively improve it with local moves. The policies use contextual features for a problem instance as input, which enables learning across instances and generalization to new ones. We also propose learning a policy to compute a bound on the objective using dual decomposition. Benchmark results show that on test instances our approach rapidly achieves approximately 30{\%} to 2000{\%} better objective value, which a state of the art integer programming solver (SCIP) requires more than an order of magnitude more running time to match. Our approach also achieves better solution quality on seven out of eight benchmark problems than standard baselines such as Tabu Search and Progressive Hedging.},
author = {Nair, V. and Dvijotham, K. and Dunning, I. and Vinyals, O.},
booktitle = {UAI},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Nair et al. - Learning Fast Optimizers for Contextual Stochastic Integer Programs.pdf:pdf},
title = {{Learning Fast Optimizers for Contextual Stochastic Integer Programs}},
url = {http://auai.org/uai2018/proceedings/papers/217.pdf},
year = {2018}
}
@inproceedings{Sun2016,
abstract = {Compilers are critical, widely-used complex software. Bugs in them have significant impact, and can cause serious dam-age when they silently miscompile a safety-critical applica-tion. An in-depth understanding of compiler bugs can help detect and fix them. To this end, we conduct the first em-pirical study on the characteristics of the bugs in two main-stream compilers, GCC and LLVM. Our study is significant in scale — it exhaustively examines about 50K bugs and 30K bug fix revisions over more than a decade's span. This paper details our systematic study. Summary find-ings include: (1) In both compilers, C++ is the most buggy component, accounting for around 20{\%} of the total bugs and twice as many as the second most buggy component; (2) the bug revealing test cases are typically small, with 80{\%} having fewer than 45 lines of code; (3) most of the bug fixes touch a single source file with small modifications (43 lines for GCC and 38 for LLVM on average); (4) the average lifetime of GCC bugs is 200 days, and 111 days for LLVM; and (5) high priority tends to be assigned to optimizer bugs, most notably 30{\%} of the bugs in GCC's inter-procedural analysis component are labeled P1 (the highest priority). This study deepens our understanding of compiler bugs. For application developers, it shows that even mature pro-duction compilers still have many bugs, which may affect development. For researchers and compiler developers, it sheds light on interesting characteristics of compiler bugs, and highlights challenges and opportunities to more effec-tively test and debug compilers.},
author = {Sun, C. and Le, V. and Zhang, Q. and Su, Z.},
booktitle = {ISSTA},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Sun et al. - Toward Understanding Compiler Bugs in GCC and LLVM.pdf:pdf},
keywords = {Keywords empirical studies,compiler bugs,compiler testing},
title = {{Toward Understanding Compiler Bugs in GCC and LLVM}},
year = {2016}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Bengio - Practical recommendations for gradient-based training of deep architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
volume = {7700 LECTU},
year = {2012}
}
@article{Klockner2012,
abstract = {High-performance computing has recently seen a surge of interest in heterogeneous systems, with an emphasis on modern Graphics Processing Units (GPUs). These devices offer tremendous potential for performance and efficiency in important large-scale applications of computational science. However, exploiting this potential can be challenging, as one must adapt to the specialized and rapidly evolving computing environment currently exhibited by GPUs. One way of addressing this challenge is to embrace better techniques and develop tools tailored to their needs. This article presents one simple technique, GPU run-time code generation (RTCG), along with PyCUDA and PyOpenCL, two open-source toolkits that supports this technique. In introducing PyCUDA and PyOpenCL, this article proposes the combination of a dynamic, high-level scripting language with the massive performance of a GPU as a compelling two-tiered computing platform, potentially offering significant performance and productivity advantages over conventional single-tier, static systems. The concept of RTCG is simple and easily implemented using existing, robust infrastructure. Nonetheless it is powerful enough to support (and encourage) the creation of custom application-specific tools by its users. The premise of the paper is illustrated by a wide range of examples where the technique has been applied with considerable success. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {0911.3456},
author = {Kl{\"{o}}ckner, Andreas and Pinto, Nicolas and Lee, Yunsup and Catanzaro, Bryan and Ivanov, Paul and Fasih, Ahmed},
doi = {10.1016/j.parco.2011.09.001},
eprint = {0911.3456},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kl{\"{o}}ckner et al. - PyCUDA and PyOpenCL A scripting-based approach to GPU run-time code generation.pdf:pdf},
isbn = {0167-8191},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Automated tuning,CUDA,Code generation,GPU,High-level languages,Many-core,Massive parallelism,OpenCL,Single-instruction multiple-data,Software engineering},
number = {3},
title = {{PyCUDA and PyOpenCL: A scripting-based approach to GPU run-time code generation}},
volume = {38},
year = {2012}
}
@article{Chang2006,
abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Fi- nance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the sim- ple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we de- scribe the design and implementation of Bigtable.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chang, F. and Dean, J. and Ghemawat, S. and Hsieh, W. C. and Wallach, D. A. and Burrows, M. and Chandra, T. and Fikes, A. and Gruber, R. E.},
doi = {10.1145/1365815.1365816},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Chang et al. - Bigtable A distributed storage system for structured data.pdf:pdf},
isbn = {1931971471},
issn = {07342071},
journal = {OSDI},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Bigtable: A distributed storage system for structured data}},
url = {http://research.google.com/archive/bigtable-osdi06.pdf},
year = {2006}
}
@misc{UniversityofEdinburgh2014t,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 11.pdf:pdf},
title = {{IAML 11}},
year = {2014}
}
@incollection{Yearning-draft,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library//2018 - Ng - Machine Learning Yearning (Chapters 23-27).pdf:pdf},
title = {{Machine Learning Yearning (Chapters 23-27)}},
year = {2018}
}
@inproceedings{Benoit2005,
abstract = {We define the concepts of nesting mode and interaction mode as they arise in the description of skeletal parallel programming systems. We suggest that these new concepts encapsulate fundamental design is- sues and may play a useful role in defining and distinguishing between the capabilities of competing systems. We present the decisions taken in our own Edinburgh Skeleton Library eSkel, and review the approaches chosen by a selection of other skeleton libraries.},
annote = {NULL},
author = {Benoit, A. and Cole, M.},
booktitle = {ICCS},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Benoit, Cole - Two fundamental concepts in skeletal parallel programming.pdf:pdf},
publisher = {Springer},
title = {{Two fundamental concepts in skeletal parallel programming}},
year = {2005}
}
@inproceedings{Betts2012,
abstract = {We present a technique for verifying race- and divergence- freedom of GPU kernels that are written in mainstream ker- nel programming languages such as OpenCL and CUDA. Our approach is founded on a novel formal operational se- mantics forGPUprogramming termed synchronous, delayed visibility (SDV) semantics. The SDV semantics provides a precise definition of barrier divergence in GPU kernels and allows kernel verification to be reduced to analysis of a sequential program, thereby completely avoiding the need to reason about thread interleavings, and allowing existing modular techniques for program verification to be leveraged. We describe an efficient encoding for data race detection and propose a method for automatically inferring loop invari- ants required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, which can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 163 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, auto- matic verification of a large number of real-world kernels. Categories},
annote = {NULL},
author = {Betts, A. and Chong, N. and Donaldson, A.},
booktitle = {OOPSLA},
doi = {10.1145/2398857.2384625},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Betts, Chong, Donaldson - GPUVerify A Verifier for GPU Kernels.pdf:pdf},
isbn = {9781450315616},
issn = {15232867},
keywords = {barrier synchronization,concurrency,data races,gpus,verification},
publisher = {ACM},
title = {{GPUVerify: A Verifier for GPU Kernels}},
year = {2012}
}
@inproceedings{Sahoo2013,
abstract = {We propose an automatic diagnosis technique for isolating the root cause(s) of software failures. We use likely program invariants, automatically generated using correct inputs that are close to the fault-triggering input, to select a set of candidate program locations which are possible root causes. We then trim the set of candidate root causes using software-implemented dynamic backwards slicing, plus two new filtering heuristics: dependence filtering, and filtering via multiple failing inputs that are also close to the failing input. Experimental results on reported software bugs of three large open-source servers show that we are able to narrow down the number of candidate bug locations to between 5 and 17 program expressions, even in programs that are hundreds of thousands of lines long.},
author = {Sahoo, S. K. and Criswell, J. and Geigle, C. and Adve, V.},
booktitle = {ASPLOS},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Sahoo et al. - Using Likely Invariants for Automated Software Fault Localization.pdf:pdf},
keywords = {bug diagnosis,debugging,fault localization,invariants,program analysis,root cause,software reliability,testing},
title = {{Using Likely Invariants for Automated Software Fault Localization}},
year = {2013}
}
@article{Ghavamzadeh2015,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1405.4980},
author = {Ghavamzadeh, M. and Mannor, S. and Pineau, J. and Tamar, A.},
doi = {10.1561/2200000049},
eprint = {1405.4980},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ghavamzadeh et al. - Bayesian Reinforcement Learning A Survey.pdf:pdf},
isbn = {2200000049},
issn = {1935-8237},
journal = {arXiv:1609.04436},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Bayesian Reinforcement Learning: A Survey}},
url = {http://www.nowpublishers.com/article/Details/MAL-049},
year = {2015}
}
@inproceedings{Kim,
annote = {NULL},
author = {Kim, Jungwon and Lee, Seyong and Vetter, Jeffrey S},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Kim, Lee, Vetter - An OpenACC-Based Unified Programming Model for Multi-accelerator Systems.pdf:pdf},
isbn = {9781450332057},
keywords = {Accelerator,Heterogeneous computing,OpenACC,Programming models},
title = {{An OpenACC-Based Unified Programming Model for Multi-accelerator Systems}},
year = {2015}
}
@article{Langdon2015,
annote = {NULL},
author = {Langdon, W. B. and Harman, M.},
doi = {10.1109/5254.846288},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Langdon, Harman - Optimising Existing Software with Genetic Programming.pdf:pdf},
isbn = {0-262-11170-5 978-0-262-11170-6},
issn = {1094-7167},
journal = {TEVC},
number = {1},
publisher = {IEEE},
title = {{Optimising Existing Software with Genetic Programming}},
volume = {19},
year = {2015}
}
@article{Liu2018,
abstract = {Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on the use of graphs to represent chemical molecules, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is more successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.},
archivePrefix = {arXiv},
arxivId = {1805.09076},
author = {Liu, Q. and Allamanis, M. and Brockschmidt, M. and Gaunt, A. L.},
doi = {arXiv:1805.09076v1},
eprint = {1805.09076},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Liu et al. - Constrained Graph Variational Autoencoders for Molecule Design.pdf:pdf},
journal = {arXiv:1805.09076},
title = {{Constrained Graph Variational Autoencoders for Molecule Design}},
year = {2018}
}
@article{Ayguade2009a,
abstract = {OpenMP has been very successful in exploiting structured parallelism in applications. With increasing application complexity, there is a growing need for addressing irregular parallelism in the presence of complicated control structures. This is evident in various efforts by the industry and research communities to provide a solution to this challenging problem. One of the primary goals of OpenMP 3.0 was to define a standard dialect to express and to exploit unstructured parallelism efficiently. This paper presents the design of the OpenMP tasking model by members of the OpenMP 3.0 tasking subcommittee which was formed for this purpose. This paper summarizes the efforts of the subcommittee (spanning over two years) in designing, evaluating, and seamlessly integrating the tasking model into the OpenMP specification. In this paper, we present the design goals and key features of the tasking model, including a rich set of examples and an in-depth discussion of the rationale behind various design choices. We compare a prototype implementation of the tasking model with existing models, and evaluate it on a wide range of applications. The comparison shows that the OpenMP tasking model provides expressiveness, flexibility, and huge potential for performance and scalability.},
annote = {NULL},
author = {Ayguade, E. and Copty, Nawal and Duran, Alejandro and Hoeflinger, Jay and Massaioli, Federico and Teruel, Xavier and Unnikrishnan, Priya and Society, Ieee Computer and Lin, Yuan and Zhang, Guansong},
doi = {10.1109/TPDS.2008.105},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Ayguade et al. - The Design of OpenMP Tasks.pdf:pdf},
isbn = {2008010031},
issn = {1045-9219},
journal = {TPDS},
keywords = {Concurrent,Concurrent Programming,Concurrent programming structures,OpenMP,OpenMP 3.0,OpenMP specification,OpenMP tasks,Parallel programming,and parallel languages,application complexity,application program interfaces,computational complexity,distributed,formal specification,irregular parallelism,parallel programming,shared-memory parallel programming,systems analysis,task parallelism},
month = {mar},
number = {3},
shorttitle = {Parallel and Distributed Systems, IEEE Transaction},
title = {{The Design of OpenMP Tasks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4553700},
volume = {20},
year = {2009}
}
@phdthesis{Bash2015a,
annote = {NULL},
author = {Manson, Jeremy},
doi = {10.1017/CBO9781107415324.004},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Manson - The Java Memory Model.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{The Java Memory Model}},
year = {2004}
}
@article{Khamassi2014,
abstract = {General-purpose shared memory multicore architectures are becoming widely available. They are likely to stand as attractive alternatives to more specialized processing architectures such as FPGA and DSP-based platforms to perform real-time digital signal processing. In this paper, we show how we can ease parallelism expression on shared memory multicore architecture through the XPU high-level programming model and we describe a parallel implementation of radar signal processing application. This study case shows how we can improve programmer productivity through easing parallel programming without sacrificing performances.},
annote = {NULL},
author = {Khamassi, N and Lann, JC Le},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Khamassi, Lann - Tackling Real-Time Signal Processing Applications on Shared Memory Multicore Architectures Using XPU.pdf:pdf},
journal = {ERTS},
keywords = {Digital Signal Processing,Multicore,Parallel Construct,Parallel Parallel Programming Model,Patterns,Pipeline Parallelism,Radar,Skeleton},
title = {{Tackling Real-Time Signal Processing Applications on Shared Memory Multicore Architectures Using XPU}},
url = {http://www.erts2014.org/Site/0R4UXE94/Fichier/erts2014{\_}3A1.pdf},
year = {2014}
}
@article{Bundy,
annote = {NULL},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Bundy - A scientific Checklist.pdf:pdf},
title = {{A scientific Checklist}}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
annote = {NULL},
author = {Graves, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Graves - Generating Sequences with Recurrent Neural Networks.pdf:pdf},
journal = {arXiv:1308.0850},
title = {{Generating Sequences with Recurrent Neural Networks}},
year = {2013}
}
@article{Gregor2014,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
annote = {NULL},
author = {Gregor, K. and Danihelka, I. and Graves, A. and Rezende, D. J. and Wierstra, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Gregor et al. - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
journal = {arXiv:1502.04623},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
year = {2015}
}
@inproceedings{Das2015,
annote = {NULL},
author = {Das, Madan and Southern, Gabriel and Renau, Jose},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Das, Southern, Renau - Section Based Program Analysis to Reduce Overhead of Detecting Unsynchronized Thread Communication.pdf:pdf},
isbn = {9781450332057},
keywords = {deterministic ex-,ecution,it then identifies two,program analysis,race detection,sections as defined below,sections of the program,software transactional memory,types of thread},
title = {{Section Based Program Analysis to Reduce Overhead of Detecting Unsynchronized Thread Communication}},
year = {2015}
}
@misc{Patterson2014,
annote = {Patterson has compiled a list of common errors in postgraduate writing, drawn largely (though not entirely) from Elements of Style. He offers simple methods for checking to see if your writing falls into these traps.


A useful list, which will be valuable when reviewing my own writing.},
author = {Patterson, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Patterson - Dave Patterson's Writing Advice.pdf:pdf},
title = {{Dave Patterson's Writing Advice}},
year = {2014}
}
@article{Ernsting2012,
annote = {Muesli GPGPU. Cited by 32.},
author = {Ernsting, Steffen and Kuchen, Herbert},
journal = {IJHPCN},
number = {2},
title = {{Algorithmic skeletons for multi-core, multi-GPU systems and clusters}},
volume = {7},
year = {2012}
}
@article{Hoschele2017,
abstract = {Knowing the precise format of a program's input is a necessary prerequisite for systematic testing. Given a program and a small set of sample inputs, we (1) track the data flow of inputs to aggregate input fragments that share the same data flow through program execution into lexical and syntactic entities; (2) assign these entities names that are based on the associated variable and function identifiers; and (3) systematically generalize production rules by means of membership queries. As a result, we need only a minimal set of sample inputs to obtain human-readable context-free grammars that reflect valid input structure. In our evaluation on inputs like URLs, spreadsheets, or configuration files, our AUTOGRAM prototype obtains input grammars that are both accurate and very readable - and that can be directly fed into test generators for comprehensive automated testing.},
author = {H{\"{o}}schele, M. and Kampmann, A. and Zeller, A.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - H{\"{o}}schele, Kampmann, Zeller - Active Learning of Input Grammars.pdf:pdf},
journal = {arXiv:1708.08731},
title = {{Active Learning of Input Grammars}},
year = {2017}
}
@inproceedings{Rodrigo2015,
annote = {NULL},
author = {Rodrigo, Prabodha Srimal and Bandara, H M N Dilum},
booktitle = {HiPC},
doi = {10.1109/HiPC.2015.36},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Rodrigo, Bandara - Accelerating Complex Event Processing through GPUs.pdf:pdf},
isbn = {9781467384889},
keywords = {CUDA,Complex Event Processing,Graphics Processing,Parallelism,Units},
title = {{Accelerating Complex Event Processing through GPUs}},
year = {2015}
}
@inproceedings{Lee2014,
annote = {NULL},
author = {Lee, Shin-ying and Wu, Carole-jean},
booktitle = {PACT},
doi = {10.1145/2628071.2628107},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Lee, Wu - CAWS criticality-aware warp scheduling for GPGPU workloads.pdf:pdf},
isbn = {9781450328098},
issn = {1089795X},
keywords = {GPGPU,GPUperformance characterization,warp/wavefront scheduling},
publisher = {ACM},
title = {{CAWS: criticality-aware warp scheduling for GPGPU workloads}},
url = {http://dl.acm.org/citation.cfm?doid=2628071.2628107},
year = {2014}
}
@misc{UniversityofEdinburgh2014bb,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 12.pdf:pdf},
title = {{IAML 12}},
year = {2014}
}
@inproceedings{Bienia2009,
abstract = {The second version of the Princeton Application Repository for Shared-Memory Computers (PARSEC) has been released. PAR- SEC is a benchmark suite for Chip-Multiprocessors (CMPs) that focuses on emerging applications. It includes a diverse set of work- loads from different domains such as interactive animation or sys- tems applications that mimic large-scale commercial workloads. The next version of PARSEC features several improved and one new workload. It also supports an additional parallelization model. Many patches and changeswere includedwhich simplify the use of PARSEC in practice. The benchmarks of the new suite have higher scalability and cover a larger number of emerging applications. In this paper we discuss the major changes in detail and provide the information necessary to interpret results obtained with PARSEC 2.0 correctly.},
annote = {PARSEC is a bencharmk for CMPs which supports Pthreads, OpenMP, and TBB. See [1] for details of PARSEC 1.0.








[1] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC Benchmark Suite: Characterization and Architectural Implications,” in Proceedings of the 17th international conference on Parallel architectures and compilation techniques, 2008, pp. 72–81.},
author = {Bienia, Christian and Li, Kai},
booktitle = {MoBS},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Bienia, Li - PARSEC 2.0 A New Benchmark Suite for Chip-Multiprocessors.pdf:pdf},
keywords = {benchmark suite,multithreading,performance measurement,shared-memory computers},
title = {{PARSEC 2.0: A New Benchmark Suite for Chip-Multiprocessors}},
year = {2009}
}
@article{Totoo2014,
abstract = {This paper provides an assessment of high-level parallel programming models for multi-core programming by implementing two versions of the n-body problem. We compare three different parallel programming models based on parallel Haskell, differing in the ways how potential parallelism is identified and managed. We assess the performance of each implementation, discuss the sequential and parallel tuning steps leading to the final versions, and draw general conclusions on the suitability of high-level parallel programming models for multi-core programming.We achieve speed-ups of up to 7.2 for the all-pairs algorithm and up to 6.5 for the Barnes-Hut algorithm on an 8-core machine.},
annote = {NULL},
author = {Totoo, P and Loidl, HW},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Totoo, Loidl - Parallel Haskell implementations of the N-body problem.pdf:pdf},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {functional programming,high-level parallel programming models,n-body problem},
number = {4},
title = {{Parallel Haskell implementations of the N-body problem}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.3087/full},
volume = {26},
year = {2014}
}
@misc{Williams2008a,
annote = {NULL},
author = {Williams, C.},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Williams - Support Vector Machines.pdf:pdf},
number = {October},
title = {{Support Vector Machines}},
year = {2008}
}
@phdthesis{Ansel2014,
abstract = {The process of optimizing programs and libraries, both for performance and quality of service, can be viewed as a search problem over the space of implementation choices. This search is traditionally manually conducted by the programmer and often must be repeated when systems, tools, or requirements change. The overriding goal of this work is to automate this search so that programs can change themselves and adapt to achieve performance portability across different environments and requirements. To achieve this, first, this work presents the PetaBricks programming language which focuses on ways for expressing program implementation search spaces at the language level. Second, this work presents OpenTuner which provides sophisticated techniques for searching these search spaces in a way that can easily be adopted by other projects. PetaBricks is a implicitly parallel language and compiler where havingmultiple implementations of multiple algorithms to solve a problem is the natural way of programming. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. PetaBricks also introduces novel techniques to autotune algorithms for different convergence criteria or quality of service requirements. We show that the PetaBricks autotuner is often able to find non-intuitive poly-algorithms that outperform more traditional hand written solutions. OpenTuner is a open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. OpenTuner has been shown to perform well on complex search spaces up to 103000 possible configurations in size.},
annote = {NULL},
author = {Ansel, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Ansel - Autotuning Programs with Algorithmic Choice.pdf:pdf},
keywords = {OpenTuner,PetaBricks},
school = {Massachusetts Institute of Technology},
title = {{Autotuning Programs with Algorithmic Choice}},
year = {2014}
}
@article{Klein2016a,
abstract = {Bayesian optimization has become a successful tool for hyperparameter optimiza-tion of machine learning algorithms, such as support vector machines or deep neural networks. But it is still costly if each evaluation of the objective requires training and validating the algorithm being optimized, which, for large datasets, often takes hours, days, or even weeks. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which mod-els loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.07079},
author = {Klein, A. and Falkner, S. and Bartels, S. and Hennig, P. and Hutter, F.},
eprint = {1605.07079},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Klein et al. - Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets.pdf:pdf},
journal = {arXiv cs.LG},
title = {{Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}},
volume = {54},
year = {2016}
}
@article{Kipf2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.02907v4},
author = {Kipf, T. N. and Welling, M.},
eprint = {arXiv:1609.02907v4},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Kipf, Welling - Semi-supervised Classification with Graph Convolutional Networks.pdf:pdf},
journal = {arXiv:1609.02907},
title = {{Semi-supervised Classification with Graph Convolutional Networks}},
year = {2017}
}
@inproceedings{Besta2018,
author = {Besta, M. and Stanojevic, D. and Zivic, T. and Singh, J. and Hoerold, M. and Hoefler, T.},
booktitle = {PACT},
doi = {10.1145/3243176.3243198},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Besta et al. - Log(Graph) A Near-Optimal High-Performance Graph Representation.pdf:pdf},
isbn = {9781450359863},
title = {{Log(Graph): A Near-Optimal High-Performance Graph Representation}},
year = {2018}
}
@inproceedings{Stephenson2005,
annote = {NULL},
author = {Stephenson, M. and Amarasinghe, S.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Stephenson, Amarasinghe - Predicting Unroll Factors Using Supervised Classification.pdf:pdf},
publisher = {IEEE},
title = {{Predicting Unroll Factors Using Supervised Classification}},
year = {2005}
}
@article{Fabeiro2013,
abstract = {Nowadays, computers include several computational devices with parallel capacities, such as multicore processors and Graphic Processing Units (GPUs). OpenCL enables the programming of all these kinds of devices. An OpenCL program consists of a host code which discovers the computational devices available in the host system and it queues up commands to the devices, and the kernel code which defines the core of the parallel computation executed in the devices. This work addresses two of the most important problems faced by an OpenCL programmer: (1) hosts codes are quite verbose but they can be automatically generated if some parameters are known; (2) OpenCL codes that are hand-optimized for a given device do not get necessarily a good performance in a different one. This paper presents a source-to-source iterative optimization tool, called OCLoptimizer, that aims to generate host codes automatically and to optimize OpenCL kernels taking as inputs an annotated version of the original kernel and a configuration file. Iterative optimization is a well-known technique which allows to optimize a given code by exploring different configuration parameters in a systematic manner. For example, we can apply tiling on one loop and the iterative optimizer would select the optimal tile size by exploring the space of possible tile sizes. The experimental results show that the tool can automatically optimize a set of OpenCL kernels for multicore processors. ?? 2013 The Authors. Published by Elsevier B.V.},
annote = {NULL},
author = {Fabeiro, J. F. and Andrade, D. and Fraguela, B. B.},
doi = {10.1016/j.procs.2013.05.299},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Fabeiro, Andrade, Fraguela - OCLoptimizer An iterative optimization tool for openCL.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Genetic algorithms,Iterative optimization,OpenCL},
publisher = {Elsevier B.V.},
title = {{OCLoptimizer: An iterative optimization tool for openCL}},
url = {http://dx.doi.org/10.1016/j.procs.2013.05.299},
volume = {18},
year = {2013}
}
@inproceedings{Mikolov2015,
annote = {NULL},
author = {Mikolov, T. and Karafiat, M. and Burget, L. and Cernocky, J. and Khudanpur, S.},
booktitle = {Interspeech},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Mikolov - Recurrent Neural Network based Language Model.pdf:pdf},
title = {{Recurrent Neural Network based Language Model}},
year = {2015}
}
@inproceedings{Ponusamy1993,
abstract = { The authors describe ways in which an HPF compiler can deal with irregular computations effectively. The first mechanism invokes a user specified mapping procedure via a set of compiler directives. The directives allow the user to use program arrays to describe graph connectivity, spatial location of array elements and computational load. The second is a simple conservative method that in many cases enables a compiler to recognize that it is possible to reuse previously computed results from inspectors (e.g.  communication schedules, loop iteration partitions, information that associates off-processor data copies with on-processor buffer locations). The authors present performance results for these mechanisms from a Fortran 90D compiler implementation.},
annote = {NULL},
author = {Ponusamy, Ravi and Slatz, Joel and Choudhary, Alok},
booktitle = {SC},
doi = {10.1109/SUPERC.1993.1263480},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Ponusamy, Slatz, Choudhary - Runtime compilation techniques for data partitioning and communication schedule reuse.pdf:pdf},
isbn = {0818643404},
issn = {10639535},
title = {{Runtime compilation techniques for data partitioning and communication schedule reuse}},
year = {1993}
}
@misc{Wheeler2010,
annote = {NULL},
author = {Wheeler, Ric},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Wheeler - One Billion Files Scalability Limits in Linux File Systems.pdf:pdf},
title = {{One Billion Files: Scalability Limits in Linux File Systems}},
year = {2010}
}
@article{Pereira2015,
abstract = {{\textcopyright} 2015John Wiley {\&} Sons, Ltd. The use of Graphics Processing Units (GPUs) for high-performance computing has gained growing momentum in recent years. Unfortunately, GPU-programming platforms like Compute Unified Device Architecture (CUDA) are complex, user unfriendly, and increase the complexity of developing high-performance parallel applications. In addition, runtime systems that execute those applications often fail to fully utilize the parallelism of modern CPU-GPU systems. Typically, parallel kernels run entirely on the most powerful device available, leaving other devices idle. These observations sparked research in two directions: (1) high-level approaches to software development for GPUs, which strike a balance between performance and ease of programming; and (2) task partitioning to fully utilize the available devices. In this paper, we propose a framework, called PSkel, that provides a single high-level abstraction for stencil programming on heterogeneous CPU-GPU systems, while allowing the programmer to partition and assign data and computation to both CPU and GPU. Our current implementation uses parallel skeletons to transparently leverage Intel Threading Building Blocks (Intel Corporation, Santa Clara, CA, USA) and NVIDIA CUDA (Nvidia Corporation, Santa Clara, CA, USA). In our experiments, we observed that parallel applications with task partitioning can improve average performance by up to 76{\%} and 28{\%} compared with CPU-only and GPU-only parallel applications, respectively.},
annote = {NULL},
author = {Pereira, Alyson D. and Ramos, Luiz and Goes, Luis F. W.},
doi = {10.1002/cpe.3479},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Pereira, Ramos, Goes - PSkel A stencil programming framework for CPU-GPU systems.pdf:pdf},
issn = {15320634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {CPU-GPU systems,CUDA,Parallel skeletons,Stencil pattern,TBB,Task partitioning},
number = {7},
title = {{PSkel: A stencil programming framework for CPU-GPU systems}},
volume = {27},
year = {2015}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and joint training of deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep architectures. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Y. and Courville, A. and Vincent, P.},
doi = {3C2DBCEE-8A96-493B-B88B-36B1F52ECA58},
eprint = {1206.5538},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Bengio, Courville, Vincent - Representation Learning A Review and New Perspectives.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {TPAMI},
number = {8},
pmid = {23459267},
publisher = {IEEE},
title = {{Representation Learning: A Review and New Perspectives}},
volume = {35},
year = {2013}
}
@article{Lee2006,
annote = {Cited by 784.},
author = {Lee, Edward a},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Lee - The Problem with Threads.pdf:pdf},
journal = {Computer},
number = {5},
publisher = {IEEE},
title = {{The Problem with Threads}},
volume = {39},
year = {2006}
}
@inproceedings{Lattner2008,
annote = {Similar to "Introduction to LLVM Compiler System", but with an additional section covering OpenGL LLVM on Mac OS X.},
author = {Lattner, Chris},
booktitle = {BSDcan},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Lattner - LLVM and Clang Next Generation Compiler Technology.pdf:pdf},
title = {{LLVM and Clang : Next Generation Compiler Technology}},
year = {2008}
}
@inproceedings{Acharya,
annote = {NULL},
author = {Acharya, A.},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Acharya - PLUTO Near-Complete Modeling of Affine Transformations for Parallelism and Locality.pdf:pdf},
isbn = {9781450332057},
keywords = {affine scheduling,affine transformations,automatic,parallelization,polyhedral model,stencil computations,tiling},
title = {{PLUTO+: Near-Complete Modeling of Affine Transformations for Parallelism and Locality}},
year = {2015}
}
@misc{Recruitment,
abstract = {This guide provides practical information to those who have been asked to submit a research proposal as part of their application for admissions to a research degree as well as those who are applying to external bodies for postgraduate research funding. A research degree (e.g. Masters by Research, PhD, EdD, DMus) can be one of the best experiences of your life. What you gain along the way will serve you for the rest of life, if only to make you a more confident and knowledgeable person. In addition to making friends, meeting eminent researchers and being part of the research community, it will allow you to develop research skills as well as invaluable transferable skills which you can apply to academic life, your current employment or a variety of professions outside of academia.},
annote = {* Write clearly, with short sentences, and realistically.{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}* Ask yourself why the research is important and timely, why anyone should fund it, and how will wider society benefit from it.},
author = {Recruitment, Student},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Recruitment - How to Write a Good Postgraduate Research Proposal.pdf:pdf},
title = {{How to Write a Good Postgraduate Research Proposal}}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
annote = {NULL},
author = {Jia, Y. and Shelhamer, E. and Donahue, J. and Karayev, S. and Long, J. and Girshick, R. and Guadarrama, S. and Darrell, T.},
booktitle = {ACMMM},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Jia et al. - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@misc{UniversityofEdinburgha,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 6. Computational Complexity.pdf:pdf},
title = {{6. Computational Complexity}},
year = {2015}
}
@phdthesis{CaetanoDeOliveiraRicga2017,
author = {{Caetano De Oliveira Ricga}, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Caetano De Oliveira Ricga - Online Iterative Compilation Guided by Work-based Profiling.pdf:pdf},
title = {{Online Iterative Compilation Guided by Work-based Profiling}},
year = {2017}
}
@inproceedings{Burrows2006,
abstract = {We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed file system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences.},
annote = {NULL},
author = {Burrows, M.},
booktitle = {OSDI},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Burrows - The Chubby lock service for loosely-coupled distributed systems.pdf:pdf},
isbn = {1-931971-47-1},
issn = {{\textless}null{\textgreater}},
keywords = {consensus,distributed-systems,fault-tolerance},
title = {{The Chubby lock service for loosely-coupled distributed systems}},
url = {citeulike-article-id:6502774{\%}5Cnhttp://portal.acm.org/citation.cfm?id=1298487},
year = {2006}
}
@inproceedings{Zhuang2005,
abstract = {We examine the problem of keyboard acoustic emanations. We present a novel attack taking as input a 10-minute sound recording of a user typing English text using a keyboard, and then recovering up to 96{\%} of typed characters. There is no need for a labeled training recording. Moreover the recognizer bootstrapped this way can even recognize random text such as passwords: In our experiments, 90{\%} of 5-character random passwords using only letters can be generated in fewer than 20 attempts by an adversary; 80{\%} of 10-character passwords can be generated in fewer than 75 attempts. Our attack uses the statistical constraints of the underlying content, English language, to reconstruct text from sound recordings without any labeled training data. The attack uses a combination of standard machine learning and speech recognition techniques, including cepstrum features, Hidden Markov Models, linear classification, and feedback-based incremental learning.},
annote = {NULL},
author = {Zhuang, L. and Zhou, F. and Tygar, J. D.},
booktitle = {CCS},
doi = {10.1145/1102120.1102169},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Zhuang, Zhou, Tygar - Keyboard acoustic emanations revisited.pdf:pdf},
isbn = {1595932267},
issn = {10949224},
keywords = {Acoustic Emanations,Cepstrum,Computer Security,Electronic Eavesdropping,HMM,Hidden Markov Models,Human Factors,Keyboards,Learning Theory,Privacy,Signal Analysis},
title = {{Keyboard acoustic emanations revisited}},
url = {http://portal.acm.org/citation.cfm?doid=1102120.1102169},
year = {2005}
}
@inproceedings{Cabezas,
annote = {NULL},
author = {Cabezas, Javier and Jord{\`{a}}, Marc and Gelado, Isaac},
booktitle = {PPoPP},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Cabezas, Jord{\`{a}}, Gelado - GPU-SM Shared Memory Multi-GPU Programming Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450334075},
keywords = {gpgpu,i,o interconnects,shared memory machines},
title = {{GPU-SM : Shared Memory Multi-GPU Programming Categories and Subject Descriptors}},
year = {2015}
}
@article{Miller1998,
abstract = {Forlarge UNIX projects, the traditional method of building the project is to use recursive make. On some projects, this results in build times which are unacceptably large, when all you want to do is change one file. In examining the source of the overly long build times, it became evident that a number of apparently unrelated problems combine to pro- duce the delay,but on analysis all have the same root cause. This paper explores a number of problems regarding the use of recursive make, and shows that theyare all symptoms of the same problem. Symptoms that the UNIX com- munity have long accepted as a fact of life, but which need not be endured anylonger. These problems include recursive makeswhich take“forever” to work out that theyneed to do nothing, recursive makeswhich do too much, or too little, recursive makeswhich are overly sensitive tochanges in the source code and require constant Makefile inter- vention to keep them working. The resolution of these problems can be found by looking at what make does, from first principles, and then analyzing the effects of introducing recursive make to this activity. The analysis shows that the problem stems from the artificial partitioning of the build into separate subsets. This, in turn, leads to the symptoms described. To avoid the symptoms, it is only necessary to avoid the separation; to use a single make session to build the whole project, which is not quite the same as a single Makefile. This conclusion runs counter to much accumulated folk wisdom in building large projects on UNIX. Some of the main objections raised by this folk wisdom are examined and shown to be unfounded. The results of actual use are far more encouraging, with routine development performance improvements significantly faster than intuition may indicate, and without the intuitvely expected compromise of modularity.The use of a whole project make is not as difficult to put into practice as it may at first appear.},
annote = {NULL},
author = {Miller, Peter},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Miller - Recursive Make Considered Harmful.pdf:pdf},
journal = {AUUGN},
number = {1},
title = {{Recursive Make Considered Harmful}},
volume = {19},
year = {1998}
}
@inproceedings{Jiang2005,
abstract = {In order to utilize the tremendous computing power of graphics hardware and to automatically adapt to the fast and frequent changes in its architecture and performance characteristics, this paper implements an automatic tuning system to generate high-performance matrix-multiplication implementation on graphics hardware. The automatic tuning system uses a parameterized code generator to generate multiple versions of matrix multiplication, whose performances are empirically evaluated by actual execution on the target platform. An ad-hoc search engine is employed to search over the implementation space for the version that yields the best performance. In contrast to similar systems on CPUs, which utilize cache blocking, register tiling, instruction scheduling tuning strategies, this paper identifies and exploits several tuning strategies that are unique for graphics hardware. These tuning strategies include optimizing for multiple-render-targets, SIMD instructions with data packing, overcoming limitations on instruction count and dynamic branch instruction. The generated implementations have comparable performance with expert manually tuned version in spite of the significant overhead incurred due to the use of the high-level BrookGPU language.},
annote = {Cited by 46.},
author = {Jiang, Changhao and Snir, Marc},
booktitle = {PACT},
doi = {10.1109/PACT.2005.10},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Jiang, Snir - Automatic tuning matrix multiplication performance on graphics hardware.pdf:pdf},
isbn = {076952429X},
issn = {1089795X},
publisher = {ACM},
title = {{Automatic tuning matrix multiplication performance on graphics hardware}},
volume = {2005},
year = {2005}
}
@article{Fuller2011,
abstract = {The end of dramatic exponential growth in single-processor performance marks the end of the dominance of the single microproessor in computing. The era of sequential computing must give way to an era in which parallelism holds the forefront. Although important scientific and engineering challenges lie ahead, this is an opportune time for innovation in programming systems and computing architectures.},
annote = {NULL},
author = {Fuller, Samuel H. and Millett, Lynette I.},
doi = {10.1109/MC.2011.15},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Fuller, Millett - Computing performance Game over or next level.pdf:pdf},
isbn = {9780309159517},
issn = {00189162},
journal = {Computer},
keywords = {High-performance computing,IT Roadmap,Moore's law,Parallel computing},
number = {1},
title = {{Computing performance: Game over or next level?}},
volume = {44},
year = {2011}
}
@misc{Guide2010,
annote = {Lots of good advice in here:},
author = {Google},
doi = {doi:10.1201/9781584889304.axf},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Google - Python Style Guide.pdf:pdf},
isbn = {978-1-58488-929-8},
title = {{Python Style Guide}},
year = {2010}
}
@inproceedings{Lidbury2015a,
abstract = {We address the compiler correctness problem for many-core systems through novel applications of fuzz testing to OpenCL compilers. Focusing on two methods from prior work, random differential testing and testing via equivalence modulo inputs (EMI), we present several strategies for random generation of deterministic, communicating OpenCL kernels, and an injection mechanism that allows EMI testing to be applied to kernels that otherwise exhibit little or no dynamically-dead code. We use these methods to conduct a large, controlled testing campaign with respect to 21 OpenCL (device, compiler) configurations, covering a range of CPU, GPU, accelerator, FPGA and emulator implementations. Our study provides independent validation of claims in prior work related to the effectiveness of random differential testing and EMI testing, proposes novel methods for lifting these techniques to the many-core setting and reveals a significant number of OpenCL compiler bugs in commercial implementations.},
annote = {NULL},
author = {Lidbury, C. and Lascu, A. and Chong, N. and Donaldson, A.},
booktitle = {PLDI},
doi = {10.1145/2737924.2737986},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Lidbury et al. - Many-Core Compiler Fuzzing.pdf:pdf},
isbn = {9781450334686},
issn = {15232867},
keywords = {Compilers,GPUs,OpenCL,concurrency,metamor- phic testing,random testing},
title = {{Many-Core Compiler Fuzzing}},
year = {2015}
}
@misc{UniversityofEdinburgh2014d,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 4. Dataflow Analysis.pdf:pdf},
title = {{4. Dataflow Analysis}},
year = {2015}
}
@inproceedings{Balaprakash2013,
abstract = {Performance models have profound impact on hardware-software codesign, architectural explorations, and performance tuning of scientific applications. Developing algebraic performance models is becoming an increasingly challenging task. In such situations, a statistical surrogate-based performance model, fitted to a small number of input-output points obtained from empirical evaluation on the target machine, provides a range of benefits. Accurate surrogates can emulate the output of the expensive empirical evaluation at new inputs and therefore can be used to test and/or aid search, compiler, and autotuning algorithms. We present an iterative parallel algorithm that builds surrogate performance models for scientific kernels and workloads on single-core and multicore and multinode architectures. We tailor to our unique parallel environment an active learning heuristic popular in the literature on the sequential design of computer experiments in order to identify the code variants whose evaluations have the best potential to improve the surrogate. We use the proposed approach in a number of case studies to illustrate its effectiveness.},
annote = {They propose a technique for parallelising active learning using dynamic trees. Their algorithm relies on speculatively taking observations *assuming* that an as-yet unmade optimisation is true. This intends to overcome the sequential nature of active learning, or the problem of diminishing returns when using parallel evaluation of batches. A solid evaluation. Cited by 4.},
author = {Balaprakash, P. and Gramacy, R. B. and Wild, S. M.},
booktitle = {CLUSTER},
doi = {10.1109/CLUSTER.2013.6702683},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Balaprakash, Gramacy, Wild - Active-learning-based surrogate models for empirical performance tuning.pdf:pdf},
isbn = {978-1-4799-0898-1},
keywords = {algebra,hardware-software codesign,iterative metho},
publisher = {IEEE},
title = {{Active-learning-based surrogate models for empirical performance tuning}},
year = {2013}
}
@misc{Bundy2014d,
annote = {NULL},
author = {Bundy, Alan},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bundy - The Need for Hypothesis in Informatics.pdf:pdf},
title = {{The Need for Hypothesis in Informatics}},
url = {http://www.inf.ed.ac.uk/teaching/courses/irm/notes/hypotheses.html},
year = {2014}
}
@inproceedings{Wahib2015a,
annote = {NULL},
author = {Wahib, M. and Maruyama, N.},
booktitle = {HPDC},
doi = {10.1145/2749246.2749255},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Wahib, Maruyama - Automated GPU Kernel Transformations in Large-Scale Production Stencil Applications.pdf:pdf},
isbn = {9781450335508},
keywords = {cuda,gpu,source-to-source translation,stencil computations},
title = {{Automated GPU Kernel Transformations in Large-Scale Production Stencil Applications}},
year = {2015}
}
@article{Zhou2018,
archivePrefix = {arXiv},
arxivId = {1812.08434},
author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
doi = {arXiv:1812.08434v1},
eprint = {1812.08434},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Zhou et al. - Graph Neural Networks A Review of Methods and Applications.pdf:pdf},
pages = {1--20},
title = {{Graph Neural Networks: A Review of Methods and Applications}},
url = {https://arxiv.org/abs/1812.08434},
year = {2018}
}
@book{VanderLinden1994,
annote = {NULL},
author = {{Van der Linden}, Peter},
publisher = {Prentice Hall Professional},
title = {{Expert C programming: deep C secrets}},
year = {1994}
}
@article{Nuzman2013,
abstract = {The growing gap between the advanced capabilities of static compilers as reflected in benchmarking results and the actual performance that users experience in real-life scenariosmakes client-side dynamic optimiza- tion technologies imperative to the domain of static languages.Dynamic optimization of software distributed in the form of a platform-agnostic Intermediate-Representation (IR) has been very successful in the domain of managed languages, greatly improving upon interpreted code, especially when online profiling is used. However, can such feedback-directed IR-based dynamic code generation be viable in the domain of statically compiled, rather than interpreted, languages? We show that fat binaries, which combine the IR together with the statically compiled executable, can provide a practical solution for software vendors, allowing their software to be dynamically optimized without the limitation of binary-level approaches, which lack the high- level IR of the program, and without the warm-up costs associated with the IR-only software distribution approach.We describe and evaluate the fat-binary-based runtime compilation approach using SPECint2006, demonstrating that the overheads it incurs are low enough to be successfully surmounted by dynamic op- timization. Building on Java JIT technologies, our results already improve upon common real-world usage scenarios, including very small workloads.},
annote = {The paper describes the use of "fat binaries" (native binaries packaged together with IR) to implement efficient dynamic optimisation for C/C++. It consists of a runtime which is based on a Java JIT, uses a runtime sampling profiling and instrumentation to indenfitify host paths, and then recompiles those in a separate thread and patching call instructions to point to the optimised version. By including the native binary, they effectively reduce the startup time associated with JIT.},
author = {Nuzman, D. and Eres, R. and Dyshel, S. and Zalmanovici, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Nuzman et al. - JIT Technology with C C Feedback-Directed Dynamic Recompilation for Statically Compiled Languages.pdf:pdf},
journal = {TACO},
keywords = {Dynamic optimization,just-in-time compilation},
number = {4},
title = {{JIT Technology with C / C++: Feedback-Directed Dynamic Recompilation for Statically Compiled Languages}},
volume = {10},
year = {2013}
}
@article{Potvin2016,
abstract = {EARLY GOOGLE EMPLOYEES decided to work with a shared codebase managed through a centralized source control system. This approach has served Google well for more than 16 years, and today the vast majority of Google's software assets continues to be stored in a single, shared repository. Meanwhile, the number of Google software developers has steadily increased, and the size of the Google codebase has grown exponentially (see Figure 1). As a result, the technology used to host the codebase has also evolved significantly.},
annote = {NULL},
author = {Potvin, R. and Levenberg, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Potvin, Levenberg - Why Google Stores Billions of Lines of Code in a Single Repository.pdf:pdf},
journal = {Communications of the ACM},
number = {7},
title = {{Why Google Stores Billions of Lines of Code in a Single Repository}},
volume = {59},
year = {2016}
}
@article{Vinyalsa,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03134v2},
author = {Vinyals, O. and Fortunato, M. and Jaitly, N.},
eprint = {arXiv:1506.03134v2},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Vinyals, Fortunato, Jaitly - Pointer Networks.pdf:pdf},
journal = {arXiv:1506.03134},
title = {{Pointer Networks}}
}
@article{Tip1995,
abstract = {A program slice consists of the parts of a program that (potentially) affect the values computed at some point of interest. Such a point of interest is referred to as a slicing criterion, and is typically specified by a location in the program in combination with a subset of the programs variables. The task of computing program slices is called program slicing. The original definition of a program slice was presented by Weiser in 1979. Since then, various slightly different notions of programslices have been proposed, as well as a numberof methods to compute them. An important distinction is that between a static and a dynamic slice. Static slices are computed without making assumptions regarding a programs input, whereas the computation of dynamic slices relies on a specific test case. This survey presents an overview of program slicing, including the various general ap- proaches used to compute slices, as well as the specific techniques used to address a variety of language features such as procedures, unstructured control flow, composite data types and pointers, and concurrency. Static and dynamic slicing methods for each of these features are compared and classified in terms of their accuracy and efficiency. Moreover, the possibilities for combining solutions for different features are investigated. Recent work on the use of compiler-optimization and symbolic execution techniques for obtaining more accurate slices is discussed. The paper concludes with an overview of the applications of program slicing, which include debugging, program integration, dataflow testing, and software maintenance.},
annote = {NULL},
author = {Tip, Frank},
doi = {10.1.1.43.3782},
file = {:Users/cec/Google Drive/Mendeley Library/1995 - Tip - A Survey of Program Slicing Techniques.pdf:pdf},
isbn = {0963-9306},
journal = {Journal of Programming Languages},
title = {{A Survey of Program Slicing Techniques}},
volume = {5399},
year = {1995}
}
@inproceedings{Chakraborty2016,
abstract = {The LLVM compiler follows closely the concurrency model of C/C++ 2011, but with a crucial difference. While in C/C++ a data race between a non-atomic read and a write is declared to be undefined behavior, in LLVM such a race has defined behavior: the read returns the special 'undef' value. This subtle difference in the semantics of racy programs has profound consequences on the set of allowed program trans-formations, but it has been not formally been studied before. This work closes this gap by providing a formal memory model for a substantial fragment of LLVM and showing that it is correct as a concurrency model for a compiler intermediate language: (1) it is stronger than the C/C++ model, (2) weaker than the known hardware models, and (3) supports the expected program transformations. In order to support LLVM's semantics for racy accesses, our formal model does not work on the level of single executions as the hardware and the C/C++ models do, but rather uses more elaborate structures called event structures.},
annote = {CGO'17 Best paper nominees},
author = {Chakraborty, S. and Vafeiadis, V.},
booktitle = {CGO},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Chakraborty, Vafeiadis - Formalizing the Concurrency Semantics of an LLVM Fragment.pdf:pdf},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
publisher = {IEEE},
title = {{Formalizing the Concurrency Semantics of an LLVM Fragment}},
year = {2017}
}
@techreport{Dolan2013,
abstract = {It is well-known that the x86 instruction set is baroque, overcom- plicated, and redundantly redundant.We show just how much fluff it has by demonstrating that it remains Turing-complete when re- duced to just one instruction. The instruction we choose is mov, which can do both loads and stores. We use no unusual addressing modes, self-modifying code, or runtime code generation. Using just this instruction (and a single unconditional branch at the end of the program to make nontermination possible), we demonstrate how an arbitrary Turing machine can be simulated.},
annote = {NULL},
author = {Dolan, Stephen},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Dolan - mov is Turing-complete.pdf:pdf},
title = {{mov is Turing-complete}},
url = {http://www.cl.cam.ac.uk/{~}sd601/papers/mov.pdf},
year = {2013}
}
@article{Goldberg1991,
abstract = {Floating-point arithmetic is considered an esotoric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating- point representation and rounding error, continues with a discussion of the IEEE floating-point standard, and concludes with examples of how computer system builders can better support floating point.},
annote = {NULL},
author = {Goldberg, David},
doi = {10.1145/103162.103163},
file = {:Users/cec/Google Drive/Mendeley Library/1991 - Goldberg - What every computer scientist should know about floating-point arithmetic.pdf:pdf},
isbn = {0000000000000},
issn = {03600300},
journal = {CSUR},
keywords = {NaN,denormalized number,exception,floating-point,floating-point standard,gradual underflow,guard digit,overflow,relative error,rounding error,rounding mode,ulp,underflow},
number = {1},
pmid = {15809826},
title = {{What every computer scientist should know about floating-point arithmetic}},
volume = {23},
year = {1991}
}
@article{Allamanis2016a,
abstract = {Mutation analysis measures test suite adequacy, the degree to which a test suite detects seeded faults: one test suite is better than another if it detects more mutants. Mutation analysis effectiveness rests on the assumption that mutants are coupled with real faults i.e. mutant detection is strongly correlated with real fault detection. The work that validated this also showed that a large portion of defects remain out of reach. We introduce tailored mutation operators to reach and capture these defects. Tailored mutation operators are built from and apply to an existing codebase and its history. They can, for instance, identify and replay errors specific to the project for which they are tailored. As our point of departure, we define tailored mutation operators for identifiers, which mutation analysis has largely ignored, because there are too many ways to mutate them. Evaluated on the Defects4J dataset, our new mutation operators creates mutants coupled to 14{\%} more faults, compared to traditional mutation operators. These new mutation operators, however, quadruple the number of mutants. To combat this problem, we propose a new approach to mutant selection focusing on the location at which to apply mutation operators and the unnaturalness of the mutated code. The results demonstrate that the location selection heuristics produce mutants more closely coupled to real faults for a given budget of mutation operator applications. In summary, this paper defines and explores tailored mutation operators, advancing the state of the art in mutation testing in two ways: 1) it suggests mutation operators that mutate identifiers and literals, extending mutation analysis to a new class of faults and 2) it demonstrates that selecting the location where a mutation operator is applied decreases the number of generated mutants without affecting the coupling of mutants and real faults.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1611.02516},
author = {Allamanis, M. and Barr, E. T. and Just, R. and Sutton, C.},
eprint = {1611.02516},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Allamanis et al. - Tailored Mutants Fit Bugs Better.pdf:pdf},
journal = {arXiv:1611.02516},
keywords = {TOSKIM},
mendeley-tags = {TOSKIM},
title = {{Tailored Mutants Fit Bugs Better}},
url = {http://arxiv.org/abs/1611.02516},
year = {2016}
}
@inproceedings{Bondhugula2008,
address = {New York, New York, USA},
annote = {Cited by 317.},
author = {Artono, A. and Ramanujam, J. and Sadayappan, P.},
booktitle = {PLDI},
doi = {10.1145/1375581.1375595},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Artono, Ramanujam, Sadayappan - A practical automatic polyhedral parallelizer and locality optimizer.pdf:pdf},
isbn = {9781595938602},
keywords = {affine transformations,automatic parallelization,hedral model,locality optimization,loop transformations,poly-,tiling},
publisher = {ACM Press},
title = {{A practical automatic polyhedral parallelizer and locality optimizer}},
url = {http://portal.acm.org/citation.cfm?doid=1375581.1375595},
year = {2008}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
annote = {NULL},
author = {Cho, K. and van Merrienboer, B. and Gulcehre, C. and Bahdanau, D. and Bougares, F. and Schwenk, H. and Bengio, Y.},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1179},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Cho et al. - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
keywords = {TOREAD,decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
mendeley-tags = {TOREAD},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
year = {2014}
}
@misc{UniversityofEdinburgh2014y,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - University of Edinburgh - IAML 9.pdf:pdf},
title = {{IAML 9}},
year = {2014}
}
@article{Bernstein2005a,
annote = {NULL},
author = {Bernstein, D. J.},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Bernstein - Salsa20 speed.pdf:pdf},
title = {{Salsa20 speed}},
year = {2005}
}
@article{Jo2017,
author = {Jo, Y.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Jo - Simplifying the Bible and Wikipedia Using Statistical Machine Translation.pdf:pdf},
journal = {arXiv:1703.08646},
title = {{Simplifying the Bible and Wikipedia Using Statistical Machine Translation}},
year = {2017}
}
@article{Bayer2014,
abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1411.7610},
author = {Bayer, J. and Osendorfer, C.},
eprint = {1411.7610},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bayer, Osendorfer - Learning Stochastic Recurrent Networks.pdf:pdf},
title = {{Learning Stochastic Recurrent Networks}},
url = {http://arxiv.org/abs/1411.7610},
year = {2014}
}
@article{Vasilescu2015b,
abstract = {Continuous integration is a software engineering practice of frequently merging all developer working copies with a shared main branch, e.g., several times a day. With the advent of GitHub, a platform well known for its "social coding" features that aid collaboration and sharing, and currently the largest code host in the open source world, collaborative software development has never been more prominent. In GitHub development one can distinguish between two types of developer contributions to a project: direct ones, coming from a typically small group of developers with write access to the main project repository, and indirect ones, coming from developers who fork the main repository, update their copies locally, and submit pull requests for review and merger. In this paper we explore how GitHub developers use continuous integration as well as whether the contribution type (direct versus indirect) and different project characteristics (e.g., main programming language, or project age) are associated with the success of the automatic builds.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1512.01862},
author = {Vasilescu, Bogdan and van Schuylenburg, Stef and Wulms, Jules and Serebrenik, Alexander and van den Brand, Mark G. J.},
doi = {10.1109/ICSME.2014.62},
eprint = {1512.01862},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Vasilescu et al. - Continuous integration in a social-coding world Empirical evidence from GitHub. Updated version with correctio.pdf:pdf},
isbn = {9780769553030},
number = {June},
title = {{Continuous integration in a social-coding world: Empirical evidence from GitHub. **Updated version with corrections**}},
url = {http://arxiv.org/abs/1512.01862 http://dx.doi.org/10.1109/ICSME.2014.62},
year = {2015}
}
@book{Cole1989,
abstract = {In the past, most significant improvements in computer performance have been achieved as a result of advances in simple device technology. The introduction of large scale parallelism at the inter-processor level now represents a viable alter- native. However, this method also introduces new difficulties, most notably the conceptual barrier encountered by the user of such a system in efficiently coor- dinating many concurrent activities towards a single goal. Thus, the design and implementation of software systems which can ease this burden is of increasing importance. Such a system must find a good balance between the simplicity of the interface presented and the efficiency with which it can be implemented. This book considers existing work in the area and proposes a new approach. The new system presents the user with a selection of independent “algorithmic skeletons”, each of which describes the structure of a particular style of algorithm, in the way in which “higher order functions” represent general computational frameworks in the context of functional programming languages. The user must describe a solution to a problem as an instance of the appropriate skeleton. The implementation task is simplified by the fact that each skeleton may be considered independently, in contrast to the monolithic programming interfaces of existing systems at a similar level of abstraction. The four skeletons presented here are based on the notions of “fixed degree di- vide and conquer”, “iterative combination” “clustering” and “task queues”. Each skeleton is introduced in terms of the abstraction it presents to the user. Imple- mentation on a square grid of autonomous processor-memory pairs is considered, and examples of problems which could be solved in terms of the skeleton are presented. In conclusion, the strengths and weaknesses of the “skeletal” approach are assessed in the context of the existing alternatives.},
annote = {Description:
* Three categories of systems for creating parallelising a solution. The highly abstract systems take full responsibility for both decomposition and distribution, the idealised paralel systems take responsibility for only distribution, and the low level systems leave both decomposition and distribution to the user.
* Highly abstract systems are typically declarative, and so are neithe parallel or non-parallel. This means that "execution may progress from the initial to final representation in any fashion which maintains equivalence".
* Idealised parallel systems require the programmer to decompose the solution into a set of concurrent processes.
* Low level systems requires explitic parallelism and understanding of the harware topology in solution designs.
* Algorithmic skeletons take familliar concepts (e.g. DaC) and produces a generic version with is parallel from the ground up.
* The programmer sees each skeleton as a HOF (or program "template" for imperative base languages) which is one of a collection of algorithmic tools from which a problem specific program must be fashioned.
* To the skeleton implementor, each skeleton is a generic computational pattern for which an equivalent, efficient parallel "harness" must be defined.








Description - DaC
* Use Fixed Degree Divide and Conquer (FDCC) to pre-assign a distribution map by requiring the programmer to specify the number of subproblems (k) that a problem will generate. For example, mergesort uses k = 2 (split divides the list into halves, join merges two sublists). Cited by 1186.},
author = {Cole, M.},
file = {:Users/cec/Google Drive/Mendeley Library/1989 - Cole - Algorithmic Skeletons Structured Management of Parallel Computation.pdf:pdf},
publisher = {Pitman London},
title = {{Algorithmic Skeletons: Structured Management of Parallel Computation}},
year = {1989}
}
@article{Goodman2016,
annote = {NULL},
author = {Goodman, N. D. and Frank, M. C.},
doi = {10.1016/j.tics.2016.08.005},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Goodman, Frank - Pragmatic Language Interpretation as Probabilistic Inference.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
title = {{Pragmatic Language Interpretation as Probabilistic Inference}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S136466131630122X},
year = {2016}
}
@article{Whiteson2004,
abstract = {Computer systems are rapidly becoming so complex that maintaining them with human support staffs will be prohibitively expensive and inefficient. In response, vi- sionaries have begun proposing that computer systems be imbued with the ability to configure themselves, diagnose failures, and ultimately repair themselves in response to these failures. However, despite convincing arguments that such a shift would be desirable, as of yet there has been little concrete progress made towards this goal. We view these problems as fundamentally machine learning challenges. Hence, this article presents a new network simulator designed to study the application of machine learning methods from a system-wide perspective. We also introduce learning-based methods for addressing the problems of job routing and CPU scheduling in the networks we simulate. Our experimental results verify that methods using machine learning out- performreasonable heuristic and hand-coded approaches on example networks designed to capture many of the complexities that exist in real systems.},
annote = {Machine learning methods are applied to system-wide job routing and scheduling for optimising networks.},
author = {Whiteson, Shimon and Stone, Peter},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Whiteson, Stone - Adaptive Job Routing and Scheduling.pdf:pdf},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {autonomic computing,q-learning,reinforcement learning,routing},
number = {7},
publisher = {Elsevier},
title = {{Adaptive Job Routing and Scheduling}},
volume = {17},
year = {2004}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
annote = {NULL},
author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
booktitle = {CVPR},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - He et al. - Deep Residual Learning for Image Recognition.pdf:pdf},
keywords = {deep learning,denoising auto-encoder,image denoising},
publisher = {IEEE},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@inproceedings{Xu2014,
annote = {NULL},
author = {Xu, Q. and Annavaram, M.},
booktitle = {PACT},
doi = {10.1145/2628071.2628105},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Xu, Annavaram - PATS Pattern Aware Scheduling and Power Gating for GPGPUs.pdf:pdf},
isbn = {978-1-4503-2809-8},
issn = {1089795X},
keywords = {branch divergence,gpgpu,pattern,power gating,warp},
publisher = {ACM},
title = {{PATS: Pattern Aware Scheduling and Power Gating for GPGPUs}},
year = {2014}
}
@article{Mcauley2016a,
annote = {NULL},
author = {Mcauley, J. and Yang, A.},
doi = {10.1145/2872427.2883044},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Mcauley, Yang - Addressing Complex and Subjective Product-Related Queries with Customer Reviews.pdf:pdf},
isbn = {9781450341431},
keywords = {question answering,relevance ranking,reviews,text modeling},
title = {{Addressing Complex and Subjective Product-Related Queries with Customer Reviews}},
year = {2016}
}
@article{Mashey2004,
abstract = {For decades, computer benchmarkers have fought a War of Means. Although many have raised concerns with the geometric mean (GM), it continues to be used by SPEC and others. This war is an unnecessarymisunderstanding due to inadequately articulated implicit assumptions, plus confusio namong populations, their parameters, sampling methods, and sample statistics. In fact, all the Means have their uses, sometimes in combination. Metrics may be algebraically correct, but statistically irrelevant or misleading if applied to population distributions for which they are inappropriate. Normal (Gaussian) distributions are so useful that they are often assumed without question,but many important distributions are not normal.They require different analyses, most commonly by finding a mathematical transformations that yields a normal distribution,computing the metrics, and then back-transforming to the original scale. Consider the distribution of relative performance ratios of programs on two computers. The normal distribution is a good fit only when variance and skew are small, but otherwise generates logical impossibilities and misleading statistical measures. A much better choice is the lognormal (or log-normal) distribution, not just on theoretical grounds, but through the (necessary) validation with real data. Normal and lognormal distributions are similar for low variance and skew, but the lognormal handles skewed distributions reasonably, unlike the normal. Lognormal distributions occur frequently elsewhere are well-understood, and have standard methods of analysis.Everyone agrees that "Performance is not a single number," ... and then argues about which number is better. It is more important to understanding populations, appropriate methods, and proper ways to convey uncertainty. When population parameters are estimated via samples, statistically correct methods must be used to produce the appropriate means, measures of dispersion, Skew, confidence levels, and perhaps goodness-of-fit estimators. If the wrong Mean is chosen, it is difficult to achieve much. The GM predicts the mean relative performance of programs, not of workloads. The usual GM formula is rather unintuitive, and is often claimed to have no physical meaning. However, it is the back-transformed average of a lognormal distribution, as can be seen by the mathematical identity below. Its use is not onlystatistically appropriate in some cases, but enables straightforward computation of other useful statistics.{\textless}display equation{\textgreater}"If a man will begin in certainties, he shall end in doubts, but if he will be content to begin with doubts, he shall end with certainties." — Francis Bacon, in Savage.},
annote = {NULL},
author = {Mashey, John R.},
doi = {10.1145/1040136.1040137},
file = {:Users/cec/Google Drive/Mendeley Library/2004 - Mashey - War of the benchmark means time for a truce.pdf:pdf},
issn = {0163-5964},
journal = {ACM SIGARCH Computer Architecture News},
number = {4},
title = {{War of the benchmark means: time for a truce}},
url = {http://doi.acm.org/10.1145/1040136.1040137{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=1040137{\&}type=pdf},
volume = {32},
year = {2004}
}
@misc{Esmaeilzadeh2012,
abstract = {Since 2005, processor designers have increased core counts to exploit Moore's Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21{\%} of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50{\%}. Through 2024, only 7.9× average speedup is possible across commonly used parallel workloads, leaving a nearly 24-fold gap from a target of doubled performance per generation.},
annote = {Cited by 543.},
author = {Esmaeilzadeh, Hadi and Blem, Emily and {St. Amant}, Ren{\'{e}}e and Sankaralingam, Karthikeyan and Burger, Doug},
booktitle = {ISCA},
doi = {10.1109/MM.2012.17},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Esmaeilzadeh et al. - Dark silicon and the end of multicore scaling.pdf:pdf},
isbn = {9781450304726},
issn = {02721732},
keywords = {Moore's law,dark silicon,modeling,multicore,power,technology scaling},
title = {{Dark silicon and the end of multicore scaling}},
year = {2011}
}
@article{Pritzel2017,
abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1703.01988},
author = {Pritzel, A. and Uria, B. and Srinivasan, S. and Puigdom{\`{e}}nech, A. and Vinyals, O. and Hassabis, D. and Wierstra, D. and Blundell, C.},
eprint = {1703.01988},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Pritzel et al. - Neural Episodic Control.pdf:pdf},
journal = {arXiv:1703.01988},
keywords = {TOSTUDY},
mendeley-tags = {TOSTUDY},
title = {{Neural Episodic Control}},
year = {2017}
}
@inproceedings{Boehm2005,
abstract = {In many environments, multi-threaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.We first review why the approach almost works, and then examine some of the surprising behavior it may entail. We further illustrate that there are very simple cases in which a pure library-based approach seems incapable of expressing an efficient parallel algorithm.Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably well specified, and does not attempt to ensure type-safety, which would entail even stronger constraints. The issues we raise are not specific to that context.},
annote = {NULL},
author = {Boehm, Hans-J.},
booktitle = {PLDI},
doi = {10.1145/1064978.1065042},
file = {:Users/cec/Google Drive/Mendeley Library/2005 - Boehm - Threads cannot be implemented as a library.pdf:pdf},
isbn = {1595930566},
issn = {03621340},
keywords = {data race,optimization,pthreads,register,threads},
title = {{Threads cannot be implemented as a library}},
year = {2005}
}
@inproceedings{Yi2007a,
abstract = {The performance of many scientific applications depends on a small number of key computational kernels which require a level of efficiency rarely satisfied by existing native com- pilers. We present a new approach to high performance ker- nel optimization, where a general-purpose transformation engine automates the production of highly efficient library routines. The library routines are then empirically tested until an implementation with a satisfactory performance level is found. Our framework requires an annotated ker- nel specification and can automatically produce optimized implementations based on tuning parameters controlled by a search driver. The transformation engine includes an ex- tensive suite of optimizations which can be easily expanded using a custom transformation language. We have applied our framework to generate code for key linear algebra ker- nels and have achieved similar performance as that achieved by ATLAS's highly tuned kernels. In several cases, our ker- nels were faster than ATLAS's native kernels; we have made these kernels available to ATLAS, which results in speedups for the ATLAS library, as we show.},
address = {New York, New York, USA},
annote = {This paper presents a transformation engine for iteratively testing computational kernels in order to find the best performance. The framework requires input kernels to be annotated. The transformation engine uses the POET scripting language and is language agnostic.




The idea of a language agnostic autotuner is really nice. This implementation suffes from requring the user to laboriously annotate the kernel, which will hinder it's widespread adoption.




They only benchmark their program on a single program, and their experimental methodology and results seem suspicious.},
author = {Yi, Qing and Whaley, R. Clint},
booktitle = {LCSD},
doi = {10.1145/1512762.1512773},
file = {:Users/cec/Google Drive/Mendeley Library/2007 - Yi, Whaley - Automated transformation for performance-critical kernels.pdf:pdf},
isbn = {9781605580869},
publisher = {ACM Press},
title = {{Automated transformation for performance-critical kernels}},
url = {http://portal.acm.org/citation.cfm?doid=1512762.1512773},
year = {2007}
}
@inproceedings{Ren2015a,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1505.02074},
author = {Ren, M. and Kiros, R. and Zemel, R.},
booktitle = {NIPS},
eprint = {1505.02074},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Ren, Kiros, Zemel - Exploring Models and Data for Image Question Answering.pdf:pdf},
title = {{Exploring Models and Data for Image Question Answering}},
url = {http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering.pdf},
year = {2015}
}
@article{Ting2016,
abstract = {The success of the application of machine-learning techniques to compilation tasks can be largely attributed to the recent development and advancement of program characterization, a process that numerically or structurally quantifies a target program. While great achievements have been made in identifying key features to characterize programs, choosing a correct set of features for a specific compiler task remains an ad hoc procedure. In order to guarantee a comprehensive coverage of features, compiler engineers usually need to select excessive number of features. This, unfortunately, would potentially lead to a selection of multiple similar features, which in turn could create a new problem of bias that emphasizes certain aspects of a program's characteristics, hence reducing the accuracy and performance of the target compiler task. In this paper, we propose FEAture Selection for compilation Tasks (FEAST), an efficient and automated framework for determining the most relevant and representative features from a feature pool. Specifically, FEAST utilizes widely used statistics and machine-learning tools, including LASSO, sequential forward and backward selection, for automatic feature selection, and can in general be applied to any numerical feature set. This paper further proposes an automated approach to compiler parameter assignment for assessing the performance of FEAST. Intensive experimental results demonstrate that, under the compiler parameter assignment task, FEAST can achieve comparable results with about 18{\%} of features that are automatically selected from the entire feature pool. We also inspect these selected features and discuss their roles in program execution.},
annote = {NULL},
author = {Ting, P. and Tu, C. and Chen, P. and Lo, Y. and Cheng, S.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Ting et al. - FEAST An Automated Feature Selection Framework for Compilation Tasks.pdf:pdf},
journal = {arXiv:1610.09543},
title = {{FEAST: An Automated Feature Selection Framework for Compilation Tasks}},
year = {2016}
}
@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, We introduce a three stage pipeline: pruning, quantization and Huffman encoding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman encoding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory, which has 180x less access energy.},
annote = {NULL},
author = {Han, S. and Mao, H. and Dally, W. J.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Han, Mao, Dally - Deep Compression Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.pdf:pdf},
journal = {arXiv:1510.00149},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
year = {2015}
}
@inproceedings{Diniz1997,
abstract = {This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environ- ments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a dif- ferent optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phasemea- sures the overhead of each version in the current environment. Each production phase uses the versionwith the least overhead in the pre- vious sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment. We have implemented dynamic feedback in the context of a par- allelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchro- nization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies fromprogram to program, that the compiler is unable to stat- ically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.},
annote = {A seminal paper on dynamic optimisation. In dynamic feedback, a compiler produces several version of the source code, with different optimisation strategies. The best implementation for a given environment is then chosen by alternating between sampling and production phases.},
author = {Diniz, P. and Rinard, M.},
booktitle = {PLDI},
file = {:Users/cec/Google Drive/Mendeley Library/1997 - Diniz, Rinard - Dynamic Feedback An Effective Technique for Adaptive Computing.pdf:pdf},
publisher = {ACM},
title = {{Dynamic Feedback: An Effective Technique for Adaptive Computing}},
year = {1997}
}
@article{Asanovic2009,
abstract = {Writing programs that scale with increasing numbers of cores should be as easy as writing programs for sequential computers.},
annote = {The paper projects the future 10 years of parallel computing, with a top-down view of research, from application to software to hardware. Of particular relevance is a description of the increasing importance of autotuners, and of algorithmic skeletons and parallel patterns (p.63).},
author = {Asanovic, K. and Wawrzynek, J. and Wessel, D. and Yelick, K. and Bodik, R. and Demmel, J. and Keaveny, T. and Keutzer, K. and Kubiatowicz, J. and Morgan, N. and Patterson, D. and Sen, K.},
doi = {10.1145/1562764.1562783},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - Asanovic et al. - A view of the parallel computing landscape.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
title = {{A view of the parallel computing landscape}},
url = {http://portal.acm.org/citation.cfm?doid=1562764.1562783},
volume = {52},
year = {2009}
}
@article{He2017,
annote = {The current state of the art. Vision is now a "solved problem" lol},
author = {He, K. and Gkioxari, G. and Dollar, P. and Girshick, R.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - He et al. - Mask R-CNN.pdf:pdf},
journal = {arXiv:1703.06870},
title = {{Mask R-CNN}},
year = {2017}
}
@misc{Stork1982,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 07. Functions.pdf:pdf},
title = {{07. Functions}},
volume = {1297},
year = {2015}
}
@article{Fursin2011,
abstract = {Tuning compiler optimizations for rapidly evolving hardwaremakes port- ing and extending an optimizing compiler for each new platform extremely chal- lenging. Iterative optimization is a popular approach to adapting programs to a new architecture automatically using feedback-directed compilation. However, the large number of evaluations required for each program has prevented iterative compilation from widespread take-up in production compilers. Machine learning has been pro- posed to tune optimizations across programs systematically but is currently limited to a few transformations, long training phases and critically lacks publicly released, stable tools. Our approach is to develop a modular, extensible, self-tuning optimization infrastructure to automatically learn the best optimizations across multiple programs andarchitectures basedonthe correlationbetweenprogramfeatures, run-timebehavior and optimizations. In this paperwedescribeMilepostGCC, the first publicly-available open-source machine learning-based compiler. It consists of an Interactive Compila- tion Interface (ICI) and plugins to extract program features and exchange optimization data with the cTuning.org open public repository. It automatically adapts the inter- nal optimization heuristic at function-level granularity to improve execution time, code size and compilation time of a new program on a given architecture. Part of theMILEPOST technology together with low-level ICI-inspired plugin framework is now included in themainline GCC.We developed machine learning plugins based on probabilistic and transductive approaches to predict good combinations of optimiza- tions. Our preliminary experimental results show that it is possible to automatically reduce the execution time of individual MiBench programs, some by more than a fac- tor of 2,while also improving compilation time and code size. On average we are able to reduce the execution time of the MiBench benchmark suite by 11{\%} for the ARC reconfigurable processor.We also present a realistic multi-objective optimization sce- nario for Berkeley DB library using Milepost GCC and improve execution time by approximately 17{\%}, while reducing compilation time and code size by 12{\%} and 7{\%} respectively on Intel Xeon processor.},
annote = {NULL},
author = {Fursin, G. and Kashnikov, Y. and Memon, A. W. and Chamski, Z. and Temam, O. and Namolaru, M. and Yom-Tov, E. and Mendelson, B. and Zaks, A. and Courtois, E. and Bodin, F. and Barnard, P. and Ashton, E. and Bonilla, E. and Thomson, J. and Williams, C. K. I. and O'Boyle, M.},
doi = {10.1007/s10766-010-0161-2},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Fursin et al. - Milepost GCC Machine Learning Enabled Self-tuning Compiler.pdf:pdf},
issn = {0885-7458},
journal = {IJPP},
keywords = {Adaptive compilation,Adaptive compiler,Automatic performance tuning,Collective optimization,Continuous optimization,Empirical performance tuning,Feedback-directed compilation,Iterative compilation,Machine learning,Machine learning compiler,Multi-objective optimization,Optimization prediction,Optimization repository,Portable optimization,Program characterization,Program features,Self-tuning compiler},
number = {3},
publisher = {Springer},
title = {{Milepost GCC: Machine Learning Enabled Self-tuning Compiler}},
volume = {39},
year = {2011}
}
@inproceedings{Sorensen2016,
annote = {NULL},
author = {Sorensen, T. and Donaldson, A.},
booktitle = {PLDI},
doi = {10.1145/2908080.2908114},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Sorensen, Donaldson - Exposing Errors Related to Weak Memory in GPU Applications.pdf:pdf},
isbn = {9781450342612},
keywords = {CUDA,GPU,Nvidia,concurrency,memory fences,stress test- ing,synchronisation,weak memory},
title = {{Exposing Errors Related to Weak Memory in GPU Applications}},
year = {2016}
}
@inproceedings{Loscher2017,
abstract = {We introduce targeted property-based testing, an enhanced form of property-based testing that aims to make the input generation com-ponent of a property-based testing tool guided by a search strategy rather than being completely random. Thus, this testing technique combines the advantages of both search-based and property-based testing. We demonstrate the technique with the framework we have built, called Target, and show its effectiveness on three case studies. The first of them demonstrates how Target can employ simulated annealing to generate sensor network topologies that form configurations with high energy consumption. The second case study shows how the generation of routing trees for a wireless network equipped with directional antennas can be guided to fulfill different energy metrics. The third case study employs Target to test the noninterference property of information-flow control abstract machine designs, and compares it with a sophisticated hand-written generator for programs of these abstract machines.},
author = {L{\"{o}}scher, A. and Sagonas, K.},
booktitle = {ISSTA},
doi = {10.1145/3092703.3092711},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - L{\"{o}}scher, Sagonas - Targeted property-based testing.pdf:pdf},
isbn = {9781450350761},
keywords = {2017,acm reference format,andreas l{\"{o}}scher and konstantinos,proper,property-based testing,quickcheck,sagonas,search-based testing,targeted property-based},
title = {{Targeted property-based testing}},
url = {http://dl.acm.org/citation.cfm?doid=3092703.3092711},
year = {2017}
}
@inproceedings{Moscovici2017a,
abstract = {We propose a design for a fine-grained lock-based skiplist optimized for Graphics Processing Units (GPUs). While GPUs are often used to accelerate streaming parallel com-putations, it remains a significant challenge to efficiently offload concurrent computations with more complicated data-irregular access and fine-grained synchronization. Nat-ural building blocks for such computations would be con-current data structures, such as skiplists, which are widely used in general purpose computations. Our design utilizes array-based nodes which are accessed and updated by warp-cooperative functions, thus taking advantage of the fact that GPUs are most efficient when memory accesses are coa-lesced and execution divergence is minimized. The proposed design has been implemented, and measurements demon-strate improved performance of up to 2.6x over skiplist de-signs for the GPU existing today.},
author = {Moscovici, N. and Cohen, N. and Petrank, E.},
booktitle = {PACT},
doi = {10.1145/3018743.3019032},
file = {:Users/cec/Library/Application Support/Mendeley Desktop/Downloaded/Moscovici, Cohen, Petrank - 2017 - A GPU-Friendly Skiplist Algorithm.pdf:pdf},
isbn = {9781450344937},
keywords = {Data Structures,GPU,SIMD,Skip List},
title = {{A GPU-Friendly Skiplist Algorithm}},
year = {2017}
}
@article{Wirtzer2015,
annote = {NULL},
author = {Rossum, Guido Van},
doi = {10.1111/j.1524},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - What ' S New in Fillers What ' S New.pdf:pdf},
isbn = {1201202515},
number = {6},
title = {{What ' S New in Fillers ? What ' S New ?}},
volume = {12},
year = {2016}
}
@article{Klein2016,
abstract = {Academic publishers claim that they add value to scholarly communications by coordinating reviews and contributing and enhancing text during publication. These contributions come at a considerable cost: U.S. academic libraries paid {\$}1.7 billion for serial subscriptions in 2008 alone. Library budgets, in contrast, are flat and not able to keep pace with serial price inflation. We have investigated the publishers' value proposition by conducting a comparative study of pre-print papers and their final published counterparts. This comparison had two working assumptions: 1) if the publishers' argument is valid, the text of a pre-print paper should vary measurably from its corresponding final published version, and 2) by applying standard similarity measures, we should be able to detect and quantify such differences. Our analysis revealed that the text contents of the scientific papers generally changed very little from their pre-print to final published versions. These findings contribute empirical indicators to discussions of the added value of commercial publishers and therefore should influence libraries' economic decisions regarding access to scholarly publications.},
annote = {The growth of serial subscriptions costs is wildly outpacing the growth in library budgets. By analyzing the value proposition of publishers (coordinating reviews and enhancing text), the authors find that this cost is unjustified. To judge the value, the authors compare final published texts against arXiv preprints.},
archivePrefix = {arXiv},
arxivId = {1604.05363},
author = {Klein, Martin and Broadwell, Peter and Farb, Sharon E. and Grappone, Todd},
eprint = {1604.05363},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Klein et al. - Comparing Published Scientific Journal Articles to Their Pre-print Versions.pdf:pdf},
journal = {arXiv:1604.05363},
keywords = {open access,pre-print,publishing,similarity},
title = {{Comparing Published Scientific Journal Articles to Their Pre-print Versions}},
url = {http://arxiv.org/abs/1604.05363},
year = {2016}
}
@article{Campbell2017,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.05648},
author = {Campbell, B. A. and Treude, C.},
eprint = {1701.05648},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Campbell, Treude - NLP2Code Code Snippet Content Assist via Natural Language Tasks.pdf:pdf},
journal = {arXiv:1701.05648},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{NLP2Code: Code Snippet Content Assist via Natural Language Tasks}},
year = {2017}
}
@inproceedings{Goli2013,
abstract = {Algorithmic skeletons (‘skeletons') abstract commo; communication; and interaction. They provide top-down design comp; executing a coarse-grained resource-intensive skel; ultimately; better job makespan on heterogeneous systems due t; a widely-used skeletal framework. Our back-end all},
annote = {NULL},
author = {Goli, Mehdi and Gonz{\'{a}}lez-V{\'{e}}lez, Horacio},
booktitle = {PDP},
doi = {10.1109/PDP.2013.29},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Goli, Gonz{\'{a}}lez-V{\'{e}}lez - Heterogeneous Algorithmic Skeletons for Fast Flow with Seamless Coordination over Hybrid Architectures.pdf:pdf},
isbn = {9780769549392},
keywords = {Algorithmic Skeletons,GPU,OpenCL,Parallel Patterns,Parallel Programming,Structured Parallelism},
publisher = {IEEE},
title = {{Heterogeneous Algorithmic Skeletons for Fast Flow with Seamless Coordination over Hybrid Architectures}},
year = {2013}
}
@misc{Griebler2014,
annote = {NULL},
author = {Griebler, D. and Adornes, D. and Fernandes, L. G.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Griebler, Adornes, Fernandes - Performance and Usability Evaluation of a Pattern-Oriented Parallel Programming Interface for Mult.pdf:pdf},
title = {{Performance and Usability Evaluation of a Pattern-Oriented Parallel Programming Interface for Multi-Core Architectures}},
url = {http://gmap.pucrs.br/gmap/files/publications/slides/2dcbc413e2297fc3aa0faf7423dbdfd2.pdf},
year = {2014}
}
@article{Bacoyannis2018,
abstract = {We outline the idiosyncrasies of neural information processing and machine learning in quantitative finance. We also present some of the approaches we take towards solving the fundamental challenges we face. 1},
archivePrefix = {arXiv},
arxivId = {1811.09549v1},
author = {Bacoyannis, V. and Glukhov, V. and Jin, T. and Kochems, J. and {Re Song}, D.},
eprint = {1811.09549v1},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Bacoyannis et al. - Idiosyncrasies and challenges of data driven learning in electronic trading.pdf:pdf},
journal = {arXiv:1811.09549},
title = {{Idiosyncrasies and challenges of data driven learning in electronic trading}},
year = {2018}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, {\'{E}}.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {1726670X},
journal = {JMLR},
keywords = {Advertising,Capitalism,Cultural industry,Economics,Manipulation,Mass deception,Phishing,Phools},
number = {Oct},
pmid = {15003161},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2016}
}
@inproceedings{Heo2017a,
abstract = {—We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use a machine learning technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.},
author = {Heo, K. and Oh, H. and Yi, K.},
booktitle = {ICSE},
doi = {10.1109/ICSE.2017.54},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Heo, Oh, Yi - Machine-Learning-Guided Selectively Unsound Static Analysis.pdf:pdf},
isbn = {978-1-5386-3868-2},
keywords = {-static analysis,machine learning,unsoundness},
title = {{Machine-Learning-Guided Selectively Unsound Static Analysis}},
year = {2017}
}
@article{Wang2016,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.07905v1},
author = {Wang, S. and Jiang, J.},
eprint = {arXiv:1608.07905v1},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Wang, Jiang - Machine Comprehension Using Match-LSTM and Answer Pointer.pdf:pdf},
journal = {arXiv:1608.07905},
title = {{Machine Comprehension Using Match-LSTM and Answer Pointer}},
year = {2016}
}
@article{Wanga,
annote = {NULL},
author = {Wang, S. and Liu, T. and Tan, L.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Wang, Liu, Tan - Automatically Learning Semantic Features for Defect Prediction.pdf:pdf},
isbn = {9781450339001},
title = {{Automatically Learning Semantic Features for Defect Prediction}}
}
@inproceedings{Gregg2011,
abstract = {General purpose GPU Computing (GPGPU) has taken off in the past few years, with great promises for increased desktop processing power due to the large number of fast computing cores on high-end graphics cards. Many publications have demonstrated phenomenal performance and have reported speedups as much as 1000{\&}{\#}x00D7; over code running on multi-core CPUs. Other studies have claimed that well-tuned CPU code reduces the performance gap significantly. We demonstrate that this important discussion is missing a key aspect, specifically the question of where in the system data resides, and the overhead to move the data to where it will be used, and back again if necessary. We have benchmarked a broad set of GPU kernels on a number of platforms with different GPUs and our results show that when memory transfer times are included, it can easily take between 2 to 50{\&}{\#}x00D7; longer to run a kernel than the GPU processing time alone. Therefore, it is necessary to either include memory transfer overhead when reporting GPU performance, or to explain why this is not relevant for the application in question. We suggest a taxonomy for future CPU/GPU comparisons, and we argue that this is not only germane for reporting performance, but is important to heterogeneous scheduling research in general.},
annote = {Cited by 126.},
author = {Gregg, Chris and Hazelwood, Kim},
booktitle = {ISPASS},
doi = {10.1109/ISPASS.2011.5762730},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Gregg, Hazelwood - Where is the data Why you cannot debate CPU vs. GPU performance without the answer.pdf:pdf},
isbn = {9781612843681},
title = {{Where is the data? Why you cannot debate CPU vs. GPU performance without the answer}},
year = {2011}
}
@article{Wager2013,
abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2012) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B on the order of n{\^{}}{\{}1.5{\}} bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B on the order of n replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1311.4555},
author = {Wager, S. and Hastie, T. and Efron, B.},
eprint = {1311.4555},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Wager, Hastie, Efron - Confidence Intervals for Random Forests The Jackknife and the Infinitesimal Jackknife.pdf:pdf},
month = {nov},
title = {{Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife}},
url = {http://arxiv.org/abs/1311.4555},
year = {2013}
}
@phdthesis{MacKay1992,
abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better.$\backslash$n$\backslash$nWhen applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance.$\backslash$n$\backslash$nComparisons of the inferences of the Bayesian Framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models.$\backslash$n$\backslash$nThe relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness data measurements, in the context of both interpolation and classification problems.$\backslash$n$\backslash$nThe concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation.},
annote = {NULL},
author = {MacKay, D. J. C.},
booktitle = {Thesis},
file = {:Users/cec/Google Drive/Mendeley Library/1992 - MacKay - Bayesian Methods for Adaptive Models Thesis by.pdf:pdf},
school = {California Institute of Technology},
title = {{Bayesian Methods for Adaptive Models Thesis by}},
year = {1992}
}
@inproceedings{Steuwer2012,
abstract = {Application programming for GPUs (Graphics Processing Units) is complex and error-prone, because the popular approaches—CUDA and OpenCL—are intrinsically low-level and offer no special support for systems consisting of multiple GPUs. The SkelCL library presented in this paper is built on top of the OpenCL standard and offers pre- implemented recurring computation and communication pat- terns (skeletons) which greatly simplify programming for multi- GPU systems. The library also provides an abstract vector data type and a high-level data (re)distribution mechanism to shield the programmer from the low-level data transfers between the system's main memory and multiple GPUs. In this paper, we focus on the specific support in SkelCL for systems with multiple GPUs and use a real-world application study from the area of medical imaging to demonstrate the reduced programming effort and competitive performance of SkelCL as compared to OpenCL and CUDA. Besides, we illustrate how SkelCL adapts to large-scale, distributed heterogeneous systems in order to simplify their programming.},
annote = {This paper describes the implementation of a real-world medical imaging application with SkelCL.},
author = {Steuwer, Michel and Kegel, Philipp and Gorlatch, Sergei},
booktitle = {IPDPSW},
doi = {10.1109/IPDPSW.2012.229},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Steuwer, Kegel, Gorlatch - Towards High-Level Programming of Multi-GPU Systems Using the SkelCL Library.pdf:pdf},
isbn = {978-1-4673-0974-5},
keywords = {Algorithmic Skeletons,GPU Computing,GPU Programming,Multi-GPU,OpenCL,SkelCL,Systems},
month = {may},
publisher = {IEEE},
title = {{Towards High-Level Programming of Multi-GPU Systems Using the SkelCL Library}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6270864},
year = {2012}
}
@article{Orhan2017,
abstract = {Skip connections made the training of very deep neural networks possible and have become an indispendable compo-nent in a variety of neural architectures. A satisfactory explanation for their success remains elusive. Here, we present an explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.},
annote = {NULL},
author = {Orhan, E.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Orhan - Skip Connections as Effective Symmetry-Breaking.pdf:pdf},
journal = {arXiv:1701.09175},
title = {{Skip Connections as Effective Symmetry-Breaking}},
year = {2017}
}
@article{Jin2018,
abstract = {Recently a variety of methods have been developed to encode graphs into low-dimensional vectors that can be easily exploited by machine learning algorithms. The majority of these methods start by embedding the graph nodes into a low-dimensional vector space, followed by using some scheme to aggregate the node embeddings. In this work, we develop a new approach to learn graph-level representations, which includes a combination of unsupervised and supervised learning components. We start by learning a set of node representations in an unsupervised fashion. Graph nodes are mapped into node sequences sampled from random walk approaches approximated by the Gumbel-Softmax distribution. Recurrent neural network (RNN) units are modified to accommodate both the node representations as well as their neighborhood information. Experiments on standard graph classification benchmarks demonstrate that our proposed approach achieves superior or comparable performance relative to the state-of-the-art algorithms in terms of convergence speed and classification accuracy. We further illustrate the effectiveness of the different components used by our approach.},
archivePrefix = {arXiv},
arxivId = {1805.07683},
author = {Jin, Yu and JaJa, Joseph F.},
doi = {arXiv:1805.07683v4},
eprint = {1805.07683},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Jin, JaJa - Learning Graph-Level Representations with Recurrent Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Learning Graph-Level Representations with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1805.07683},
year = {2018}
}
@inproceedings{Price2015,
abstract = {We describe Oclgrind, a platform designed to enable the creation of developer tools for analysis and debugging of OpenCL programs. Oclgrind simulates how OpenCL kernels execute with respect to the OpenCL standard, adhering to the execution and memory models that it defines. A simple plugin interface allows developer tools to observe the sim- ulation and collect execution information to provide useful analysis, or catch bugs that would be otherwise difficult to spot when running the application on a real device. We give details about the implementation of the simula- tor, and describe how it can be extended with plugins that provide useful developer tools. We also present several ex- ample use-cases that have already been created using this platform, motivated by real-world problems that OpenCL developers face.},
annote = {NULL},
author = {Price, J. and Mcintosh-Smith, S.},
booktitle = {IWOCL},
doi = {10.1145/2791321.2791333},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Price, Mcintosh-Smith - Oclgrind An Extensible OpenCL Device Simulator.pdf:pdf},
isbn = {9781450334846},
keywords = {debugging,gpgpu,opencl,simulation,spir},
publisher = {ACM},
title = {{Oclgrind: An Extensible OpenCL Device Simulator}},
year = {2015}
}
@article{Silver2017a,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, D. and Schrittwieser, J. and Simonyan, K. and Antonoglou, I. and Huang, A. and Guez, A. and Hubert, T. and Baker, L. and Lai, M. and Bolton, A. and Chen, Y. and Lillicrap, T. and Hui, F. and Sifre, L. and {Van Den Driessche}, G. and Graepel, T. and Hassabis, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Silver et al. - Mastering the game of Go without human knowledge.pdf:pdf},
isbn = {3013372370},
journal = {Nature},
number = {7676},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
volume = {550},
year = {2017}
}
@misc{Muscat2013,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 05. Sets.pdf:pdf},
title = {{05. Sets}},
year = {2015}
}
@misc{Haller2012,
annote = {NULL},
author = {Haller, Philipp},
booktitle = {AGERE!},
doi = {10.1145/2414639.2414641},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Haller - On the integration of the actor model in mainstream technologies.pdf:pdf},
isbn = {9781450316309},
keywords = {actors,concurrent programming,distributed programming,scala,threads},
title = {{On the integration of the actor model in mainstream technologies}},
url = {http://dl.acm.org/citation.cfm?id=2414639.2414641},
year = {2012}
}
@inproceedings{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
annote = {NULL},
author = {Zaremba, W. and Sutskever, I. and Vinyals, O.},
booktitle = {ICLR},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Zaremba, Sutskever, Vinyals - Recurrent Neural Network Regularization.pdf:pdf},
keywords = {Natural Language Processing,Recurrent Neural Netw},
title = {{Recurrent Neural Network Regularization}},
year = {2014}
}
@inproceedings{Wang,
annote = {NULL},
author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
booktitle = {PPoPP},
doi = {10.1145/2621934.2621936.},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - Wang et al. - Gunrock A High-Performance Graph Processing Library on the GPU.pdf:pdf},
isbn = {9781450332057},
keywords = {developers,for use by external,github,gpu,graph processing,gunrock,io,is available in an,open-source repository at http,runtime framework},
number = {Figure 1},
title = {{Gunrock : A High-Performance Graph Processing Library on the GPU}},
year = {2015}
}
@inproceedings{Matsuzaki2006,
abstract = {With the increasing popularity of parallel programming environments such as PC clusters, more and more sequen- tial programmers, with little knowledge about parallel ar- chitectures and parallel programming, are hoping to write parallel programs. Numerous attempts have been made to develop high-level parallel programming libraries that use abstraction to hide low-level concerns and reduce difficul- ties in parallel programming. Among them, libraries of par- allel skeletons have emerged as a promising way towards this direction. Unfortunately, these libraries are not well ac- cepted by sequential programmers, because of incomplete elimination of lower-level details, ad-hoc selection of li- brary functions, unsatisfactory performance, or lack of con- vincing application examples. This paper addresses prin- ciple of designing skeleton libraries of parallel program- ming and reports implementation details and practical ap- plications of a skeleton library SkeTo. The SkeTo library is unique in its feature that it has a solid theoretical founda- tion based on the theory of Constructive Algorithmics, and is practical to be used to describe various parallel compu- tations in a sequential manner.},
annote = {This applies applies Constrive Algorithms theory to SkeTo.},
author = {Matsuzaki, Kiminori and Iwasaki, Hideya and Emoto, Kento and Hu, Zhenjiang},
booktitle = {ICST},
file = {:Users/cec/Google Drive/Mendeley Library/2006 - Matsuzaki et al. - A Library of Constructive Skeletons for Sequential Style of Parallel Programming.pdf:pdf},
publisher = {ACM},
title = {{A Library of Constructive Skeletons for Sequential Style of Parallel Programming}},
year = {2006}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, M. and Denil, M. and Gomez, S. and Hoffman, M. W. and Pfau, D. and Schaul, T. and de Freitas, N.},
doi = {10.1007/s10115-008-0151-5},
eprint = {1606.04474},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Andrychowicz et al. - Learning to learn by gradient descent by gradient descent.pdf:pdf},
isbn = {1011500801515},
issn = {0219-1377},
journal = {arXiv:1606.04474 [cs]},
number = {Nips},
pmid = {207591},
title = {{Learning to learn by gradient descent by gradient descent}},
url = {http://arxiv.org/abs/1606.04474},
year = {2016}
}
@inproceedings{Chen2014a,
annote = {NULL},
author = {Chen, J. and Hu, W. and Hao, D. and Xiong, Y. and Zhang, H. and Zhang, L. and Xie, B.},
booktitle = {ICSE},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Chen et al. - An Empirical Comparison of Compiler Testing Techniques.pdf:pdf},
isbn = {9781450339001},
title = {{An Empirical Comparison of Compiler Testing Techniques}},
year = {2016}
}
@inproceedings{Jouppi2017,
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a ​ Tensor Processing Unit (TPU) ​ — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, {\ldots}) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95{\%} of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. Index terms–DNN, MLP, CNN, RNN, LSTM, neural network, domain-specific architecture, accelerator},
annote = {First empirical comparisons of TPUs. Blog post: https://cloudplatform.googleblog.com/2017/04/quantifying-the-performance-of-the-TPU-our-first-machine-learning-chip.html},
author = {Jouppi, N. P. and Young, C. and Patil, N. and Patterson, D. and Agrawal, G. and Bajwa, R. and Bates, S. and Bhatia, S. and Boden, N. and Borchers, A. and Boyle, R. and Cantin, P. and Chao, C. and Clark, C. and Coriell, J. and Daley, M. and Dau, M. and Dean, J. and Gelb, B. and Ghaemmaghami, T. V. and Gottipati, R. and Gulland, W. and Hagmann, R. and Ho, C. R. and Hogberg, D. and Hu, J. and Hundt, R. and Hurt, D. and Ibarz, J. and Jaffey, A. and Jaworski, A. and Kaplan, A. and Khaitan, H. and Koch, A. and Kumar, N. and Lacy, S. and Laudon, J. and Law, J. and Le, D. and Leary, C. and Liu, Z. and Lucke, K. and Lundin, A. and Mackean, G. and Maggiore, A. and Mahony, M. and Miller, K. and Nagarajan, R. and Narayanaswami, R. and Ni, R. and Nix, K. and Norrie, T. and Omernick, M. and Penukonda, N. and Phelps, A. and Ross, J. and Ross, M. and Salek, A. and Samadiani, E. and Severn, C. and Sizikov, G. and Snelham, M. and Souter, J. and Steinberg, D. and Swing, A. and Tan, M. and Thorson, G. and Tian, B. and Toma, H. and Tuttle, E. and Vasudevan, V. and Walter, R. and Wang, W. and Wilcox, E. and Yoon, G. H.},
booktitle = {ISCA},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Jouppi et al. - In-Datacenter Performance Analysis of a Tensor Processing Unit​ TM.pdf:pdf},
title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit​ TM}},
year = {2017}
}
@misc{UniversityofEdinburgh2014s,
annote = {NULL},
author = {{University of Edinburgh}},
doi = {10.1145/365719.366426},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - University of Edinburgh - 7. Compiling techniques.pdf:pdf},
issn = {00010782},
title = {{7. Compiling techniques}},
volume = {9},
year = {2015}
}
@inproceedings{Ren,
annote = {NULL},
author = {Ren, J. and Gao, L. and Wang, Z.},
booktitle = {INFOCOM},
file = {:Users/cec/Google Drive/Mendeley Library/2017 - Ren, Gao, Wang - Optimise Web Browsing on Heterogeneous Mobile Platforms A Machine Learning Based Approach.pdf:pdf},
keywords = {big.LITTLE,energy optimisation,mobile web browsing,mobile workloads},
title = {{Optimise Web Browsing on Heterogeneous Mobile Platforms: A Machine Learning Based Approach}},
year = {2017}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
author = {Dumoulin, V. and Visin, F.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Dumoulin, Visin - A guide to Convolution Arithmetic for Deep Learning.pdf:pdf},
journal = {arXiv:1603.07285},
title = {{A guide to Convolution Arithmetic for Deep Learning}},
year = {2016}
}
@inproceedings{Mohr2003,
annote = {NULL},
author = {Mohr, B. and Wolf, F.},
booktitle = {Euro-Par},
file = {:Users/cec/Google Drive/Mendeley Library/2003 - Mohr, Wolf - KOJAK - A Tool Set for Automatic Performance Analysis of Parallel Programs.pdf:pdf},
title = {{KOJAK - A Tool Set for Automatic Performance Analysis of Parallel Programs}},
year = {2003}
}
@article{Darlington1993b,
abstract = {Prograxnming parallel machines is notoriously difficult. Factors contribut- ing to this difficulty include the complexity of concurrency, the effect of resource allocation on performance and the current diversity of parallel machine models. The net result is that effective portability, which de- pends crucially on the predictability of performance, has been lost. Functional programming languages have been put forward as solutions to these problems, because of the availability of implicit parallelism. How- ever, performance will be generally poor unless the issue of resource alloca- tion is addressed explicitly, diminishing the advantage of using a functional language in the first place. We present a methodology which is a compromise between the extremes of explicit imperative programming and implicit functional programming. We use a repertoire of higher-order parallel forms, skeletons, as the basic building blocks for parallel implementations and provide program transfor- mations which can convert between skeletons, giving portability between differing machines. Resource allocation issues are documented for each skeleton/machine pair and are addressed explicitly during implementation in an interactive, selective manner, rather than by explicit programming.},
annote = {NULL},
author = {Darlington, J and Field, A and Harrison, P},
doi = {10.1007/3-540-56891-3_12},
file = {:Users/cec/Google Drive/Mendeley Library/1993 - Darlington, Field, Harrison - Parallel programming using skeleton functions(2).pdf:pdf},
isbn = {3540568913},
journal = {PARLE},
number = {3},
title = {{Parallel programming using skeleton functions}},
url = {http://www.springerlink.com/index/7702G7745Q12362W.pdf},
year = {1993}
}
@inproceedings{Oleynik2014,
address = {Cham},
annote = {Cited by 1.},
author = {Oleynik, Y. and Mijakovi, R. and Ure{\~{n}}a, I. A. C. and Firbach, M. and Gerndt, M.},
booktitle = {HLRS},
doi = {10.1007/978-3-319-08144-1},
editor = {Kn{\"{u}}pfer, Andreas and Gracia, Jos{\'{e}} and Nagel, Wolfgang E. and Resch, Michael M.},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Oleynik et al. - Recent Advances in Periscope for Performance Analysis and Tuning.pdf:pdf},
isbn = {978-3-319-08143-4},
publisher = {Springer International Publishing},
title = {{Recent Advances in Periscope for Performance Analysis and Tuning}},
url = {http://link.springer.com/10.1007/978-3-319-08144-1},
year = {2014}
}
@article{Sander2012,
annote = {NULL},
author = {Sander, Ben},
journal = {AMD Fusion Developer Summit},
title = {{Bolt: A C++ Templater Library for HSA}},
volume = {12},
year = {2012}
}
@article{Zhanga,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.05703v2},
author = {Zhang, N. and Paluri, M. and Taigman, Y. and Fergus, R. and Bourdev, L.},
eprint = {arXiv:1501.05703v2},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Zhang et al. - Beyond Frontal Faces Improving Person Recognition Using Multiple Cues.pdf:pdf},
title = {{Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues}}
}
@inproceedings{Leather2008,
abstract = {Heuristics in compilers are often designed by manually analyzing sample programs. Recent advances have successfully applied machine learning to automatically generate heuristics. The typical format of these approaches reduces the input loops, functions or programs to a finite vector of features. A machine learning algorithm then learns a mapping from these features to the desired heuristic parameters. Choosing the right features is important and requires expert knowledge since no machine learning tool will work well with poorly chosen features. This paper introduces a novel mechanism to generate features. Grammars describ- ing languages of features are defined and from these grammars sentences are randomly produced. The features are then evaluated over input data and computed values are given to machine learning tools. We propose the construction of domain specific feature languages for different pur- poses in different parts of the compiler. Using these feature languages, complex, ma- chine generated features are extracted from program code. Using our observation that some functions can benefit from setting different compiler options, while others cannot, we demonstrate the use of a decision tree classifier to automatically identify the former using the automatically generated features. We showthat our method outperform human generated features on problems of loop unrolling and phase ordering, achieving a statistically significant decrease in run-time compared to programs compiled using GCC's heuristics.},
annote = {NULL},
author = {Leather, H. and Yom-tov, E. and Namolaru, M. and Freund, A.},
booktitle = {SMART},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Leather et al. - Automatic Feature Generation for Setting Compilers Heuristics.pdf:pdf},
title = {{Automatic Feature Generation for Setting Compilers Heuristics}},
year = {2008}
}
@article{Aldinucci2012,
abstract = {We propose a data flow based run time system as an efficient tool for supporting execution of parallel code on heterogeneous architectures hosting both multicore CPUs and GPUs. We discuss how the proposed run time system may be the target of both structured parallel applications developed using algorithmic skeletons/parallel design patterns and also more “domain specific” programming models. Experimental results demonstrating the feasibility of the approach are presented.},
annote = {This paper describes an attempt at implementing dataflow parallel processing for heterogeneous architectures.




Pretty boring, but I only gave it a quick skim.},
author = {Aldinucci, M. and Svizzera, C. S. and Danelutto, M. and Kilpatrick, P. and Torquati, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Aldinucci et al. - Targeting heterogeneous architectures via macro data flow.pdf:pdf},
journal = {Parallel Processing Letters},
keywords = {algorithmic skeletons,data flow,heterogeneous architectures,parallel design pat- terns,structured parallelism},
number = {02},
publisher = {World Scientific},
title = {{Targeting heterogeneous architectures via macro data flow}},
volume = {22},
year = {2012}
}
@inproceedings{McAuley2015,
abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1506.04757},
author = {McAuley, J. and Targett, C. and Shi, Q. and Hengel, A. V. D.},
booktitle = {SIGIR},
doi = {10.1145/2766462.2767755},
eprint = {1506.04757},
file = {:Users/cec/Google Drive/Mendeley Library/2015 - McAuley et al. - Image-based Recommendations on Styles and Substitutes.pdf:pdf},
isbn = {9781450336215},
title = {{Image-based Recommendations on Styles and Substitutes}},
url = {http://arxiv.org/abs/1506.04757},
year = {2015}
}
@article{Kl2010a,
annote = {NULL},
author = {Kl, A. and Brown, J. H. and Garcia, D.},
file = {:Users/cec/Google Drive/Mendeley Library/2012 - Kl, Brown, Garcia - Scripting GPUs with PyOpenCL.pdf:pdf},
journal = {Parallel Computing},
number = {3},
title = {{Scripting GPUs with PyOpenCL}},
volume = {38},
year = {2012}
}
@article{Fousek2011,
abstract = {When implementing a function mapping on the contemporary GPU, several contradictory performance factors affecting distribution of computation into GPU kernels have to be balanced. A decomposition-fusion scheme suggests to decompose computational problem to be solved by several simple functions implemented as standalone kernels and to fuse some of these functions later into more complex kernels to improve memory locality.$\backslash$nIn this paper, a prototype of source-to-source compiler automating the fusion phase is presented and the im-$\backslash$npact of fusions generated by the compiler as well as compiler efficiency is experimentally evaluated.},
annote = {NULL},
author = {Fousek, Jan and Filipovi{\v{c}}, Jiři and Madzin, Matu{\v{s}}},
doi = {10.1145/2082156.2082183},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Fousek, Filipovi{\v{c}}, Madzin - Automatic fusions of CUDA-GPU kernels for parallel map.pdf:pdf},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
number = {4},
title = {{Automatic fusions of CUDA-GPU kernels for parallel map}},
volume = {39},
year = {2011}
}
@inproceedings{Magni2,
abstract = {OpenCL has become the de-facto data parallel programming model for parallel devices in today's high-performance su- percomputers. OpenCL was designed with the goal of guar- anteeing program portability across hardware from different vendors. However, achieving good performance is hard, re- quiring manual tuning of the program and expert knowledge of each target device. In this paper we consider a data parallel compiler trans- formation — thread-coarsening — and evaluate its effects across a range of devices by developing a source-to-source OpenCL compiler based on LLVM. We thoroughly evaluate this transformation on 17 benchmarks and five platforms with different coarsening parameters giving over 43,000 different experiments. We achieve speedups over 9x on indi- vidual applications and average speedups ranging from 1.15x on the Nvidia Kepler GPU to 1.50x on the AMD Cypress GPU. Finally, we use statistical regression to analyse and explain program performance in terms of hardware-based performance counters.},
annote = {Magni develops a source-to-source thread coarsening transformation for LLVM's OpenCL compiler, and evaluates the effects on performance using regression trees on 5 architectures across 17 benchmarks. The transformation is a function-pass which performance divergence analysis to determine which instructions differ between threads, then duplicates those instructions in the function body, changing the index variables so that memory access is coalesced. The evaluation compares speedups for each device, and uses performance counters to measure the number of SIMD instructions, memory loads, branches, and cache utilisation. Cited by 15.},
author = {Magni, A. and Dubach, C. and O'Boyle, M.},
booktitle = {SC},
doi = {10.1145/2503210.2503268},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Magni, Dubach, O'Boyle - A Large-Scale Cross-Architecture Evaluation of Thread-Coarsening.pdf:pdf},
isbn = {9781450323789},
issn = {21674337},
keywords = {gpu,opencl,regression trees,thread coarsening},
title = {{A Large-Scale Cross-Architecture Evaluation of Thread-Coarsening}},
year = {2013}
}
@unpublished{Zhao,
annote = {NULL},
author = {Zhao, B. and Li, Z. and Jannesari, A. and Wolf, F. and Wu, W.},
file = {:Users/cec/Google Drive/Mendeley Library/Unknown - Zhao et al. - Dependence-Based Code Transformation for Coarse-Grained Parallelism.pdf:pdf},
keywords = {coarse-,code transformation,data dependence analysis,tbb},
title = {{Dependence-Based Code Transformation for Coarse-Grained Parallelism}}
}
@inproceedings{Falcou2008,
annote = {NULL},
author = {Falcou, J and Serot, J},
booktitle = {PARCO},
file = {:Users/cec/Google Drive/Mendeley Library/2008 - Falcou, Serot - Formal semantics applied to the implementation of a skeleton-based parallel programming library.pdf:pdf},
title = {{Formal semantics applied to the implementation of a skeleton-based parallel programming library}},
url = {http://books.google.co.uk/books?hl=en{\&}lr={\&}id=WCOTC2UmXT8C{\&}oi=fnd{\&}pg=PA243{\&}dq={\%}22skeleton*{\%}22+parallel{\&}ots=88hZNU9vol{\&}sig=BaMtaRJghfIdANkC7XbslyGSuf0},
year = {2008}
}
@inproceedings{Bouchard2016,
abstract = {To learn text understanding models with millions of parameters one needs massive amounts of data. In this work, we argue that generating data can compensate for this need. While defining generic data generators is dif-ficult, we propose to allow generators to be " weakly " specified in the sense that a set of parameters controls how the data is generated. Consider for example generators where the ex-ample templates, grammar, and/or vocabulary is determined by this set of parameters. In-stead of manually tuning these parameters, we learn them from the limited training data at our disposal. To achieve this, we derive an ef-ficient algorithm called GENERE that jointly estimates the parameters of the model and the undetermined generation parameters. We il-lustrate its benefits by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network.},
author = {Bouchard, G. and Stenetorp, P. and Riedel, S.},
booktitle = {EMNLP},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Bouchard, Stenetorp, Riedel - Learning to Generate Textual Data.pdf:pdf},
title = {{Learning to Generate Textual Data}},
year = {2016}
}
@inproceedings{Rul2010,
abstract = {Accelerator processors allow energy-efficient compu- tation at high performance, especially for computation- intensive applications. There exists a plethora of differ- ent accelerator architectures, such as GPUs and the Cell Broadband Engine. Each accelerator has its own program- ming language, but the recently introduced OpenCL lan- guage unifies accelerator programming languages. Hereby, OpenCL achieves functional protability, allowing to reduce the development time of kernels. Functional portability however has limited value without performance portabil- ity: the possibility to re-use optimized kernels with good performance. This paper investigates the specificity of code optimizations to accelerator architecture and the severity of lack of performance portability.},
annote = {This paper investigates the performance protability of OpenCL kernels. Multiple benchmarks are timed on different architectures with different optimisation parameters. They find that each architecture is particularly sensitive to one of the optimisations. This is a lightweight fluff piece, with no real substance. The optimisation space is small, the benchmarks are few, and the experimental method is undocumented.},
author = {Rul, S. and Vandierendonck, H. and Haene, J. D. and Bosschere, K. D.},
booktitle = {SAAHPC},
file = {:Users/cec/Google Drive/Mendeley Library/2010 - Rul et al. - An Experimental Study on Performance Portability of OpenCL Kernels.pdf:pdf},
title = {{An Experimental Study on Performance Portability of OpenCL Kernels}},
year = {2010}
}
@article{Han2014,
abstract = {The use of local memory is important to improve the performance of OpenCL programs. However, its use may not always benefit performance, depending on various application characteristics, and there is no simple heuristic for deciding when to use it. We develop a machine learning model to decide if the optimization is beneficial or not. We train the model with millions of synthetic benchmarks and show that it can predict if the optimization should be applied for a single array, in both synthetic and real benchmarks, with high accuracy.},
annote = {From Duplicate 1 (Automatic Tuning of Local Memory Use on GPGPUs - Han, Tianyi David; Abdelrahman, Tarek S.)

Shitty.},
archivePrefix = {arXiv},
arxivId = {1412.6986},
author = {David, T. and Tarek, H. and Abdelrahman, S. and Han, T. D. and Abdelrahman, T. S.},
eprint = {1412.6986},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - David et al. - Automatic Tuning of Local Memory Use on GPGPUs.pdf:pdf},
journal = {arXiv:1412.6986},
keywords = {()},
title = {{Automatic Tuning of Local Memory Use on GPGPUs}},
url = {http://arxiv.org/abs/1412.6986},
year = {2014}
}
@misc{UniversityofEdinburgh2009b,
annote = {NULL},
author = {{University of Edinburgh}},
file = {:Users/cec/Google Drive/Mendeley Library/2009 - University of Edinburgh - IAML 2014 Resit Exam.pdf:pdf},
number = {August},
title = {{IAML 2014 Resit Exam}},
year = {2009}
}
@inproceedings{Bardsley2014,
abstract = {We report on practical experiences over the last 2.5 years related to the engineering of GPUVerify, a static verification tool for OpenCL and CUDA GPU kernels, plotting the progress of GPUVerify from a prototype to a fully functional and relatively efficient analysis tool. Our hope is that this experience report will serve the verification community by helping to inform future tooling efforts.},
author = {Bardsley, E. and Betts, A. and Chong, N. and Collingbourne, P. and Deligiannis, P. and Donaldson, A. and Ketema, J. and Liew, D. and Qadeer, S.},
booktitle = {CAV},
file = {:Users/cec/Google Drive/Mendeley Library/2014 - Bardsley et al. - Engineering a Static Verification Tool for GPU Kernels.pdf:pdf},
title = {{Engineering a Static Verification Tool for GPU Kernels}},
year = {2014}
}
@incollection{Ng2018b,
author = {Ng, A.},
booktitle = {Machine Learning Yearning},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Ng - Machine Learning Yearning, Ch 53-58.pdf:pdf},
title = {{Machine Learning Yearning, Ch 53-58}},
year = {2018}
}
@inproceedings{Whaley1998,
abstract = {This paper describes an approac h for the automatic generation and optimization of n umer? ical soft are for processors with deep memory hierarc w hies and pipelined functional units? The production of suc h soft are for mac w hines ranging from desktop w orkstations to em bed? ded processors can be a tedious and time consuming process? The w ork described here can help in automating m h of this process? W uc e will concen trate our e?orts on the widely used linear algebra k ernels called the Basic Linear Algebra Subroutines ?BLAS?? In particular? the w ork presen ted here is for general matrix m ultiply ? DGEMM? Ho ev w er m h of the uc tec hnology and approac h dev eloped here can be applied to the other Lev el ? BLAS and the general strategy can ha e an impact on basic linear algebra operations in general and ma v y be extended to other importan tk ernel operations.},
annote = {Print these.},
author = {Whaley, R. C. and Dongarra, J. J.},
booktitle = {SC},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Whaley, Dongarra - Automatically tuned linear algebra software.pdf:pdf},
publisher = {IEEE Computer Society},
title = {{Automatically tuned linear algebra software}},
year = {1998}
}
@article{Rossum2012g,
annote = {NULL},
author = {Rossum, Guido Van},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Rossum - HOWTO Fetch Internet Resources Using The urllib Package.pdf:pdf},
title = {{HOWTO Fetch Internet Resources Using The urllib Package}},
year = {2016}
}
@inproceedings{Indyk1998,
annote = {NULL},
author = {Indyk, P. and Motwani, R.},
booktitle = {STOC},
file = {:Users/cec/Google Drive/Mendeley Library/1998 - Indyk, Motwani - Approximate nearest neighbors Towards removing the curse of dimensionality.pdf:pdf},
publisher = {ACM},
title = {{Approximate nearest neighbors: Towards removing the curse of dimensionality.}},
year = {1998}
}
@inproceedings{Wang2013,
abstract = {Hybrid memory designs, such as DRAM plus Phase Change Memory (PCM), have shown some promise for alleviating power and density issues faced by traditional memory systems. But previous studies have concentrated on CPU systems with a modest level of parallelism. This work studies the problem in a massively parallel setting. Specifically, it investigates the special implications to hybrid memory imposed by the massive parallelism in GPU. It empirically shows that, contrary to promising results demonstrated for CPU, previous designs of PCM-based hybrid memory result in significant degradation to the energy efficiency of GPU. It reveals that the fundamental reason comes from a multi-facet mismatch between those designs and the massive parallelism in GPU. It presents a solution that centers around a close cooperation between compiler-directed data placement and hardware-assisted runtime adaptation. The co-design approach helps tap into the full potential of hybrid memory for GPU without requiring dramatic hardware changes over previous designs, yielding 6{\%} and 49{\%} energy saving on average compared to pure DRAM and pure PCM respectively, and keeping performance loss less than 2{\%}.},
annote = {NULL},
author = {Wang, Bin and Wu, Bo and Li, Dong and Shen, Xipeng and Yu, Weikuan and Jiao, Yizheng and Vetter, Jeffrey S.},
booktitle = {PACT},
doi = {10.1109/PACT.2013.6618807},
file = {:Users/cec/Google Drive/Mendeley Library/2013 - Wang et al. - Exploring hybrid memory for GPU energy efficiency through software-hardware co-design.pdf:pdf},
isbn = {9781479910212},
issn = {1089795X},
keywords = {Co-Design,Energy Efficiency,GPU,NVRAM},
number = {Section VII},
publisher = {ACM},
title = {{Exploring hybrid memory for GPU energy efficiency through software-hardware co-design}},
year = {2013}
}
@misc{Brownlee2016,
annote = {NULL},
author = {Brownlee, J.},
file = {:Users/cec/Google Drive/Mendeley Library/2016 - Brownlee - Machine Learning Performance Improvement Cheat Sheet.pdf:pdf},
title = {{Machine Learning Performance Improvement Cheat Sheet}},
year = {2016}
}
@article{Zhang2018c,
author = {Wang, Z. and O'Boyle, M.},
file = {:Users/cec/Google Drive/Mendeley Library/2018 - Wang, O'Boyle - Machine learning in Compilers.pdf:pdf},
isbn = {9781450333993},
journal = {Proceedings of the IEEE},
keywords = {TOREAD},
mendeley-tags = {TOREAD},
title = {{Machine learning in Compilers}},
year = {2018}
}
@inproceedings{Marlow2011,
abstract = {We present a new programming model for deterministic parallel computation in a pure functional language. The model is monadic and has explicit granularity, but allows dynamic construction of dataflow networks that are scheduled at runtime, while remaining deterministic and pure. The implementation is based on monadic concurrency, which has until now only been used to simulate con- currency in functional languages, rather than to provide parallelism. We present the API with its semantics, and argue that parallel exe- cution is deterministic. Furthermore, we present a complete work- stealing scheduler implemented as a Haskell library, and we show that it performs at least as well as the existing parallel programming models in Haskell.},
annote = {NULL},
author = {Marlow, S. and Newton, R. and Jones, S. P.},
booktitle = {Haskell},
file = {:Users/cec/Google Drive/Mendeley Library/2011 - Marlow, Newton, Jones - A monad for deterministic parallelism.pdf:pdf},
keywords = {concurrent programming,languages,performance},
number = {12},
title = {{A monad for deterministic parallelism}},
url = {http://dl.acm.org/citation.cfm?id=2034685},
volume = {46},
year = {2011}
}
@article{Petersen,
annote = {NULL},
author = {Petersen, Paul M PM and Padua, David a},
file = {:Users/cec/Google Drive/Mendeley Library/1992 - Petersen, Padua - Machine-Independent Evaluation of Parallelizing Compilers.pdf:pdf},
journal = {Computer},
publisher = {Citeseer},
title = {{Machine-Independent Evaluation of Parallelizing Compilers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.2792{\&}rep=rep1{\&}type=pdf},
volume = {559},
year = {1992}
}
