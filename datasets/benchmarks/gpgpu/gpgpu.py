"""A collection of seven GPGPU benchmark suites.

These seven benchmark suites were responsible for 92% of GPU results published
in 25 top tier conference papers. For more details, see:

  ï»¿Cummins, C., Petoumenos, P., Zang, W., & Leather, H. (2017). Synthesizing
  Benchmarks for Predictive Modeling. In CGO. IEEE.

When executed as a binary, this file runs the selected benchmark suites and
dumps the execution logs generated by libcecl in a directory.

Usage:

  bazel run //datasets/benchmarks/gpgpu --
      --gpgpu_envs='Emulator|Oclgrind|Oclgrind_Simulator|Oclgrind_18.3|1.2'
      --gpgpu_benchmarks_suites=amd,npb --gpgpu_logdir=/tmp/logs
"""
import contextlib
import multiprocessing
import os
import pathlib
import subprocess
import tempfile
import typing

import humanize
from absl import app
from absl import flags
from absl import logging

from datasets.benchmarks.gpgpu import gpgpu_pb2
from gpu.cldrive.legacy import env as cldrive_env
from gpu.libcecl import libcecl_compile
from gpu.libcecl import libcecl_runtime
from gpu.oclgrind import oclgrind
from labm8 import bazelutil
from labm8 import fs
from labm8 import labdate
from labm8 import labtypes
from labm8 import pbutil
from labm8 import system
from labm8 import text

FLAGS = flags.FLAGS

# The list of all GPGPU benchmark suites.
_BENCHMARK_SUITE_NAMES = [
    'amd-app-sdk-3.0',
    'npb-3.3',
    'nvidia-4.2',
    'parboil-0.2',
    'polybench-gpu-1.0',
    'rodinia-3.1',
    'shoc-1.1.5',
    'dummy_just_for_testing',
]

flags.DEFINE_list(
    'gpgpu_benchmark_suites', _BENCHMARK_SUITE_NAMES,
    'The names of benchmark suites to run. Defaults to all '
    'benchmark suites.')
flags.DEFINE_list(
    'gpgpu_envs', ['Emulator|Oclgrind|Oclgrind_Simulator|Oclgrind_18.3|1.2'],
    'The OpenCL environment names to execute benchmark suites '
    'on. To list the available environments, run '
    '`bazel run //gpu/clinfo`.')
flags.DEFINE_string('gpgpu_logdir', '/tmp/phd/datasets/benchmarks/gpgpu',
                    'The directory to write log files to.')
flags.DEFINE_integer(
    'gpgpu_build_process_count', multiprocessing.cpu_count(),
    'The number of parallel threads to use when building '
    'GPGPU benchmark suites. Defaults to the number of '
    'processors on your system.')
flags.DEFINE_integer('gpgpu_benchmark_run_count', 1,
                     'The number of times to execute each benchmark suite.')
flags.DEFINE_string('gpgpu_log_extension', '.pb',
                    'The file extension for generated log files.')
flags.DEFINE_boolean(
    'gpgpu_record_outputs', True,
    "Record each benchmark's stdout and stderr. This "
    "information is not needed to get performance data, and "
    "can be quite large.")

_RODINIA_DATA_ROOT = bazelutil.DataPath('rodinia_data')

_MKCECL = bazelutil.DataPath('phd/gpu/libcecl/mkcecl')


def CheckCall(command: typing.Union[str, typing.List[str]],
              shell: bool = False,
              env: typing.Dict[str, str] = None):
  """Wrapper around subprocess.check_call() to log executed commands."""
  if shell:
    logging.debug('$ %s', command)
    subprocess.check_call(command, shell=True, env=env)
  else:
    command = [str(x) for x in command]
    logging.debug('$ %s', ' '.join(command))
    subprocess.check_call(command, env=env)


def RewriteClDeviceType(env: cldrive_env.OclgrindOpenCLEnvironment,
                        path: pathlib.Path):
  """Rewrite all instances of CL_DEVICE_TYPE_XXX in the given path."""
  cl_device_type = ('CL_DEVICE_TYPE_GPU' if env.device_type.lower() == 'gpu'
                    else 'CL_DEVICE_TYPE_CPU')
  CheckCall(
      f"""\
for f in $(find '{path}' -type f); do
  grep CL_DEVICE_TYPE_ $f &>/dev/null && {{
    sed -E -i 's/CL_DEVICE_TYPE_[A-Z]+/{cl_device_type}/g' $f
    echo Set {cl_device_type} in $f
  }} || true
done""",
      shell=True)


@contextlib.contextmanager
def MakeEnv(make_dir: pathlib.Path,
            opencl_headers: bool = True) -> typing.Dict[str, str]:
  """Return a build environment for GPGPU benchmarks."""
  with fs.chdir(make_dir):
    with tempfile.TemporaryDirectory(prefix='phd_gpu_libcecl_header_') as d:
      d = pathlib.Path(d)
      # Many of the benchmarks include Linux-dependent headers. Spoof them here
      # so that we can build.
      if system.is_mac():
        with open(d / 'malloc.h', 'w') as f:
          f.write('#include <stdlib.h>')
        (d / 'linux').mkdir()
        with open(d / 'linux/limits.h', 'w') as f:
          f.write("""
#ifndef _LINUX_LIMITS_H
#define _LINUX_LIMITS_H

#define NR_OPEN	        1024

#define NGROUPS_MAX    65536	/* supplemental group IDs are available */
#define ARG_MAX       131072	/* # bytes of args + environ for exec() */
#define LINK_MAX         127	/* # links a file may have */
#define MAX_CANON        255	/* size of the canonical input queue */
#define MAX_INPUT        255	/* size of the type-ahead buffer */
#define NAME_MAX         255	/* # chars in a file name */
#define PATH_MAX        4096	/* # chars in a path name including nul */
#define PIPE_BUF        4096	/* # bytes in atomic write to a pipe */
#define XATTR_NAME_MAX   255	/* # chars in an extended attribute name */
#define XATTR_SIZE_MAX 65536	/* size of an extended attribute value (64k) */
#define XATTR_LIST_MAX 65536	/* size of extended attribute namelist (64k) */

#define RTSIG_MAX	  32

#endif
""")

      env = os.environ.copy()
      cflags, ldflags = libcecl_compile.LibCeclCompileAndLinkFlags(
          opencl_headers=opencl_headers)
      env['CFLAGS'] = ' '.join(cflags) + f' -isystem {d}'
      env['CXXFLAGS'] = ' '.join(cflags) + f' -isystem {d}'
      env['LDFLAGS'] = ' '.join(ldflags)

      for flag in ['CFLAGS', 'CXXFLAGS', 'LDFLAGS']:
        env[f'EXTRA_{flag}'] = env[flag]
      yield env


def Make(
    target: typing.Optional[str],
    make_dir: pathlib.Path,
    extra_make_args: typing.Optional[typing.List[str]] = None,
) -> None:
  """Run make target in the given path."""
  if not (make_dir / 'Makefile').is_file():
    raise EnvironmentError(f"Cannot find Makefile in {make_dir}")

  # Build relative to the path, rather than using `make -c <path>`. This is
  # because some of the source codes have hard-coded relative paths.
  with MakeEnv(make_dir) as env:
    logging.debug('Running make %s in %s', target, make_dir)
    CheckCall(
        ['make', '-j', FLAGS.gpgpu_build_process_count] +
        ([target] if target else []) + (extra_make_args or []),
        env=env)


def FindExecutableInDir(path: pathlib.Path) -> pathlib.Path:
  """Find an executable file in a directory."""
  exes = [f for f in path.iterdir() if f.is_file() and os.access(f, os.X_OK)]
  if len(exes) != 1:
    raise EnvironmentError(f"Expected a single executable, found {len(exes)}")
  return exes[0]


class _BenchmarkSuite(object):
  """Abstract base class for a GPGPU benchmark suite.

  A benchmark suite provides two methods: ForceOpenCLEnvironment(), which forces
  the benchmarks within the suite to execute on a given device, and Run(), which
  executes the benchmarks and logs output to a directory. Example usage:

    with SomeBenchmarkSuite() as bs:
      bs.ForceOpenCLEnvironment('gpu')
      bs.Run('/tmp/logs/gpu')
      bs.ForceOpenCLEnvironment('cpu')
      bs.Run('/tmp/logs/cpu/1')
      bs.Run('/tmp/logs/cpu/2')
  """

  def __init__(self):
    if self.name not in _BENCHMARK_SUITE_NAMES:
      raise ValueError(f"Unknown benchmark suite: {self.name}")

    self._env = None
    self._input_files = bazelutil.DataPath(
        f'phd/datasets/benchmarks/gpgpu/{self.name}')
    self._mutable_location = None
    self._logdir = None
    self._log_paths: typing.List[pathlib.Path] = []

  def __enter__(self) -> pathlib.Path:
    prefix = f'phd_datasets_benchmarks_gpgpu_{self.name}'
    self._mutable_location = pathlib.Path(tempfile.mkdtemp(prefix=prefix))
    fs.cp(self._input_files, self._mutable_location)
    return self

  def __exit__(self, *args):
    fs.rm(self._mutable_location)
    self._mutable_location = None

  @property
  def path(self):
    """Return the path of the mutable copy of the benchmark sources."""
    if self._mutable_location is None:
      raise TypeError("Must be used as a context manager")
    return self._mutable_location

  @property
  def logs(self) -> typing.Iterable[gpgpu_pb2.GpgpuBenchmarkRun]:
    """Return an iterator of log protos."""
    return (pbutil.FromFile(log, gpgpu_pb2.GpgpuBenchmarkRun())
            for log in self._log_paths)

  @property
  def log_count(self):
    return len(self._log_paths)

  def ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment) -> None:
    """Force benchmarks to execute with the given environment."""
    self._env = env
    return self._ForceOpenCLEnvironment(env)

  @property
  def env(self) -> cldrive_env.OpenCLEnvironment:
    return self._env

  def Run(self, logdir: pathlib.Path) -> None:
    """Run benchmarks and log results to directory."""
    logdir.mkdir(parents=True, exist_ok=True)
    if self.env is None:
      raise TypeError("Must call ForceOpenCLEnvironment() before Run()")
    self._logdir = logdir
    ret = self._Run()
    self._logdir = None
    return ret

  @contextlib.contextmanager
  def RunEnv(self, path: pathlib.Path) -> typing.Dict[str, str]:
    """Return an execution environment for a GPGPU benchmark."""
    with fs.chdir(path):
      yield libcecl_runtime.RunEnv(self.env)

  def _ExecToLogFile(
      self,
      executable: pathlib.Path,
      benchmark_name: str,
      command: typing.Optional[typing.List[str]] = None,
      dataset_name: str = 'default',
      env: typing.Optional[typing.Dict[str, str]] = None) -> None:
    """Run executable using runcecl script and log output."""
    logging.info('Executing %s:%s', self.name, benchmark_name)
    self._logdir.mkdir(exist_ok=True, parents=True)

    device_name = '_'.join([x.lower() for x in self.env.device_name.split()])

    # Create the name of the logfile now, so that is timestamped to the start of
    # execution.
    timestamp = labdate.MillisecondsTimestamp()
    log_name = '.'.join([
        self.name, benchmark_name, device_name, system.HOSTNAME,
        str(timestamp)
    ])

    # Assemble the command to run.
    command = command or [executable]
    if self.env.name == oclgrind.CLINFO_DESCRIPTION.name:
      command = [str(oclgrind.OCLGRIND_PATH)] + command

    extra_env = env or dict()
    with self.RunEnv(executable.parent) as os_env:
      # Add the additional environment variables.
      os_env.update(extra_env)

      libcecl_log = libcecl_runtime.RunLibceclExecutable(
          command, self.env, os_env, record_outputs=FLAGS.gpgpu_record_outputs)

    if libcecl_log.returncode:
      log_produced = (
          self._logdir / f'{log_name}.ERROR{FLAGS.gpgpu_log_extension}')
    else:
      log_produced = self._logdir / f'{log_name}{FLAGS.gpgpu_log_extension}'

    pbutil.ToFile(
        gpgpu_pb2.GpgpuBenchmarkRun(
            benchmark_suite=self.name,
            benchmark_name=benchmark_name,
            dataset_name=dataset_name,
            hostname=system.HOSTNAME,
            run=libcecl_log,
        ), log_produced)

    logging.info('Wrote %s', log_produced)
    self._log_paths.append(log_produced)

  # Abstract attributes that must be provided by subclasses.

  @property
  def name(self) -> str:
    raise NotImplementedError("abstract property")

  @property
  def benchmarks(self) -> typing.List[str]:
    """Return a list of all benchmark names."""
    raise NotImplementedError("abstract property")

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment) -> None:
    """Set the given device type."""
    raise NotImplementedError("abstract method")

  def _Run(self) -> None:
    """Run the benchmarks and produce output log files."""
    raise NotImplementedError("abstract method")


class DummyJustForTesting(_BenchmarkSuite):
  """A dummy benchmark suite for testing purposes.

  It sill behaves like a real benchmark suite, but without running any expensive
  binaries.
  """

  @property
  def name(self) -> str:
    return "dummy_just_for_testing"

  @property
  def benchmarks(self) -> typing.List[str]:
    return ['hello']

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    logging.info("Dummy benchmark running on %s", env.name)

    CheckCall([_MKCECL, self.path / 'hello.cc'])

    with MakeEnv(self.path) as env:
      CheckCall(
          f'gcc {self.path}/hello.cc -o {self.path}/hello {env["CFLAGS"]} '
          f'{env["LDFLAGS"]}',
          shell=True)

  def _Run(self):
    logging.info("Executing dummy benchmarks!")
    self._ExecToLogFile(self.path / 'hello', 'hello')


class AmdAppSdkBenchmarkSuite(_BenchmarkSuite):
  """The AMD App SDK benchmarks.

  This is a subset of the App SDK example programs. They use a CMake build
  system. One caveat is that we have to use the OpenCL headers provided as
  part of the package, rather than the phd versions. The Ubunutu libglew-dev
  package is a build requirement.
  """

  @property
  def name(self):
    return 'amd-app-sdk-3.0'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        'AdvancedConvolution',
        'BinomialOption',
        'BitonicSort',
        'BlackScholes',
        'FastWalshTransform',
        'FloydWarshall',
        'Histogram',
        'MatrixMultiplication',
        'MatrixTranspose',
        'MonteCarloAsian',
        'NBody',
        'PrefixSum',
        'Reduction',
        'ScanLargeArrays',
        'SimpleConvolution',
        'SobelFilter',
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    RewriteClDeviceType(env, self.path / 'samples/opencl/cl/1.x')

    for benchmark in self.benchmarks:
      # Clean any existing builds.
      if (self.path / f'samples/opencl/cl/1.x/{benchmark}/Makefile').is_file():
        Make('clean', self.path / 'samples/opencl/cl/1.x' / benchmark)

    # Delete all CMake generated files.
    CheckCall([
        'find', self.path / 'samples/opencl/cl/1.x', '-iwholename', '*cmake*',
        '-not', '-name', 'CMakeLists.txt', '-delete'
    ])

    for benchmark in self.benchmarks:
      with MakeEnv(
          self.path / f'samples/opencl/cl/1.x/{benchmark}',
          opencl_headers=False) as env:
        env['CFLAGS'] = f'{env["CFLAGS"]} -isystem {self.path}/include'
        env['CXXFLAGS'] = f'{env["CXXFLAGS"]} -isystem {self.path}/include'

        logging.debug('Building %s:%s in %s', self.name, benchmark)
        CheckCall(['cmake', '.'], env=env)
        CheckCall(['make', '-j', FLAGS.gpgpu_build_process_count, 'VERBOSE=1'],
                  env=env)

  def _Run(self):
    for benchmark in self.benchmarks:
      executable = (self.path / 'samples/opencl/cl/1.x' / benchmark /
                    'bin/x86_64/Release' / benchmark)
      self._ExecToLogFile(executable, benchmark)


class NasParallelBenchmarkSuite(_BenchmarkSuite):
  """The NAS benchmark suite."""

  @property
  def name(self):
    return 'npb-3.3'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        'BT',
        'CG',
        'EP',
        'FT',
        'IS',
        'LU',
        'MG',
        'SP',
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    Make('clean', self.path)
    Make('suite', self.path)

  def _Run(self):
    for benchmark in self.benchmarks:
      for dataset in ['S', 'W', 'A', 'B', 'C']:
        executable = self.path / f'bin/{benchmark.lower()}.{dataset}.x'
        # TODO(cec): Fix a handful of build errors which prevent all
        # executables from being compiled.
        if not executable.is_file():
          continue
        self._ExecToLogFile(
            executable,
            f'{benchmark.lower()}.{dataset}',
            env={
                'OPENCL_DEVICE_TYPE':
                ('GPU' if self.env.device_type.lower() == 'gpu' else 'CPU')
            },
            dataset_name=dataset,
            command=[executable, f'../{benchmark}'])


class NvidiaBenchmarkSuite(_BenchmarkSuite):
  """NVIDIA GPU SDK."""

  @property
  def name(self):
    return 'nvidia-4.2'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        'BlackScholes',
        'ConvolutionSeparable',
        'DCT8x8',
        'DXTCompression',
        'DotProduct',
        'FDTD3d',
        'HiddenMarkovModel',
        'MatVecMul',
        'MatrixMul',
        'MersenneTwister',
        'RadixSort',
        'Reduction',
        'Scan',
        'VectorAdd',
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    RewriteClDeviceType(env, self.path / 'OpenCL/src')
    Make('clean', self.path / 'OpenCL')
    Make(None, self.path / 'OpenCL')

  def _Run(self):
    for benchmark in self.benchmarks:
      executable = self.path / f'OpenCL/bin/linux/release/ocl{benchmark}'
      self._ExecToLogFile(executable, benchmark)


class ParboilBenchmarkSuite(_BenchmarkSuite):
  """Parboil benchmark suite."""

  @property
  def name(self):
    return 'parboil-0.2'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        'bfs',
        'cutcp',
        'histo',
        'lbm',
        'mri-gridding',
        'mri-q',
        'sad',
        'sgemm',
        'spmv',
        'stencil',
        'tpacf',
    ]

  @property
  def benchmarks_and_datasets(self):
    return [
        ('bfs', '1M'),
        ('bfs', 'NY'),
        ('bfs', 'SF'),
        ('bfs', 'UT'),
        ('cutcp', 'large'),
        ('cutcp', 'small'),
        ('histo', 'default'),
        ('histo', 'large'),
        ('lbm', 'long'),
        ('lbm', 'short'),
        ('mri-gridding', 'small'),
        ('mri-q', 'large'),
        ('mri-q', 'small'),
        ('sad', 'default'),
        ('sad', 'large'),
        ('sgemm', 'medium'),
        ('sgemm', 'small'),
        ('spmv', 'large'),
        ('spmv', 'medium'),
        ('spmv', 'small'),
        ('stencil', 'default'),
        ('stencil', 'small'),
        ('tpacf', 'large'),
        ('tpacf', 'medium'),
        ('tpacf', 'small'),
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    RewriteClDeviceType(env, self.path / 'benchmarks')

    # Due to the large size of parboil benchmarks (> 900 MB uncompressed), we
    # ship compressed archives with per-benchmark datasets. These must be
    # decompressed. This must be done prior to building. Once decompressed,
    # we remove the compressed archives so that the unpacked archives are
    # re-used for the lifetime of this object.
    with fs.chdir(self.path / f'datasets'):
      for benchmark in self.benchmarks:
        dataset_archive = self.path / f'datasets/{benchmark}.tar.bz2'
        if dataset_archive.is_file():
          logging.info('Unpacking datasets for %s:%s', self.name, benchmark)
          CheckCall(['tar', 'xjvf', dataset_archive])
        elif pathlib.Path(f'{dataset_archive}.part1').is_file():
          logging.info('Unpacking datasets for %s:%s', self.name, benchmark)
          CheckCall(
              f'cat {dataset_archive}.part1 {dataset_archive}.part2 '
              f'> {dataset_archive}',
              shell=True)
          pathlib.Path(f'{dataset_archive}.part1').unlink()
          pathlib.Path(f'{dataset_archive}.part2').unlink()
          CheckCall(['tar', 'xjvf', dataset_archive])

        if dataset_archive.is_file():
          dataset_archive.unlink()

    CheckCall(['find', self.path, '-name', '*.o', '-delete'])
    with MakeEnv(self.path) as env:
      for benchmark in self.benchmarks:
        CheckCall(['python2', './parboil', 'compile', benchmark, 'opencl_base'],
                  env=env)

  def _Run(self):
    for benchmark, dataset in self.benchmarks_and_datasets:
      self._ExecToLogFile(
          self.path / 'parboil',
          f'{benchmark}.{dataset}',
          command=[
              'python2', './parboil', 'run', benchmark, 'opencl_base', dataset
          ])


class PolybenchGpuBenchmarkSuite(_BenchmarkSuite):
  """PolyBench/GPU 1.0 Benchmarks."""

  @property
  def name(self):
    return 'polybench-gpu-1.0'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        '2DCONV',
        '2MM',
        '3DCONV',
        '3MM',
        'ATAX',
        'BICG',
        'CORR',
        'COVAR',
        # Bad: 'FDTD-2D',
        'GEMM',
        'GESUMMV',
        'GRAMSCHM',
        'MVT',
        'SYR2K',
        'SYRK',
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    RewriteClDeviceType(env, self.path / 'OpenCL')
    for benchmark in self.benchmarks:
      logging.info('Building benchmark %s', benchmark)
      Make('clean', self.path / 'OpenCL' / benchmark)
      Make('all', self.path / 'OpenCL' / benchmark)

  def _Run(self):
    for benchmark in self.benchmarks:
      executable = FindExecutableInDir(self.path / 'OpenCL' / benchmark)
      self._ExecToLogFile(executable, benchmark)


class RodiniaBenchmarkSuite(_BenchmarkSuite):
  """Rodinia Benchmark Suite 3.1.

  The University of Virginia Rodinia Benchmark Suite is a collection of parallel
  programs which targets heterogeneous computing platforms with both multicore
  CPUs and GPUs.

  Copyright (c)2008-2011 University of Virginia.

  For further details, see:

      S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, Sang-Ha Lee and
      K. Skadron. "Rodinia: A Benchmark Suite for Heterogeneous Computing".
      IISWC, 2009.
  """

  @property
  def name(self) -> str:
    return 'rodinia-3.1'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        'b_tree',
        'backprop',
        'bfs',
        'cfd',
        'dwt2d',
        'gaussian',
        'heartwall',
        'hotspot',
        'hotspot3D',
        'hybridsort',
        'kmeans',
        'lavaMD',
        'leukocyte',
        'lud',
        'myocyte',
        'nn',
        'nw',
        'particlefilter',
        'pathfinder',
        'srad',
        'streamcluster',
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    RewriteClDeviceType(env, self.path / 'opencl')

    logging.info("Building Rodinia benchmarks")

    # This directory is not generated by the Makefile, but is needed by it.
    (self.path / 'bin/linux/opencl').mkdir(parents=True, exist_ok=True)

    # Copy and unpack the data sets, which come from a data-only file tree.
    logging.info('Unpacking compressed data archives.')
    fs.cp(_RODINIA_DATA_ROOT, self.path / 'data')
    Make('all', self.path / 'data')

    Make('OCL_clean', self.path)
    Make('OPENCL', self.path)
    # TODO(cec): Original script then deleted the opencl/hotspot3D/3D file. Is
    # it not working?

  def _Run(self):
    for benchmark in self.benchmarks:
      executable = self.path / 'opencl' / benchmark / 'run'
      self._ExecToLogFile(executable, benchmark, command=['bash', './run'])


class ShocBenchmarkSuite(_BenchmarkSuite):
  """SHOC Benchmarks."""

  @property
  def name(self):
    return 'shoc-1.1.5'

  @property
  def benchmarks(self) -> typing.List[str]:
    return [
        'BFS',
        'FFT',
        'GEMM',
        'MD',
        'MD5Hash',
        'Reduction',
        'Scan',
        'Sort',
        'Spmv',
        'Stencil2D',
        'Triad',
        'S3D',
    ]

  def _ForceOpenCLEnvironment(self, env: cldrive_env.OpenCLEnvironment):
    RewriteClDeviceType(env, self.path / 'src/opencl')
    if (self.path / 'Makefile').is_file():
      Make('distclean', self.path)

    with fs.chdir(self.path):
      CheckCall(['./configure'])

    Make(None, self.path / 'src/common')
    Make(None, self.path / 'src/opencl')

  def _Run(self):
    for benchmark in self.benchmarks:
      level1 = self.path / f'src/opencl/level1/{benchmark.lower()}/{benchmark}'
      level2 = self.path / f'src/opencl/level2/{benchmark.lower()}/{benchmark}'
      if level1.is_file():
        executable = level1
      else:
        executable = level2
      self._ExecToLogFile(executable, benchmark)


# A map of benchmark suite names to classes.
BENCHMARK_SUITES = {
    bs().name: bs for bs in labtypes.AllSubclassesOfClass(_BenchmarkSuite)
}


def ResolveBenchmarkSuiteClassesFromNames(names: typing.List[str]):
  trie = text.BuildPrefixTree(_BENCHMARK_SUITE_NAMES)
  benchmark_suite_classes = []

  for name in names:
    try:
      options = list(text.AutoCompletePrefix(name, trie))
    except KeyError:
      raise app.UsageError(f"Unknown benchmark suite: '{name}'. "
                           f"Legal values: {BENCHMARK_SUITES.keys()}")

    if len(options) > 1:
      raise app.UsageError(f"Ambiguous benchmark suite: '{name}'. "
                           f"Candidates: {options}")

    benchmark_suite_classes.append(BENCHMARK_SUITES[options[0]])

  return benchmark_suite_classes


def main(argv: typing.List[str]):
  """Main entry point."""
  if len(argv) > 1:
    raise app.UsageError("Unknown arguments: '{}'.".format(' '.join(argv[1:])))

  # Get the OpenCL environments.
  envs = [
      cldrive_env.OpenCLEnvironment.FromName(env) for env in FLAGS.gpgpu_envs
  ]

  # Run the requested benchmark suites on the requested devices.
  outdir = pathlib.Path(FLAGS.gpgpu_logdir)
  for benchmark_suite_class in ResolveBenchmarkSuiteClassesFromNames(
      FLAGS.gpgpu_benchmark_suites):
    with benchmark_suite_class() as benchmark_suite:
      for env in envs:
        logging.info('Building and running %s on %s', benchmark_suite.name,
                     env.name)
        benchmark_suite.ForceOpenCLEnvironment(env)
        for i in range(FLAGS.gpgpu_benchmark_run_count):
          logging.info('Starting run %d of %s', i, benchmark_suite.name)
          benchmark_suite.Run(outdir)

      kernel_invocation_count = sum(
          len(log.run.kernel_invocation) for log in benchmark_suite.logs)
      logging.info('Extracted %s kernel invocations from %s logs',
                   humanize.intcomma(kernel_invocation_count),
                   humanize.intcomma(benchmark_suite.log_count))


if __name__ == '__main__':
  app.run(main)
