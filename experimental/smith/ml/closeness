#!/usr/bin/env python3
#
# TODO: Line style http://matplotlib.org/examples/lines_bars_and_markers/line_styles_reference.html
#
import matplotlib
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import math
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sklearn
import sys
import scipy

from argparse import ArgumentParser
from collections import Counter
from copy import copy
from functools import partial
from itertools import combinations, zip_longest
from math import sqrt,ceil
from numpy.random import RandomState
from os import system
from random import seed,random
from sklearn.base import clone
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier
from sklearn.preprocessing import normalize
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, export_graphviz

import labm8
from labm8 import fmt
from labm8 import fs
from labm8 import math as labmath
from labm8 import viz

import smith
from smith import cgo13

# Seaborn configuration:
sns.set(style="ticks", color_codes=True)
plt.style.use(["seaborn-white", "seaborn-paper"])


def get_cgo13_features(D):
    return np.array([
        (D["transfer"].values / (D["comp"].values + D["mem"].values)),
        (D["coalesced"].values / D["mem"].values),
        ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
        (D["comp"].values / D["mem"].values),
    ]).T


def get_raw_features(D):
    return np.array([
        D["comp"].values,
        D["rational"].values,
        D["mem"].values,
        D["localmem"].values,
        D["coalesced"].values,
        D["atomic"].values,
        D["transfer"].values,
        D["wgsize"].values,
    ]).T


def get_our_features(D):
    return np.array([
        D["comp"].values,
        D["rational"].values,
        D["mem"].values,
        D["localmem"].values,
        D["coalesced"].values,
        D["transfer"].values,
        D["wgsize"].values,
        (D["transfer"].values / (D["comp"].values + D["mem"].values)),
        (D["coalesced"].values / D["mem"].values),
        ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
        (D["comp"].values / D["mem"].values),
    ]).T


def get_static_features(D):
    return np.array([
        D["comp"].values,
        # D["rational"].values,
        D["mem"].values,
        D["localmem"].values,
        D["coalesced"].values,
        # D["F2:coalesced/mem"].values,
        # D["F4:comp/mem"].values,
    ], dtype=float).T


def get_labels(D):
    return D["oracle"]


def normalize(*args):
    # num_features = len(args[0][0])
    # print(num_features, "features")
    # for feature_idx in range(num_features):
    #     divisor = max([np.max(arg[:,feature_idx]) for arg in args])
    #     # divisor = max(np.median([np.median(arg[:,feature_idx]) for arg in args]), 1)
    #     print("Feature", feature_idx, "max:", divisor)

    #     for arg in args:
    #         arg[:,feature_idx] /= divisor

    # # Sanity-check that normalization worked:
    # for j in range(num_features):
    #     for arg in args:
    #         for i in arg[:,j]:
    #             assert(i <= 1)
    #             assert(i >= 0)

    return args


def closeness(platform_id):
    # Load datasets
    B = pd.read_csv("data/{}/benchmarks.csv".format(platform_id.lower()))
    B["synthetic"] = np.zeros(len(B))
    S = pd.read_csv("data/{}/synthetics.csv".format(platform_id.lower()))
    S["synthetic"] = np.ones(len(S))
    BS = pd.concat((B, S), ignore_index=True)

    features = get_cgo13_features
    # features = get_static_features

    # Transform to feature space:
    X1 = features(B)
    X2 = features(S)
    X3 = features(BS)

    ytest2 = get_labels(S)
    ytest3 = get_labels(BS)

    # Neighbours to check for:
    Xtest = features(B)
    ytest = get_labels(B)

    # Get nearest neighbours for kernels:
    nbrs = NearestNeighbors(n_neighbors=2, algorithm='brute').fit(X1)
    distances1, indices1 = nbrs.kneighbors(Xtest)

    nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(X2)
    distances2, indices2 = nbrs.kneighbors(Xtest)

    nbrs = NearestNeighbors(n_neighbors=2, algorithm='brute').fit(X3)
    distances3, indices3 = nbrs.kneighbors(Xtest)

    # Find out whether the nearest neighbours have the same behaviour:
    behaviour1 = np.ones(len(indices1), dtype=bool)
    behaviour2 = np.ones(len(indices2), dtype=bool)
    behaviour3 = np.ones(len(indices3), dtype=bool)

    assert(len(distances1) == len(distances2))
    assert(len(distances2) == len(distances3))

    for i,index in enumerate(indices1):
        # index[0] is index of origin, index[1] is nearest neighbour:
        if index[1] == i:
            index[0], index[1] = index[1], index[0]
        if ytest[index[0]] != ytest[index[1]]:
            behaviour1[i] = False

    for i,index in enumerate(indices2):
        if ytest[i] != ytest2[int(index)]:
            behaviour2[i] = False

    for i,index in enumerate(indices3):
        # index[0] is index of origin, index[1] is nearest neighbour:
        if index[1] == i:
            index[0], index[1] = index[1], index[0]
        if ytest[index[0]] != ytest3[index[1]]:
            behaviour3[i] = False

    synthetic_neighbours3 = [BS.loc[i[1]]["synthetic"] for i in indices3]
    synthetic_distances3 = [
        d[1] for d,s in zip(distances3,synthetic_neighbours3) if s
    ]

    # USED TO IDENTIFY KERNELS WITH THE SAME FEATURES BUT DIFFERENT BEHAVIOUR:
    #
    # print()
    # print("Zero feature distance, different behaviour:")
    # for i,row in enumerate(zip(distances1, behaviour1)):
    #     distance, behaviour = row
    #     if distance[1] == 0 and not behaviour:
    #         print(" ",
    #               B.loc[indices1[i][0]]["benchmark"],
    #               B.loc[indices1[i][1]]["benchmark"])
    # print()

    # print()
    # print("Zero feature distance, different behaviour:")
    # for i,row in enumerate(zip(distances2, behaviour2)):
    #     distance, behaviour = row
    #     if distance == 0  and not behaviour:
    #         print(" ",
    #               B.loc[indices1[i][0]]["benchmark"],
    #               S.loc[indices1[i][1]]["benchmark"])
    # print()

    print()
    print("Closeness on Platform {}".format(platform_id.upper()))
    print("  Benchmarks:")
    print("  Median dist to nearest neighbour:     {:.1f}"
          .format(labmath.median(distances1[:,1])))
    print("  Mean dist to nearest neighbour:       {:.1f}"
          .format(labmath.mean(distances1[:,1])))
    print("  Nearest Neighbour has same behaviour: {:.1f} %"
          .format((sum(behaviour1) / len(behaviour1)) * 100))
    print()
    print("  Synthetics:")
    print("  Median dist to nearest neighbour:     {:.1f}"
          .format(labmath.median(distances2[:,0])))
    print("  Mean dist to nearest neighbour:       {:.1f}"
          .format(labmath.mean(distances2[:,0])))
    print("  Nearest Neighbour has same behaviour: {:.1f} %"
          .format((sum(behaviour2) / len(behaviour2)) * 100))
    print("  Closest neighbour:                    {:.1f}"
          .format(min(distances2[:,0])))
    print()
    print("  Benchmarks w. synthetics:")
    print("  Median dist to nearest neighbour:     {:.1f}"
          .format(labmath.median(distances3[:,1])))
    print("  Mean dist to nearest neighbour:       {:.1f}"
          .format(labmath.mean(distances3[:,1])))
    print("  Median dist to synthetic neighbour:   {:.1f}"
          .format(labmath.median(synthetic_distances3)))
    print("  Nearest Neighbour has same behaviour: {:.1f} %"
          .format((sum(behaviour3) / len(behaviour3)) * 100))
    print("  Closer neighbours:                    {} {:.1f} %"
          .format(int(sum(synthetic_neighbours3)),
                  (sum(synthetic_neighbours3) / len(B)) * 100))
    print("  Closest synthetic neighbour:          {:.1f}"
          .format(min(synthetic_neighbours3)))
    print("  Median dist to nearest synthetic:     {:.1f}"
          .format(labmath.median(synthetic_neighbours3)))
    print("  Farthest synthetic neighbour:         {:.1f}"
          .format(max(synthetic_neighbours3)))
    print()

    # def distplot(D):
    #     opts = {"kde": False, "bins": 25}
    #     ax = sns.distplot(D["distance"], **opts)
    #     ax.set_yscale("log", nonposy='clip')
    #     plt.ylim(0, 1000)
    #     plt.xlabel("")
    #     plt.ylabel("Count")

    MAXDIST = 1000
    benchmarks = pd.DataFrame(
        [(d,b) for d,b in zip(distances1[:,1],behaviour1)
         if d < MAXDIST],
        columns=["distance", "behaviour"])
    synthetics = pd.DataFrame(
        [(d,b) for d,b in zip(distances3[:,1],behaviour2) if d < MAXDIST],
        columns=["distance", "behaviour"])

    # plt.subplot(2, 2, 1)
    # distplot(benchmarks[benchmarks["behaviour"] == 1])
    # plt.title("B, same behaviour")

    # plt.subplot(2, 2, 3)
    # distplot(benchmarks[benchmarks["behaviour"] == 0])
    # plt.title("B, different behaviour")

    # plt.subplot(2, 2, 2)
    # distplot(synthetics[synthetics["behaviour"] == 1])
    # plt.title("BS, same behaviour")

    # plt.subplot(2, 2, 4)
    # distplot(synthetics[synthetics["behaviour"] == 0])
    # plt.title("BS, different behaviour")

    # plt.show()


def get_nearest_neighbour_distance(F1, F2):
    """
    Return nearest-neighbour distances from F1 to F2.
    """
    nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(F2)
    distances, indices = nbrs.kneighbors(F1)
    # for d,i,f in zip(distances,indices,F1):
    #     if d == 0:
    #         print("distance: ", d, "clgen:", F2[i], "target:", f)
    return distances


def remove_duplicates(g):
    uniq = set()
    out = []
    for row in g.to_dict('records'):
        key = (row['benchmark'])

        if key not in uniq:
            out.append(row)
            uniq.add(key)

    g = pd.DataFrame(out)
    return g


def static_kernel_closeness():
    features = get_static_features

    # Set a seed for reproducible results:
    np.random.seed(204)

    # Load datasets:
    Benchmarks = pd.read_csv("data/a/benchmarks.csv")
    CLgen = pd.read_csv(
        fs.path("~/kernels/clgen/features.csv"))
    GitHub = pd.read_csv(
        fs.path("~/kernels/github/features.csv"))
    CLSmith = pd.read_csv(
        fs.path("~/kernels/clsmith/features.csv"))

    # Extract features:
    BenchmarksFeatures = features(remove_duplicates(Benchmarks))
    CLgenFeatures = features(CLgen)
    GitHubFeatures = features(GitHub)
    CLSmithFeatures = features(CLSmith)

    BenchmarksFeatures, CLgenFeatures, GitHubFeatures, CLSmithFeatures = normalize(
        BenchmarksFeatures, CLgenFeatures, GitHubFeatures, CLSmithFeatures)

    stepMax = max(
        len(GitHubFeatures), len(CLgenFeatures), len(CLSmithFeatures))

    LOG_SCALE = True
    if LOG_SCALE:
        stepMin = 10
        steps = np.array(list(exp_range(stepMin, stepMax, 1.5 )), dtype=np.int32)
    else:
        stepMin = 100
        steps = np.array(list(range(stepMin, stepMax, 2500)), dtype=np.int32)

    GitHubSteps = [n for n in steps if n <= len(GitHubFeatures)]
    CLSmithSteps = [n for n in steps if n <= len(CLSmithFeatures)]
    CLgenSteps = [n for n  in steps if n <= len(CLgenFeatures)]

    numGitHubSteps = len(GitHubSteps)
    numCLSmithSteps = len(CLSmithSteps)
    numCLgenSteps = len(CLgenSteps)

    print("#. GitHub kernels", GitHubSteps[-1])
    print("#. CLSmith kernels", CLSmithSteps[-1])
    print("#. CLgen kernels", CLgenSteps[-1])

    summarize_distance = np.median
    summarize_results = np.mean
    def error(x):
        scale = np.std(x) / sqrt(len(x))
        mean = np.mean(x)
        ci = scipy.stats.norm.interval(.95, loc=mean, scale=scale)
        return ci[1] - mean

    print("Steps:", steps)

    datasets = [
        {
            "name": "GitHub",
            "color": "b",
            "linestyle": ":",
            "steps": GitHubSteps,
            "features": GitHubFeatures,
            "distances": np.zeros(numGitHubSteps, dtype=float),
            "errors": np.zeros(numGitHubSteps, dtype=float)
        },
        {
            "name": "CLSmith",
            "color": "r",
            "linestyle": "--",
            "steps": CLSmithSteps,
            "features": CLSmithFeatures,
            "distances": np.zeros(numCLSmithSteps, dtype=float),
            "errors": np.zeros(numCLSmithSteps, dtype=float)
        },
        {
            "name": "CLgen",
            "color": "g",
            "linestyle": "-",
            "steps": CLgenSteps,
            "features": CLgenFeatures,
            "distances": np.zeros(numCLgenSteps, dtype=float),
            "errors": np.zeros(numCLgenSteps, dtype=float)
        },
    ]

    # Ratio improvements:
    GitHubDistances = get_nearest_neighbour_distance(BenchmarksFeatures, GitHubFeatures)
    CLSmithDistances = get_nearest_neighbour_distance(BenchmarksFeatures, CLSmithFeatures)
    CLgenDistances = get_nearest_neighbour_distance(BenchmarksFeatures, CLgenFeatures)
    CLSmithCloser = [True if y < x else False for x,y in zip(CLSmithDistances, CLgenDistances)]
    CLgenCloser = [True if y < x else False for x,y in zip(GitHubDistances, CLgenDistances)]
    print("Closer CLSmith features:  {} ({:.1f} %)"
          .format(sum(CLSmithCloser), (sum(CLSmithCloser) / len(CLSmithCloser)) * 100))
    print("Closer CLgen features:  {} ({:.1f} %)"
          .format(sum(CLgenCloser), (sum(CLgenCloser) / len(CLgenCloser)) * 100))

    numIterations = 30
    # numIterations = 30

    for dataset in datasets:
        for i, n in enumerate(dataset["steps"]):
            numSteps = len(dataset["steps"])
            print("\r", dataset["name"], i + 1, "of", numSteps, end="")
            r = np.zeros(numIterations, dtype=float)
            for j in range(numIterations):
                np.random.shuffle(dataset["features"])
                r[j] = summarize_distance(get_nearest_neighbour_distance(
                    BenchmarksFeatures, dataset["features"][:n]
                ))
            dataset["distances"][i] = summarize_results(r)
            dataset["errors"][i] = error(r)
        print("\nMin value of", dataset["name"], "with", dataset["steps"][-1],
              "kernels = {:.2f}".format(dataset["distances"][-1]))
        plt.errorbar(dataset["steps"], dataset["distances"],
                     yerr=dataset["errors"], c=dataset["color"],
                     linestyle=dataset["linestyle"],
                     label=dataset["name"], ecolor="k", capthick=1)
    ax = plt.gca()

    # No legend title
    plt.legend(loc='upper right')
    ax.get_legend().set_title("")
    ax.get_legend().draw_frame(True)

    plt.xlim(stepMin, stepMax)
    plt.ylim(0, )

    if LOG_SCALE:
        ax.set_xscale("log", nonposy='clip')

    plt.xlabel("#. OpenCL kernels")
    plt.ylabel("Feature Distance")

    figsize = (4.5, 2.15)
    viz.finalise(fs.path("~/cgo17/img/closeness.pdf"), figsize=figsize)


def zero_distance():
    features = get_static_features

    # Set a seed for reproducible results:
    np.random.seed(204)

    # Load datasets:
    Benchmarks = pd.read_csv("data/a/benchmarks.csv")
    CLgen = pd.read_csv(
        fs.path("~/kernels/clgen/features.csv"))
    GitHub = pd.read_csv(
        fs.path("~/kernels/github/features.csv"))
    CLSmith = pd.read_csv(
        fs.path("~/kernels/clsmith/features.csv"))

    # Extract features:
    BenchmarksFeatures = features(remove_duplicates(Benchmarks))
    CLgenFeatures = features(CLgen)
    GitHubFeatures = features(GitHub)
    CLSmithFeatures = features(CLSmith)

    BenchmarksFeatures, CLgenFeatures, GitHubFeatures, CLSmithFeatures = normalize(
        BenchmarksFeatures, CLgenFeatures, GitHubFeatures, CLSmithFeatures)

    # stepMax = max(
    #     len(GitHubFeatures), len(CLgenFeatures), len(CLSmithFeatures))
    stepMin = 0
    stepMax = 10000
    linStepSize = int((stepMax - stepMin) / 20)
    closeness_threshold = 0

    LOG_SCALE = False
    steps = np.array(list(range(stepMin, stepMax + linStepSize, linStepSize)),
                     dtype=np.int32)
    steps[0] = max(steps[0], 1)

    GitHubSteps = [n for n in steps if n <= len(GitHubFeatures)]
    CLSmithSteps = [n for n in steps if n <= len(CLSmithFeatures)]
    CLgenSteps = [n for n  in steps if n <= len(CLgenFeatures)]

    numGitHubSteps = len(GitHubSteps)
    numCLSmithSteps = len(CLSmithSteps)
    numCLgenSteps = len(CLgenSteps)

    print("#. GitHub kernels", GitHubSteps[-1])
    print("#. CLSmith kernels", CLSmithSteps[-1])
    print("#. CLgen kernels", CLgenSteps[-1])

    # Zero-distance
    def summarize_distance(distances):
        return sum([1 if d <= closeness_threshold else 0 for d in distances])

    summarize_results = np.mean
    error = np.std
    # def error(x):
    #     scale = np.std(x) / sqrt(len(x))
    #     mean = np.mean(x)
    #     ci = scipy.stats.norm.interval(.95, loc=mean, scale=scale)
    #     return ci[1] - mean

    print("Steps:", steps)

    datasets = [
        {
            "name": "GitHub",
            "color": "b",
            "linestyle": ":",
            "steps": GitHubSteps,
            "features": GitHubFeatures,
            "distances": np.zeros(numGitHubSteps, dtype=float),
            "errors": np.zeros(numGitHubSteps, dtype=float)
        },
        {
            "name": "CLSmith",
            "color": "r",
            "linestyle": "--",
            "steps": CLSmithSteps,
            "features": CLSmithFeatures,
            "distances": np.zeros(numCLSmithSteps, dtype=float),
            "errors": np.zeros(numCLSmithSteps, dtype=float)
        },
        {
            "name": "CLgen",
            "color": "g",
            "linestyle": "-",
            "steps": CLgenSteps,
            "features": CLgenFeatures,
            "distances": np.zeros(numCLgenSteps, dtype=float),
            "errors": np.zeros(numCLgenSteps, dtype=float)
        },
    ]

    numIterations = 10

    # Plot an oracle line:
    # plt.plot(datasets[2]["steps"], datasets[2]["steps"],
    #          label="Oracle", color="k", linestyle="--")

    for dataset in datasets:
        for i, n in enumerate(dataset["steps"]):
            numSteps = len(dataset["steps"])
            print("\r", dataset["name"], i + 1, "of", numSteps, end="")
            r = np.zeros(numIterations, dtype=float)
            for j in range(numIterations):
                np.random.shuffle(dataset["features"])
                r[j] = summarize_distance(get_nearest_neighbour_distance(
                    dataset["features"][:n], BenchmarksFeatures
                ))
            dataset["distances"][i] = summarize_results(r)
            dataset["errors"][i] = error(r)
        print("\nMin value of", dataset["name"], "with", dataset["steps"][-1],
              "kernels = {:.2f}".format(dataset["distances"][-1]))
        plt.errorbar(dataset["steps"], dataset["distances"],
                     yerr=dataset["errors"], c=dataset["color"],
                     linestyle=dataset["linestyle"],
                     label=dataset["name"], ecolor="k", capthick=1)
    ax = plt.gca()

    # No legend title
    plt.legend(loc='upper left')
    ax.get_legend().set_title("")
    ax.get_legend().draw_frame(True)

    plt.xlim(stepMin, stepMax)
    plt.ylim(-50, )

    if LOG_SCALE:
        ax.set_xscale("log", nonposy='clip')

    plt.xlabel("#. kernels")
    plt.ylabel("#. matches")

    figsize = (4.5, 2.0)
    viz.finalise(fs.path("~/cgo17/img/closeness.pdf"), figsize=figsize)


def closer_than():
    features = get_static_features

    # Set a seed for reproducible results:
    np.random.seed(204)

    # Load datasets:
    Benchmarks = pd.read_csv("data/a/benchmarks.csv")
    CLgen = pd.read_csv(
        fs.path("~/kernels/clgen/features.csv"))
    GitHub = pd.read_csv(
        fs.path("~/kernels/github/features.csv"))
    CLSmith = pd.read_csv(
        fs.path("~/kernels/clsmith/features.csv"))

    # Extract features:
    BenchmarksFeatures = features(remove_duplicates(Benchmarks))
    CLgenFeatures = features(CLgen)
    GitHubFeatures = features(GitHub)
    CLSmithFeatures = features(CLSmith)

    BenchmarksFeatures, CLgenFeatures, GitHubFeatures, CLSmithFeatures = normalize(
        BenchmarksFeatures, CLgenFeatures, GitHubFeatures, CLSmithFeatures)

    stepMax = max(
        len(GitHubFeatures), len(CLgenFeatures), len(CLSmithFeatures))

    LOG_SCALE = True
    if LOG_SCALE:
        stepMin = 10
        steps = np.array(list(exp_range(stepMin, stepMax, 2 )), dtype=np.int32)
    else:
        stepMin = 100
        steps = np.array(list(range(stepMin, stepMax, 2500)), dtype=np.int32)

    GitHubSteps = [n for n in steps if n <= len(GitHubFeatures)]
    CLSmithSteps = [n for n in steps if n <= len(CLSmithFeatures)]
    CLgenSteps = [n for n  in steps if n <= len(CLgenFeatures)]

    numGitHubSteps = len(GitHubSteps)
    numCLSmithSteps = len(CLSmithSteps)
    numCLgenSteps = len(CLgenSteps)

    print("#. GitHub kernels", GitHubSteps[-1])
    print("#. CLSmith kernels", CLSmithSteps[-1])
    print("#. CLgen kernels", CLgenSteps[-1])

    # summarize_distance = np.median

    # Zero-distance
    def summarize_distance(distances, notzero):
        if notzero:
            return sum([1 if d < 5 and d > 0 else 0 for d in distances])
        else:
            return sum([1 if d < 5 else 0 for d in distances])

    summarize_results = np.mean
    def error(x):
        scale = np.std(x) / sqrt(len(x))
        mean = np.mean(x)
        ci = scipy.stats.norm.interval(.95, loc=mean, scale=scale)
        return ci[1] - mean

    print("Steps:", steps)

    datasets = [
        {
            "name": "GitHub",
            "color": "b",
            "linestyle": ":",
            "steps": GitHubSteps,
            "features": GitHubFeatures,
            "distances": np.zeros(numGitHubSteps, dtype=float),
            "errors": np.zeros(numGitHubSteps, dtype=float)
        },
        {
            "name": "CLSmith",
            "color": "r",
            "linestyle": "--",
            "steps": CLSmithSteps,
            "features": CLSmithFeatures,
            "distances": np.zeros(numCLSmithSteps, dtype=float),
            "errors": np.zeros(numCLSmithSteps, dtype=float)
        },
        {
            "name": "CLgen",
            "color": "g",
            "linestyle": "-",
            "steps": CLgenSteps,
            "features": CLgenFeatures,
            "distances": np.zeros(numCLgenSteps, dtype=float),
            "errors": np.zeros(numCLgenSteps, dtype=float)
        },
    ]

    # Ratio improvements:
    GitHubDistances = get_nearest_neighbour_distance(BenchmarksFeatures, GitHubFeatures)
    CLSmithDistances = get_nearest_neighbour_distance(BenchmarksFeatures, CLSmithFeatures)
    CLgenDistances = get_nearest_neighbour_distance(BenchmarksFeatures, CLgenFeatures)
    CLSmithCloser = [True if y < x else False for x,y in zip(CLSmithDistances, CLgenDistances)]
    CLgenCloser = [True if y < x else False for x,y in zip(GitHubDistances, CLgenDistances)]
    print("Closer CLSmith features:  {} ({:.1f} %)"
          .format(sum(CLSmithCloser), (sum(CLSmithCloser) / len(CLSmithCloser)) * 100))
    print("Closer CLgen features:  {} ({:.1f} %)"
          .format(sum(CLgenCloser), (sum(CLgenCloser) / len(CLgenCloser)) * 100))

    numIterations = 10
    # numIterations = 30

    # for i, row in enumerate(zip_longest(*[d["steps"] for d in datasets])):
    #     print(row)

    GitHub = datasets[0]
    CLSmith = datasets[1]
    CLgen = datasets[2]
    for i, n in enumerate(GitHub["steps"]):
        r = np.zeros(numIterations, dtype=float)
        for j in range(numIterations):
            np.random.shuffle(GitHub["features"])
            np.random.shuffle(CLSmith["features"])
            np.random.shuffle(CLgen["features"])

            GitHubDistances = get_nearest_neighbour_distance(
                BenchmarksFeatures, GitHub["features"][:n]
            )
            CLSmithDistances = get_nearest_neighbour_distance(
                BenchmarksFeatures, CLSmith["features"][:n]
            )
            CLgenDistances = get_nearest_neighbour_distance(
                BenchmarksFeatures, CLgen["features"][:n]
            )

    #         r[j] = summarize_distance()
    #         dataset["distances"][i] = summarize_results(r)
    #         dataset["errors"][i] = error(r)
    #     print("\nMin value of", dataset["name"], "with", dataset["steps"][-1],
    #           "kernels = {:.2f}".format(dataset["distances"][-1]))
    #     plt.errorbar(dataset["steps"], dataset["distances"],
    #                  yerr=dataset["errors"], c=dataset["color"],
    #                  linestyle=dataset["linestyle"],
    #                  label=dataset["name"], ecolor="k", capthick=1)
    # ax = plt.gca()

    # # No legend title
    # plt.legend(loc='upper left right')
    # ax.get_legend().set_title("")
    # ax.get_legend().draw_frame(True)

    # plt.xlim(stepMin, stepMax)
    # plt.ylim(0, )

    # if LOG_SCALE:
    #     ax.set_xscale("log", nonposy='clip')

    # plt.xlabel("#. kernels")
    # plt.ylabel("#. close kernels")

    # figsize = (4.5, 2.15)
    # viz.finalise(fs.path("~/cgo17/img/closer-than.pdf"), figsize=figsize)


def exp_range(start, end, mul):
    while start < end:
        yield start
        start *= mul
    yield end


def main():
    # closeness("A")
    # closeness("B")

    # static_kernel_closeness()
    zero_distance()
    # closer_than()


if __name__ == "__main__":
    main()
