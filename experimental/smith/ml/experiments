#!/usr/bin/env python3
#
from collections import Counter

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
import seaborn as sns
import sys
from numpy.random import RandomState
from os import system
from phd.lib.labm8 import fs
from phd.lib.labm8 import io
from phd.lib.labm8 import math as labmath
from phd.lib.labm8 import viz
from sklearn.base import clone
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from smith import cgo13


# Seaborn configuration:
sns.set(style="ticks", color_codes=True)
plt.style.use(["seaborn-white", "seaborn-paper"])


def get_cgo13_model():
  # return KNeighborsClassifier(1)
  return DecisionTreeClassifier(
      random_state=204,
      splitter="best",
      criterion="entropy",
      # max_depth=4,
      # min_samples_split=6,
      # min_samples_leaf=3
  )


def get_cgo13_features(D):
  return np.array([
    (D["transfer"].values / (D["comp"].values + D["mem"].values)),
    (D["coalesced"].values / D["mem"].values),
    ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
    (D["comp"].values / D["mem"].values),
  ]).T


def get_our_model():
  return KNeighborsClassifier(1)


def get_our_features(D):
  return np.array([
    D["comp"].values,
    D["rational"].values,
    D["mem"].values,
    D["localmem"].values,
    D["coalesced"].values,
    # D["atomic"].values,
    D["transfer"].values,
    D["wgsize"].values,
    (D["transfer"].values / (D["comp"].values + D["mem"].values)),
    (D["coalesced"].values / D["mem"].values),
    ((D["localmem"].values / D["mem"].values) * D["wgsize"].values),
    (D["comp"].values / D["mem"].values),
  ]).T


def get_raw_features(D):
  return np.array([
    D["comp"].values,
    D["rational"].values,
    D["mem"].values,
    D["localmem"].values,
    D["coalesced"].values,
    D["atomic"].values,
    D["transfer"].values,
    D["wgsize"].values,
  ]).T


def get_labels(D):
  return D["oracle"]


def customaxis(ax, c_left='k', c_bottom='k', c_right='none', c_top='none',
               lw=1, size=12, pad=8):
  """
  Credit: http://stackoverflow.com/a/11417222
  """
  for c_spine, spine in zip([c_left, c_bottom, c_right, c_top],
                            ['left', 'bottom', 'right', 'top']):
    if c_spine != 'none':
      ax.spines[spine].set_color(c_spine)
      ax.spines[spine].set_linewidth(lw)
    else:
      ax.spines[spine].set_color('none')
  if (c_bottom == 'none') & (c_top == 'none'):  # no bottom and no top
    ax.xaxis.set_ticks_position('none')
  elif (c_bottom != 'none') & (c_top != 'none'):  # bottom and top
    ax.tick_params(axis='x', direction='out', width=lw, length=7,
                   color=c_bottom, labelsize=size, pad=pad)
  elif (c_bottom != 'none') & (c_top == 'none'):  # bottom but not top
    ax.xaxis.set_ticks_position('bottom')
    ax.tick_params(axis='x', direction='out', width=lw, length=7,
                   color=c_bottom, labelsize=size, pad=pad)
  elif (c_bottom == 'none') & (c_top != 'none'):  # no bottom but top
    ax.xaxis.set_ticks_position('top')
    ax.tick_params(axis='x', direction='out', width=lw, length=7,
                   color=c_top, labelsize=size, pad=pad)
  if (c_left == 'none') & (c_right == 'none'):  # no left and no right
    ax.yaxis.set_ticks_position('none')
  elif (c_left != 'none') & (c_right != 'none'):  # left and right
    ax.tick_params(axis='y', direction='out', width=lw, length=7,
                   color=c_left, labelsize=size, pad=pad)
  elif (c_left != 'none') & (c_right == 'none'):  # left but not right
    ax.yaxis.set_ticks_position('left')
    ax.tick_params(axis='y', direction='out', width=lw, length=7,
                   color=c_left, labelsize=size, pad=pad)
  elif (c_left == 'none') & (c_right != 'none'):  # no left but right
    ax.yaxis.set_ticks_position('right')
    ax.tick_params(axis='y', direction='out', width=lw, length=7,
                   color=c_right, labelsize=size, pad=pad)


def leave_one_benchmark_out(clf, get_features, D, benchmark):
  # Create data masks. For training we exclude all results from
  # the test benchmark.
  test_mask = D["benchmark"].str.contains(r"^" + benchmark)
  train_mask = ~test_mask

  # Create training and testing data:
  X_train = get_features(D[train_mask])
  y_train = get_labels(D[train_mask])

  D_test = D[test_mask]
  X_test = get_features(D_test)
  y_test = get_labels(D_test)

  # Train classifier:
  clf.fit(X_train, y_train)

  # Make predictions
  predicted = clf.predict(X_test)
  D_out = []
  for d, y, p in zip(D_test.to_dict('records'), y_test, predicted):
    d["p"] = p
    d["p_correct"] = 1 if y == p else 0
    D_out.append(d)

  # Return a list of dicts
  return D_out


def train_on_synthetics(clf, get_features, D, benchmark):
  # Create data masks. For training we exclude all results from
  # the test benchmark.
  test_mask = D["benchmark"].str.contains(r"^" + benchmark)
  train_mask = D["synthetic"] == 1

  # Create training and testing data:
  X_train = get_features(D[train_mask])
  y_train = get_labels(D[train_mask])

  D_test = D[test_mask]
  X_test = get_features(D_test)
  y_test = get_labels(D_test)

  # Train classifier:
  clf.fit(X_train, y_train)

  # Make predictions
  predicted = clf.predict(X_test)
  D_out = []
  for d, y, p in zip(D_test.to_dict('records'), y_test, predicted):
    d["p"] = p
    d["p_correct"] = 1 if y == p else 0
    D_out.append(d)

  # Return a list of dicts
  return D_out


def rand_jitter(arr, factor=0.01, randomstate=RandomState(204)):
  stdev = factor * (max(arr) - min(arr))
  return arr + randomstate.randn(len(arr)) * stdev


def scatter_with_jitter(plt, x, y, **kwargs):
  jitter_opts = kwargs.get("jitter_opts", {})
  if "jitter_opts" in kwargs: kwargs.pop("jitter_opts")

  jitter_factor = kwargs.get("jitter", None)
  if jitter_factor is not None:
    jitter_opts["factor"] = jitter_factor
    kwargs.pop("jitter")

  return plt.scatter(rand_jitter(x, **jitter_opts),
                     rand_jitter(y, **jitter_opts), **kwargs)


def motivation():
  #######################
  # Motivation Baseline #
  #######################
  clf = get_cgo13_model()
  platform = "b"
  suite = "parboil"

  # Load data and mask off the benchmark suite in use:
  B = pd.read_csv("data/{}/benchmarks.csv".format(platform))
  test_mask = B["benchmark"].str.contains(r"^{}-".format(suite))
  B = B[test_mask]

  benchmark_names = sorted(set([
    re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)", b).group(1)
    for b in B["benchmark"] if b.startswith(suite)
  ]))

  B_out = []
  for benchmark in benchmark_names:
    B_out += leave_one_benchmark_out(clf, get_cgo13_features, B, benchmark)
  B_out = pd.DataFrame(B_out)
  assert (len(B) == len(B_out))

  pca = PCA(n_components=2)
  pca.fit(get_raw_features(B))

  X = pca.transform(get_raw_features(B))
  jitter = .075

  # Apply jitter and repack
  x, y = zip(*X)
  x = rand_jitter(x, jitter, RandomState(204))
  y = rand_jitter(y, jitter, RandomState(205))
  X = list(zip(x, y))

  correct = [x for x, b in zip(X, B_out.to_dict('records')) if b["p_correct"]]
  incorrect = [x for x, b in zip(X, B_out.to_dict('records')) if
               not b["p_correct"]]

  print()
  print("Motivation:")
  print("  #. correct:   ", len(correct),
        "({:.1f}%)".format((len(correct) / len(X)) * 100))
  print("  #. incorrect: ", len(incorrect))
  print()

  # Scatter type:
  plot_opts = {"s": 85, "alpha": .65}
  plt.scatter(*zip(*incorrect),
              color="r", marker="v", label='Incorrect', **plot_opts)
  plt.scatter(*zip(*correct),
              color="b", marker="^", label='Correct', **plot_opts)

  ax = plt.gca()
  # No tick labels:
  ax.set_xticklabels([])
  ax.set_yticklabels([])
  # customaxis(ax)

  # Axis labels
  plt.xlabel(r"Principle Component 1 $\rightarrow$", ha="right")
  plt.ylabel(r"Principle Component 2 $\rightarrow$", ha="right")

  # Position axis labels at end of axis
  ax.xaxis.set_label_coords(1, -.025)
  ax.yaxis.set_label_coords(-.025, 1)

  # Show legend
  handles, labels = ax.get_legend_handles_labels()
  ax.legend(handles[::-1], labels[::-1])
  ax.get_legend().draw_frame(True)

  xlim = ax.get_xlim()
  ylim = ax.get_ylim()

  figsize = (2.5, 2.5)
  viz.finalise(fs.path("~/cgo17/img/motivation-a.pdf"),
               figsize=figsize, tight=True)

  # Reset classifier
  clf = clone(clf)

  ##############################
  # With additional benchmarks #
  ##############################
  B = pd.read_csv("data/{}/benchmarks.csv".format(platform))
  others = [
    "shoc-1.1.5-FFT-ifft1D_512",
    "shoc-1.1.5-S3D-ratt6_kernel",
    "shoc-1.1.5-S3D-ratx2_kernel",
    "shoc-1.1.5-S3D-ratt9_kernel",
  ]

  train_mask = B["benchmark"].str.contains(r"^{}|{}"
                                           .format(suite, "|".join(others)))
  test_mask = B["benchmark"].str.contains(r"^{}".format(suite))
  other_mask = B["benchmark"].str.contains("|".join(others))

  Btrain = B[train_mask]
  Btest = B[test_mask]
  Bother = B[other_mask]

  B_out = []
  for benchmark in benchmark_names:
    B_out += leave_one_benchmark_out(
        clf, get_cgo13_features, Btrain, benchmark)
  B_out = pd.DataFrame(B_out)
  assert (len(Btest) == len(B_out))

  X = pca.transform(get_raw_features(Btest))

  # Apply jitter
  x, y = zip(*X)
  x = rand_jitter(x, jitter, RandomState(204))
  y = rand_jitter(y, jitter, RandomState(205))
  X = list(zip(x, y))

  correct = [
    x for x, b in zip(X, B_out.to_dict('records')) if b["p_correct"]]
  incorrect = [
    x for x, b in zip(X, B_out.to_dict('records')) if not b["p_correct"]]
  other = pca.transform(get_raw_features(Bother))

  print()
  print("Motivation w. other suite:")
  print("  #. correct:   ", len(correct),
        "({:.1f}%)".format((len(correct) / len(X)) * 100))
  print("  #. incorrect: ", len(incorrect))
  print("  #. additional:", len(Bother))
  print()

  scatter_with_jitter(plt, *zip(*other[:3]),
                      color="g", marker="o", label="Additional",
                      jitter=jitter, **plot_opts)
  plt.scatter(*zip(*incorrect),
              color="r", marker="v", label='Incorrect', **plot_opts)
  plt.scatter(*zip(*correct),
              color="b", marker="^", label='Correct', **plot_opts)

  ax = plt.gca()
  # No tick labels:
  ax.set_xticklabels([])
  ax.set_yticklabels([])
  # customaxis(ax)

  # Axis labels
  plt.xlabel(r"Principle Component 1 $\rightarrow$", ha="right")
  plt.ylabel(r"Principle Component 2 $\rightarrow$", ha="right")

  # Position axis labels at end of axis
  ax.xaxis.set_label_coords(1, -.025)
  ax.yaxis.set_label_coords(-.025, 1)

  # Show legend
  handles, labels = ax.get_legend_handles_labels()
  ax.legend(handles[::-1], labels[::-1])
  ax.get_legend().draw_frame(True)

  # Set same axis limits as before
  ax.set_xlim(xlim)
  ax.set_ylim(ylim)

  # plt.show()
  viz.finalise(fs.path("~/cgo17/img/motivation-b.pdf"),
               figsize=figsize, tight=True)

  ##########################
  # Survey of GPGPU papers #
  ##########################

  # Load data from literature review:
  data = pd.read_csv("data/motivation-survey.csv", delimiter="\t")

  benchmark_counts = [x[1] for x in data.groupby("Paper").size().iteritems()]
  avg_benchmark_count = labmath.mean(benchmark_counts)
  median_benchmark_count = labmath.median(benchmark_counts)

  suites = pd.DataFrame(
      [(x[0], x[1] / avg_benchmark_count)
       for x in data.groupby(["Benchmark Suite"]).size().iteritems()],
      columns=["Suite", "#. benchmarks"])

  # To sort by frequency count:
  suites.sort_values("#. benchmarks", inplace=True, ascending=False)
  # To sort by alphabetical:
  # suites.sort_values("Suite", inplace=True)

  big4 = suites[:4]
  big4_suites = [x["Suite"] for x in big4.to_dict('records')]
  big4_count = sum([x["#. benchmarks"] for x in big4.to_dict('records')])
  total_count = sum([x["#. benchmarks"] for x in suites.to_dict('records')])

  print()
  print("Literature review:")
  print("  Average number of benchmarks:", avg_benchmark_count)
  print("  Median number of benchmarks: ", median_benchmark_count)
  print("  Big4:                        ", ",".join(big4_suites))
  print("  Big4 ratio:                   {:.1f}%"
        .format((big4_count / total_count) * 100))
  print()
  print(suites)
  print()

  ax = sns.barplot(x="Suite", y="#. benchmarks", data=suites)

  # customaxis(ax)

  # Add threshold line:
  plt.axhline(y=.5, color="k", lw=1)

  # Rotate x labels:
  plt.setp(ax.get_xticklabels(), rotation=90)

  plt.xlabel("")
  plt.ylabel("#. benchmarks used")

  figsize = (4, 2.25)
  viz.finalise(fs.path("~/cgo17/img/motivation-c.pdf"),
               figsize=figsize, tight=True)

  # Train on one benchmark suite, test on another:
  with open(fs.path("~/cgo17/tab/motivation-amd.tex"), "w") as outfile:
    xval_benchmarks("data/a/benchmarks.csv", file=outfile)
  with open(fs.path("~/cgo17/tab/motivation-nvidia.tex"), "w") as outfile:
    xval_benchmarks("data/b/benchmarks.csv", file=outfile)


def xval_benchmarks(path, file=sys.stdout):
  # Cross-validate across benchmarks
  data = cgo13.LabelledData.from_csv(
      path, group_by="suite")
  metrics = cgo13.classification(data, group_by="suite")
  results = np.zeros((len(metrics), len(metrics[0])))
  for j in range(len(metrics)):
    for i in range(len(metrics[j])):
      results[j, i] = metrics[j][i].oracle
  results = results.T

  groups = sorted(set(data["Group"]))

  def shortlabels(groups):
    return [re.sub("-.+$", "", x) for x in groups]

  # Print table:
  print("\\begin{{tabular}}{{ R{{1.5cm}} {} }}"
        .format(" ".join(["C{1.5cm}"] * len(groups))),
        file=file)
  print("\\toprule", file=file)
  print("& " + " & ".join(["\\textbf{" + escape_suite_name(g) + "}"
                           for g in groups]) + "\\\\",
        file=file)
  print("\\midrule", file=file)
  for i, row in enumerate(results):
    print("\\textbf{" + escape_suite_name(groups[i]) + "} & " +
          " & ".join(["{:.1f}\\%".format(x * 100) if j != i else "-"
                      for j, x in enumerate(row)]) + "\\\\",
          file=file)
  print("\\bottomrule", file=file)
  print("\\end{tabular}", file=file)


def speedup_with_synthetic_benchmarks(platform_id):
  #
  # Cross-validate CGO13 model across NPB benchmarks, with and
  # without synthetic benchmarks.
  #
  # Datasets:
  #
  #   B -  benchmarks
  #   S -  synthetics
  #   BS - benchmarks + synthetics
  #
  B = pd.read_csv("data/{platform_id}/benchmarks.csv"
                  .format(platform_id=platform_id.lower()))
  B["group"] = ["B"] * len(B)

  S = pd.read_csv("~/cldrive/data/clgen/platform-{platform_id}.csv"
                  .format(platform_id=platform_id.lower()))
  S["group"] = ["S"] * len(S)

  G = pd.read_csv(fs.path("~/cldrive/data/github/platform-{platform_id}.csv")
                  .format(platform_id=platform_id.lower()))
  G["group"] = ["G"] * len(G)

  BS = pd.concat((B, S))
  BG = pd.concat((B, G))

  # Find the ZeroR. This is the device which is most frequently
  # optimal:
  Bmask = B[B["benchmark"].str.contains("npb-3.3-")]

  zeror = Counter(Bmask["oracle"]).most_common(1)[0][0]
  zeror_freq = Counter(Bmask["oracle"]).most_common(1)[0][1]
  zeror_ratio = zeror_freq / len(Bmask)
  zeror_runtime = "runtime_" + zeror.lower()

  # Get the names of the benchmarks, in the form: $suite-$version-$benchmark
  benchmark_names = sorted(set([
    re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)-", b).group(1)
    for b in B["benchmark"] if b.startswith("npb-")
  ]))

  B_out, BS_out, BG_out = [], [], []
  for benchmark in benchmark_names:
    # CGO13 predictive model:
    clf = get_cgo13_model()
    features = get_cgo13_features
    # clf = KNeighborsClassifier(n_neighbors=1)

    # Cross validate on baseline:
    B_out += leave_one_benchmark_out(clf, get_cgo13_features, B, benchmark)

    # Reset model.
    clf = get_cgo13_model()

    # Repeate cross-validation with synthetic kernels:
    BS_out += leave_one_benchmark_out(clf, get_cgo13_features, BS, benchmark)

    # Reset model.
    clf = get_cgo13_model()

    # Repeate cross-validation with GitHub kernels:
    BG_out += leave_one_benchmark_out(clf, get_cgo13_features, BG, benchmark)

    assert (len(B_out) == len(BS_out))
    assert (len(B_out) == len(BG_out))

  # Create results frame:
  R_out = []
  for b, bs, bg in zip(B_out, BS_out, BG_out):
    # Get runtimes of device using predicted device.
    b_p_runtime = b["runtime_" + b["p"].lower()]
    bs_p_runtime = bs["runtime_" + bs["p"].lower()]
    bg_p_runtime = bg["runtime_" + bg["p"].lower()]

    # Speedup is the ratio of runtime using the predicted device
    # over runtime using ZeroR device.
    b["p_speedup"] = b_p_runtime / b[zeror_runtime]
    bs["p_speedup"] = bs_p_runtime / bs[zeror_runtime]
    bg["p_speedup"] = bg_p_runtime / bg[zeror_runtime]

    # Oracle is the ratio of runtime using the best device vs
    # runtime using predicted device.
    b["p_oracle"] = b["runtime"] / b_p_runtime
    bs["p_oracle"] = bs["runtime"] / bs_p_runtime
    bg["p_oracle"] = bg["runtime"] / bg_p_runtime

    # Get the group label, in the form $benchmark.$dataset:
    group = re.sub(r"[^-]+-[0-9\.]+-([^-]+)-.+", r"\1",
                   b["benchmark"]) + "." + b["dataset"]
    b["group"] = group
    bg["group"] = group
    bs["group"] = group

    # Set the training data type.
    b["training"] = "Grewe et al."
    bg["training"] = "w. GitHub"
    bs["training"] = "w. CLgen"

    R_out.append(b)
    # R_out.append(bg)
    R_out.append(bs)
  R = pd.DataFrame(R_out)

  b_mask = R["training"] == "Grewe et al."
  bs_mask = R["training"] == "w. CLgen"
  bg_mask = R["training"] == "w. GitHub"

  B_acc = labmath.mean(R[b_mask].groupby(["group"])["p_correct"].mean())
  BG_acc = labmath.mean(R[bg_mask].groupby(["group"])["p_correct"].mean())
  BS_acc = labmath.mean(R[bs_mask].groupby(["group"])["p_correct"].mean())

  B_oracle = labmath.mean(R[b_mask].groupby(["group"])["p_oracle"].mean())
  BG_oracle = labmath.mean(R[bg_mask].groupby(["group"])["p_oracle"].mean())
  BS_oracle = labmath.mean(R[bs_mask].groupby(["group"])["p_oracle"].mean())

  B_speedups = R[b_mask].groupby(["group"])["p_speedup"].mean()
  BS_speedups = R[bs_mask].groupby(["group"])["p_speedup"].mean()

  B_speedup = labmath.mean(B_speedups)
  BG_speedup = labmath.mean(R[bg_mask].groupby(["group"])["p_speedup"].mean())
  BS_speedup = labmath.mean(BS_speedups)

  B_geo = labmath.geomean(R[b_mask].groupby(["group"])["p_speedup"].mean())
  BG_geo = labmath.geomean(R[bg_mask].groupby(["group"])["p_speedup"].mean())
  BS_geo = labmath.geomean(R[bs_mask].groupby(["group"])["p_speedup"].mean())

  # Print analytics:
  print()
  print("Experiment 1 on Platform {}:".format(platform_id.upper()))
  print("  #. benchmarks:          ",
        len(set(B["benchmark"])), "kernels,",
        len(B), "observations")
  print("  #. githubs:             ",
        len(set(G["benchmark"])), "kernels,",
        len(G), "observations")
  print("  #. synthetic:           ",
        len(set(S["benchmark"])), "kernels,",
        len(S), "observations")
  print()
  print("  ZeroR device:            {} ({:.1f} %)"
        .format(zeror, zeror_ratio * 100))
  print()
  print("  Accuracy of baseline:    {:.1f} %".format(B_acc * 100))
  print("  Accuracy w. GitHub:      {:.1f} %".format(BG_acc * 100))
  print("  Accuracy w. CLgen:       {:.1f} %".format(BS_acc * 100))
  print()
  print("  Oracle of baseline:      {:.1f} %".format(B_oracle * 100))
  print("  Oracle w. GitHub:        {:.1f} %".format(BG_oracle * 100))
  print("  Oracle w. CLgen:         {:.1f} %".format(BS_oracle * 100))
  print()
  print("  Speedup of baseline:     {:.2f} x".format(B_speedup))
  print("  Speedup w. GitHub:       {:.2f} x".format(BG_speedup))
  print("  Speedup w. CLgen:        {:.2f} x".format(BS_speedup))
  print()
  print("  Geomean Speedup of baseline:    {:.2f} x".format(B_geo))
  print("  Geomean Speedup w. GitHub:      {:.2f} x".format(BG_geo))
  print("  Geomean Speedup w. CLgen:       {:.2f} x".format(BS_geo))

  matched_or_improved = [True for x, y in zip(B_speedups, BS_speedups)
                         if y >= x]
  improved = [True for x, y in zip(B_speedups, BS_speedups)
              if y > x]
  worsened = [True for x, y in zip(B_speedups, BS_speedups)
              if y < x - .02]

  print()
  print("  Improved:                {} ({:.1f} %)"
        .format(len(improved), (len(improved) / len(B_speedups)) * 100))
  print("  Num matched or improved: {} ({:.1f} %)"
        .format(len(matched_or_improved),
                (len(matched_or_improved) / len(B_speedups)) * 100))
  print("  Worsened:                {} ({:.1f} %)"
        .format(len(worsened), (len(worsened) / len(B_speedups)) * 100))
  print()

  # Plot summary:
  num_hues = len(set(R["training"]))
  palette = sns.cubehelix_palette(num_hues, rot=-.4, light=.85, dark=.35)

  # average bars:
  R = R.append({
    "group": "Average",
    "p_speedup": B_speedup,
    "training": "Grewe et al."
  }, ignore_index=True)
  R = R.append({
    "group": "Average",
    "p_speedup": BS_speedup,
    "training": "w. CLgen"
  }, ignore_index=True)
  # # geomean bars:
  # R = R.append({
  #     "group": "Geomean",
  #     "p_speedup": B_geo,
  #     "training": "Grewe et al."
  # }, ignore_index=True)
  # R = R.append({
  #     "group": "Geomean",
  #     "p_speedup": BS_geo,
  #     "training": "w. CLgen"
  # }, ignore_index=True)

  # Negative offset so that bars start at 1:
  R["p_speedup"] -= 1

  ax = sns.barplot(
      x="group", y="p_speedup", data=R, ci=None, hue="training",
      palette=palette)
  plt.ylabel("Speedup")
  plt.xlabel("")

  # Speedup line
  plt.axhline(y=0, color="k", lw=1)
  plt.axvline(x=plt.xlim()[1] - 1, color="k", lw=1, linestyle="--")

  # No legend title
  ax.get_legend().set_title("")
  plt.legend(loc='upper right')
  ax.get_legend().draw_frame(True)

  figsize = (9, 2.2)
  if platform_id == "B":
    plt.ylim(-1, 16)
    typecast = int
  else:
    typecast = float

  # Counter negative offset:
  ax.set_yticklabels([typecast(i) + 1 for i in ax.get_yticks()])

  plt.setp(ax.get_xticklabels(), rotation=90)

  # plt.show()
  viz.finalise(fs.path("~/cgo17/img/ex1-{}.pdf"
                       .format(platform_id.upper())),
               figsize=figsize, tight=True)


def eigenspace(platform_id):
  B = pd.read_csv("data/{}/benchmarks.csv".format(platform_id))
  S = pd.read_csv("data/{}/synthetics.csv".format(platform_id))
  BS = pd.concat((B, S))

  features = get_raw_features
  pca = PCA(n_components=2)
  pca.fit(features(BS))

  # benchmarks = pd.DataFrame([x for x in pca.transform(features(B))][:100],
  #                           columns=["E1", "E2"])
  # benchmarks["synthetic"] = np.zeros(len(benchmarks))
  # synthetics = pd.DataFrame([x for x in pca.transform(features(S))][:100],
  #                           columns=["E1", "E2"])
  # synthetics["synthetic"] = np.ones(len(synthetics))
  # eigens = pd.concat((benchmarks, synthetics))
  #
  # sns.pairplot(eigens, hue="synthetic")

  benchmarks = pca.transform(features(B))
  synthetics = pca.transform(features(S))[:100]

  plot_opts = {"s": 35, "alpha": .65}
  plt.scatter(*zip(*benchmarks),
              color="r", marker="v", label='Benchmarks', **plot_opts)
  plt.scatter(*zip(*synthetics),
              color="b", marker="^", label='Synthetics', **plot_opts)
  # plt.xlim(3200000, 3700000)
  plt.ylim(2, 5)

  plt.show()


def predictive_models():
  B = pd.read_csv("data/b/benchmarks.csv")
  S = pd.read_csv("data/b/synthetics.csv")
  BS = pd.concat((B, S))

  model1 = get_cgo13_model()
  model2 = get_cgo13_model()
  features = get_cgo13_features

  Btrain_mask = ~B["benchmark"].str.contains(r"^npb-3.3-CG")
  BStrain_mask = ~BS["benchmark"].str.contains(r"^npb-3.3-CG")

  BX_train = features(B[Btrain_mask])
  By_train = get_labels(B[Btrain_mask])

  BSX_train = features(BS[BStrain_mask])
  BSy_train = get_labels(BS[BStrain_mask])

  print("")
  model1.fit(BX_train, By_train)

  def visualize_tree(model, path, max_depth=None):
    path = fs.path(path)
    export_graphviz(model, label=None, filled=True,
                    feature_names=["F1", "F2", "F3", "F4"],
                    impurity=False, proportion=True,
                    rounded=True, max_depth=max_depth)
    # Remove the 2nd and 3rd lines from split nodes:
    system(r"sed -r 's/\\n[0-9\.]+%\\n\[[0-9\.]+, [0-9\.]+\]//' -i tree.dot")

    # Convert .dot to .ps:
    system("dot -Tpdf tree.dot -o " + path)
    io.info("Wrote", path)

    # Clean up
    system("rm tree.dot")

  model1.fit(BX_train, By_train)
  visualize_tree(model1, "~/cgo17/img/tree-a.pdf", 2)

  model2.fit(BSX_train, BSy_train)
  visualize_tree(model2, "~/cgo17/img/tree-b.pdf", 2)


def experiment1():
  # eigenspace("B")
  predictive_models()
  speedup_with_synthetic_benchmarks("A")
  speedup_with_synthetic_benchmarks("B")


def escape_suite_name(g):
  c = g.split('-')
  if (c[0] == "shoc" or
      c[0] == "npb" or
      c[0] == "nvidia" or
      c[0] == "amd"):
    return c[0].upper()
  else:
    return c[0].capitalize()


def escape_benchmark_name(g):
  c = g.split('-')
  return escape_suite_name(g) + "." + c[-2]


def experiment2():
  #
  # Cross-validate across all benchmarks using CGO13 model and our
  # own, with and without synthetic benchmarks. Report per-platform
  # speedup of our model over CGO13.
  #
  def compare_clfs(clf1, get_features1, clf2, get_features2, D1, D2, benchmark):
    test1_mask = D1["benchmark"].str.contains(r"^" + benchmark)
    test2_mask = D2["benchmark"].str.contains(r"^" + benchmark)
    assert (len(D1[test1_mask]) == len(D2[test2_mask]))

    # Create data masks. For training we exclude all results from
    # benchmark.
    train1_mask = ~test1_mask
    train2_mask = ~test2_mask

    # Create training and testing data:
    X1_train = get_features1(D1.loc[train1_mask])
    X2_train = get_features2(D2.loc[train2_mask])
    y1_train = get_labels(D1[train1_mask])
    y2_train = get_labels(D2[train2_mask])

    D1_test = D1[test1_mask]
    D2_test = D2[test2_mask]
    X1_test = get_features1(D1.loc[test1_mask])
    X2_test = get_features2(D2.loc[test2_mask])
    y1_test = get_labels(D1_test)
    y2_test = get_labels(D2_test)

    # Train classifiers:
    clf1.fit(X1_train, y1_train)
    clf2.fit(X2_train, y2_train)

    # Make predictions
    predicted1 = clf1.predict(X1_test)
    predicted2 = clf2.predict(X2_test)

    D_out = []
    for d, y, p1, p2 in zip(D1_test.to_dict('records'), y1_test,
                            predicted1, predicted2):
      d["p1"], d["p2"] = p1, p2
      d["p1_correct"] = 1 if y == p1 else 0
      d["p2_correct"] = 1 if y == p2 else 0
      D_out.append(d)

    # Return a list of dicts
    return D_out

  aB = pd.read_csv("data/a/benchmarks.csv")
  aB["synthetic"] = np.zeros(len(aB))
  bB = pd.read_csv("data/b/benchmarks.csv")
  bB["synthetic"] = np.zeros(len(bB))
  B = pd.concat((aB, bB))

  aS = pd.read_csv("data/a/synthetics.csv")
  aS["synthetic"] = np.ones(len(aS))
  bS = pd.read_csv("data/b/synthetics.csv")
  bS["synthetic"] = np.ones(len(bS))
  S = pd.concat((aS, bS))

  aBS = pd.concat((aB, aS))
  bBS = pd.concat((bB, bS))
  BS = pd.concat((B, S))

  assert (len(B) == len(aB) + len(bB))
  assert (len(S) == len(aS) + len(bS))
  assert (len(BS) == len(aBS) + len(bBS))

  # Get benchmark names: <suite>-<benchmark>
  benchmark_names = sorted(set([
    re.match(r"^([^0-9]+-[0-9\.]+-[^-]+)", b).group(1)
    for b in B["benchmark"]
  ]))

  # Perform cross-validation:
  B_out = []
  for i, benchmark in enumerate(benchmark_names):
    print("\r", i + 1, benchmark, end="")
    cgo13_clf = get_cgo13_model()
    our_clf = get_our_model()

    cgo13_features = get_cgo13_features
    our_features = get_our_features

    # Cross validate on baseline:
    tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,
                       aBS, aBS, benchmark)
    for d in tmp:
      d["platform"] = "AMD Tahiti 7970"
    B_out += tmp

    # Reset models.
    cgo13_clf = get_cgo13_model()
    our_clf = get_our_model()

    # Same as before, on other platform:
    tmp = compare_clfs(cgo13_clf, cgo13_features, our_clf, our_features,
                       bBS, bBS, benchmark)
    for d in tmp:
      d["platform"] = "NVIDIA GTX 970"
    B_out += tmp
  print()

  # Create results frame:
  R_out = []
  # Get runtimes of device using predicted device.
  for b in B_out:
    p1_runtime = b["runtime_" + b["p1"].lower()]
    p2_runtime = b["runtime_" + b["p2"].lower()]

    # Speedup is the ratio of runtime using our predicted device
    # over runtime using CGO13 predicted device.
    b["p_speedup"] = p2_runtime / p1_runtime

    # Oracle is the ratio of runtime using the best device vs
    # runtime using predicted device.
    b["p_oracle"] = b["runtime"] / p2_runtime

    # Get the benchmark name:
    b["group"] = escape_benchmark_name(b["benchmark"])

    R_out.append(b)
  R = pd.DataFrame(R_out)

  improved = R[R["p_speedup"] > 1]

  Amask = R["platform"] == "AMD Tahiti 7970"
  Bmask = R["platform"] == "NVIDIA GTX 970"
  a = R[Amask]
  b = R[Bmask]

  a_p1_accuracy = labmath.mean(a.groupby(["group"])["p1_correct"].mean())
  b_p1_accuracy = labmath.mean(b.groupby(["group"])["p1_correct"].mean())

  a_p2_accuracy = labmath.mean(a.groupby(["group"])["p2_correct"].mean())
  b_p2_accuracy = labmath.mean(b.groupby(["group"])["p2_correct"].mean())

  a_speedups = a.groupby(["group"])["p_speedup"].mean()
  b_speedups = b.groupby(["group"])["p_speedup"].mean()

  a_speedup = labmath.mean(a_speedups)
  b_speedup = labmath.mean(b_speedups)

  a_geo = labmath.geomean(a_speedups)
  b_geo = labmath.geomean(b_speedups)

  a_matched_or_improved = [x for x in a_speedups if x >= 1]
  b_matched_or_improved = [x for x in b_speedups if x >= 1]

  a_improved = [x for x in a_speedups if x > 1]
  b_improved = [x for x in b_speedups if x > 1]

  a_worsened = [x for x in a_speedups if x < 1]
  b_worsened = [x for x in b_speedups if x < 1]

  # Sanity-checks:
  assert (len(R) == len(a) + len(b))
  assert (len(a_matched_or_improved) + len(a_worsened) == len(a_speedups))
  assert (len(b_matched_or_improved) + len(b_worsened) == len(b_speedups))

  print()
  print("Experiment 2 on AMD Tahiti 7970:")
  print("  Accuracy of baseline:    {:.1f} %".format(a_p1_accuracy * 100))
  print("  Accuracy of our model:   {:.1f} %".format(a_p2_accuracy * 100))
  print()
  print("  Speedup over CGO13:      {:.2f} x".format(a_speedup))
  print("  Geomean over CGO13:      {:.2f} x".format(a_geo))
  print()
  print("  Num matched or improved: {} ({:.1f} %)"
        .format(len(a_matched_or_improved),
                (len(a_matched_or_improved) / len(a_speedups)) * 100))
  print("  Num improved:            {} ({:.1f} %)"
        .format(len(a_improved),
                (len(a_improved) / len(a_speedups)) * 100))
  print("  Num worsened:            {} ({:.1f} %)"
        .format(len(a_worsened),
                (len(a_worsened) / len(a_speedups)) * 100))
  print()
  print("Experiment 2 on NVIDIA GTX 970:")
  print("  Accuracy of baseline:    {:.1f} %".format(b_p1_accuracy * 100))
  print("  Accuracy of our model:   {:.1f} %".format(b_p2_accuracy * 100))
  print("  Speedup over CGO13:      {:.2f} x".format(b_speedup))
  print("  Geomean over CGO13:      {:.2f} x".format(b_geo))
  print()
  print("  Num matched or improved: {} ({:.1f} %)"
        .format(len(b_matched_or_improved),
                (len(b_matched_or_improved) / len(b_speedups)) * 100))
  print("  Num improved:            {} ({:.1f} %)"
        .format(len(b_improved),
                (len(b_improved) / len(b_speedups)) * 100))
  print("  Num worsened:            {} ({:.1f} %)"
        .format(len(b_worsened),
                (len(b_worsened) / len(b_speedups)) * 100))
  print()

  num_hues = len(set(R["platform"]))
  palette = sns.cubehelix_palette(num_hues, start=4, rot=.8, light=.8, dark=.3)

  # average bars:
  R = R.append({
    "group": "Average",
    "p_speedup": a_speedup,
    "platform": "AMD Tahiti 7970"
  }, ignore_index=True)
  R = R.append({
    "group": "Average",
    "p_speedup": b_speedup,
    "platform": "NVIDIA GTX 970"
  }, ignore_index=True)
  # # geomean bars:
  # R = R.append({
  #     "group": "Geomean",
  #     "p_speedup": a_geo,
  #     "training": "Grewe et al."
  # }, ignore_index=True)
  # R = R.append({
  #     "group": "Geomean",
  #     "p_speedup": b_geo,
  #     "training": "w. CLgen"
  # }, ignore_index=True)

  # Negative offset so that bars start at 1:
  R["p_speedup"] -= 1

  ax = sns.barplot(x="group", y="p_speedup", hue="platform", data=R,
                   palette=palette, ci=None)

  plt.ylabel("Speedup over Grewe et al.")
  plt.xlabel("")

  plt.axhline(y=0, color="k", lw=1)
  plt.axvline(x=plt.xlim()[1] - 1, color="k", lw=1, linestyle="--")

  # Rotate x ticks
  plt.setp(ax.get_xticklabels(), rotation=90)

  # Set plot limit:
  plt.ylim(-1, 9)

  # Counter negative offset:
  ax.set_yticklabels([int(i) + 1 for i in ax.get_yticks()])

  # No legend title
  ax.get_legend().set_title("")
  plt.legend(loc='upper right')
  ax.get_legend().draw_frame(True)

  figsize = (9, 4)
  viz.finalise(fs.path("~/cgo17/img/ex2.pdf"), figsize=figsize, tight=True)


def main():
  motivation()
  experiment1()
  experiment2()


if __name__ == "__main__":
  main()
