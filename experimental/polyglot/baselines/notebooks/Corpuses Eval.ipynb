{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "from datasets.github.scrape_repos import contentfiles\n",
    "from deeplearning.clgen.corpuses import corpuses\n",
    "from deeplearning.clgen.proto import corpus_pb2\n",
    "from lib.labm8 import bazelutil\n",
    "from lib.labm8 import pbutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 corpuses\n"
     ]
    }
   ],
   "source": [
    "def LoadCorpuses(directory, proto_class):\n",
    "    return {\n",
    "        name.stem: corpuses.Corpus(pbutil.FromFile(directory / name, proto_class()))\n",
    "        for name in directory.iterdir()\n",
    "        if not str(name.stem).startswith('c-')\n",
    "    }\n",
    "\n",
    "corpuses_dir = bazelutil.DataPath('phd/experimental/polyglot/baselines/corpuses')\n",
    "corps = LoadProtos(corpuses_dir, corpus_pb2.Corpus)\n",
    "print('Loaded', len(corps), 'corpuses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputContentfiles(local_directory: pathlib.Path) -> pathlib.Path:\n",
    "    path = pathlib.Path(f'/var/phd/datasets/github/repos_by_lang/{local_directory.stem}.db')\n",
    "    if path.is_file():\n",
    "        return path\n",
    "    else:\n",
    "        raise FileNotFoundError(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8ae3450c4e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m df = pd.DataFrame(\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mCorpusStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     columns=['Language', 'Repositories', 'Files', 'Lines', 'Encoding', \n\u001b[1;32m     24\u001b[0m              'Vocab size', 'Corpus size'])\n",
      "\u001b[0;32m<ipython-input-51-8ae3450c4e93>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m df = pd.DataFrame(\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mCorpusStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     columns=['Language', 'Repositories', 'Files', 'Lines', 'Encoding', \n\u001b[1;32m     24\u001b[0m              'Vocab size', 'Corpus size'])\n",
      "\u001b[0;32m<ipython-input-51-8ae3450c4e93>\u001b[0m in \u001b[0;36mCorpusStats\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lines'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{:.1f}M'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontentfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContentFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinecount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Character'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'Ascii'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'Token'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Vocab size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Corpus size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{:.1f}M'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_count\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cec/phd/bazel-bin/experimental/polyglot/baselines/notebooks/run.runfiles/phd/deeplearning/clgen/corpuses/corpuses.py\u001b[0m in \u001b[0;36matomizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atomizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_LoadAtomizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atomizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CreateAtomizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atomizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cec/phd/bazel-bin/experimental/polyglot/baselines/notebooks/run.runfiles/phd/deeplearning/clgen/corpuses/corpuses.py\u001b[0m in \u001b[0;36m_CreateAtomizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii_character_atomizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m       \u001b[0matomizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matomizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsciiCharacterAtomizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'greedy_multichar_atomizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0matoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_multichar_atomizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cec/phd/bazel-bin/experimental/polyglot/baselines/notebooks/run.runfiles/phd/deeplearning/clgen/corpuses/atomizers.py\u001b[0m in \u001b[0;36mFromText\u001b[0;34m(cls, text)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mcount_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcount_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mAsciiCharacterAtomizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "def CorpusStats(corpus: corpuses.Corpus):\n",
    "    corpus.Create()\n",
    "    stats = {}\n",
    "    stats['Language'] = {\n",
    "        'opencl': 'OpenCL',\n",
    "        'c': 'C',\n",
    "        'java': 'Java'\n",
    "    }[pathlib.Path(corpus.config.local_directory).stem]\n",
    "\n",
    "    input_db_path = InputContentfiles(pathlib.Path(corpus.config.local_directory))\n",
    "    input_db = contentfiles.ContentFiles(input_db_path)\n",
    "    with input_db.Session() as session:\n",
    "        stats['Repositories'] = '{:.1f}k'.format(session.query(contentfiles.ContentFile.clone_from_url).distinct().count() / 1e3)\n",
    "        stats['Files'] = '{:.1f}k'.format(session.query(contentfiles.ContentFile).count() / 1e3)\n",
    "        stats['Lines'] = '{:.1f}M'.format(session.query(func.sum(contentfiles.ContentFile.linecount)).scalar() / 1e6)\n",
    "        \n",
    "    stats['Encoding'] = 'Character' if 'Ascii' in str(corpus.atomizer) else 'Token'\n",
    "    stats['Vocab size'] = corpus.vocab_size\n",
    "    stats['Corpus size'] = '{:.1f}M'.format((corpus.encoded.token_count) / 1e6)\n",
    "    return stats\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [CorpusStats(c) for c in corps.values()],\n",
    "    columns=['Language', 'Repositories', 'Files', 'Lines', 'Encoding', \n",
    "             'Vocab size', 'Corpus size'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
