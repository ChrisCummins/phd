{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import typing\n",
    "import collections\n",
    "\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from labm8 import humanize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets.github.scrape_repos import contentfiles\n",
    "from deeplearning.clgen import clgen\n",
    "from deeplearning.clgen import errors\n",
    "from deeplearning.clgen.corpuses import corpuses\n",
    "from deeplearning.clgen.proto import corpus_pb2\n",
    "from deeplearning.clgen.proto import clgen_pb2\n",
    "from deeplearning.clgen.proto import model_pb2\n",
    "from deeplearning.clgen.proto import sampler_pb2\n",
    "from labm8 import bazelutil\n",
    "from labm8 import pbutil\n",
    "from labm8 import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48 instances\n"
     ]
    }
   ],
   "source": [
    "instances_config = pathlib.Path('~/data/experimental/deeplearning/polyglot/instances.pbtxt').expanduser()\n",
    "instances = [\n",
    "    clgen.Instance(i) for i in\n",
    "    pbutil.FromFile(instances_config, clgen_pb2.Instances()).instance\n",
    "]\n",
    "print(\"Loaded {} instances\".format(len(instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{PosixPath('/mnt/cc/data/datasets/github/corpuses/java'),\n",
       " PosixPath('/mnt/cc/data/datasets/github/corpuses/opencl')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetContentfileDirectories(instances: typing.List[clgen.Instance]) -> typing.List[pathlib.Path]:\n",
    "    \"\"\"Return the list of contentfiles directories.\"\"\"\n",
    "    preprocessed_dirs = {i.model.corpus.preprocessed.database_path.parent for i in instances}\n",
    "    contentfiles = {(p / 'contentfiles').resolve() for p in preprocessed_dirs}\n",
    "    return contentfiles\n",
    "\n",
    "GetContentfileDirectories(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/cc/data/datasets/github/repos_by_lang/java.db'),\n",
       " PosixPath('/mnt/cc/data/datasets/github/repos_by_lang/opencl.db')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetContentfileDatabase(local_directory: pathlib.Path) -> pathlib.Path:\n",
    "    path = pathlib.Path(f'/mnt/cc/data/datasets/github/repos_by_lang/{local_directory.stem}.db')\n",
    "    if path.is_file():\n",
    "        return path\n",
    "    else:\n",
    "        raise FileNotFoundError(path)\n",
    "        \n",
    "contentfiles_dbs = [GetContentfileDatabase(p) for p in GetContentfileDirectories(instances)]\n",
    "contentfiles_dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOutputCorpus(instance: clgen.Instance) -> corpuses.Corpus:\n",
    "    with instance.Session():\n",
    "        out_dir = instance.model.SamplerCache(instance.sampler)\n",
    "        if not out_dir.is_dir():\n",
    "            return None\n",
    "        output_corpus_config = corpus_pb2.Corpus()\n",
    "        output_corpus_config.CopyFrom(instance.model.corpus.config)\n",
    "        output_corpus_config.local_directory = str(out_dir) + '.contentfiles'\n",
    "        if not pathlib.Path(output_corpus_config.local_directory).is_dir():\n",
    "            return None\n",
    "        return corpuses.Corpus(output_corpus_config)\n",
    "\n",
    "output_corpuses = [GetOutputCorpus(i) for i in instances]\n",
    "print(\"Loaded {} output corpuses\".format(len([x for x in output_corpuses if x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Character'), ('Vocab size', 92), ('Corpus size', '21.8M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05)])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Character'), ('Vocab size', 92), ('Corpus size', '21.8M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05)])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Character'), ('Vocab size', 92), ('Corpus size', '21.8M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 50), ('Final Loss', '0.187'), ('Training time', '12 hours'), ('Time / epoch', '15 minutes'), ('Sample temperature', '1.0'), ('Output samples', '10,034'), ('Output vocab size', '71'), ('Time / sample (ms)', 385), ('Samples / day', '224.3k'), ('Efficiency', '3.27%'), ('Throughput / day', '7.3k')])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Character'), ('Vocab size', 92), ('Corpus size', '21.8M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 50), ('Final Loss', '0.187'), ('Training time', '12 hours'), ('Time / epoch', '15 minutes'), ('Sample temperature', '0.5'), ('Output samples', '10,027'), ('Output vocab size', '69'), ('Time / sample (ms)', 473), ('Samples / day', '182.8k'), ('Efficiency', '5.83%'), ('Throughput / day', '10.7k')])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Token'), ('Vocab size', 166), ('Corpus size', '18.9M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 50), ('Final Loss', '0.269'), ('Training time', '4 hours'), ('Time / epoch', '4 minutes'), ('Sample temperature', '1.0'), ('Output samples', '10,059'), ('Output vocab size', '112'), ('Time / sample (ms)', 520), ('Samples / day', '166.0k'), ('Efficiency', '2.22%'), ('Throughput / day', '3.7k')])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Token'), ('Vocab size', 166), ('Corpus size', '18.9M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 50), ('Final Loss', '0.269'), ('Training time', '4 hours'), ('Time / epoch', '4 minutes'), ('Sample temperature', '0.5'), ('Output samples', '10,002'), ('Output vocab size', '132'), ('Time / sample (ms)', 511), ('Samples / day', '169.1k'), ('Efficiency', '1.92%'), ('Throughput / day', '3.2k')])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Token'), ('Vocab size', 166), ('Corpus size', '18.9M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 50), ('Final Loss', '0.236'), ('Training time', '11 hours'), ('Time / epoch', '13 minutes'), ('Sample temperature', '1.0'), ('Output samples', '10,040'), ('Output vocab size', '109'), ('Time / sample (ms)', 524), ('Samples / day', '164.7k'), ('Efficiency', '3.68%'), ('Throughput / day', '6.1k')])\n",
      "OrderedDict([('Language', 'OpenCL'), ('Encoding', 'Token'), ('Vocab size', 166), ('Corpus size', '18.9M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 50), ('Final Loss', '0.236'), ('Training time', '11 hours'), ('Time / epoch', '13 minutes'), ('Sample temperature', '0.5'), ('Output samples', '10,040'), ('Output vocab size', '100'), ('Time / sample (ms)', 535), ('Samples / day', '161.5k'), ('Efficiency', '3.75%'), ('Throughput / day', '6.0k')])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Character'), ('Vocab size', 3085), ('Corpus size', '286.9M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 38), ('Final Loss', '1.081'), ('Training time', '2 days'), ('Time / epoch', 'an hour')])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Character'), ('Vocab size', 3085), ('Corpus size', '286.9M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 38), ('Final Loss', '1.081'), ('Training time', '2 days'), ('Time / epoch', 'an hour')])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Character'), ('Vocab size', 3085), ('Corpus size', '286.9M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 4), ('Final Loss', '0.821'), ('Training time', '15 hours'), ('Time / epoch', '3 hours')])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Character'), ('Vocab size', 3085), ('Corpus size', '286.9M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05), ('Epochs', 4), ('Final Loss', '0.821'), ('Training time', '15 hours'), ('Time / epoch', '3 hours')])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Token'), ('Vocab size', 3133), ('Corpus size', '262.2M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05)])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Token'), ('Vocab size', 3133), ('Corpus size', '262.2M'), ('Model size', '512x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05)])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Token'), ('Vocab size', 3133), ('Corpus size', '262.2M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05)])\n",
      "OrderedDict([('Language', 'Java'), ('Encoding', 'Token'), ('Vocab size', 3133), ('Corpus size', '262.2M'), ('Model size', '1024x2'), ('Optimizer', 'Adam'), ('Learning rate', 0.002), ('Decay', 0.05)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Vocab size</th>\n",
       "      <th>Corpus size</th>\n",
       "      <th>Model size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Decay</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Final Loss</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Time / epoch</th>\n",
       "      <th>Sample temperature</th>\n",
       "      <th>Output samples</th>\n",
       "      <th>Output vocab size</th>\n",
       "      <th>Time / sample (ms)</th>\n",
       "      <th>Samples / day</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Throughput / day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Character</td>\n",
       "      <td>92</td>\n",
       "      <td>21.8M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Character</td>\n",
       "      <td>92</td>\n",
       "      <td>21.8M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Character</td>\n",
       "      <td>92</td>\n",
       "      <td>21.8M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.187</td>\n",
       "      <td>12 hours</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10,034</td>\n",
       "      <td>71</td>\n",
       "      <td>385</td>\n",
       "      <td>224.3k</td>\n",
       "      <td>3.27%</td>\n",
       "      <td>7.3k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Character</td>\n",
       "      <td>92</td>\n",
       "      <td>21.8M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.187</td>\n",
       "      <td>12 hours</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10,027</td>\n",
       "      <td>69</td>\n",
       "      <td>473</td>\n",
       "      <td>182.8k</td>\n",
       "      <td>5.83%</td>\n",
       "      <td>10.7k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Token</td>\n",
       "      <td>166</td>\n",
       "      <td>18.9M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.269</td>\n",
       "      <td>4 hours</td>\n",
       "      <td>4 minutes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10,059</td>\n",
       "      <td>112</td>\n",
       "      <td>520</td>\n",
       "      <td>166.0k</td>\n",
       "      <td>2.22%</td>\n",
       "      <td>3.7k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Token</td>\n",
       "      <td>166</td>\n",
       "      <td>18.9M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.269</td>\n",
       "      <td>4 hours</td>\n",
       "      <td>4 minutes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10,002</td>\n",
       "      <td>132</td>\n",
       "      <td>511</td>\n",
       "      <td>169.1k</td>\n",
       "      <td>1.92%</td>\n",
       "      <td>3.2k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Token</td>\n",
       "      <td>166</td>\n",
       "      <td>18.9M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.236</td>\n",
       "      <td>11 hours</td>\n",
       "      <td>13 minutes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10,040</td>\n",
       "      <td>109</td>\n",
       "      <td>524</td>\n",
       "      <td>164.7k</td>\n",
       "      <td>3.68%</td>\n",
       "      <td>6.1k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OpenCL</td>\n",
       "      <td>Token</td>\n",
       "      <td>166</td>\n",
       "      <td>18.9M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.236</td>\n",
       "      <td>11 hours</td>\n",
       "      <td>13 minutes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10,040</td>\n",
       "      <td>100</td>\n",
       "      <td>535</td>\n",
       "      <td>161.5k</td>\n",
       "      <td>3.75%</td>\n",
       "      <td>6.0k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Java</td>\n",
       "      <td>Character</td>\n",
       "      <td>3085</td>\n",
       "      <td>286.9M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>38</td>\n",
       "      <td>1.081</td>\n",
       "      <td>2 days</td>\n",
       "      <td>an hour</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Java</td>\n",
       "      <td>Character</td>\n",
       "      <td>3085</td>\n",
       "      <td>286.9M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>38</td>\n",
       "      <td>1.081</td>\n",
       "      <td>2 days</td>\n",
       "      <td>an hour</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Java</td>\n",
       "      <td>Character</td>\n",
       "      <td>3085</td>\n",
       "      <td>286.9M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.821</td>\n",
       "      <td>15 hours</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Java</td>\n",
       "      <td>Character</td>\n",
       "      <td>3085</td>\n",
       "      <td>286.9M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.821</td>\n",
       "      <td>15 hours</td>\n",
       "      <td>3 hours</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Java</td>\n",
       "      <td>Token</td>\n",
       "      <td>3133</td>\n",
       "      <td>262.2M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Java</td>\n",
       "      <td>Token</td>\n",
       "      <td>3133</td>\n",
       "      <td>262.2M</td>\n",
       "      <td>512x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Java</td>\n",
       "      <td>Token</td>\n",
       "      <td>3133</td>\n",
       "      <td>262.2M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Java</td>\n",
       "      <td>Token</td>\n",
       "      <td>3133</td>\n",
       "      <td>262.2M</td>\n",
       "      <td>1024x2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Language   Encoding  Vocab size Corpus size Model size Optimizer  \\\n",
       "0    OpenCL  Character          92       21.8M      512x2      Adam   \n",
       "1    OpenCL  Character          92       21.8M      512x2      Adam   \n",
       "2    OpenCL  Character          92       21.8M     1024x2      Adam   \n",
       "3    OpenCL  Character          92       21.8M     1024x2      Adam   \n",
       "4    OpenCL      Token         166       18.9M      512x2      Adam   \n",
       "5    OpenCL      Token         166       18.9M      512x2      Adam   \n",
       "6    OpenCL      Token         166       18.9M     1024x2      Adam   \n",
       "7    OpenCL      Token         166       18.9M     1024x2      Adam   \n",
       "8      Java  Character        3085      286.9M      512x2      Adam   \n",
       "9      Java  Character        3085      286.9M      512x2      Adam   \n",
       "10     Java  Character        3085      286.9M     1024x2      Adam   \n",
       "11     Java  Character        3085      286.9M     1024x2      Adam   \n",
       "12     Java      Token        3133      262.2M      512x2      Adam   \n",
       "13     Java      Token        3133      262.2M      512x2      Adam   \n",
       "14     Java      Token        3133      262.2M     1024x2      Adam   \n",
       "15     Java      Token        3133      262.2M     1024x2      Adam   \n",
       "\n",
       "    Learning rate  Decay Epochs Final Loss Training time Time / epoch  \\\n",
       "0           0.002   0.05      -          -             -            -   \n",
       "1           0.002   0.05      -          -             -            -   \n",
       "2           0.002   0.05     50      0.187      12 hours   15 minutes   \n",
       "3           0.002   0.05     50      0.187      12 hours   15 minutes   \n",
       "4           0.002   0.05     50      0.269       4 hours    4 minutes   \n",
       "5           0.002   0.05     50      0.269       4 hours    4 minutes   \n",
       "6           0.002   0.05     50      0.236      11 hours   13 minutes   \n",
       "7           0.002   0.05     50      0.236      11 hours   13 minutes   \n",
       "8           0.002   0.05     38      1.081        2 days      an hour   \n",
       "9           0.002   0.05     38      1.081        2 days      an hour   \n",
       "10          0.002   0.05      4      0.821      15 hours      3 hours   \n",
       "11          0.002   0.05      4      0.821      15 hours      3 hours   \n",
       "12          0.002   0.05      -          -             -            -   \n",
       "13          0.002   0.05      -          -             -            -   \n",
       "14          0.002   0.05      -          -             -            -   \n",
       "15          0.002   0.05      -          -             -            -   \n",
       "\n",
       "   Sample temperature Output samples Output vocab size Time / sample (ms)  \\\n",
       "0                   -              -                 -                  -   \n",
       "1                   -              -                 -                  -   \n",
       "2                 1.0         10,034                71                385   \n",
       "3                 0.5         10,027                69                473   \n",
       "4                 1.0         10,059               112                520   \n",
       "5                 0.5         10,002               132                511   \n",
       "6                 1.0         10,040               109                524   \n",
       "7                 0.5         10,040               100                535   \n",
       "8                   -              -                 -                  -   \n",
       "9                   -              -                 -                  -   \n",
       "10                  -              -                 -                  -   \n",
       "11                  -              -                 -                  -   \n",
       "12                  -              -                 -                  -   \n",
       "13                  -              -                 -                  -   \n",
       "14                  -              -                 -                  -   \n",
       "15                  -              -                 -                  -   \n",
       "\n",
       "   Samples / day Efficiency Throughput / day  \n",
       "0              -          -                -  \n",
       "1              -          -                -  \n",
       "2         224.3k      3.27%             7.3k  \n",
       "3         182.8k      5.83%            10.7k  \n",
       "4         166.0k      2.22%             3.7k  \n",
       "5         169.1k      1.92%             3.2k  \n",
       "6         164.7k      3.68%             6.1k  \n",
       "7         161.5k      3.75%             6.0k  \n",
       "8              -          -                -  \n",
       "9              -          -                -  \n",
       "10             -          -                -  \n",
       "11             -          -                -  \n",
       "12             -          -                -  \n",
       "13             -          -                -  \n",
       "14             -          -                -  \n",
       "15             -          -                -  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def InstanceStats(instance: clgen.Instance, output_corpus: corpuses.Corpus) -> typing.Dict[str, typing.Any]:\n",
    "    stats = collections.OrderedDict()\n",
    "    stats['Language'] = {\n",
    "        'opencl': 'OpenCL',\n",
    "        'java': 'Java',\n",
    "    }[pathlib.Path(instance.model.corpus.config.local_directory).stem]\n",
    "    stats['Encoding'] = 'Character' if 'Ascii' in str(instance.model.corpus.atomizer) else 'Token'\n",
    "    stats['Vocab size'] = instance.model.corpus.atomizer.vocab_size\n",
    "    stats['Corpus size'] = '{:.1f}M'.format(instance.model.corpus.encoded.token_count / 1e6)\n",
    "    # stats['Embedding'] = instance.model.config.architecture.embedding_size\n",
    "    stats['Model size'] = f'{instance.model.config.architecture.neurons_per_layer}x{instance.model.config.architecture.num_layers}'\n",
    "    # stats['Dropout'] = instance.model.config.architecture.post_layer_dropout_micros / 1e6\n",
    "    if instance.model.config.training.HasField('adam_optimizer'):\n",
    "        stats['Optimizer'] = 'Adam'\n",
    "        stats['Learning rate'] = instance.model.config.training.adam_optimizer.initial_learning_rate_micros / 1e6\n",
    "        stats['Decay'] = instance.model.config.training.adam_optimizer.learning_rate_decay_per_epoch_micros / 1e6\n",
    "    else:\n",
    "        stats['Optimizer'] = 'RMSProp'\n",
    "        stats['Learning rate'] = instance.model.config.training.rmsprop_optimizer.initial_learning_rate_micros / 1e6\n",
    "        stats['Decay'] = instance.model.config.training.rmsprop_optimizer.learning_rate_decay_per_epoch_micros / 1e6\n",
    "    telemetry = instance.model.TrainingTelemetry()\n",
    "    if telemetry:\n",
    "        stats['Epochs'] = len(telemetry)\n",
    "        stats['Final Loss'] = '{:.3f}'.format(telemetry[-1].loss)\n",
    "        stats['Training time'] = humanize.Duration(sum(t.epoch_wall_time_ms for t in telemetry) / 1e3)\n",
    "        stats['Time / epoch'] = humanize.Duration(np.array([t.epoch_wall_time_ms for t in telemetry]).mean() / 1e3)\n",
    "\n",
    "    if output_corpus:\n",
    "#         with instance.Session():\n",
    "#             try:\n",
    "#                 output_corpus.Create()\n",
    "#             except errors.EmptyCorpusException:\n",
    "#                 pass\n",
    "        samples_dir = instance.model.SamplerCache(instance.sampler)\n",
    "        sample_times = np.array([\n",
    "            pbutil.FromFile(samples_dir / f, model_pb2.Sample, uninitialized_okay=True).wall_time_ms for f in samples_dir.iterdir()\n",
    "        ], dtype=np.int32)\n",
    "        # TODO(cec): Use the number of extracted kernels, not the number of samples themselves.\n",
    "        # Sample times is in milliseconds, and we want time per thousand, so they cancel out.\n",
    "        # Average sample time in seconds.\n",
    "        sample_time_seconds = sample_times.mean() / 1000\n",
    "        stats['Sample temperature'] = humanize.Commas(instance.sampler.temperature)\n",
    "        # stats['Output samples'] = humanize.Commas(output_corpus.preprocessed.input_size)\n",
    "        stats['Output vocab size'] = humanize.Commas(output_corpus.vocab_size)\n",
    "        stats['Time / sample (ms)'] = int(round(sample_times.mean()))\n",
    "        sample_throughput = (24 * 3600) / sample_time_seconds\n",
    "        stats['Samples / day'] = '{:.1f}k'.format(sample_throughput / 1000)\n",
    "        # stats['Time / 1k samples'] = humanize.Duration(samples_time_seconds * 1000)\n",
    "        if output_corpus.preprocessed.size:\n",
    "            efficiency = (output_corpus.preprocessed.size / \n",
    "                          (output_corpus.preprocessed.input_size or 1))\n",
    "            good_sample_throughput = efficiency * sample_throughput\n",
    "            stats['Efficiency'] = '{:.2%}'.format(efficiency)\n",
    "            stats['Throughput / day'] = '{:.1f}k'.format(good_sample_throughput / 1000)\n",
    "    print(stats)\n",
    "    return stats\n",
    "\n",
    "stats = pd.DataFrame([InstanceStats(i, o) for i, o in zip(instances, output_corpuses)]).fillna('-')\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (_, row), instance in zip(stats.iterrows(), instances):\n",
    "    plt.plot([t.epoch_num for t in instance.model.TrainingTelemetry()], \n",
    "             [t.loss for t in instance.model.TrainingTelemetry()], \n",
    "             label=f\"{row['Language']}-{row['Model size']}\")\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.title('Training Losses')\n",
    "\n",
    "# X axis.\n",
    "# plt.xlim((0, 50 - 1))\n",
    "# ax.set_xticklabels([i + 1 for i in ax.get_xticks()])\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Y axis.\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "viz.finalise(size=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
