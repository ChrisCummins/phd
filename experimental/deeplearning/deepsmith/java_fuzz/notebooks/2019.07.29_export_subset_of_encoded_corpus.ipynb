{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "\n",
    "import sqlalchemy as sql\n",
    "\n",
    "from labm8 import app\n",
    "from labm8 import sqlutil\n",
    "from labm8 import humanize\n",
    "from deeplearning.clgen import clgen\n",
    "from deeplearning.clgen import samples_database\n",
    "from deeplearning.clgen.corpuses import encoded\n",
    "from deeplearning.clgen.corpuses import corpuses\n",
    "from deeplearning.clgen.proto import clgen_pb2\n",
    "from deeplearning.clgen.proto import corpus_pb2\n",
    "from deeplearning.clgen.proto import model_pb2\n",
    "from deeplearning.clgen.proto import sampler_pb2\n",
    "from experimental.deeplearning.deepsmith.java_fuzz import sample_java_model\n",
    "from experimental.deeplearning.deepsmith.java_fuzz import sample_opencl_model\n",
    "from research.cummins_2017_cgo import generative_model as opencl\n",
    "\n",
    "app.FLAGS(['argv0', '--clgen_corpus_dir=/var/phd/datasets/github/corpuses/opencl', '--clgen_multichar_tokenizer'])\n",
    "\n",
    "FLAGS = app.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pre-encoded OpenCL corpus database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OpenCL instance to export pre-encoded corpus from.\n",
    "config = sample_java_model.MakeClgenInstanceConfig(\n",
    "      working_dir='/var/phd/experimental/deeplearning/deepsmith/java_fuzz/opencl_clgen_cache',\n",
    "      encoded_db=encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16?charset=utf8'), num_training_epochs=50, seed_text='kernel void A (',\n",
    "      neurons_per_layer=512)\n",
    "\n",
    "# Replace the Java corpus with an OpenCL one.\n",
    "config.model.corpus.CopyFrom(opencl.CreateCorpusProtoFromFlags())\n",
    "\n",
    "output_db = encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?opencl_enc_2019.07.29?charset=utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing vocab with 166 entries\n",
      "Copying encoded content files ...\n",
      "Imported 4015 contentfiles\n",
      "Exported 19171581 token corpus\n"
     ]
    }
   ],
   "source": [
    "def ExportPreEncodedCorpus(instance: clgen.Instance, dst: encoded.EncodedContentFiles):\n",
    "    with instance.Session() as instance_session:\n",
    "        src_corpus = instance_session.model.corpus\n",
    "        src_corpus.Create()\n",
    "        with dst.Session(commit=True) as dst_session:\n",
    "            print(\"Storing vocab with\", len(src_corpus.atomizer.vocab), \"entries\")\n",
    "            corpuses.StoreVocabInMetaTable(dst_session, src_corpus.atomizer.vocab)\n",
    "            print(\"Copying encoded content files ...\")\n",
    "            dst_session.query(encoded.EncodedContentFile).delete()\n",
    "            with src_corpus.encoded.Session() as src_session:\n",
    "                query = src_session.query(encoded.EncodedContentFile)\n",
    "                for i, row in enumerate(query):\n",
    "                    dst_session.merge(row)\n",
    "                print(\"Imported\", i, \"contentfiles\")\n",
    "\n",
    "ExportPreEncodedCorpus(clgen.Instance(config), output_db)\n",
    "\n",
    "with output_db.Session() as session:\n",
    "    query = session.query(sql.func.sum(encoded.EncodedContentFile.tokencount))\n",
    "    print(\"Exported\", query.one()[0], \"token corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export subset of Java corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_count = 19171581\n",
    "\n",
    "input_db = encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16?charset=utf8')\n",
    "output_db = encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16_mini?charset=utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1,000 methods, token count 247,292 of 19,171,581 ...\n",
      "processed 2,000 methods, token count 517,808 of 19,171,581 ...\n",
      "processed 3,000 methods, token count 756,441 of 19,171,581 ...\n",
      "processed 4,000 methods, token count 1,001,015 of 19,171,581 ...\n",
      "processed 5,000 methods, token count 1,232,256 of 19,171,581 ...\n",
      "processed 6,000 methods, token count 1,504,923 of 19,171,581 ...\n",
      "processed 7,000 methods, token count 1,727,197 of 19,171,581 ...\n",
      "processed 8,000 methods, token count 2,026,271 of 19,171,581 ...\n",
      "processed 9,000 methods, token count 2,286,418 of 19,171,581 ...\n",
      "processed 10,000 methods, token count 2,571,847 of 19,171,581 ...\n",
      "processed 11,000 methods, token count 2,812,830 of 19,171,581 ...\n",
      "processed 12,000 methods, token count 3,081,203 of 19,171,581 ...\n",
      "processed 13,000 methods, token count 3,366,257 of 19,171,581 ...\n",
      "processed 14,000 methods, token count 3,609,967 of 19,171,581 ...\n",
      "processed 15,000 methods, token count 3,893,045 of 19,171,581 ...\n",
      "processed 16,000 methods, token count 4,141,226 of 19,171,581 ...\n",
      "processed 17,000 methods, token count 4,363,098 of 19,171,581 ...\n",
      "processed 18,000 methods, token count 4,611,023 of 19,171,581 ...\n",
      "processed 19,000 methods, token count 4,845,178 of 19,171,581 ...\n",
      "processed 20,000 methods, token count 5,089,660 of 19,171,581 ...\n",
      "processed 21,000 methods, token count 5,348,192 of 19,171,581 ...\n",
      "processed 22,000 methods, token count 5,596,414 of 19,171,581 ...\n",
      "processed 23,000 methods, token count 5,882,508 of 19,171,581 ...\n",
      "processed 24,000 methods, token count 6,198,163 of 19,171,581 ...\n",
      "processed 25,000 methods, token count 6,466,949 of 19,171,581 ...\n",
      "processed 26,000 methods, token count 6,616,405 of 19,171,581 ...\n",
      "processed 27,000 methods, token count 6,839,955 of 19,171,581 ...\n",
      "processed 28,000 methods, token count 7,097,852 of 19,171,581 ...\n",
      "processed 29,000 methods, token count 7,375,301 of 19,171,581 ...\n",
      "processed 30,000 methods, token count 7,665,186 of 19,171,581 ...\n",
      "processed 31,000 methods, token count 7,920,509 of 19,171,581 ...\n",
      "processed 32,000 methods, token count 8,172,904 of 19,171,581 ...\n",
      "processed 33,000 methods, token count 8,429,337 of 19,171,581 ...\n",
      "processed 34,000 methods, token count 8,696,840 of 19,171,581 ...\n",
      "processed 35,000 methods, token count 8,949,218 of 19,171,581 ...\n",
      "processed 36,000 methods, token count 9,197,452 of 19,171,581 ...\n",
      "processed 37,000 methods, token count 9,494,493 of 19,171,581 ...\n",
      "processed 38,000 methods, token count 9,746,908 of 19,171,581 ...\n",
      "processed 39,000 methods, token count 9,995,924 of 19,171,581 ...\n",
      "processed 40,000 methods, token count 10,233,516 of 19,171,581 ...\n",
      "processed 41,000 methods, token count 10,488,993 of 19,171,581 ...\n",
      "processed 42,000 methods, token count 10,818,661 of 19,171,581 ...\n",
      "processed 43,000 methods, token count 11,083,253 of 19,171,581 ...\n",
      "processed 44,000 methods, token count 11,361,657 of 19,171,581 ...\n",
      "processed 45,000 methods, token count 11,673,493 of 19,171,581 ...\n",
      "processed 46,000 methods, token count 11,934,949 of 19,171,581 ...\n",
      "processed 47,000 methods, token count 12,236,067 of 19,171,581 ...\n",
      "processed 48,000 methods, token count 12,473,391 of 19,171,581 ...\n",
      "processed 49,000 methods, token count 12,741,672 of 19,171,581 ...\n",
      "processed 50,000 methods, token count 12,992,139 of 19,171,581 ...\n",
      "processed 51,000 methods, token count 13,247,532 of 19,171,581 ...\n",
      "processed 52,000 methods, token count 13,534,501 of 19,171,581 ...\n",
      "processed 53,000 methods, token count 13,808,931 of 19,171,581 ...\n",
      "processed 54,000 methods, token count 14,075,933 of 19,171,581 ...\n",
      "processed 55,000 methods, token count 14,326,308 of 19,171,581 ...\n",
      "processed 56,000 methods, token count 14,595,985 of 19,171,581 ...\n",
      "processed 57,000 methods, token count 14,863,525 of 19,171,581 ...\n",
      "processed 58,000 methods, token count 15,197,264 of 19,171,581 ...\n",
      "processed 59,000 methods, token count 15,571,993 of 19,171,581 ...\n",
      "processed 60,000 methods, token count 15,826,717 of 19,171,581 ...\n",
      "processed 61,000 methods, token count 16,071,687 of 19,171,581 ...\n",
      "processed 62,000 methods, token count 16,357,838 of 19,171,581 ...\n",
      "processed 63,000 methods, token count 16,635,056 of 19,171,581 ...\n",
      "processed 64,000 methods, token count 16,878,782 of 19,171,581 ...\n",
      "processed 65,000 methods, token count 17,118,088 of 19,171,581 ...\n",
      "processed 66,000 methods, token count 17,268,823 of 19,171,581 ...\n",
      "processed 67,000 methods, token count 17,492,386 of 19,171,581 ...\n",
      "processed 68,000 methods, token count 17,748,605 of 19,171,581 ...\n",
      "processed 69,000 methods, token count 18,029,422 of 19,171,581 ...\n",
      "processed 70,000 methods, token count 18,322,733 of 19,171,581 ...\n",
      "processed 71,000 methods, token count 18,483,042 of 19,171,581 ...\n",
      "processed 72,000 methods, token count 18,605,994 of 19,171,581 ...\n",
      "processed 73,000 methods, token count 18,862,762 of 19,171,581 ...\n",
      "processed 74,000 methods, token count 19,108,838 of 19,171,581 ...\n",
      "Exported 19,171,738 token corpus of 74,298 methods from a possible 3,430,397 methods\n"
     ]
    }
   ],
   "source": [
    "def ExportMetaTable(input_session, output_session):\n",
    "    output_session.query(encoded.Meta).delete()\n",
    "    for row in input_session.query(encoded.Meta):\n",
    "        output_session.merge(row)\n",
    "\n",
    "def ExportTokenCount(input_db, output_db, target_token_count):\n",
    "    \"\"\"Iteratively build-up corpus to export.\"\"\"\n",
    "    row_batch_size = 100\n",
    "\n",
    "    with input_db.Session() as input_session, output_db.Session(commit=True) as output_session:\n",
    "        ExportMetaTable(input_session, output_session)\n",
    "        \n",
    "        output_session.query(encoded.EncodedContentFile).delete()\n",
    "        query = input_session.query(encoded.EncodedContentFile)\n",
    "        batches = sqlutil.OffsetLimitBatchedQuery(query=query, batch_size=row_batch_size, compute_max_rows=True)\n",
    "        actual_token_count, method_count = 0, 0\n",
    "        for batch in batches:\n",
    "            for row in batch.rows:\n",
    "                method_count += 1\n",
    "                actual_token_count += row.tokencount\n",
    "                output_session.merge(row)\n",
    "                # Periodically print a progress update.\n",
    "                if method_count % 1000 == 0:\n",
    "                    print('processed', humanize.Commas(method_count), \n",
    "                          'methods, token count', humanize.Commas(actual_token_count), \n",
    "                          'of', humanize.Commas(target_token_count),'...')\n",
    "                # We're done.\n",
    "                if actual_token_count >= target_token_count:\n",
    "                    print(\"Exported\", humanize.Commas(actual_token_count), \"token corpus of\", \n",
    "                          humanize.Commas(method_count), \n",
    "                          \"methods from a possible\", humanize.Commas(batch.max_rows), \"methods\")\n",
    "                    return\n",
    "\n",
    "ExportTokenCount(input_db, output_db, target_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
