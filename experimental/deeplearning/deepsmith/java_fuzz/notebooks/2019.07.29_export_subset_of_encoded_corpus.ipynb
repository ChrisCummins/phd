{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "\n",
    "import sqlalchemy as sql\n",
    "\n",
    "from labm8 import app\n",
    "from labm8 import sqlutil\n",
    "from labm8 import humanize\n",
    "from deeplearning.clgen import clgen\n",
    "from deeplearning.clgen import samples_database\n",
    "from deeplearning.clgen.corpuses import encoded\n",
    "from deeplearning.clgen.corpuses import corpuses\n",
    "from deeplearning.clgen.proto import clgen_pb2\n",
    "from deeplearning.clgen.proto import corpus_pb2\n",
    "from deeplearning.clgen.proto import model_pb2\n",
    "from deeplearning.clgen.proto import sampler_pb2\n",
    "from experimental.deeplearning.deepsmith.java_fuzz import sample_java_model\n",
    "from experimental.deeplearning.deepsmith.java_fuzz import sample_opencl_model\n",
    "from research.cummins_2017_cgo import generative_model as opencl\n",
    "\n",
    "app.FLAGS(['argv0', '--clgen_corpus_dir=/var/phd/datasets/github/corpuses/opencl', '--clgen_multichar_tokenizer'])\n",
    "\n",
    "FLAGS = app.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pre-encoded OpenCL corpus database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OpenCL instance to export pre-encoded corpus from.\n",
    "config = sample_java_model.MakeClgenInstanceConfig(\n",
    "      working_dir='/var/phd/experimental/deeplearning/deepsmith/java_fuzz/opencl_clgen_cache',\n",
    "      encoded_db=encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16?charset=utf8'), num_training_epochs=50, seed_text='kernel void A (',\n",
    "      neurons_per_layer=512)\n",
    "\n",
    "# Replace the Java corpus with an OpenCL one.\n",
    "config.model.corpus.CopyFrom(opencl.CreateCorpusProtoFromFlags())\n",
    "\n",
    "output_db = encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?opencl_enc_2019.07.29?charset=utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing vocab with 166 entries\n",
      "Copying encoded content files ...\n",
      "Imported 4015 contentfiles\n",
      "Exported 19171581 token corpus\n"
     ]
    }
   ],
   "source": [
    "def ExportPreEncodedCorpus(instance: clgen.Instance, dst: encoded.EncodedContentFiles):\n",
    "    with instance.Session() as instance_session:\n",
    "        src_corpus = instance_session.model.corpus\n",
    "        src_corpus.Create()\n",
    "        with dst.Session(commit=True) as dst_session:\n",
    "            print(\"Storing vocab with\", len(src_corpus.atomizer.vocab), \"entries\")\n",
    "            corpuses.StoreVocabInMetaTable(dst_session, src_corpus.atomizer.vocab)\n",
    "            print(\"Copying encoded content files ...\")\n",
    "            dst_session.query(encoded.EncodedContentFile).delete()\n",
    "            with src_corpus.encoded.Session() as src_session:\n",
    "                query = src_session.query(encoded.EncodedContentFile)\n",
    "                for i, row in enumerate(query):\n",
    "                    dst_session.merge(row)\n",
    "                print(\"Imported\", i, \"contentfiles\")\n",
    "\n",
    "ExportPreEncodedCorpus(clgen.Instance(config), output_db)\n",
    "\n",
    "with output_db.Session() as session:\n",
    "    query = session.query(sql.func.sum(encoded.EncodedContentFile.tokencount))\n",
    "    print(\"Exported\", query.one()[0], \"token corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export subset of Java corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_count = int(50e6)\n",
    "\n",
    "input_db = encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16?charset=utf8')\n",
    "output_db = encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16_T50M?charset=utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 5,000 methods, token count 1,232,256 of 50,000,000 ...\n",
      "processed 10,000 methods, token count 2,571,847 of 50,000,000 ...\n",
      "processed 15,000 methods, token count 3,893,045 of 50,000,000 ...\n",
      "processed 20,000 methods, token count 5,089,660 of 50,000,000 ...\n",
      "processed 25,000 methods, token count 6,466,949 of 50,000,000 ...\n",
      "processed 30,000 methods, token count 7,665,186 of 50,000,000 ...\n",
      "processed 35,000 methods, token count 8,949,218 of 50,000,000 ...\n",
      "processed 40,000 methods, token count 10,233,516 of 50,000,000 ...\n",
      "processed 45,000 methods, token count 11,673,493 of 50,000,000 ...\n",
      "processed 50,000 methods, token count 12,992,139 of 50,000,000 ...\n",
      "processed 55,000 methods, token count 14,326,308 of 50,000,000 ...\n",
      "processed 60,000 methods, token count 15,826,717 of 50,000,000 ...\n",
      "processed 65,000 methods, token count 17,118,088 of 50,000,000 ...\n",
      "processed 70,000 methods, token count 18,322,733 of 50,000,000 ...\n",
      "processed 75,000 methods, token count 19,327,871 of 50,000,000 ...\n",
      "processed 80,000 methods, token count 20,400,074 of 50,000,000 ...\n",
      "processed 85,000 methods, token count 21,520,050 of 50,000,000 ...\n",
      "processed 90,000 methods, token count 22,556,972 of 50,000,000 ...\n",
      "processed 95,000 methods, token count 23,597,056 of 50,000,000 ...\n",
      "processed 100,000 methods, token count 24,449,840 of 50,000,000 ...\n",
      "processed 105,000 methods, token count 25,380,992 of 50,000,000 ...\n",
      "processed 110,000 methods, token count 26,533,267 of 50,000,000 ...\n",
      "processed 115,000 methods, token count 27,800,614 of 50,000,000 ...\n",
      "processed 120,000 methods, token count 29,037,091 of 50,000,000 ...\n",
      "processed 125,000 methods, token count 30,216,519 of 50,000,000 ...\n",
      "processed 130,000 methods, token count 31,327,012 of 50,000,000 ...\n",
      "processed 135,000 methods, token count 32,290,409 of 50,000,000 ...\n",
      "processed 140,000 methods, token count 33,720,041 of 50,000,000 ...\n",
      "processed 145,000 methods, token count 34,748,781 of 50,000,000 ...\n",
      "processed 150,000 methods, token count 35,964,538 of 50,000,000 ...\n",
      "processed 155,000 methods, token count 37,298,848 of 50,000,000 ...\n",
      "processed 160,000 methods, token count 38,513,397 of 50,000,000 ...\n",
      "processed 165,000 methods, token count 39,802,372 of 50,000,000 ...\n",
      "processed 170,000 methods, token count 41,139,491 of 50,000,000 ...\n",
      "processed 175,000 methods, token count 42,436,303 of 50,000,000 ...\n",
      "processed 180,000 methods, token count 43,663,118 of 50,000,000 ...\n",
      "processed 185,000 methods, token count 44,871,529 of 50,000,000 ...\n",
      "processed 190,000 methods, token count 46,107,111 of 50,000,000 ...\n",
      "processed 195,000 methods, token count 49,597,650 of 50,000,000 ...\n",
      "Exported 50,000,044 token corpus of 196,604 methods from a possible 3,430,397 methods\n"
     ]
    }
   ],
   "source": [
    "def ExportMetaTable(input_session, output_session):\n",
    "    output_session.query(encoded.Meta).delete()\n",
    "    for row in input_session.query(encoded.Meta):\n",
    "        output_session.merge(row)\n",
    "\n",
    "def ExportTokenCount(input_db, output_db, target_token_count):\n",
    "    \"\"\"Iteratively build-up corpus to export.\"\"\"\n",
    "    row_batch_size = 500\n",
    "\n",
    "    with input_db.Session() as input_session, output_db.Session(commit=True) as output_session:\n",
    "        ExportMetaTable(input_session, output_session)\n",
    "        \n",
    "        output_session.query(encoded.EncodedContentFile).delete()\n",
    "        query = input_session.query(encoded.EncodedContentFile)\n",
    "        batches = sqlutil.OffsetLimitBatchedQuery(query=query, batch_size=row_batch_size, compute_max_rows=True)\n",
    "        actual_token_count, method_count = 0, 0\n",
    "        for batch in batches:\n",
    "            for row in batch.rows:\n",
    "                method_count += 1\n",
    "                actual_token_count += row.tokencount\n",
    "                output_session.merge(row)\n",
    "                # Periodically print a progress update.\n",
    "                if method_count % 5000 == 0:\n",
    "                    print('processed', humanize.Commas(method_count), \n",
    "                          'methods, token count', humanize.Commas(actual_token_count), \n",
    "                          'of', humanize.Commas(target_token_count),'...')\n",
    "                # We're done.\n",
    "                if actual_token_count >= target_token_count:\n",
    "                    print(\"Exported\", humanize.Commas(actual_token_count), \"token corpus of\", \n",
    "                          humanize.Commas(method_count), \n",
    "                          \"methods from a possible\", humanize.Commas(batch.max_rows), \"methods\")\n",
    "                    return\n",
    "\n",
    "ExportTokenCount(input_db, output_db, target_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating OpenCL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with corpus dir.\n",
    "config = sample_java_model.MakeClgenInstanceConfig(\n",
    "    working_dir='/var/phd/experimental/deeplearning/deepsmith/java_fuzz/opencl_clgen_cache',\n",
    "    encoded_db=encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.16?charset=utf8'), \n",
    "    num_training_epochs=50, \n",
    "    seed_text='kernel void A (',\n",
    "    neurons_per_layer=512)\n",
    "config.model.corpus.CopyFrom(opencl.CreateCorpusProtoFromFlags())\n",
    "opencl_model_with_corpus_dir = clgen.Instance(config)\n",
    "\n",
    "# Model with corpus database.\n",
    "config = sample_java_model.MakeClgenInstanceConfig(\n",
    "    working_dir='/var/phd/experimental/deeplearning/deepsmith/java_fuzz/opencl_clgen_cache',\n",
    "    encoded_db=encoded.EncodedContentFiles('file:///var/phd/db/cc1.mysql?opencl_enc_2019.07.29?charset=utf8'), \n",
    "    num_training_epochs=50, \n",
    "    seed_text='kernel void A (',\n",
    "    neurons_per_layer=512)\n",
    "opencl_model_with_corpus_db = clgen.Instance(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus dir trained? False\n",
      "Corpus db trained? False\n"
     ]
    }
   ],
   "source": [
    "with opencl_model_with_corpus_dir.Session() as session:\n",
    "    print('Corpus dir trained?', session.model.is_trained)\n",
    "    \n",
    "with opencl_model_with_corpus_db.Session() as session:\n",
    "    print('Corpus db trained?', session.model.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
