{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import collections\n",
    "import pathlib\n",
    "import shutil\n",
    "import typing\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sql\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from labm8.py import app\n",
    "from labm8.py import prof\n",
    "from labm8.py import sqlutil\n",
    "from labm8.py import pdutil\n",
    "from labm8.py import viz\n",
    "from labm8.py import labtypes\n",
    "from labm8.py import humanize\n",
    "\n",
    "from datasets.github.scrape_repos import contentfiles\n",
    "from deeplearning.clgen.corpuses import preprocessed\n",
    "from deeplearning.clgen.corpuses import encoded\n",
    "\n",
    "FLAGS = app.FLAGS(['argv0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_db = 'file:///var/phd/db/cc1.mysql?github_java?charset=utf8'\n",
    "methods_db = 'file:///var/phd/db/cc1.mysql?github_java_methods_2019.06.28?charset=utf8'\n",
    "pp_db = 'file:///var/phd/db/cc1.mysql?github_java_methods_pp_2019.07.02?charset=utf8'\n",
    "enc_db = 'file:///var/phd/db/cc1.mysql?github_java_methods_enc_2019.07.02?charset=utf8'\n",
    "\n",
    "# Connect to databases.\n",
    "scraper_db = contentfiles.ContentFiles(scraper_db, must_exist=True)\n",
    "methods_db = contentfiles.ContentFiles(methods_db, must_exist=True)\n",
    "pp_db = preprocessed.PreprocessedContentFiles(pp_db, must_exist=True)\n",
    "enc_db = encoded.EncodedContentFiles(enc_db, must_exist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries in 3m 29s 667ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average charcount</th>\n",
       "      <th>Average linecount</th>\n",
       "      <th>Contentfile count</th>\n",
       "      <th>Repo_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>static_methods</th>\n",
       "      <td>428</td>\n",
       "      <td>12</td>\n",
       "      <td>533,083</td>\n",
       "      <td>13,983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preprocessed_methods</th>\n",
       "      <td>313</td>\n",
       "      <td>14</td>\n",
       "      <td>460,072</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Average charcount Average linecount Contentfile count  \\\n",
       "static_methods                     428                12           533,083   \n",
       "preprocessed_methods               313                14           460,072   \n",
       "\n",
       "                     Repo_count  \n",
       "static_methods           13,983  \n",
       "preprocessed_methods          -  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a table of file and line counts.\n",
    "\n",
    "def CfOverview(db):\n",
    "  with db.Session() as s:\n",
    "    repo_count = s.query(sql.func.count(contentfiles.GitHubRepository.clone_from_url)).one()[0]\n",
    "    contentfile_count = s.query(sql.func.count(contentfiles.ContentFile.id)).one()[0]\n",
    "    avg_charcount = int(round(s.query(sql.func.avg(contentfiles.ContentFile.charcount)).one()[0]))\n",
    "    avg_linecount = int(round(s.query(sql.func.avg(contentfiles.ContentFile.linecount)).one()[0]))\n",
    "    return {\n",
    "        'Repo_count': humanize.Commas(repo_count),\n",
    "        'Contentfile count': humanize.Commas(contentfile_count),\n",
    "        'Average charcount': humanize.Commas(avg_charcount),\n",
    "        'Average linecount': humanize.Commas(avg_linecount),\n",
    "    }\n",
    "\n",
    "def PpOverview(db):\n",
    "  with db.Session() as s:\n",
    "    cf_count = s.query(sql.func.count(preprocessed.PreprocessedContentFile.id))\\\n",
    "        .filter(preprocessed.PreprocessedContentFile.preprocessing_succeeded == True).one()[0]\n",
    "    avg_charcount = int(round(s.query(sql.func.avg(preprocessed.PreprocessedContentFile.charcount))\\\n",
    "                              .filter(preprocessed.PreprocessedContentFile.preprocessing_succeeded == True).one()[0]))\n",
    "    avg_linecount = int(round(s.query(sql.func.avg(preprocessed.PreprocessedContentFile.linecount))\\\n",
    "                              .filter(preprocessed.PreprocessedContentFile.preprocessing_succeeded == True).one()[0]))\n",
    "    return {\n",
    "        'Repo_count': '-',\n",
    "        'Contentfile count': humanize.Commas(cf_count),\n",
    "        'Average charcount': humanize.Commas(avg_charcount),\n",
    "        'Average linecount': humanize.Commas(avg_linecount),\n",
    "    }\n",
    "\n",
    "def EnvOverview(db):\n",
    "  with db.Session() as s:\n",
    "    cf_count = s.query(sql.func.count(encoded.EncodedContentFile.id))[0]\n",
    "    avg_charcount = int(round(s.query(sql.func.avg(encoded.EncodedContentFile.tokencount)).one()[0]))\n",
    "    return {\n",
    "        'Repo_count': '-',\n",
    "        'Contentfile count': humanize.Commas(cf_count),\n",
    "        'Average charcount': humanize.Commas(avg_charcount),\n",
    "        'Average linecount': '-',\n",
    "    }\n",
    "\n",
    "with prof.ProfileToStdout('queries'):\n",
    "  df = pd.DataFrame([\n",
    "      # CfOverview(scraper_db),\n",
    "      CfOverview(methods_db),\n",
    "      PpOverview(pp_db),\n",
    "      EncOverview(enc_db),\n",
    "  ], index=[\n",
    "      # 'scraper_db', \n",
    "      'static_methods',\n",
    "      'preprocessed_methods',\n",
    "      'encoded_methods',\n",
    "  ])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReposFeatures(db):\n",
    "  \"\"\"Get a table of repo features.\"\"\"\n",
    "  with db.Session() as s:\n",
    "    q = s.query(sql.func.count(contentfiles.ContentFile.id).label('file_count'),\n",
    "                contentfiles.GitHubRepository.owner,\n",
    "                contentfiles.GitHubRepository.name,\n",
    "                contentfiles.GitHubRepository.clone_from_url,\n",
    "                contentfiles.GitHubRepository.num_stars,\n",
    "                contentfiles.GitHubRepository.num_forks,\n",
    "                contentfiles.GitHubRepository.num_watchers)\\\n",
    "        .join(contentfiles.GitHubRepository)\\\n",
    "        .group_by(contentfiles.ContentFile.clone_from_url)\n",
    "    df = pdutil.QueryToDataFrame(s, q)\n",
    "    df.set_index('clone_from_url', inplace=True)\n",
    "  return df\n",
    "\n",
    "def GetPpRepoFeatures(db, df):\n",
    "  \"\"\"Get the subset of repos used in a preprocessed db.\"\"\"\n",
    "  with db.Session() as s:\n",
    "    q = s.query(preprocessed.PreprocessedContentFile.input_relpath)\n",
    "    clone_from_urls = {\":\".join(x[0].split(\":\")[:2]) for x in q}\n",
    "  return df.loc[list(clone_from_urls)]\n",
    "\n",
    "scraper_db_repo_features = GetReposFeatures(scraper_db)\n",
    "methods_db_repo_features = GetReposFeatures(methods_db)\n",
    "pp_db_repo_features = GetPpRepoFeatures(pp_db, methods_db_repo_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Github Stargazers\n",
    "\n",
    "import math\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "@ticker.FuncFormatter\n",
    "def y_formatter(x, pos):\n",
    "    return humanize.DecimalPrefix(x, '', precision=2)\n",
    "\n",
    "@ticker.FuncFormatter\n",
    "def x_formatter(x, pos):\n",
    "    return humanize.DecimalPrefix(np.expm1(x), '', precision=2)\n",
    "  \n",
    "def PlotStarCount(df, ax):\n",
    "  viz.Distplot(x='num_stars', data=df, log1p_x=True, nbins=12)\n",
    "  plt.xlabel('Log Github Stargazers')\n",
    "  plt.ylabel('Frequencey')\n",
    "\n",
    "  ax.xaxis.set_major_formatter(x_formatter)\n",
    "  ax.yaxis.set_major_formatter(y_formatter)\n",
    "  \n",
    "ax = plt.subplot(1, 3, 1)\n",
    "PlotStarCount(scraper_db_repo_features, ax=ax)\n",
    "ax.set_title(f'{humanize.Commas(len(scraper_db_repo_features))} scraped repos')\n",
    "  \n",
    "ax = plt.subplot(1, 3, 2)\n",
    "PlotStarCount(methods_db_repo_features, ax=ax)\n",
    "ax.set_title(f'{humanize.Commas(len(methods_db_repo_features))} \"usable\" repos')\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "PlotStarCount(pp_db_repo_features, ax=ax)\n",
    "ax.set_title(f'{humanize.Commas(len(pp_db_repo_features))} preprocessed repos')\n",
    "\n",
    "viz.Finalize(figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def GetSecretsFeatures():\n",
    "  with pp_db.Session() as s:\n",
    "    q = s.query(preprocessed.PreprocessedContentFile.input_relpath, \n",
    "                preprocessed.PreprocessedContentFile.text) \\\n",
    "        .filter(preprocessed.PreprocessedContentFile.preprocessing_succeeded == False)\\\n",
    "        .filter(preprocessed.PreprocessedContentFile.text.like('Text contains secrets: %'))\\\n",
    "        .all()\n",
    "\n",
    "    clone_from_urls = [':'.join(r[0].split(':')[:2]) for r in q]\n",
    "    relpaths = [r[0].split(':')[2] for r in q]\n",
    "    artifact_indices = [int(r[0].split(':')[-1]) for r in q]\n",
    "    secret_types = [r[1][len('Text contains secrets: '):] for r in q]\n",
    "\n",
    "  tuples = list(zip(clone_from_urls, relpaths, artifact_indices, secret_types))\n",
    "  \n",
    "  with methods_db.Session() as s:\n",
    "    q = s.query(contentfiles.ContentFile).filter(sql.tuple_(\n",
    "        contentfiles.ContentFile.clone_from_url, \n",
    "        contentfiles.ContentFile.relpath,\n",
    "        contentfiles.ContentFile.artifact_index\n",
    "    ).in_(tuples)).all()\n",
    "\n",
    "    texts = [r.text for r in q]\n",
    "    assert len(q) == len(tuples)\n",
    "    \n",
    "  return pd.DataFrame(zip(clone_from_urls, relpaths, artifact_indices, secret_types, texts), columns=[\n",
    "    'clone_from_url',\n",
    "    'relpath',\n",
    "    'artifact_index',\n",
    "    'secret_type',\n",
    "    'text',\n",
    "])\n",
    "\n",
    "secrets_df = GetSecretsFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot data  { form-width: \"30%\" }\n",
    "\n",
    "secrets_df.groupby('secret_type').count()[['text']].plot.bar()\n",
    "plt.gca().get_legend().remove()\n",
    "plt.xlabel('')\n",
    "plt.ylabel('#. methods filtered')\n",
    "viz.Finalize(figsize=(5, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
