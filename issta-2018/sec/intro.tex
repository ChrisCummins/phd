\section{Introduction}\label{sec:intro}

Compilers should produce correct code for valid inputs, and meaningful errors
for invalid inputs. Failure to do so can hinder software development or even
cause catastrophic runtime errors. Still, properly testing compilers is hard.
Modern optimizing compilers are large and complex programs, and their input
space is huge. Hand designed suites of test programs, while important, are
inadequate for covering such a large space and will not touch all parts of the
compiler.

Random test case generation --- \emph{fuzzing} --- is a well established and
effective method for identifying compiler
bugs~\cite{Chen2014a,Chen2013,Kossatchev2005}. When fuzzing, randomly generated
valid or semi-valid inputs are fed to the compiler. Any kind of unexpected
behavior, including crashes, freezes, or wrong binaries, indicates a compiler
bug. While crashes and freezes in the compiler are easy to detect, determining
that binaries are correctly compiled is not generally possible without either
developer provided validation for the particular program's behavior or a gold
standard compiler from which to create reference outputs. In the absence of
those, Differential Testing~\cite{McKeeman1998} can be used. The generated code
and a set of inputs form a \emph{test case} which is compiled and executed on
multiple \emph{testbeds}. If the test case should have deterministic behavior,
but the output differs between testbeds, then a bug has been discovered.

Compiler fuzzing requires efficiently generating test cases that trigger
compiler bugs. The state-of-the-art approach, CSmith~\cite{Yang2011}, generates
large random programs by defining and sampling a probabilistic grammar which
covers a subset of the C programming language. Through this grammar, CSmith
ensures that the generated code easily passes the compiler front-end and
stresses the most complex part of the compiler, the middle-end. Complex static
and dynamic analyses make sure that programs are free from undefined behavior.
The programs are then differentially tested.

While CSmith has been successfully used to identify hundreds of bugs in
compilers, it and similar approaches have a significant drawback. They represent
a huge undertaking and require a thorough understanding of the target
programming language. CSmith was developed over the course of years, and
consists of over 41k lines of handwritten C++ code. By tightly coupling the
generation logic with the target programming language, each feature of the
grammar must be painstakingly and expertly engineered for each new target
language. For example, lifting CSmith from C to OpenCL~\cite{Lidbury2015a} --- a
superficially simple task --- took 9 months and an additional 8k lines of code.
Given the difficulty of defining a new grammar, typically only a subset of the
language is implemented.

What we propose is a fast, effective, and low effort approach to the generation
of random programs for compiler fuzzing. Our methodology uses recent advances in
deep learning to automatically construct probabilistic models of how humans
write code, instead of painstakingly defining a grammar to the same end. By
training a deep neural network on a corpus of handwritten code, it is able to
infer both the syntax and semantics of the programming language and the common
constructs and patterns. Our approach essentially frames the generation of
random programs as a language modelling problem. This greatly simplifies and
accelerates the process. The expressiveness of the generated programs is limited
only by what is contained in the corpus, not the developer's expertise or
available time. Such a corpus can readily be assembled from open source
repositories.

In this work we primarily target OpenCL, an open standard for programming
heterogeneous systems, though our approach is largely language agnostic. We
chose OpenCL for three reasons: it is an emerging standard with the challenging
promise of functional portability across a diverse range of heterogeneous
hardware; OpenCL is compiled ``online'', meaning that even compiler crashes and
freezes may not be discovered until a product is deployed to customers; and
there is already a hand written random program generator for the language to
compare against. We provide preliminary results supporting DeepSmith's potential
for multi-lingual compiler fuzzing.

We make the following contributions:
%
\begin{itemize}
  \item a novel, automatic, and fast approach for the generation of expressive
  random programs for compiler fuzzing. We \emph{infer} programming language
  syntax, structure, and use from real-world examples, not through an expert-
  defined grammar. Our system needs two orders of magnitude less code than the
  state-of–the-art, and takes less than a day to train;

  \item we discover a similar number of bugs as the state-of–the-art, but also
  find bugs which prior work cannot, covering more components of the compiler;

  \item in modelling real handwritten code, our test cases are more
  interpretable than other approaches. Average test case size is two orders of
  magnitude smaller than state-of-the-art, without any expensive reduction
  process.
\end{itemize}
